 AFTAggregator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AFTAggregator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class AFTAggregator Object org.apache.spark.ml.regression.AFTAggregator All Implemented Interfaces: java.io.Serializable public class AFTAggregator extends Object implements scala.Serializable AFTAggregator computes the gradient and loss for a AFT loss function, as used in AFT survival regression for samples in sparse or dense vector in an online fashion. The loss function and likelihood function under the AFT model based on: Lawless, J. F., Statistical Models and Methods for Lifetime Data, New York: John Wiley & Sons, Inc. 2003. Two AFTAggregator can be merged together to have a summary of loss and gradient of the corresponding joint dataset. Given the values of the covariates x^{'}, for random lifetime t_{i} of subjects i = 1, ..., n, with possible right-censoring, the likelihood function under the AFT model is given as L(\beta,\sigma)=\prod_{i=1}^n[\frac{1}{\sigma}f_{0} (\frac{\log{t_{i}}-x^{'}\beta}{\sigma})]^{\delta_{i}}S_{0} (\frac{\log{t_{i}}-x^{'}\beta}{\sigma})^{1-\delta_{i}} Where \delta_{i} is the indicator of the event has occurred i.e. uncensored or not. Using \epsilon_{i}=\frac{\log{t_{i}}-x^{'}\beta}{\sigma}, the log-likelihood function assumes the form \iota(\beta,\sigma)=\sum_{i=1}^{n}[-\delta_{i}\log\sigma+ \delta_{i}\log{f_{0}}(\epsilon_{i})+(1-\delta_{i})\log{S_{0}(\epsilon_{i})}] Where S_{0}(\epsilon_{i}) is the baseline survivor function, and f_{0}(\epsilon_{i}) is corresponding density function. The most commonly used log-linear survival regression method is based on the Weibull distribution of the survival time. The Weibull distribution for lifetime corresponding to extreme value distribution for log of the lifetime, and the S_{0}(\epsilon) function is S_{0}(\epsilon_{i})=\exp(-e^{\epsilon_{i}}) the f_{0}(\epsilon_{i}) function is f_{0}(\epsilon_{i})=e^{\epsilon_{i}}\exp(-e^{\epsilon_{i}}) The log-likelihood function for Weibull distribution of lifetime is \iota(\beta,\sigma)= -\sum_{i=1}^n[\delta_{i}\log\sigma-\delta_{i}\epsilon_{i}+e^{\epsilon_{i}}] Due to minimizing the negative log-likelihood equivalent to maximum a posteriori probability, the loss function we use to optimize is -\iota(\beta,\sigma). The gradient functions for \beta and \log\sigma respectively are \frac{\partial (-\iota)}{\partial \beta}= \sum_{1=1}^{n}[\delta_{i}-e^{\epsilon_{i}}]\frac{x_{i}}{\sigma} \frac{\partial (-\iota)}{\partial (\log\sigma)}= \sum_{i=1}^{n}[\delta_{i}+(\delta_{i}-e^{\epsilon_{i}})\epsilon_{i}] param: parameters including three part: The log of scale parameter, the intercept and regression coefficients corresponding to the features. param: fitIntercept Whether to fit an intercept term. param: featuresStd The standard deviation values of the features. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description AFTAggregator(breeze.linalg.DenseVector<Object> parameters, boolean fitIntercept, double[] featuresStd)  Method Summary Methods  Modifier and Type Method and Description AFTAggregator add(org.apache.spark.ml.regression.AFTPoint data) Add a new training data to this AFTAggregator, and update the loss and gradient of the objective function. long count()  breeze.linalg.DenseVector<Object> gradient()  double loss()  AFTAggregator merge(AFTAggregator other) Merge another AFTAggregator, and update the loss and gradient of the objective function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AFTAggregator public AFTAggregator(breeze.linalg.DenseVector<Object> parameters, boolean fitIntercept, double[] featuresStd) Method Detail count public long count() loss public double loss() gradient public breeze.linalg.DenseVector<Object> gradient() add public AFTAggregator add(org.apache.spark.ml.regression.AFTPoint data) Add a new training data to this AFTAggregator, and update the loss and gradient of the objective function. Parameters:data - The AFTPoint representation for one data point to be added into this aggregator. Returns:This AFTAggregator object. merge public AFTAggregator merge(AFTAggregator other) Merge another AFTAggregator, and update the loss and gradient of the objective function. (Note that it's in place merging; as a result, this object will be modified.) Parameters:other - The other AFTAggregator to be merged. Returns:This AFTAggregator object. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AFTCostFun (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AFTCostFun (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class AFTCostFun Object org.apache.spark.ml.regression.AFTCostFun All Implemented Interfaces: breeze.optimize.DiffFunction<breeze.linalg.DenseVector<Object>>, breeze.optimize.StochasticDiffFunction<breeze.linalg.DenseVector<Object>>, scala.Function1<breeze.linalg.DenseVector<Object>,Object> public class AFTCostFun extends Object implements breeze.optimize.DiffFunction<breeze.linalg.DenseVector<Object>> AFTCostFun implements Breeze's DiffFunction[T] for AFT cost. It returns the loss and gradient at a particular point (parameters). It's used in Breeze's convex optimization routines. Constructor Summary Constructors  Constructor and Description AFTCostFun(RDD<org.apache.spark.ml.regression.AFTPoint> data, boolean fitIntercept, double[] featuresStd)  Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<Object,breeze.linalg.DenseVector<Object>> calculate(breeze.linalg.DenseVector<Object> parameters)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface breeze.optimize.DiffFunction cached, throughLens Methods inherited from interface breeze.optimize.StochasticDiffFunction apply, gradientAt, valueAt Methods inherited from interface scala.Function1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Constructor Detail AFTCostFun public AFTCostFun(RDD<org.apache.spark.ml.regression.AFTPoint> data, boolean fitIntercept, double[] featuresStd) Method Detail calculate public scala.Tuple2<Object,breeze.linalg.DenseVector<Object>> calculate(breeze.linalg.DenseVector<Object> parameters) Specified by: calculate in interface breeze.optimize.StochasticDiffFunction<breeze.linalg.DenseVector<Object>> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AFTSurvivalRegression (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AFTSurvivalRegression (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class AFTSurvivalRegression Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<AFTSurvivalRegressionModel> org.apache.spark.ml.regression.AFTSurvivalRegression All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class AFTSurvivalRegression extends Estimator<AFTSurvivalRegressionModel> implements DefaultParamsWritable :: Experimental :: Fit a parametric survival regression model named accelerated failure time (AFT) model (https://en.wikipedia.org/wiki/Accelerated_failure_time_model) based on the Weibull distribution of the survival time. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description AFTSurvivalRegression()  AFTSurvivalRegression(String uid)  Method Summary Methods  Modifier and Type Method and Description static Param<String> censorCol()  Param<String> censorCol() Param for censor column name. static Params clear(Param<?> param)  AFTSurvivalRegression copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  AFTSurvivalRegressionModel fit(Dataset<?> dataset) Fits a model to the input data. static BooleanParam fitIntercept()  static <T> scala.Option<T> get(Param<T> param)  static String getCensorCol()  String getCensorCol()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  static boolean getFitIntercept()  static String getLabelCol()  static int getMaxIter()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  static double[] getQuantileProbabilities()  double[] getQuantileProbabilities()  static String getQuantilesCol()  String getQuantilesCol()  static double getTol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  boolean hasQuantilesCol() Checks whether the input has quantiles column name. static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  static AFTSurvivalRegression load(String path)  static IntParam maxIter()  static Param<?>[] params()  static Param<String> predictionCol()  static DoubleArrayParam quantileProbabilities()  DoubleArrayParam quantileProbabilities() Param for quantile probabilities array. static Param<String> quantilesCol()  Param<String> quantilesCol() Param for quantiles column name. static void save(String path)  static <T> Params set(Param<T> param, T value)  AFTSurvivalRegression setCensorCol(String value)  AFTSurvivalRegression setFeaturesCol(String value)  AFTSurvivalRegression setFitIntercept(boolean value) Set if we should fit the intercept Default is true. AFTSurvivalRegression setLabelCol(String value)  AFTSurvivalRegression setMaxIter(int value) Set the maximum number of iterations. AFTSurvivalRegression setPredictionCol(String value)  AFTSurvivalRegression setQuantileProbabilities(double[] value)  AFTSurvivalRegression setQuantilesCol(String value)  AFTSurvivalRegression setTol(double value) Set the convergence tolerance of iterations. static DoubleParam tol()  static String toString()  StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting) Validates and transforms the input schema with the provided param map. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Constructor Detail AFTSurvivalRegression public AFTSurvivalRegression(String uid) AFTSurvivalRegression public AFTSurvivalRegression() Method Detail load public static AFTSurvivalRegression load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() tol public static final DoubleParam tol() getTol public static final double getTol() fitIntercept public static final BooleanParam fitIntercept() getFitIntercept public static final boolean getFitIntercept() censorCol public static final Param<String> censorCol() getCensorCol public static String getCensorCol() quantileProbabilities public static final DoubleArrayParam quantileProbabilities() getQuantileProbabilities public static double[] getQuantileProbabilities() quantilesCol public static final Param<String> quantilesCol() getQuantilesCol public static String getQuantilesCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setFeaturesCol public AFTSurvivalRegression setFeaturesCol(String value) setLabelCol public AFTSurvivalRegression setLabelCol(String value) setCensorCol public AFTSurvivalRegression setCensorCol(String value) setPredictionCol public AFTSurvivalRegression setPredictionCol(String value) setQuantileProbabilities public AFTSurvivalRegression setQuantileProbabilities(double[] value) setQuantilesCol public AFTSurvivalRegression setQuantilesCol(String value) setFitIntercept public AFTSurvivalRegression setFitIntercept(boolean value) Set if we should fit the intercept Default is true. Parameters:value - (undocumented) Returns:(undocumented) setMaxIter public AFTSurvivalRegression setMaxIter(int value) Set the maximum number of iterations. Default is 100. Parameters:value - (undocumented) Returns:(undocumented) setTol public AFTSurvivalRegression setTol(double value) Set the convergence tolerance of iterations. Smaller value will lead to higher accuracy with the cost of more iterations. Default is 1E-6. Parameters:value - (undocumented) Returns:(undocumented) fit public AFTSurvivalRegressionModel fit(Dataset<?> dataset) Description copied from class: Estimator Fits a model to the input data. Specified by: fit in class Estimator<AFTSurvivalRegressionModel> Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public AFTSurvivalRegression copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Estimator<AFTSurvivalRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) censorCol public Param<String> censorCol() Param for censor column name. The value of this column could be 0 or 1. If the value is 1, it means the event has occurred i.e. uncensored; otherwise censored. Returns:(undocumented) getCensorCol public String getCensorCol() quantileProbabilities public DoubleArrayParam quantileProbabilities() Param for quantile probabilities array. Values of the quantile probabilities array should be in the range (0, 1) and the array should be non-empty. Returns:(undocumented) getQuantileProbabilities public double[] getQuantileProbabilities() quantilesCol public Param<String> quantilesCol() Param for quantiles column name. This column will output quantiles of corresponding quantileProbabilities if it is set. Returns:(undocumented) getQuantilesCol public String getQuantilesCol() hasQuantilesCol public boolean hasQuantilesCol() Checks whether the input has quantiles column name. validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fitting or prediction Returns:output schema Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AFTSurvivalRegressionModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AFTSurvivalRegressionModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class AFTSurvivalRegressionModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<AFTSurvivalRegressionModel> org.apache.spark.ml.regression.AFTSurvivalRegressionModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class AFTSurvivalRegressionModel extends Model<AFTSurvivalRegressionModel> implements MLWritable :: Experimental :: Model produced by AFTSurvivalRegression. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static Param<String> censorCol()  Param<String> censorCol() Param for censor column name. static Params clear(Param<?> param)  Vector coefficients()  AFTSurvivalRegressionModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  static BooleanParam fitIntercept()  static <T> scala.Option<T> get(Param<T> param)  static String getCensorCol()  String getCensorCol()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  static boolean getFitIntercept()  static String getLabelCol()  static int getMaxIter()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  static double[] getQuantileProbabilities()  double[] getQuantileProbabilities()  static String getQuantilesCol()  String getQuantilesCol()  static double getTol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  boolean hasQuantilesCol() Checks whether the input has quantiles column name. double intercept()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  static AFTSurvivalRegressionModel load(String path)  static IntParam maxIter()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  double predict(Vector features)  static Param<String> predictionCol()  Vector predictQuantiles(Vector features)  static DoubleArrayParam quantileProbabilities()  DoubleArrayParam quantileProbabilities() Param for quantile probabilities array. static Param<String> quantilesCol()  Param<String> quantilesCol() Param for quantiles column name. static MLReader<AFTSurvivalRegressionModel> read()  static void save(String path)  double scale()  static <T> Params set(Param<T> param, T value)  AFTSurvivalRegressionModel setFeaturesCol(String value)  static M setParent(Estimator<M> parent)  AFTSurvivalRegressionModel setPredictionCol(String value)  AFTSurvivalRegressionModel setQuantileProbabilities(double[] value)  AFTSurvivalRegressionModel setQuantilesCol(String value)  static DoubleParam tol()  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting) Validates and transforms the input schema with the provided param map. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.MLWritable save Method Detail read public static MLReader<AFTSurvivalRegressionModel> read() load public static AFTSurvivalRegressionModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() tol public static final DoubleParam tol() getTol public static final double getTol() fitIntercept public static final BooleanParam fitIntercept() getFitIntercept public static final boolean getFitIntercept() censorCol public static final Param<String> censorCol() getCensorCol public static String getCensorCol() quantileProbabilities public static final DoubleArrayParam quantileProbabilities() getQuantileProbabilities public static double[] getQuantileProbabilities() quantilesCol public static final Param<String> quantilesCol() getQuantilesCol public static String getQuantilesCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) coefficients public Vector coefficients() intercept public double intercept() scale public double scale() setFeaturesCol public AFTSurvivalRegressionModel setFeaturesCol(String value) setPredictionCol public AFTSurvivalRegressionModel setPredictionCol(String value) setQuantileProbabilities public AFTSurvivalRegressionModel setQuantileProbabilities(double[] value) setQuantilesCol public AFTSurvivalRegressionModel setQuantilesCol(String value) predictQuantiles public Vector predictQuantiles(Vector features) predict public double predict(Vector features) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public AFTSurvivalRegressionModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<AFTSurvivalRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) censorCol public Param<String> censorCol() Param for censor column name. The value of this column could be 0 or 1. If the value is 1, it means the event has occurred i.e. uncensored; otherwise censored. Returns:(undocumented) getCensorCol public String getCensorCol() quantileProbabilities public DoubleArrayParam quantileProbabilities() Param for quantile probabilities array. Values of the quantile probabilities array should be in the range (0, 1) and the array should be non-empty. Returns:(undocumented) getQuantileProbabilities public double[] getQuantileProbabilities() quantilesCol public Param<String> quantilesCol() Param for quantiles column name. This column will output quantiles of corresponding quantileProbabilities if it is set. Returns:(undocumented) getQuantilesCol public String getQuantilesCol() hasQuantilesCol public boolean hasQuantilesCol() Checks whether the input has quantiles column name. validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fitting or prediction Returns:output schema Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ALS.InBlock$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ALS.InBlock$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.recommendation Class ALS.InBlock$ Object org.apache.spark.ml.recommendation.ALS.InBlock$ All Implemented Interfaces: java.io.Serializable Enclosing class: ALS public static class ALS.InBlock$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static ALS.InBlock$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description ALS.InBlock$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final ALS.InBlock$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail ALS.InBlock$ public ALS.InBlock$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ALS.Rating$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ALS.Rating$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.recommendation Class ALS.Rating$ Object org.apache.spark.ml.recommendation.ALS.Rating$ All Implemented Interfaces: java.io.Serializable Enclosing class: ALS public static class ALS.Rating$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static ALS.Rating$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description ALS.Rating$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final ALS.Rating$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail ALS.Rating$ public ALS.Rating$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ALS.Rating (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ALS.Rating (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.recommendation Class ALS.Rating<ID> Object org.apache.spark.ml.recommendation.ALS.Rating<ID> All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: ALS public static class ALS.Rating<ID> extends Object implements scala.Product, scala.Serializable :: DeveloperApi :: Rating class for better code readability. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ALS.Rating(ID user, ID item, float rating)  Method Summary Methods  Modifier and Type Method and Description ID item()  float rating()  ID user()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail ALS.Rating public ALS.Rating(ID user, ID item, float rating) Method Detail user public ID user() item public ID item() rating public float rating() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ALS.RatingBlock$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ALS.RatingBlock$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.recommendation Class ALS.RatingBlock$ Object org.apache.spark.ml.recommendation.ALS.RatingBlock$ All Implemented Interfaces: java.io.Serializable Enclosing class: ALS public static class ALS.RatingBlock$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static ALS.RatingBlock$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description ALS.RatingBlock$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final ALS.RatingBlock$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail ALS.RatingBlock$ public ALS.RatingBlock$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ALS (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ALS (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.recommendation Class ALS Object org.apache.spark.mllib.recommendation.ALS All Implemented Interfaces: java.io.Serializable public class ALS extends Object implements scala.Serializable Alternating Least Squares matrix factorization. ALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. X * Yt = R. Typically these approximations are called 'factor' matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix. This is a blocked implementation of the ALS factorization algorithm that groups the two sets of factors (referred to as "users" and "products") into blocks and reduces communication by only sending one copy of each user vector to each product block on each iteration, and only for the product blocks that need that user's feature vector. This is achieved by precomputing some information about the ratings matrix to determine the "out-links" of each user (which blocks of products it will contribute to) and "in-link" information for each product (which of the feature vectors it receives from each user block it will depend on). This allows us to send only an array of feature vectors between each user block and product block, and have the product block find the users' ratings and update the products based on these messages. For implicit preference data, the algorithm used is based on "Collaborative Filtering for Implicit Feedback Datasets", available at http://dx.doi.org/10.1109/ICDM.2008.22, adapted for the blocked approach used here. Essentially instead of finding the low-rank approximations to the rating matrix R, this finds the approximations for a preference matrix P where the elements of P are 1 if r > 0 and 0 if r <= 0. The ratings then act as 'confidence' values related to strength of indicated user preferences rather than explicit ratings given to items. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ALS() Constructs an ALS instance with default parameters: {numBlocks: -1, rank: 10, iterations: 10, lambda: 0.01, implicitPrefs: false, alpha: 1.0}. Method Summary Methods  Modifier and Type Method and Description MatrixFactorizationModel run(JavaRDD<Rating> ratings) Java-friendly version of ALS.run. MatrixFactorizationModel run(RDD<Rating> ratings) Run ALS with the configured parameters on an input RDD of Rating objects. ALS setAlpha(double alpha) Sets the constant used in computing confidence in implicit ALS. ALS setBlocks(int numBlocks) Set the number of blocks for both user blocks and product blocks to parallelize the computation into; pass -1 for an auto-configured number of blocks. ALS setCheckpointInterval(int checkpointInterval) :: DeveloperApi :: Set period (in iterations) between checkpoints (default = 10). ALS setFinalRDDStorageLevel(StorageLevel storageLevel) :: DeveloperApi :: Sets storage level for final RDDs (user/product used in MatrixFactorizationModel). ALS setImplicitPrefs(boolean implicitPrefs) Sets whether to use implicit preference. ALS setIntermediateRDDStorageLevel(StorageLevel storageLevel) :: DeveloperApi :: Sets storage level for intermediate RDDs (user/product in/out links). ALS setIterations(int iterations) Set the number of iterations to run. ALS setLambda(double lambda) Set the regularization parameter, lambda. ALS setNonnegative(boolean b) Set whether the least-squares problems solved at each iteration should have nonnegativity constraints. ALS setProductBlocks(int numProductBlocks) Set the number of product blocks to parallelize the computation. ALS setRank(int rank) Set the rank of the feature matrices computed (number of features). ALS setSeed(long seed) Sets a random seed to have deterministic results. ALS setUserBlocks(int numUserBlocks) Set the number of user blocks to parallelize the computation. static MatrixFactorizationModel train(RDD<Rating> ratings, int rank, int iterations) Train a matrix factorization model given an RDD of ratings by users for a subset of products. static MatrixFactorizationModel train(RDD<Rating> ratings, int rank, int iterations, double lambda) Train a matrix factorization model given an RDD of ratings by users for a subset of products. static MatrixFactorizationModel train(RDD<Rating> ratings, int rank, int iterations, double lambda, int blocks) Train a matrix factorization model given an RDD of ratings by users for a subset of products. static MatrixFactorizationModel train(RDD<Rating> ratings, int rank, int iterations, double lambda, int blocks, long seed) Train a matrix factorization model given an RDD of ratings by users for a subset of products. static MatrixFactorizationModel trainImplicit(RDD<Rating> ratings, int rank, int iterations) Train a matrix factorization model given an RDD of 'implicit preferences' of users for a subset of products. static MatrixFactorizationModel trainImplicit(RDD<Rating> ratings, int rank, int iterations, double lambda, double alpha) Train a matrix factorization model given an RDD of 'implicit preferences' of users for a subset of products. static MatrixFactorizationModel trainImplicit(RDD<Rating> ratings, int rank, int iterations, double lambda, int blocks, double alpha) Train a matrix factorization model given an RDD of 'implicit preferences' of users for a subset of products. static MatrixFactorizationModel trainImplicit(RDD<Rating> ratings, int rank, int iterations, double lambda, int blocks, double alpha, long seed) Train a matrix factorization model given an RDD of 'implicit preferences' given by users to some products, in the form of (userID, productID, preference) pairs. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ALS public ALS() Constructs an ALS instance with default parameters: {numBlocks: -1, rank: 10, iterations: 10, lambda: 0.01, implicitPrefs: false, alpha: 1.0}. Method Detail train public static MatrixFactorizationModel train(RDD<Rating> ratings, int rank, int iterations, double lambda, int blocks, long seed) Train a matrix factorization model given an RDD of ratings by users for a subset of products. The ratings matrix is approximated as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, ALS is run iteratively with a configurable level of parallelism. Parameters:ratings - RDD of Rating objects with userID, productID, and ratingrank - number of features to useiterations - number of iterations of ALSlambda - regularization parameterblocks - level of parallelism to split computation intoseed - random seed for initial matrix factorization model Returns:(undocumented) train public static MatrixFactorizationModel train(RDD<Rating> ratings, int rank, int iterations, double lambda, int blocks) Train a matrix factorization model given an RDD of ratings by users for a subset of products. The ratings matrix is approximated as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, ALS is run iteratively with a configurable level of parallelism. Parameters:ratings - RDD of Rating objects with userID, productID, and ratingrank - number of features to useiterations - number of iterations of ALSlambda - regularization parameterblocks - level of parallelism to split computation into Returns:(undocumented) train public static MatrixFactorizationModel train(RDD<Rating> ratings, int rank, int iterations, double lambda) Train a matrix factorization model given an RDD of ratings by users for a subset of products. The ratings matrix is approximated as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, ALS is run iteratively with a level of parallelism automatically based on the number of partitions in ratings. Parameters:ratings - RDD of Rating objects with userID, productID, and ratingrank - number of features to useiterations - number of iterations of ALSlambda - regularization parameter Returns:(undocumented) train public static MatrixFactorizationModel train(RDD<Rating> ratings, int rank, int iterations) Train a matrix factorization model given an RDD of ratings by users for a subset of products. The ratings matrix is approximated as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, ALS is run iteratively with a level of parallelism automatically based on the number of partitions in ratings. Parameters:ratings - RDD of Rating objects with userID, productID, and ratingrank - number of features to useiterations - number of iterations of ALS Returns:(undocumented) trainImplicit public static MatrixFactorizationModel trainImplicit(RDD<Rating> ratings, int rank, int iterations, double lambda, int blocks, double alpha, long seed) Train a matrix factorization model given an RDD of 'implicit preferences' given by users to some products, in the form of (userID, productID, preference) pairs. We approximate the ratings matrix as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, we run a given number of iterations of ALS. This is done using a level of parallelism given by blocks. Parameters:ratings - RDD of (userID, productID, rating) pairsrank - number of features to useiterations - number of iterations of ALSlambda - regularization parameterblocks - level of parallelism to split computation intoalpha - confidence parameterseed - random seed for initial matrix factorization model Returns:(undocumented) trainImplicit public static MatrixFactorizationModel trainImplicit(RDD<Rating> ratings, int rank, int iterations, double lambda, int blocks, double alpha) Train a matrix factorization model given an RDD of 'implicit preferences' of users for a subset of products. The ratings matrix is approximated as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, ALS is run iteratively with a configurable level of parallelism. Parameters:ratings - RDD of Rating objects with userID, productID, and ratingrank - number of features to useiterations - number of iterations of ALSlambda - regularization parameterblocks - level of parallelism to split computation intoalpha - confidence parameter Returns:(undocumented) trainImplicit public static MatrixFactorizationModel trainImplicit(RDD<Rating> ratings, int rank, int iterations, double lambda, double alpha) Train a matrix factorization model given an RDD of 'implicit preferences' of users for a subset of products. The ratings matrix is approximated as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, ALS is run iteratively with a level of parallelism determined automatically based on the number of partitions in ratings. Parameters:ratings - RDD of Rating objects with userID, productID, and ratingrank - number of features to useiterations - number of iterations of ALSlambda - regularization parameteralpha - confidence parameter Returns:(undocumented) trainImplicit public static MatrixFactorizationModel trainImplicit(RDD<Rating> ratings, int rank, int iterations) Train a matrix factorization model given an RDD of 'implicit preferences' of users for a subset of products. The ratings matrix is approximated as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, ALS is run iteratively with a level of parallelism determined automatically based on the number of partitions in ratings. Parameters:ratings - RDD of Rating objects with userID, productID, and ratingrank - number of features to useiterations - number of iterations of ALS Returns:(undocumented) setBlocks public ALS setBlocks(int numBlocks) Set the number of blocks for both user blocks and product blocks to parallelize the computation into; pass -1 for an auto-configured number of blocks. Default: -1. Parameters:numBlocks - (undocumented) Returns:(undocumented) setUserBlocks public ALS setUserBlocks(int numUserBlocks) Set the number of user blocks to parallelize the computation. Parameters:numUserBlocks - (undocumented) Returns:(undocumented) setProductBlocks public ALS setProductBlocks(int numProductBlocks) Set the number of product blocks to parallelize the computation. Parameters:numProductBlocks - (undocumented) Returns:(undocumented) setRank public ALS setRank(int rank) Set the rank of the feature matrices computed (number of features). Default: 10. setIterations public ALS setIterations(int iterations) Set the number of iterations to run. Default: 10. setLambda public ALS setLambda(double lambda) Set the regularization parameter, lambda. Default: 0.01. setImplicitPrefs public ALS setImplicitPrefs(boolean implicitPrefs) Sets whether to use implicit preference. Default: false. setAlpha public ALS setAlpha(double alpha) Sets the constant used in computing confidence in implicit ALS. Default: 1.0. Parameters:alpha - (undocumented) Returns:(undocumented) setSeed public ALS setSeed(long seed) Sets a random seed to have deterministic results. setNonnegative public ALS setNonnegative(boolean b) Set whether the least-squares problems solved at each iteration should have nonnegativity constraints. Parameters:b - (undocumented) Returns:(undocumented) setIntermediateRDDStorageLevel public ALS setIntermediateRDDStorageLevel(StorageLevel storageLevel) :: DeveloperApi :: Sets storage level for intermediate RDDs (user/product in/out links). The default value is MEMORY_AND_DISK. Users can change it to a serialized storage, e.g., MEMORY_AND_DISK_SER and set spark.rdd.compress to true to reduce the space requirement, at the cost of speed. Parameters:storageLevel - (undocumented) Returns:(undocumented) setFinalRDDStorageLevel public ALS setFinalRDDStorageLevel(StorageLevel storageLevel) :: DeveloperApi :: Sets storage level for final RDDs (user/product used in MatrixFactorizationModel). The default value is MEMORY_AND_DISK. Users can change it to a serialized storage, e.g. MEMORY_AND_DISK_SER and set spark.rdd.compress to true to reduce the space requirement, at the cost of speed. Parameters:storageLevel - (undocumented) Returns:(undocumented) setCheckpointInterval public ALS setCheckpointInterval(int checkpointInterval) :: DeveloperApi :: Set period (in iterations) between checkpoints (default = 10). Checkpointing helps with recovery (when nodes fail) and StackOverflow exceptions caused by long lineage. It also helps with eliminating temporary shuffle files on disk, which can be important when there are many ALS iterations. If the checkpoint directory is not set in SparkContext, this setting is ignored. Parameters:checkpointInterval - (undocumented) Returns:(undocumented) run public MatrixFactorizationModel run(RDD<Rating> ratings) Run ALS with the configured parameters on an input RDD of Rating objects. Returns a MatrixFactorizationModel with feature vectors for each user and product. Parameters:ratings - (undocumented) Returns:(undocumented) run public MatrixFactorizationModel run(JavaRDD<Rating> ratings) Java-friendly version of ALS.run. Parameters:ratings - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ALSModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ALSModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.recommendation Class ALSModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<ALSModel> org.apache.spark.ml.recommendation.ALSModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class ALSModel extends Model<ALSModel> implements MLWritable Model fitted by ALS. param: rank rank of the matrix factorization model param: userFactors a DataFrame that stores user factors in two columns: id and features param: itemFactors a DataFrame that stores item factors in two columns: id and features See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description UserDefinedFunction checkedCast() Attempts to safely cast a user/item id to an Int. static Params clear(Param<?> param)  ALSModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getItemCol()  String getItemCol()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  static String getUserCol()  String getUserCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> itemCol()  Param<String> itemCol() Param for the column name for item ids. Dataset<Row> itemFactors()  static ALSModel load(String path)  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static Param<String> predictionCol()  int rank()  static MLReader<ALSModel> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  ALSModel setItemCol(String value)  static M setParent(Estimator<M> parent)  ALSModel setPredictionCol(String value)  ALSModel setUserCol(String value)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static Param<String> userCol()  Param<String> userCol() Param for the column name for user ids. Dataset<Row> userFactors()  static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.MLWritable save Method Detail read public static MLReader<ALSModel> read() load public static ALSModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() userCol public static Param<String> userCol() getUserCol public static String getUserCol() itemCol public static Param<String> itemCol() getItemCol public static String getItemCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) rank public int rank() userFactors public Dataset<Row> userFactors() itemFactors public Dataset<Row> itemFactors() setUserCol public ALSModel setUserCol(String value) setItemCol public ALSModel setItemCol(String value) setPredictionCol public ALSModel setPredictionCol(String value) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public ALSModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<ALSModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) userCol public Param<String> userCol() Param for the column name for user ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: "user" Returns:(undocumented) getUserCol public String getUserCol() itemCol public Param<String> itemCol() Param for the column name for item ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: "item" Returns:(undocumented) getItemCol public String getItemCol() checkedCast public UserDefinedFunction checkedCast() Attempts to safely cast a user/item id to an Int. Throws an exception if the value is out of integer range. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AbsoluteError (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AbsoluteError (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.loss Class AbsoluteError Object org.apache.spark.mllib.tree.loss.AbsoluteError public class AbsoluteError extends Object :: DeveloperApi :: Class for absolute error loss calculation (for regression). The absolute (L1) error is defined as: |y - F(x)| where y is the label and F(x) is the model prediction for features x. Constructor Summary Constructors  Constructor and Description AbsoluteError()  Method Summary Methods  Modifier and Type Method and Description static double gradient(double prediction, double label) Method to calculate the gradients for the gradient boosting calculation for least absolute error calculation. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AbsoluteError public AbsoluteError() Method Detail gradient public static double gradient(double prediction, double label) Method to calculate the gradients for the gradient boosting calculation for least absolute error calculation. The gradient with respect to F(x) is: sign(F(x) - y) Parameters:prediction - Predicted label.label - True label. Returns:Loss gradient Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Accumulable (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Accumulable (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class Accumulable<R,T> Object org.apache.spark.Accumulable<R,T> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: Accumulator Deprecated.  use AccumulatorV2. Since 2.0.0. public class Accumulable<R,T> extends Object implements java.io.Serializable A data type that can be accumulated, i.e. has a commutative and associative "add" operation, but where the result type, R, may be different from the element type being added, T. You must define how to add data, and how to merge two of these together. For some data types, such as a counter, these might be the same operation. In that case, you can use the simpler Accumulator. They won't always be the same, though -- e.g., imagine you are accumulating a set. You will add items to the set, and you will union two sets together. Operations are not thread-safe. param: id ID of this accumulator; for internal use only. param: initialValue initial value of accumulator param: param helper object defining how to add elements of type R and T param: name human-readable name for use in Spark's web UI param: countFailedValues whether to accumulate values from failed tasks. This is set to true for system and time metrics like serialization time or bytes spilled, and false for things with absolute values like number of input rows. This should be used for internal metrics only. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Accumulable(R initialValue, AccumulableParam<R,T> param) Deprecated.    Method Summary Methods  Modifier and Type Method and Description void add(T term) Deprecated.  Add more data to this accumulator / accumulable long id() Deprecated.    R localValue() Deprecated.  Get the current value of this accumulator from within a task. void merge(R term) Deprecated.  Merge two accumulable objects together scala.Option<String> name() Deprecated.    void setValue(R newValue) Deprecated.  Set the accumulator's value. String toString() Deprecated.    R value() Deprecated.  Access the accumulator's current value; only allowed on driver. R zero() Deprecated.    Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail Accumulable public Accumulable(R initialValue, AccumulableParam<R,T> param) Deprecated.  Method Detail id public long id() Deprecated.  name public scala.Option<String> name() Deprecated.  zero public R zero() Deprecated.  add public void add(T term) Deprecated.  Add more data to this accumulator / accumulable Parameters:term - the data to add merge public void merge(R term) Deprecated.  Merge two accumulable objects together Normally, a user will not want to use this version, but will instead call add. Parameters:term - the other R that will get merged with this value public R value() Deprecated.  Access the accumulator's current value; only allowed on driver. Returns:(undocumented) localValue public R localValue() Deprecated.  Get the current value of this accumulator from within a task. This is NOT the global value of the accumulator. To get the global value after a completed operation on the dataset, call value. The typical use of this method is to directly mutate the local value, eg., to add an element to a Set. Returns:(undocumented) setValue public void setValue(R newValue) Deprecated.  Set the accumulator's value. For internal use only. Parameters:newValue - (undocumented) toString public String toString() Deprecated.  Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulableInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulableInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class AccumulableInfo Object org.apache.spark.status.api.v1.AccumulableInfo public class AccumulableInfo extends Object Method Summary Methods  Modifier and Type Method and Description long id()  String name()  scala.Option<String> update()  String value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail id public long id() name public String name() update public scala.Option<String> update() value public String value() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulableParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulableParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Interface AccumulableParam<R,T> All Superinterfaces: java.io.Serializable All Known Subinterfaces: AccumulatorParam<T> All Known Implementing Classes: AccumulatorParam.DoubleAccumulatorParam$, AccumulatorParam.FloatAccumulatorParam$, AccumulatorParam.IntAccumulatorParam$, AccumulatorParam.LongAccumulatorParam$, AccumulatorParam.StringAccumulatorParam$ Deprecated.  use AccumulatorV2. Since 2.0.0. public interface AccumulableParam<R,T> extends java.io.Serializable Helper object defining how to accumulate values of a particular type. An implicit AccumulableParam needs to be available when you create Accumulables of a specific type. Method Summary Methods  Modifier and Type Method and Description R addAccumulator(R r, T t) Deprecated.  Add additional data to the accumulator value. R addInPlace(R r1, R r2) Deprecated.  Merge two accumulated values together. R zero(R initialValue) Deprecated.  Return the "zero" (identity) value for an accumulator type, given its initial value. Method Detail addAccumulator R addAccumulator(R r, T t) Deprecated.  Add additional data to the accumulator value. Is allowed to modify and return r for efficiency (to avoid allocating objects). Parameters:r - the current value of the accumulatort - the data to be added to the accumulator Returns:the new value of the accumulator addInPlace R addInPlace(R r1, R r2) Deprecated.  Merge two accumulated values together. Is allowed to modify and return the first value for efficiency (to avoid allocating objects). Parameters:r1 - one set of accumulated datar2 - another set of accumulated data Returns:both data sets merged together zero R zero(R initialValue) Deprecated.  Return the "zero" (identity) value for an accumulator type, given its initial value. For example, if R was a vector of N dimensions, this would return a vector of N zeroes. Parameters:initialValue - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Accumulator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Accumulator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class Accumulator<T> Object org.apache.spark.Accumulable<T,T> org.apache.spark.Accumulator<T> All Implemented Interfaces: java.io.Serializable Deprecated.  use AccumulatorV2. Since 2.0.0. public class Accumulator<T> extends Accumulable<T,T> A simpler value of Accumulable where the result type being accumulated is the same as the types of elements being merged, i.e. variables that are only "added" to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric value types, and programmers can add support for new types. An accumulator is created from an initial value v by calling SparkContext.accumulator. Tasks running on the cluster can then add to it using the += operator. However, they cannot read its value. Only the driver program can read the accumulator's value, using its value() method. The interpreter session below shows an accumulator being used to add up the elements of an array: scala> val accum = sc.accumulator(0) accum: org.apache.spark.Accumulator[Int] = 0 scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x) ... 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s scala> accum.value res2: Int = 10 param: initialValue initial value of accumulator param: param helper object defining how to add elements of type T param: name human-readable name associated with this accumulator param: countFailedValues whether to accumulate values from failed tasks See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static void $plus$eq(T term) Deprecated.    static void $plus$plus$eq(R term) Deprecated.    static void add(T term) Deprecated.    static long id() Deprecated.    static R localValue() Deprecated.    static void merge(R term) Deprecated.    static scala.Option<String> name() Deprecated.    static void setValue(R newValue) Deprecated.    static String toString() Deprecated.    static void value_$eq(R newValue) Deprecated.    static R value() Deprecated.    static R zero() Deprecated.    Methods inherited from class org.apache.spark.Accumulable add, id, localValue, merge, name, setValue, toString, value, zero Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Method Detail id public static long id() Deprecated.  name public static scala.Option<String> name() Deprecated.  zero public static R zero() Deprecated.  $plus$eq public static void $plus$eq(T term) Deprecated.  add public static void add(T term) Deprecated.  $plus$plus$eq public static void $plus$plus$eq(R term) Deprecated.  merge public static void merge(R term) Deprecated.  value public static R value() Deprecated.  localValue public static R localValue() Deprecated.  value_$eq public static void value_$eq(R newValue) Deprecated.  setValue public static void setValue(R newValue) Deprecated.  toString public static String toString() Deprecated.  Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulatorContext (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulatorContext (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class AccumulatorContext Object org.apache.spark.util.AccumulatorContext public class AccumulatorContext extends Object An internal class used to track accumulators by Spark itself. Constructor Summary Constructors  Constructor and Description AccumulatorContext()  Method Summary Methods  Modifier and Type Method and Description static void clear() Clears all registered AccumulatorV2s. static scala.Option<AccumulatorV2<?,?>> get(long id) Returns the AccumulatorV2 registered with the given ID, if any. static long newId() Returns a globally unique ID for a new AccumulatorV2. static int numAccums() Returns the number of accumulators registered. static void register(AccumulatorV2<?,?> a) Registers an AccumulatorV2 created on the driver such that it can be used on the executors. static void remove(long id) Unregisters the AccumulatorV2 with the given ID, if any. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AccumulatorContext public AccumulatorContext() Method Detail newId public static long newId() Returns a globally unique ID for a new AccumulatorV2. Note: Once you copy the AccumulatorV2 the ID is no longer unique. Returns:(undocumented) numAccums public static int numAccums() Returns the number of accumulators registered. Used in testing. register public static void register(AccumulatorV2<?,?> a) Registers an AccumulatorV2 created on the driver such that it can be used on the executors. All accumulators registered here can later be used as a container for accumulating partial values across multiple tasks. This is what DAGScheduler does. Note: if an accumulator is registered here, it should also be registered with the active context cleaner for cleanup so as to avoid memory leaks. If an AccumulatorV2 with the same ID was already registered, this does nothing instead of overwriting it. We will never register same accumulator twice, this is just a sanity check. Parameters:a - (undocumented) remove public static void remove(long id) Unregisters the AccumulatorV2 with the given ID, if any. Parameters:id - (undocumented) get public static scala.Option<AccumulatorV2<?,?>> get(long id) Returns the AccumulatorV2 registered with the given ID, if any. Parameters:id - (undocumented) Returns:(undocumented) clear public static void clear() Clears all registered AccumulatorV2s. For testing only. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulatorParam.DoubleAccumulatorParam$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulatorParam.DoubleAccumulatorParam$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class AccumulatorParam.DoubleAccumulatorParam$ Object org.apache.spark.AccumulatorParam.DoubleAccumulatorParam$ All Implemented Interfaces: java.io.Serializable, AccumulableParam<Object,Object>, AccumulatorParam<Object> Enclosing interface: AccumulatorParam<T> Deprecated.  use AccumulatorV2. Since 2.0.0. public static class AccumulatorParam.DoubleAccumulatorParam$ extends Object implements AccumulatorParam<Object> See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface org.apache.spark.AccumulatorParam AccumulatorParam.DoubleAccumulatorParam$, AccumulatorParam.FloatAccumulatorParam$, AccumulatorParam.IntAccumulatorParam$, AccumulatorParam.LongAccumulatorParam$, AccumulatorParam.StringAccumulatorParam$ Field Summary Fields  Modifier and Type Field and Description static AccumulatorParam.DoubleAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description AccumulatorParam.DoubleAccumulatorParam$() Deprecated.    Method Summary Methods  Modifier and Type Method and Description double addInPlace(double t1, double t2) Deprecated.    double zero(double initialValue) Deprecated.    Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.AccumulatorParam addAccumulator Methods inherited from interface org.apache.spark.AccumulableParam addInPlace, zero Field Detail MODULE$ public static final AccumulatorParam.DoubleAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Detail AccumulatorParam.DoubleAccumulatorParam$ public AccumulatorParam.DoubleAccumulatorParam$() Deprecated.  Method Detail addInPlace public double addInPlace(double t1, double t2) Deprecated.  zero public double zero(double initialValue) Deprecated.  Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulatorParam.FloatAccumulatorParam$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulatorParam.FloatAccumulatorParam$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class AccumulatorParam.FloatAccumulatorParam$ Object org.apache.spark.AccumulatorParam.FloatAccumulatorParam$ All Implemented Interfaces: java.io.Serializable, AccumulableParam<Object,Object>, AccumulatorParam<Object> Enclosing interface: AccumulatorParam<T> Deprecated.  use AccumulatorV2. Since 2.0.0. public static class AccumulatorParam.FloatAccumulatorParam$ extends Object implements AccumulatorParam<Object> See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface org.apache.spark.AccumulatorParam AccumulatorParam.DoubleAccumulatorParam$, AccumulatorParam.FloatAccumulatorParam$, AccumulatorParam.IntAccumulatorParam$, AccumulatorParam.LongAccumulatorParam$, AccumulatorParam.StringAccumulatorParam$ Field Summary Fields  Modifier and Type Field and Description static AccumulatorParam.FloatAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description AccumulatorParam.FloatAccumulatorParam$() Deprecated.    Method Summary Methods  Modifier and Type Method and Description float addInPlace(float t1, float t2) Deprecated.    float zero(float initialValue) Deprecated.    Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.AccumulatorParam addAccumulator Methods inherited from interface org.apache.spark.AccumulableParam addInPlace, zero Field Detail MODULE$ public static final AccumulatorParam.FloatAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Detail AccumulatorParam.FloatAccumulatorParam$ public AccumulatorParam.FloatAccumulatorParam$() Deprecated.  Method Detail addInPlace public float addInPlace(float t1, float t2) Deprecated.  zero public float zero(float initialValue) Deprecated.  Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulatorParam.IntAccumulatorParam$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulatorParam.IntAccumulatorParam$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class AccumulatorParam.IntAccumulatorParam$ Object org.apache.spark.AccumulatorParam.IntAccumulatorParam$ All Implemented Interfaces: java.io.Serializable, AccumulableParam<Object,Object>, AccumulatorParam<Object> Enclosing interface: AccumulatorParam<T> Deprecated.  use AccumulatorV2. Since 2.0.0. public static class AccumulatorParam.IntAccumulatorParam$ extends Object implements AccumulatorParam<Object> See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface org.apache.spark.AccumulatorParam AccumulatorParam.DoubleAccumulatorParam$, AccumulatorParam.FloatAccumulatorParam$, AccumulatorParam.IntAccumulatorParam$, AccumulatorParam.LongAccumulatorParam$, AccumulatorParam.StringAccumulatorParam$ Field Summary Fields  Modifier and Type Field and Description static AccumulatorParam.IntAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description AccumulatorParam.IntAccumulatorParam$() Deprecated.    Method Summary Methods  Modifier and Type Method and Description int addInPlace(int t1, int t2) Deprecated.    int zero(int initialValue) Deprecated.    Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.AccumulatorParam addAccumulator Methods inherited from interface org.apache.spark.AccumulableParam addInPlace, zero Field Detail MODULE$ public static final AccumulatorParam.IntAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Detail AccumulatorParam.IntAccumulatorParam$ public AccumulatorParam.IntAccumulatorParam$() Deprecated.  Method Detail addInPlace public int addInPlace(int t1, int t2) Deprecated.  zero public int zero(int initialValue) Deprecated.  Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulatorParam.LongAccumulatorParam$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulatorParam.LongAccumulatorParam$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class AccumulatorParam.LongAccumulatorParam$ Object org.apache.spark.AccumulatorParam.LongAccumulatorParam$ All Implemented Interfaces: java.io.Serializable, AccumulableParam<Object,Object>, AccumulatorParam<Object> Enclosing interface: AccumulatorParam<T> Deprecated.  use AccumulatorV2. Since 2.0.0. public static class AccumulatorParam.LongAccumulatorParam$ extends Object implements AccumulatorParam<Object> See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface org.apache.spark.AccumulatorParam AccumulatorParam.DoubleAccumulatorParam$, AccumulatorParam.FloatAccumulatorParam$, AccumulatorParam.IntAccumulatorParam$, AccumulatorParam.LongAccumulatorParam$, AccumulatorParam.StringAccumulatorParam$ Field Summary Fields  Modifier and Type Field and Description static AccumulatorParam.LongAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description AccumulatorParam.LongAccumulatorParam$() Deprecated.    Method Summary Methods  Modifier and Type Method and Description long addInPlace(long t1, long t2) Deprecated.    long zero(long initialValue) Deprecated.    Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.AccumulatorParam addAccumulator Methods inherited from interface org.apache.spark.AccumulableParam addInPlace, zero Field Detail MODULE$ public static final AccumulatorParam.LongAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Detail AccumulatorParam.LongAccumulatorParam$ public AccumulatorParam.LongAccumulatorParam$() Deprecated.  Method Detail addInPlace public long addInPlace(long t1, long t2) Deprecated.  zero public long zero(long initialValue) Deprecated.  Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulatorParam.StringAccumulatorParam$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulatorParam.StringAccumulatorParam$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class AccumulatorParam.StringAccumulatorParam$ Object org.apache.spark.AccumulatorParam.StringAccumulatorParam$ All Implemented Interfaces: java.io.Serializable, AccumulableParam<String,String>, AccumulatorParam<String> Enclosing interface: AccumulatorParam<T> Deprecated.  use AccumulatorV2. Since 2.0.0. public static class AccumulatorParam.StringAccumulatorParam$ extends Object implements AccumulatorParam<String> See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface org.apache.spark.AccumulatorParam AccumulatorParam.DoubleAccumulatorParam$, AccumulatorParam.FloatAccumulatorParam$, AccumulatorParam.IntAccumulatorParam$, AccumulatorParam.LongAccumulatorParam$, AccumulatorParam.StringAccumulatorParam$ Field Summary Fields  Modifier and Type Field and Description static AccumulatorParam.StringAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description AccumulatorParam.StringAccumulatorParam$() Deprecated.    Method Summary Methods  Modifier and Type Method and Description String addInPlace(String t1, String t2) Deprecated.  Merge two accumulated values together. String zero(String initialValue) Deprecated.  Return the "zero" (identity) value for an accumulator type, given its initial value. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.AccumulatorParam addAccumulator Field Detail MODULE$ public static final AccumulatorParam.StringAccumulatorParam$ MODULE$ Deprecated.  Static reference to the singleton instance of this Scala object. Constructor Detail AccumulatorParam.StringAccumulatorParam$ public AccumulatorParam.StringAccumulatorParam$() Deprecated.  Method Detail addInPlace public String addInPlace(String t1, String t2) Deprecated.  Description copied from interface: AccumulableParam Merge two accumulated values together. Is allowed to modify and return the first value for efficiency (to avoid allocating objects). Specified by: addInPlace in interface AccumulableParam<String,String> Parameters:t1 - one set of accumulated datat2 - another set of accumulated data Returns:both data sets merged together zero public String zero(String initialValue) Deprecated.  Description copied from interface: AccumulableParam Return the "zero" (identity) value for an accumulator type, given its initial value. For example, if R was a vector of N dimensions, this would return a vector of N zeroes. Specified by: zero in interface AccumulableParam<String,String> Parameters:initialValue - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulatorParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulatorParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Interface AccumulatorParam<T> All Superinterfaces: AccumulableParam<T,T>, java.io.Serializable All Known Implementing Classes: AccumulatorParam.DoubleAccumulatorParam$, AccumulatorParam.FloatAccumulatorParam$, AccumulatorParam.IntAccumulatorParam$, AccumulatorParam.LongAccumulatorParam$, AccumulatorParam.StringAccumulatorParam$ Deprecated.  use AccumulatorV2. Since 2.0.0. public interface AccumulatorParam<T> extends AccumulableParam<T,T> A simpler version of AccumulableParam where the only data type you can add in is the same type as the accumulated value. An implicit AccumulatorParam object needs to be available when you create Accumulators of a specific type. Nested Class Summary Nested Classes  Modifier and Type Interface and Description static class  AccumulatorParam.DoubleAccumulatorParam$ Deprecated.  use AccumulatorV2. Since 2.0.0. static class  AccumulatorParam.FloatAccumulatorParam$ Deprecated.  use AccumulatorV2. Since 2.0.0. static class  AccumulatorParam.IntAccumulatorParam$ Deprecated.  use AccumulatorV2. Since 2.0.0. static class  AccumulatorParam.LongAccumulatorParam$ Deprecated.  use AccumulatorV2. Since 2.0.0. static class  AccumulatorParam.StringAccumulatorParam$ Deprecated.  use AccumulatorV2. Since 2.0.0. Method Summary Methods  Modifier and Type Method and Description T addAccumulator(T t1, T t2) Deprecated.  Add additional data to the accumulator value. Methods inherited from interface org.apache.spark.AccumulableParam addInPlace, zero Method Detail addAccumulator T addAccumulator(T t1, T t2) Deprecated.  Description copied from interface: AccumulableParam Add additional data to the accumulator value. Is allowed to modify and return r for efficiency (to avoid allocating objects). Specified by: addAccumulator in interface AccumulableParam<T,T> Parameters:t1 - the current value of the accumulatort2 - the data to be added to the accumulator Returns:the new value of the accumulator Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AccumulatorV2 (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AccumulatorV2 (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class AccumulatorV2<IN,OUT> Object org.apache.spark.util.AccumulatorV2<IN,OUT> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: CollectionAccumulator, DoubleAccumulator, LegacyAccumulatorWrapper, LongAccumulator public abstract class AccumulatorV2<IN,OUT> extends Object implements scala.Serializable The base class for accumulators, that can accumulate inputs of type IN, and produce output of type OUT. OUT should be a type that can be read atomically (e.g., Int, Long), or thread-safely (e.g., synchronized collections) because it will be read from other threads. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description AccumulatorV2()  Method Summary Methods  Modifier and Type Method and Description abstract void add(IN v) Takes the inputs and accumulates. abstract AccumulatorV2<IN,OUT> copy() Creates a new copy of this accumulator. AccumulatorV2<IN,OUT> copyAndReset() Creates a new copy of this accumulator, which is zero value. long id() Returns the id of this accumulator, can only be called after registration. boolean isRegistered() Returns true if this accumulator has been registered. abstract boolean isZero() Returns if this accumulator is zero value or not. abstract void merge(AccumulatorV2<IN,OUT> other) Merges another same-type accumulator into this one and update its state, i.e. scala.Option<String> name() Returns the name of this accumulator, can only be called after registration. abstract void reset() Resets this accumulator, which is zero value. String toString()  abstract OUT value() Defines the current value of this accumulator Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail AccumulatorV2 public AccumulatorV2() Method Detail isRegistered public final boolean isRegistered() Returns true if this accumulator has been registered. Note that all accumulators must be registered before use, or it will throw exception. Returns:(undocumented) id public final long id() Returns the id of this accumulator, can only be called after registration. Returns:(undocumented) name public final scala.Option<String> name() Returns the name of this accumulator, can only be called after registration. Returns:(undocumented) isZero public abstract boolean isZero() Returns if this accumulator is zero value or not. e.g. for a counter accumulator, 0 is zero value; for a list accumulator, Nil is zero value. Returns:(undocumented) copyAndReset public AccumulatorV2<IN,OUT> copyAndReset() Creates a new copy of this accumulator, which is zero value. i.e. call isZero on the copy must return true. Returns:(undocumented) copy public abstract AccumulatorV2<IN,OUT> copy() Creates a new copy of this accumulator. Returns:(undocumented) reset public abstract void reset() Resets this accumulator, which is zero value. i.e. call isZero must return true. add public abstract void add(IN v) Takes the inputs and accumulates. Parameters:v - (undocumented) merge public abstract void merge(AccumulatorV2<IN,OUT> other) Merges another same-type accumulator into this one and update its state, i.e. this should be merge-in-place. Parameters:other - (undocumented) value public abstract OUT value() Defines the current value of this accumulator Returns:(undocumented) toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AggregatedDialect (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AggregatedDialect (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class AggregatedDialect Object org.apache.spark.sql.jdbc.JdbcDialect org.apache.spark.sql.jdbc.AggregatedDialect All Implemented Interfaces: java.io.Serializable public class AggregatedDialect extends JdbcDialect AggregatedDialect can unify multiple dialects into one virtual Dialect. Dialects are tried in order, and the first dialect that does not return a neutral element will will. param: dialects List of dialects. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description AggregatedDialect(scala.collection.immutable.List<JdbcDialect> dialects)  Method Summary Methods  Modifier and Type Method and Description boolean canHandle(String url) Check if this dialect instance can handle a certain jdbc url. scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) Get the custom datatype mapping for the given jdbc meta information. scala.Option<JdbcType> getJDBCType(DataType dt) Retrieve the jdbc / sql type for a given datatype. Methods inherited from class org.apache.spark.sql.jdbc.JdbcDialect beforeFetch, getTableExistsQuery, quoteIdentifier Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AggregatedDialect public AggregatedDialect(scala.collection.immutable.List<JdbcDialect> dialects) Method Detail canHandle public boolean canHandle(String url) Description copied from class: JdbcDialect Check if this dialect instance can handle a certain jdbc url. Specified by: canHandle in class JdbcDialect Parameters:url - the jdbc url. Returns:True if the dialect can be applied on the given jdbc url. getCatalystType public scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) Description copied from class: JdbcDialect Get the custom datatype mapping for the given jdbc meta information. Overrides: getCatalystType in class JdbcDialect Parameters:sqlType - The sql type (see java.sql.Types)typeName - The sql type name (e.g. "BIGINT UNSIGNED")size - The size of the type.md - Result metadata associated with this type. Returns:The actual DataType (subclasses of DataType) or null if the default type mapping should be used. getJDBCType public scala.Option<JdbcType> getJDBCType(DataType dt) Description copied from class: JdbcDialect Retrieve the jdbc / sql type for a given datatype. Overrides: getJDBCType in class JdbcDialect Parameters:dt - The datatype (e.g. StringType) Returns:The new JdbcType if there is an override for this DataType Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AggregatingEdgeContext (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AggregatingEdgeContext (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx.impl Class AggregatingEdgeContext<VD,ED,A> Object org.apache.spark.graphx.EdgeContext<VD,ED,A> org.apache.spark.graphx.impl.AggregatingEdgeContext<VD,ED,A> public class AggregatingEdgeContext<VD,ED,A> extends EdgeContext<VD,ED,A> Constructor Summary Constructors  Constructor and Description AggregatingEdgeContext(scala.Function2<A,A,A> mergeMsg, Object aggregates, org.apache.spark.util.collection.BitSet bitset)  Method Summary Methods  Modifier and Type Method and Description ED attr() The attribute associated with the edge. VD dstAttr() The vertex attribute of the edge's destination vertex. long dstId() The vertex id of the edge's destination vertex. void sendToDst(A msg) Sends a message to the destination vertex. void sendToSrc(A msg) Sends a message to the source vertex. void set(long srcId, long dstId, int localSrcId, int localDstId, VD srcAttr, VD dstAttr, ED attr)  void setRest(long dstId, int localDstId, VD dstAttr, ED attr)  void setSrcOnly(long srcId, int localSrcId, VD srcAttr)  VD srcAttr() The vertex attribute of the edge's source vertex. long srcId() The vertex id of the edge's source vertex. Methods inherited from class org.apache.spark.graphx.EdgeContext toEdgeTriplet, unapply Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AggregatingEdgeContext public AggregatingEdgeContext(scala.Function2<A,A,A> mergeMsg, Object aggregates, org.apache.spark.util.collection.BitSet bitset) Method Detail set public void set(long srcId, long dstId, int localSrcId, int localDstId, VD srcAttr, VD dstAttr, ED attr) setSrcOnly public void setSrcOnly(long srcId, int localSrcId, VD srcAttr) setRest public void setRest(long dstId, int localDstId, VD dstAttr, ED attr) srcId public long srcId() Description copied from class: EdgeContext The vertex id of the edge's source vertex. Specified by: srcId in class EdgeContext<VD,ED,A> dstId public long dstId() Description copied from class: EdgeContext The vertex id of the edge's destination vertex. Specified by: dstId in class EdgeContext<VD,ED,A> srcAttr public VD srcAttr() Description copied from class: EdgeContext The vertex attribute of the edge's source vertex. Specified by: srcAttr in class EdgeContext<VD,ED,A> dstAttr public VD dstAttr() Description copied from class: EdgeContext The vertex attribute of the edge's destination vertex. Specified by: dstAttr in class EdgeContext<VD,ED,A> attr public ED attr() Description copied from class: EdgeContext The attribute associated with the edge. Specified by: attr in class EdgeContext<VD,ED,A> sendToSrc public void sendToSrc(A msg) Description copied from class: EdgeContext Sends a message to the source vertex. Specified by: sendToSrc in class EdgeContext<VD,ED,A> sendToDst public void sendToDst(A msg) Description copied from class: EdgeContext Sends a message to the destination vertex. Specified by: sendToDst in class EdgeContext<VD,ED,A> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Aggregator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Aggregator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.expressions Class Aggregator<IN,BUF,OUT> Object org.apache.spark.sql.expressions.Aggregator<IN,BUF,OUT> All Implemented Interfaces: java.io.Serializable public abstract class Aggregator<IN,BUF,OUT> extends Object implements scala.Serializable :: Experimental :: A base class for user-defined aggregations, which can be used in Dataset operations to take all of the elements of a group and reduce them to a single value. For example, the following aggregator extracts an int from a specific class and adds them up: case class Data(i: Int) val customSummer = new Aggregator[Data, Int, Int] { def zero: Int = 0 def reduce(b: Int, a: Data): Int = b + a.i def merge(b1: Int, b2: Int): Int = b1 + b2 def finish(r: Int): Int = r }.toColumn() val ds: Dataset[Data] = ... val aggregated = ds.select(customSummer) Based loosely on Aggregator from Algebird: https://github.com/twitter/algebird Since: 1.6.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Aggregator()  Method Summary Methods  Modifier and Type Method and Description abstract Encoder<BUF> bufferEncoder() Specifies the Encoder for the intermediate value type. abstract OUT finish(BUF reduction) Transform the output of the reduction. abstract BUF merge(BUF b1, BUF b2) Merge two intermediate values. abstract Encoder<OUT> outputEncoder() Specifies the Encoder for the final ouput value type. abstract BUF reduce(BUF b, IN a) Combine two values to produce a new value. TypedColumn<IN,OUT> toColumn() Returns this Aggregator as a TypedColumn that can be used in Dataset. abstract BUF zero() A zero value for this aggregation. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Aggregator public Aggregator() Method Detail zero public abstract BUF zero() A zero value for this aggregation. Should satisfy the property that any b + zero = b. Returns:(undocumented)Since: 1.6.0 reduce public abstract BUF reduce(BUF b, IN a) Combine two values to produce a new value. For performance, the function may modify b and return it instead of constructing new object for b. Parameters:b - (undocumented)a - (undocumented) Returns:(undocumented)Since: 1.6.0 merge public abstract BUF merge(BUF b1, BUF b2) Merge two intermediate values. Parameters:b1 - (undocumented)b2 - (undocumented) Returns:(undocumented)Since: 1.6.0 finish public abstract OUT finish(BUF reduction) Transform the output of the reduction. Parameters:reduction - (undocumented) Returns:(undocumented)Since: 1.6.0 bufferEncoder public abstract Encoder<BUF> bufferEncoder() Specifies the Encoder for the intermediate value type. Returns:(undocumented)Since: 2.0.0 outputEncoder public abstract Encoder<OUT> outputEncoder() Specifies the Encoder for the final ouput value type. Returns:(undocumented)Since: 2.0.0 toColumn public TypedColumn<IN,OUT> toColumn() Returns this Aggregator as a TypedColumn that can be used in Dataset. operations. Returns:(undocumented)Since: 1.6.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Algo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Algo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.configuration Class Algo Object org.apache.spark.mllib.tree.configuration.Algo public class Algo extends Object Enum to select the algorithm for the decision tree Constructor Summary Constructors  Constructor and Description Algo()  Method Summary Methods  Modifier and Type Method and Description static scala.Enumeration.Value apply(int x)  static scala.Enumeration.Value Classification()  static int maxId()  static scala.Enumeration.Value Regression()  static String toString()  static scala.Enumeration.ValueSet values()  static scala.Enumeration.Value withName(String s)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Algo public Algo() Method Detail Classification public static scala.Enumeration.Value Classification() Regression public static scala.Enumeration.Value Regression() toString public static String toString() values public static scala.Enumeration.ValueSet values() maxId public static final int maxId() apply public static final scala.Enumeration.Value apply(int x) withName public static final scala.Enumeration.Value withName(String s) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AllJobsCancelled (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AllJobsCancelled (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler Class AllJobsCancelled Object org.apache.spark.scheduler.AllJobsCancelled public class AllJobsCancelled extends Object Constructor Summary Constructors  Constructor and Description AllJobsCancelled()  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AllJobsCancelled public AllJobsCancelled() Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AllReceiverIds (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AllReceiverIds (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.scheduler Class AllReceiverIds Object org.apache.spark.streaming.scheduler.AllReceiverIds public class AllReceiverIds extends Object A message used by ReceiverTracker to ask all receiver's ids still stored in ReceiverTrackerEndpoint. Constructor Summary Constructors  Constructor and Description AllReceiverIds()  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AllReceiverIds public AllReceiverIds() Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AnalysisException (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AnalysisException (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class AnalysisException Object Throwable Exception org.apache.spark.sql.AnalysisException All Implemented Interfaces: java.io.Serializable public class AnalysisException extends Exception implements scala.Serializable :: DeveloperApi :: Thrown when a query fails to analyze, usually because the query itself is invalid. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static void addSuppressed(Throwable x$1)  scala.Option<Throwable> cause()  static Throwable fillInStackTrace()  static Throwable getCause()  static String getLocalizedMessage()  String getMessage()  String getSimpleMessage()  static StackTraceElement[] getStackTrace()  static Throwable[] getSuppressed()  static Throwable initCause(Throwable x$1)  scala.Option<Object> line()  String message()  scala.Option<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> plan()  static void printStackTrace()  static void printStackTrace(java.io.PrintStream x$1)  static void printStackTrace(java.io.PrintWriter x$1)  static void setStackTrace(StackTraceElement[] x$1)  scala.Option<Object> startPosition()  static String toString()  AnalysisException withPosition(scala.Option<Object> line, scala.Option<Object> startPosition)  Methods inherited from class Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Method Detail getLocalizedMessage public static String getLocalizedMessage() getCause public static Throwable getCause() initCause public static Throwable initCause(Throwable x$1) toString public static String toString() printStackTrace public static void printStackTrace() printStackTrace public static void printStackTrace(java.io.PrintStream x$1) printStackTrace public static void printStackTrace(java.io.PrintWriter x$1) fillInStackTrace public static Throwable fillInStackTrace() getStackTrace public static StackTraceElement[] getStackTrace() setStackTrace public static void setStackTrace(StackTraceElement[] x$1) addSuppressed public static final void addSuppressed(Throwable x$1) getSuppressed public static final Throwable[] getSuppressed() message public String message() line public scala.Option<Object> line() startPosition public scala.Option<Object> startPosition() plan public scala.Option<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> plan() cause public scala.Option<Throwable> cause() withPosition public AnalysisException withPosition(scala.Option<Object> line, scala.Option<Object> startPosition) getMessage public String getMessage() Overrides: getMessage in class Throwable getSimpleMessage public String getSimpleMessage() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method And (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="And (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class And Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.And All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class And extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff both left or right evaluate to true. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description And(Filter left, Filter right)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  Filter left()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Filter right()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail And public And(Filter left, Filter right) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() left public Filter left() right public Filter right() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AnyDataType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AnyDataType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class AnyDataType Object org.apache.spark.sql.types.AnyDataType public class AnyDataType extends Object An AbstractDataType that matches any concrete data types. Constructor Summary Constructors  Constructor and Description AnyDataType()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AnyDataType public AnyDataType() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ApplicationAttemptInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ApplicationAttemptInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class ApplicationAttemptInfo Object org.apache.spark.status.api.v1.ApplicationAttemptInfo public class ApplicationAttemptInfo extends Object Method Summary Methods  Modifier and Type Method and Description scala.Option<String> attemptId()  boolean completed()  long duration()  java.util.Date endTime()  long getEndTimeEpoch()  long getLastUpdatedEpoch()  long getStartTimeEpoch()  java.util.Date lastUpdated()  String sparkUser()  java.util.Date startTime()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail attemptId public scala.Option<String> attemptId() startTime public java.util.Date startTime() endTime public java.util.Date endTime() lastUpdated public java.util.Date lastUpdated() duration public long duration() sparkUser public String sparkUser() completed public boolean completed() getStartTimeEpoch public long getStartTimeEpoch() getEndTimeEpoch public long getEndTimeEpoch() getLastUpdatedEpoch public long getLastUpdatedEpoch() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ApplicationInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ApplicationInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class ApplicationInfo Object org.apache.spark.status.api.v1.ApplicationInfo public class ApplicationInfo extends Object Method Summary Methods  Modifier and Type Method and Description scala.collection.Seq<ApplicationAttemptInfo> attempts()  scala.Option<Object> coresGranted()  scala.Option<Object> coresPerExecutor()  String id()  scala.Option<Object> maxCores()  scala.Option<Object> memoryPerExecutorMB()  String name()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail id public String id() name public String name() coresGranted public scala.Option<Object> coresGranted() maxCores public scala.Option<Object> maxCores() coresPerExecutor public scala.Option<Object> coresPerExecutor() memoryPerExecutorMB public scala.Option<Object> memoryPerExecutorMB() attempts public scala.collection.Seq<ApplicationAttemptInfo> attempts() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ApplicationStatus (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ApplicationStatus (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method org.apache.spark.status.api.v1 Enum ApplicationStatus Object Enum<ApplicationStatus> org.apache.spark.status.api.v1.ApplicationStatus All Implemented Interfaces: java.io.Serializable, Comparable<ApplicationStatus> public enum ApplicationStatus extends Enum<ApplicationStatus> Enum Constant Summary Enum Constants  Enum Constant and Description COMPLETED  RUNNING  Method Summary Methods  Modifier and Type Method and Description static ApplicationStatus fromString(String str)  static ApplicationStatus valueOf(String name) Returns the enum constant of this type with the specified name. static ApplicationStatus[] values() Returns an array containing the constants of this enum type, in the order they are declared. Methods inherited from class Enum compareTo, equals, getDeclaringClass, hashCode, name, ordinal, toString, valueOf Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Enum Constant Detail COMPLETED public static final ApplicationStatus COMPLETED RUNNING public static final ApplicationStatus RUNNING Method Detail values public static ApplicationStatus[] values() Returns an array containing the constants of this enum type, in the order they are declared. This method may be used to iterate over the constants as follows: for (ApplicationStatus c : ApplicationStatus.values())   System.out.println(c); Returns:an array containing the constants of this enum type, in the order they are declared valueOf public static ApplicationStatus valueOf(String name) Returns the enum constant of this type with the specified name. The string must match exactly an identifier used to declare an enum constant in this type. (Extraneous whitespace characters are not permitted.) Parameters:name - the name of the enum constant to be returned. Returns:the enum constant with the specified name Throws: IllegalArgumentException - if this enum type has no constant with the specified name NullPointerException - if the argument is null fromString public static ApplicationStatus fromString(String str) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method ApplicationsListResource (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ApplicationsListResource (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class ApplicationsListResource Object org.apache.spark.status.api.v1.ApplicationsListResource public class ApplicationsListResource extends Object Constructor Summary Constructors  Constructor and Description ApplicationsListResource()  Method Summary Methods  Modifier and Type Method and Description static ApplicationInfo appHistoryInfoToPublicAppInfo(org.apache.spark.deploy.history.ApplicationHistoryInfo app)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ApplicationsListResource public ApplicationsListResource() Method Detail appHistoryInfoToPublicAppInfo public static ApplicationInfo appHistoryInfoToPublicAppInfo(org.apache.spark.deploy.history.ApplicationHistoryInfo app) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ApplyInPlace (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ApplyInPlace (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.ann Class ApplyInPlace Object org.apache.spark.ml.ann.ApplyInPlace public class ApplyInPlace extends Object Implements in-place application of functions in the arrays Constructor Summary Constructors  Constructor and Description ApplyInPlace()  Method Summary Methods  Modifier and Type Method and Description static void apply(breeze.linalg.DenseMatrix<Object> x1, breeze.linalg.DenseMatrix<Object> x2, breeze.linalg.DenseMatrix<Object> y, scala.Function2<Object,Object,Object> func)  static void apply(breeze.linalg.DenseMatrix<Object> x, breeze.linalg.DenseMatrix<Object> y, scala.Function1<Object,Object> func)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ApplyInPlace public ApplyInPlace() Method Detail apply public static void apply(breeze.linalg.DenseMatrix<Object> x, breeze.linalg.DenseMatrix<Object> y, scala.Function1<Object,Object> func) apply public static void apply(breeze.linalg.DenseMatrix<Object> x1, breeze.linalg.DenseMatrix<Object> x2, breeze.linalg.DenseMatrix<Object> y, scala.Function2<Object,Object,Object> func) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AreaUnderCurve (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AreaUnderCurve (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.evaluation Class AreaUnderCurve Object org.apache.spark.mllib.evaluation.AreaUnderCurve public class AreaUnderCurve extends Object Computes the area under the curve (AUC) using the trapezoidal rule. Constructor Summary Constructors  Constructor and Description AreaUnderCurve()  Method Summary Methods  Modifier and Type Method and Description static double of(scala.collection.Iterable<scala.Tuple2<Object,Object>> curve) Returns the area under the given curve. static double of(RDD<scala.Tuple2<Object,Object>> curve) Returns the area under the given curve. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AreaUnderCurve public AreaUnderCurve() Method Detail of public static double of(RDD<scala.Tuple2<Object,Object>> curve) Returns the area under the given curve. Parameters:curve - a RDD of ordered 2D points stored in pairs representing a curve Returns:(undocumented) of public static double of(scala.collection.Iterable<scala.Tuple2<Object,Object>> curve) Returns the area under the given curve. Parameters:curve - an iterator over ordered 2D points stored in pairs representing a curve Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ArrayType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ArrayType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class ArrayType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.ArrayType All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class ArrayType extends DataType implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ArrayType(DataType elementType, boolean containsNull)  Method Summary Methods  Modifier and Type Method and Description static ArrayType apply(DataType elementType) Construct a ArrayType object with the given element type. abstract static boolean canEqual(Object that)  String catalogString() String representation for the type saved in external catalogs. boolean containsNull()  int defaultSize() The default size of a value of the ArrayType is 100 * the default size of the element type. DataType elementType()  abstract static boolean equals(Object that)  static String json()  static String prettyJson()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  String simpleString() Readable string representation for the type. String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType fromJson, json, prettyJson, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail ArrayType public ArrayType(DataType elementType, boolean containsNull) Method Detail apply public static ArrayType apply(DataType elementType) Construct a ArrayType object with the given element type. The `containsNull` is true. typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() elementType public DataType elementType() containsNull public boolean containsNull() defaultSize public int defaultSize() The default size of a value of the ArrayType is 100 * the default size of the element type. (We assume that there are 100 elements). Specified by: defaultSize in class DataType Returns:(undocumented) simpleString public String simpleString() Description copied from class: DataType Readable string representation for the type. Overrides: simpleString in class DataType catalogString public String catalogString() Description copied from class: DataType String representation for the type saved in external catalogs. Overrides: catalogString in class DataType sql public String sql() Overrides: sql in class DataType Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AskPermissionToCommitOutput (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AskPermissionToCommitOutput (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler Class AskPermissionToCommitOutput Object org.apache.spark.scheduler.AskPermissionToCommitOutput All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class AskPermissionToCommitOutput extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description AskPermissionToCommitOutput(int stage, int partition, int attemptNumber)  Method Summary Methods  Modifier and Type Method and Description int attemptNumber()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  int partition()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  int stage()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail AskPermissionToCommitOutput public AskPermissionToCommitOutput(int stage, int partition, int attemptNumber) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() stage public int stage() partition public int partition() attemptNumber public int attemptNumber() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AssociationRules.Rule (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AssociationRules.Rule (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class AssociationRules.Rule<Item> Object org.apache.spark.mllib.fpm.AssociationRules.Rule<Item> All Implemented Interfaces: java.io.Serializable Enclosing class: AssociationRules public static class AssociationRules.Rule<Item> extends Object implements scala.Serializable An association rule between sets of items. param: antecedent hypotheses of the rule. Java users should call javaAntecedent() instead. param: consequent conclusion of the rule. Java users should call javaConsequent() instead. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description Object antecedent()  double confidence() Returns the confidence of the rule. Object consequent()  java.util.List<Item> javaAntecedent() Returns antecedent in a Java List. java.util.List<Item> javaConsequent() Returns consequent in a Java List. String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Method Detail antecedent public Object antecedent() consequent public Object consequent() confidence public double confidence() Returns the confidence of the rule. Returns:(undocumented) javaAntecedent public java.util.List<Item> javaAntecedent() Returns antecedent in a Java List. Returns:(undocumented) javaConsequent public java.util.List<Item> javaConsequent() Returns consequent in a Java List. Returns:(undocumented) toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AssociationRules (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AssociationRules (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class AssociationRules Object org.apache.spark.mllib.fpm.AssociationRules All Implemented Interfaces: java.io.Serializable public class AssociationRules extends Object implements scala.Serializable Generates association rules from a RDD[FreqItemset[Item]. This method only generates association rules which have a single item as the consequent. See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  AssociationRules.Rule<Item> An association rule between sets of items. Constructor Summary Constructors  Constructor and Description AssociationRules() Constructs a default instance with default parameters {minConfidence = 0.8}. Method Summary Methods  Modifier and Type Method and Description <Item> JavaRDD<AssociationRules.Rule<Item>> run(JavaRDD<FPGrowth.FreqItemset<Item>> freqItemsets) Java-friendly version of run. <Item> RDD<AssociationRules.Rule<Item>> run(RDD<FPGrowth.FreqItemset<Item>> freqItemsets, scala.reflect.ClassTag<Item> evidence$1) Computes the association rules with confidence above minConfidence. AssociationRules setMinConfidence(double minConfidence) Sets the minimal confidence (default: 0.8). Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AssociationRules public AssociationRules() Constructs a default instance with default parameters {minConfidence = 0.8}. Method Detail setMinConfidence public AssociationRules setMinConfidence(double minConfidence) Sets the minimal confidence (default: 0.8). Parameters:minConfidence - (undocumented) Returns:(undocumented) run public <Item> RDD<AssociationRules.Rule<Item>> run(RDD<FPGrowth.FreqItemset<Item>> freqItemsets, scala.reflect.ClassTag<Item> evidence$1) Computes the association rules with confidence above minConfidence. Parameters:freqItemsets - frequent itemset model obtained from FPGrowthevidence$1 - (undocumented) Returns:a Set[Rule[Item] containing the association rules. run public <Item> JavaRDD<AssociationRules.Rule<Item>> run(JavaRDD<FPGrowth.FreqItemset<Item>> freqItemsets) Java-friendly version of run. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AsyncRDDActions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AsyncRDDActions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class AsyncRDDActions<T> Object org.apache.spark.rdd.AsyncRDDActions<T> All Implemented Interfaces: java.io.Serializable public class AsyncRDDActions<T> extends Object implements scala.Serializable A set of asynchronous RDD actions available through an implicit conversion. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description AsyncRDDActions(RDD<T> self, scala.reflect.ClassTag<T> evidence$1)  Method Summary Methods  Modifier and Type Method and Description FutureAction<scala.collection.Seq<T>> collectAsync() Returns a future for retrieving all elements of this RDD. FutureAction<Object> countAsync() Returns a future for counting the number of elements in the RDD. FutureAction<scala.runtime.BoxedUnit> foreachAsync(scala.Function1<T,scala.runtime.BoxedUnit> f) Applies a function f to all elements of this RDD. FutureAction<scala.runtime.BoxedUnit> foreachPartitionAsync(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) Applies a function f to each partition of this RDD. static scala.concurrent.ExecutionContextExecutorService futureExecutionContext()  FutureAction<scala.collection.Seq<T>> takeAsync(int num) Returns a future for retrieving the first num elements of the RDD. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AsyncRDDActions public AsyncRDDActions(RDD<T> self, scala.reflect.ClassTag<T> evidence$1) Method Detail futureExecutionContext public static scala.concurrent.ExecutionContextExecutorService futureExecutionContext() countAsync public FutureAction<Object> countAsync() Returns a future for counting the number of elements in the RDD. Returns:(undocumented) collectAsync public FutureAction<scala.collection.Seq<T>> collectAsync() Returns a future for retrieving all elements of this RDD. Returns:(undocumented) takeAsync public FutureAction<scala.collection.Seq<T>> takeAsync(int num) Returns a future for retrieving the first num elements of the RDD. Parameters:num - (undocumented) Returns:(undocumented) foreachAsync public FutureAction<scala.runtime.BoxedUnit> foreachAsync(scala.Function1<T,scala.runtime.BoxedUnit> f) Applies a function f to all elements of this RDD. Parameters:f - (undocumented) Returns:(undocumented) foreachPartitionAsync public FutureAction<scala.runtime.BoxedUnit> foreachPartitionAsync(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) Applies a function f to each partition of this RDD. Parameters:f - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Attribute (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Attribute (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.attribute Class Attribute Object org.apache.spark.ml.attribute.Attribute All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: BinaryAttribute, NominalAttribute, NumericAttribute public abstract class Attribute extends Object implements scala.Serializable :: DeveloperApi :: Abstract class for ML attributes. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Attribute()  Method Summary Methods  Modifier and Type Method and Description abstract AttributeType attrType() Attribute type. abstract scala.Option<Object> index() Index of the attribute. abstract boolean isNominal() Tests whether this attribute is nominal, true for NominalAttribute and BinaryAttribute. abstract boolean isNumeric() Tests whether this attribute is numeric, true for NumericAttribute and BinaryAttribute. abstract scala.Option<String> name() Name of the attribute. Metadata toMetadata() Converts to ML metadata Metadata toMetadata(Metadata existingMetadata) Converts to ML metadata with some existing metadata. String toString()  StructField toStructField() Converts to a StructField. StructField toStructField(Metadata existingMetadata) Converts to a StructField with some existing metadata. abstract Attribute withIndex(int index) Copy with a new index. abstract Attribute withName(String name) Copy with a new name. abstract Attribute withoutIndex() Copy without the index. abstract Attribute withoutName() Copy without the name. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail Attribute public Attribute() Method Detail attrType public abstract AttributeType attrType() Attribute type. name public abstract scala.Option<String> name() Name of the attribute. None if it is not set. withName public abstract Attribute withName(String name) Copy with a new name. withoutName public abstract Attribute withoutName() Copy without the name. index public abstract scala.Option<Object> index() Index of the attribute. None if it is not set. withIndex public abstract Attribute withIndex(int index) Copy with a new index. withoutIndex public abstract Attribute withoutIndex() Copy without the index. isNumeric public abstract boolean isNumeric() Tests whether this attribute is numeric, true for NumericAttribute and BinaryAttribute. Returns:(undocumented) isNominal public abstract boolean isNominal() Tests whether this attribute is nominal, true for NominalAttribute and BinaryAttribute. Returns:(undocumented) toMetadata public Metadata toMetadata(Metadata existingMetadata) Converts to ML metadata with some existing metadata. toMetadata public Metadata toMetadata() Converts to ML metadata toStructField public StructField toStructField(Metadata existingMetadata) Converts to a StructField with some existing metadata. Parameters:existingMetadata - existing metadata to carry over Returns:(undocumented) toStructField public StructField toStructField() Converts to a StructField. toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AttributeGroup (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AttributeGroup (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.attribute Class AttributeGroup Object org.apache.spark.ml.attribute.AttributeGroup All Implemented Interfaces: java.io.Serializable public class AttributeGroup extends Object implements scala.Serializable :: DeveloperApi :: Attributes that describe a vector ML column. param: name name of the attribute group (the ML column name) param: numAttributes optional number of attributes. At most one of numAttributes and attrs can be defined. param: attrs optional array of attributes. Attribute will be copied with their corresponding indices in the array. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description AttributeGroup(String name) Creates an attribute group without attribute info. AttributeGroup(String name, Attribute[] attrs) Creates an attribute group with attributes. AttributeGroup(String name, int numAttributes) Creates an attribute group knowing only the number of attributes. Method Summary Methods  Modifier and Type Method and Description Attribute apply(int attrIndex) Gets an attribute by its index. Attribute apply(String attrName) Gets an attribute by its name. scala.Option<Attribute[]> attributes() Optional array of attributes. boolean equals(Object other)  static AttributeGroup fromStructField(StructField field) Creates an attribute group from a StructField instance. Attribute getAttr(int attrIndex) Gets an attribute by its index. Attribute getAttr(String attrName) Gets an attribute by its name. boolean hasAttr(String attrName) Test whether this attribute group contains a specific attribute. int hashCode()  int indexOf(String attrName) Index of an attribute specified by name. String name()  scala.Option<Object> numAttributes()  int size() Size of the attribute group. Metadata toMetadata() Converts to ML metadata Metadata toMetadata(Metadata existingMetadata) Converts to ML metadata with some existing metadata. String toString()  StructField toStructField() Converts to a StructField. StructField toStructField(Metadata existingMetadata) Converts to a StructField with some existing metadata. Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail AttributeGroup public AttributeGroup(String name) Creates an attribute group without attribute info. Parameters:name - name of the attribute group AttributeGroup public AttributeGroup(String name, int numAttributes) Creates an attribute group knowing only the number of attributes. Parameters:name - name of the attribute groupnumAttributes - number of attributes AttributeGroup public AttributeGroup(String name, Attribute[] attrs) Creates an attribute group with attributes. Parameters:name - name of the attribute groupattrs - array of attributes. Attributes will be copied with their corresponding indices in the array. Method Detail fromStructField public static AttributeGroup fromStructField(StructField field) Creates an attribute group from a StructField instance. name public String name() numAttributes public scala.Option<Object> numAttributes() attributes public scala.Option<Attribute[]> attributes() Optional array of attributes. At most one of numAttributes and attributes can be defined. Returns:(undocumented) size public int size() Size of the attribute group. Returns -1 if the size is unknown. hasAttr public boolean hasAttr(String attrName) Test whether this attribute group contains a specific attribute. indexOf public int indexOf(String attrName) Index of an attribute specified by name. apply public Attribute apply(String attrName) Gets an attribute by its name. getAttr public Attribute getAttr(String attrName) Gets an attribute by its name. apply public Attribute apply(int attrIndex) Gets an attribute by its index. getAttr public Attribute getAttr(int attrIndex) Gets an attribute by its index. toMetadata public Metadata toMetadata(Metadata existingMetadata) Converts to ML metadata with some existing metadata. toMetadata public Metadata toMetadata() Converts to ML metadata toStructField public StructField toStructField(Metadata existingMetadata) Converts to a StructField with some existing metadata. toStructField public StructField toStructField() Converts to a StructField. equals public boolean equals(Object other) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AttributeKeys (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AttributeKeys (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.attribute Class AttributeKeys Object org.apache.spark.ml.attribute.AttributeKeys public class AttributeKeys extends Object Keys used to store attributes. Constructor Summary Constructors  Constructor and Description AttributeKeys()  Method Summary Methods  Modifier and Type Method and Description static String ATTRIBUTES()  static String INDEX()  static String MAX()  static String MIN()  static String ML_ATTR()  static String NAME()  static String NUM_ATTRIBUTES()  static String NUM_VALUES()  static String ORDINAL()  static String SPARSITY()  static String STD()  static String TYPE()  static String VALUES()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AttributeKeys public AttributeKeys() Method Detail ML_ATTR public static String ML_ATTR() TYPE public static String TYPE() NAME public static String NAME() INDEX public static String INDEX() MIN public static String MIN() MAX public static String MAX() STD public static String STD() SPARSITY public static String SPARSITY() ORDINAL public static String ORDINAL() VALUES public static String VALUES() NUM_VALUES public static String NUM_VALUES() ATTRIBUTES public static String ATTRIBUTES() NUM_ATTRIBUTES public static String NUM_ATTRIBUTES() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method AttributeType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="AttributeType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.attribute Class AttributeType Object org.apache.spark.ml.attribute.AttributeType public abstract class AttributeType extends Object :: DeveloperApi :: An enum-like type for attribute types: AttributeType$.Numeric, AttributeType$.Nominal, and AttributeType$.Binary. Constructor Summary Constructors  Constructor and Description AttributeType(String name)  Method Summary Methods  Modifier and Type Method and Description static AttributeType Binary() Binary type. static AttributeType fromName(String name) Gets the AttributeType object from its name. String name()  static AttributeType Nominal() Nominal type. static AttributeType Numeric() Numeric type. static AttributeType Unresolved() Unresolved type. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail AttributeType public AttributeType(String name) Method Detail Numeric public static AttributeType Numeric() Numeric type. Nominal public static AttributeType Nominal() Nominal type. Binary public static AttributeType Binary() Binary type. Unresolved public static AttributeType Unresolved() Unresolved type. fromName public static AttributeType fromName(String name) Gets the AttributeType object from its name. Parameters:name - attribute type name: "numeric", "nominal", or "binary" Returns:(undocumented) name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BLAS (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BLAS (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg Class BLAS Object org.apache.spark.mllib.linalg.BLAS public class BLAS extends Object BLAS routines for MLlib's vectors and matrices. Constructor Summary Constructors  Constructor and Description BLAS()  Method Summary Methods  Modifier and Type Method and Description static void axpy(double a, Vector x, Vector y) y += a * x static void copy(Vector x, Vector y) y = x static double dot(Vector x, Vector y) dot(x, y) static void gemm(double alpha, Matrix A, DenseMatrix B, double beta, DenseMatrix C) C := alpha * A * B + beta * C static void gemv(double alpha, Matrix A, Vector x, double beta, DenseVector y) y := alpha * A * x + beta * y static void scal(double a, Vector x) x = a * x static void spr(double alpha, Vector v, DenseVector U) Adds alpha * v * v.t to a matrix in-place. static void spr(double alpha, Vector v, double[] U) Adds alpha * v * v.t to a matrix in-place. static void syr(double alpha, Vector x, DenseMatrix A) A := alpha * x * x^T^ + A Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BLAS public BLAS() Method Detail axpy public static void axpy(double a, Vector x, Vector y) y += a * x Parameters:a - (undocumented)x - (undocumented)y - (undocumented) dot public static double dot(Vector x, Vector y) dot(x, y) Parameters:x - (undocumented)y - (undocumented) Returns:(undocumented) copy public static void copy(Vector x, Vector y) y = x Parameters:x - (undocumented)y - (undocumented) scal public static void scal(double a, Vector x) x = a * x Parameters:a - (undocumented)x - (undocumented) spr public static void spr(double alpha, Vector v, DenseVector U) Adds alpha * v * v.t to a matrix in-place. This is the same as BLAS's ?SPR. Parameters:U - the upper triangular part of the matrix in a DenseVector(column major)alpha - (undocumented)v - (undocumented) spr public static void spr(double alpha, Vector v, double[] U) Adds alpha * v * v.t to a matrix in-place. This is the same as BLAS's ?SPR. Parameters:U - the upper triangular part of the matrix packed in an array (column major)alpha - (undocumented)v - (undocumented) syr public static void syr(double alpha, Vector x, DenseMatrix A) A := alpha * x * x^T^ + A Parameters:alpha - a real scalar that will be multiplied to x * x^T^.x - the vector x that contains the n elements.A - the symmetric matrix A. Size of n x n. gemm public static void gemm(double alpha, Matrix A, DenseMatrix B, double beta, DenseMatrix C) C := alpha * A * B + beta * C Parameters:alpha - a scalar to scale the multiplication A * B.A - the matrix A that will be left multiplied to B. Size of m x k.B - the matrix B that will be left multiplied by A. Size of k x n.beta - a scalar that can be used to scale matrix C.C - the resulting matrix C. Size of m x n. C.isTransposed must be false. gemv public static void gemv(double alpha, Matrix A, Vector x, double beta, DenseVector y) y := alpha * A * x + beta * y Parameters:alpha - a scalar to scale the multiplication A * x.A - the matrix A that will be left multiplied to x. Size of m x n.x - the vector x that will be left multiplied by A. Size of n x 1.beta - a scalar that can be used to scale vector y.y - the resulting vector y. Size of m x 1. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BaseRRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BaseRRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.r Class BaseRRDD<T,U> Object org.apache.spark.rdd.RDD<U> org.apache.spark.api.r.BaseRRDD<T,U> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: PairwiseRRDD, RRDD, StringRRDD public abstract class BaseRRDD<T,U> extends RDD<U> See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BaseRRDD(RDD<T> parent, int numPartitions, byte[] func, String deserializer, String serializer, byte[] packageNames, Broadcast<Object>[] broadcastVars, scala.reflect.ClassTag<T> evidence$1, scala.reflect.ClassTag<U> evidence$2)  Method Summary Methods  Modifier and Type Method and Description scala.collection.Iterator<U> compute(Partition partition, TaskContext context) :: DeveloperApi :: Implemented by subclasses to compute a given partition. Partition[] getPartitions() Implemented by subclasses to return the set of partitions in this RDD. Methods inherited from class org.apache.spark.rdd.RDD aggregate, cache, cartesian, checkpoint, coalesce, collect, collect, context, count, countApprox, countApproxDistinct, countApproxDistinct, countByValue, countByValueApprox, dependencies, distinct, distinct, doubleRDDToDoubleRDDFunctions, filter, first, flatMap, fold, foreach, foreachPartition, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupBy, id, intersection, intersection, intersection, isCheckpointed, isEmpty, iterator, keyBy, localCheckpoint, map, mapPartitions, mapPartitionsWithIndex, max, min, name, numericRDDToDoubleRDDFunctions, partitioner, partitions, persist, persist, pipe, pipe, pipe, preferredLocations, randomSplit, rddToAsyncRDDActions, rddToOrderedRDDFunctions, rddToPairRDDFunctions, rddToSequenceFileRDDFunctions, reduce, repartition, sample, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortBy, sparkContext, subtract, subtract, subtract, take, takeOrdered, takeSample, toDebugString, toJavaRDD, toLocalIterator, top, toString, treeAggregate, treeReduce, union, unpersist, zip, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail BaseRRDD public BaseRRDD(RDD<T> parent, int numPartitions, byte[] func, String deserializer, String serializer, byte[] packageNames, Broadcast<Object>[] broadcastVars, scala.reflect.ClassTag<T> evidence$1, scala.reflect.ClassTag<U> evidence$2) Method Detail getPartitions public Partition[] getPartitions() Description copied from class: RDD Implemented by subclasses to return the set of partitions in this RDD. This method will only be called once, so it is safe to implement a time-consuming computation in it. The partitions in this array must satisfy the following property: rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index } Returns:(undocumented) compute public scala.collection.Iterator<U> compute(Partition partition, TaskContext context) Description copied from class: RDD :: DeveloperApi :: Implemented by subclasses to compute a given partition. Specified by: compute in class RDD<U> Parameters:partition - (undocumented)context - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BaseRelation (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BaseRelation (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class BaseRelation Object org.apache.spark.sql.sources.BaseRelation public abstract class BaseRelation extends Object ::DeveloperApi:: Represents a collection of tuples with a known schema. Classes that extend BaseRelation must be able to produce the schema of their data in the form of a StructType. Concrete implementation should inherit from one of the descendant Scan classes, which define various abstract methods for execution. BaseRelations must also define an equality function that only returns true when the two instances will return the same data. This equality function is used when determining when it is safe to substitute cached results for a given relation. Since: 1.3.0 Constructor Summary Constructors  Constructor and Description BaseRelation()  Method Summary Methods  Modifier and Type Method and Description boolean needConversion() Whether does it need to convert the objects in Row to internal representation, for example: java.lang.String -> UTF8String java.lang.Decimal -> Decimal abstract StructType schema()  long sizeInBytes() Returns an estimated size of this relation in bytes. abstract SQLContext sqlContext()  Filter[] unhandledFilters(Filter[] filters) Returns the list of Filters that this datasource may not be able to handle. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BaseRelation public BaseRelation() Method Detail sqlContext public abstract SQLContext sqlContext() schema public abstract StructType schema() sizeInBytes public long sizeInBytes() Returns an estimated size of this relation in bytes. This information is used by the planner to decide when it is safe to broadcast a relation and can be overridden by sources that know the size ahead of time. By default, the system will assume that tables are too large to broadcast. This method will be called multiple times during query planning and thus should not perform expensive operations for each invocation. Note that it is always better to overestimate size than underestimate, because underestimation could lead to execution plans that are suboptimal (i.e. broadcasting a very large table). Returns:(undocumented)Since: 1.3.0 needConversion public boolean needConversion() Whether does it need to convert the objects in Row to internal representation, for example: java.lang.String -> UTF8String java.lang.Decimal -> Decimal If needConversion is false, buildScan() should return an RDD of InternalRow Note: The internal representation is not stable across releases and thus data sources outside of Spark SQL should leave this as true. Returns:(undocumented)Since: 1.4.0 unhandledFilters public Filter[] unhandledFilters(Filter[] filters) Returns the list of Filters that this datasource may not be able to handle. These returned Filters will be evaluated by Spark SQL after data is output by a scan. By default, this function will return all filters, as it is always safe to double evaluate a Filter. However, specific implementations can override this function to avoid double filtering when they are capable of processing a filter internally. Parameters:filters - (undocumented) Returns:(undocumented)Since: 1.6.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BatchInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BatchInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.scheduler Class BatchInfo Object org.apache.spark.streaming.scheduler.BatchInfo All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class BatchInfo extends Object implements scala.Product, scala.Serializable :: DeveloperApi :: Class having information on completed batches. param: batchTime Time of the batch param: streamIdToInputInfo A map of input stream id to its input info param: submissionTime Clock time of when jobs of this batch was submitted to the streaming scheduler queue param: processingStartTime Clock time of when the first job of this batch started processing param: processingEndTime Clock time of when the last job of this batch finished processing param: outputOperationInfos The output operations in this batch See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BatchInfo(Time batchTime, scala.collection.immutable.Map<Object,StreamInputInfo> streamIdToInputInfo, long submissionTime, scala.Option<Object> processingStartTime, scala.Option<Object> processingEndTime, scala.collection.immutable.Map<Object,OutputOperationInfo> outputOperationInfos)  Method Summary Methods  Modifier and Type Method and Description Time batchTime()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  long numRecords() The number of recorders received by the receivers in this batch. scala.collection.immutable.Map<Object,OutputOperationInfo> outputOperationInfos()  scala.Option<Object> processingDelay() Time taken for the all jobs of this batch to finish processing from the time they started processing. scala.Option<Object> processingEndTime()  scala.Option<Object> processingStartTime()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  scala.Option<Object> schedulingDelay() Time taken for the first job of this batch to start processing from the time this batch was submitted to the streaming scheduler. scala.collection.immutable.Map<Object,StreamInputInfo> streamIdToInputInfo()  long submissionTime()  scala.Option<Object> totalDelay() Time taken for all the jobs of this batch to finish processing from the time they were submitted. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BatchInfo public BatchInfo(Time batchTime, scala.collection.immutable.Map<Object,StreamInputInfo> streamIdToInputInfo, long submissionTime, scala.Option<Object> processingStartTime, scala.Option<Object> processingEndTime, scala.collection.immutable.Map<Object,OutputOperationInfo> outputOperationInfos) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() batchTime public Time batchTime() streamIdToInputInfo public scala.collection.immutable.Map<Object,StreamInputInfo> streamIdToInputInfo() submissionTime public long submissionTime() processingStartTime public scala.Option<Object> processingStartTime() processingEndTime public scala.Option<Object> processingEndTime() outputOperationInfos public scala.collection.immutable.Map<Object,OutputOperationInfo> outputOperationInfos() schedulingDelay public scala.Option<Object> schedulingDelay() Time taken for the first job of this batch to start processing from the time this batch was submitted to the streaming scheduler. Essentially, it is processingStartTime - submissionTime. Returns:(undocumented) processingDelay public scala.Option<Object> processingDelay() Time taken for the all jobs of this batch to finish processing from the time they started processing. Essentially, it is processingEndTime - processingStartTime. Returns:(undocumented) totalDelay public scala.Option<Object> totalDelay() Time taken for all the jobs of this batch to finish processing from the time they were submitted. Essentially, it is processingDelay + schedulingDelay. Returns:(undocumented) numRecords public long numRecords() The number of recorders received by the receivers in this batch. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BernoulliCellSampler (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BernoulliCellSampler (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.random Class BernoulliCellSampler<T> Object org.apache.spark.util.random.BernoulliCellSampler<T> All Implemented Interfaces: java.io.Serializable, Cloneable, Pseudorandom, RandomSampler<T,T> public class BernoulliCellSampler<T> extends Object implements RandomSampler<T,T> :: DeveloperApi :: A sampler based on Bernoulli trials for partitioning a data sequence. param: lb lower bound of the acceptance range param: ub upper bound of the acceptance range param: complement whether to use the complement of the range specified, default to false See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BernoulliCellSampler(double lb, double ub, boolean complement)  Method Summary Methods  Modifier and Type Method and Description BernoulliCellSampler<T> clone() return a copy of the RandomSampler object BernoulliCellSampler<T> cloneComplement() Return a sampler that is the complement of the range specified of the current sampler. int sample() Whether to sample the next item or not. void setSeed(long seed) Set random seed. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.util.random.RandomSampler sample Constructor Detail BernoulliCellSampler public BernoulliCellSampler(double lb, double ub, boolean complement) Method Detail setSeed public void setSeed(long seed) Description copied from interface: Pseudorandom Set random seed. Specified by: setSeed in interface Pseudorandom sample public int sample() Description copied from interface: RandomSampler Whether to sample the next item or not. Return how many times the next item will be sampled. Return 0 if it is not sampled. Specified by: sample in interface RandomSampler<T,T> Returns:(undocumented) cloneComplement public BernoulliCellSampler<T> cloneComplement() Return a sampler that is the complement of the range specified of the current sampler. Returns:(undocumented) clone public BernoulliCellSampler<T> clone() Description copied from interface: RandomSampler return a copy of the RandomSampler object Specified by: clone in interface RandomSampler<T,T> Overrides: clone in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BernoulliSampler (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BernoulliSampler (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.random Class BernoulliSampler<T> Object org.apache.spark.util.random.BernoulliSampler<T> All Implemented Interfaces: java.io.Serializable, Cloneable, Pseudorandom, RandomSampler<T,T> public class BernoulliSampler<T> extends Object implements RandomSampler<T,T> :: DeveloperApi :: A sampler based on Bernoulli trials. param: fraction the sampling fraction, aka Bernoulli sampling probability See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BernoulliSampler(double fraction, scala.reflect.ClassTag<T> evidence$1)  Method Summary Methods  Modifier and Type Method and Description BernoulliSampler<T> clone() return a copy of the RandomSampler object int sample() Whether to sample the next item or not. void setSeed(long seed) Set random seed. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.util.random.RandomSampler sample Constructor Detail BernoulliSampler public BernoulliSampler(double fraction, scala.reflect.ClassTag<T> evidence$1) Method Detail setSeed public void setSeed(long seed) Description copied from interface: Pseudorandom Set random seed. Specified by: setSeed in interface Pseudorandom sample public int sample() Description copied from interface: RandomSampler Whether to sample the next item or not. Return how many times the next item will be sampled. Return 0 if it is not sampled. Specified by: sample in interface RandomSampler<T,T> Returns:(undocumented) clone public BernoulliSampler<T> clone() Description copied from interface: RandomSampler return a copy of the RandomSampler object Specified by: clone in interface RandomSampler<T,T> Overrides: clone in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Binarizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Binarizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class Binarizer Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.feature.Binarizer All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public final class Binarizer extends Transformer implements DefaultParamsWritable Binarize a column of continuous features given a threshold. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Binarizer()  Binarizer(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  Binarizer copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  double getThreshold()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Binarizer load(String path)  static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  Binarizer setInputCol(String value)  Binarizer setOutputCol(String value)  Binarizer setThreshold(double value)  DoubleParam threshold() Param for threshold used to binarize continuous features. static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail Binarizer public Binarizer(String uid) Binarizer public Binarizer() Method Detail load public static Binarizer load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) threshold public DoubleParam threshold() Param for threshold used to binarize continuous features. The features greater than the threshold, will be binarized to 1.0. The features equal to or less than the threshold, will be binarized to 0.0. Default: 0.0 Returns:(undocumented) getThreshold public double getThreshold() setThreshold public Binarizer setThreshold(double value) setInputCol public Binarizer setInputCol(String value) setOutputCol public Binarizer setOutputCol(String value) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public Binarizer copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Transformer Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BinaryAttribute (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BinaryAttribute (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.attribute Class BinaryAttribute Object org.apache.spark.ml.attribute.Attribute org.apache.spark.ml.attribute.BinaryAttribute All Implemented Interfaces: java.io.Serializable public class BinaryAttribute extends Attribute :: DeveloperApi :: A binary attribute. param: name optional name param: index optional index param: values optional values. If set, its size must be 2. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description AttributeType attrType() Attribute type. static BinaryAttribute defaultAttr() The default binary attribute. boolean equals(Object other)  int hashCode()  scala.Option<Object> index() Index of the attribute. boolean isNominal() Tests whether this attribute is nominal, true for NominalAttribute and BinaryAttribute. boolean isNumeric() Tests whether this attribute is numeric, true for NumericAttribute and BinaryAttribute. scala.Option<String> name() Name of the attribute. static Metadata toMetadata()  static Metadata toMetadata(Metadata existingMetadata)  static String toString()  static StructField toStructField()  static StructField toStructField(Metadata existingMetadata)  scala.Option<String[]> values()  BinaryAttribute withIndex(int index) Copy with a new index. BinaryAttribute withName(String name) Copy with a new name. BinaryAttribute withoutIndex() Copy without the index. BinaryAttribute withoutName() Copy without the name. BinaryAttribute withoutValues() Copy without the values. BinaryAttribute withValues(String negative, String positive) Copy with new values. Methods inherited from class org.apache.spark.ml.attribute.Attribute toMetadata, toMetadata, toString, toStructField, toStructField Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Method Detail defaultAttr public static final BinaryAttribute defaultAttr() The default binary attribute. toMetadata public static Metadata toMetadata(Metadata existingMetadata) toMetadata public static Metadata toMetadata() toStructField public static StructField toStructField(Metadata existingMetadata) toStructField public static StructField toStructField() toString public static String toString() name public scala.Option<String> name() Description copied from class: Attribute Name of the attribute. None if it is not set. Specified by: name in class Attribute index public scala.Option<Object> index() Description copied from class: Attribute Index of the attribute. None if it is not set. Specified by: index in class Attribute values public scala.Option<String[]> values() attrType public AttributeType attrType() Description copied from class: Attribute Attribute type. Specified by: attrType in class Attribute isNumeric public boolean isNumeric() Description copied from class: Attribute Tests whether this attribute is numeric, true for NumericAttribute and BinaryAttribute. Specified by: isNumeric in class Attribute Returns:(undocumented) isNominal public boolean isNominal() Description copied from class: Attribute Tests whether this attribute is nominal, true for NominalAttribute and BinaryAttribute. Specified by: isNominal in class Attribute Returns:(undocumented) withName public BinaryAttribute withName(String name) Description copied from class: Attribute Copy with a new name. Specified by: withName in class Attribute withoutName public BinaryAttribute withoutName() Description copied from class: Attribute Copy without the name. Specified by: withoutName in class Attribute withIndex public BinaryAttribute withIndex(int index) Description copied from class: Attribute Copy with a new index. Specified by: withIndex in class Attribute withoutIndex public BinaryAttribute withoutIndex() Description copied from class: Attribute Copy without the index. Specified by: withoutIndex in class Attribute withValues public BinaryAttribute withValues(String negative, String positive) Copy with new values. Parameters:negative - name for negativepositive - name for positive Returns:(undocumented) withoutValues public BinaryAttribute withoutValues() Copy without the values. equals public boolean equals(Object other) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BinaryClassificationEvaluator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BinaryClassificationEvaluator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.evaluation Class BinaryClassificationEvaluator Object org.apache.spark.ml.evaluation.Evaluator org.apache.spark.ml.evaluation.BinaryClassificationEvaluator All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class BinaryClassificationEvaluator extends Evaluator implements DefaultParamsWritable :: Experimental :: Evaluator for binary classification, which expects two input columns: rawPrediction and label. The rawPrediction column can be of type double (binary 0/1 prediction, or probability of label 1) or of type vector (length-2 vector of raw predictions, scores, or label probabilities). See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BinaryClassificationEvaluator()  BinaryClassificationEvaluator(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  BinaryClassificationEvaluator copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. double evaluate(Dataset<?> dataset) Evaluates model output and returns a scalar metric. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getLabelCol()  String getMetricName()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getRawPredictionCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean isDefined(Param<?> param)  boolean isLargerBetter() Indicates whether the metric returned by evaluate should be maximized (true, default) or minimized (false). static boolean isSet(Param<?> param)  static Param<String> labelCol()  static BinaryClassificationEvaluator load(String path)  Param<String> metricName() param for metric name in evaluation (supports "areaUnderROC" (default), "areaUnderPR") static Param<?>[] params()  static Param<String> rawPredictionCol()  static void save(String path)  static <T> Params set(Param<T> param, T value)  BinaryClassificationEvaluator setLabelCol(String value)  BinaryClassificationEvaluator setMetricName(String value)  BinaryClassificationEvaluator setRawPredictionCol(String value)  static String toString()  String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.evaluation.Evaluator evaluate Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail BinaryClassificationEvaluator public BinaryClassificationEvaluator(String uid) BinaryClassificationEvaluator public BinaryClassificationEvaluator() Method Detail load public static BinaryClassificationEvaluator load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() rawPredictionCol public static final Param<String> rawPredictionCol() getRawPredictionCol public static final String getRawPredictionCol() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) metricName public Param<String> metricName() param for metric name in evaluation (supports "areaUnderROC" (default), "areaUnderPR") Returns:(undocumented) getMetricName public String getMetricName() setMetricName public BinaryClassificationEvaluator setMetricName(String value) setRawPredictionCol public BinaryClassificationEvaluator setRawPredictionCol(String value) setLabelCol public BinaryClassificationEvaluator setLabelCol(String value) evaluate public double evaluate(Dataset<?> dataset) Description copied from class: Evaluator Evaluates model output and returns a scalar metric. The value of isLargerBetter specifies whether larger values are better. Specified by: evaluate in class Evaluator Parameters:dataset - a dataset that contains labels/observations and predictions. Returns:metric isLargerBetter public boolean isLargerBetter() Description copied from class: Evaluator Indicates whether the metric returned by evaluate should be maximized (true, default) or minimized (false). A given evaluator may support multiple metrics which may be maximized or minimized. Overrides: isLargerBetter in class Evaluator Returns:(undocumented) copy public BinaryClassificationEvaluator copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Evaluator Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BinaryClassificationMetrics (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BinaryClassificationMetrics (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.evaluation Class BinaryClassificationMetrics Object org.apache.spark.mllib.evaluation.BinaryClassificationMetrics public class BinaryClassificationMetrics extends Object Evaluator for binary classification. param: scoreAndLabels an RDD of (score, label) pairs. param: numBins if greater than 0, then the curves (ROC curve, PR curve) computed internally will be down-sampled to this many "bins". If 0, no down-sampling will occur. This is useful because the curve contains a point for each distinct score in the input, and this could be as large as the input itself -- millions of points or more, when thousands may be entirely sufficient to summarize the curve. After down-sampling, the curves will instead be made of approximately numBins points instead. Points are made from bins of equal numbers of consecutive points. The size of each bin is floor(scoreAndLabels.count() / numBins), which means the resulting number of bins may not exactly equal numBins. The last bin in each partition may be smaller as a result, meaning there may be an extra sample at partition boundaries. Constructor Summary Constructors  Constructor and Description BinaryClassificationMetrics(RDD<scala.Tuple2<Object,Object>> scoreAndLabels) Defaults numBins to 0. BinaryClassificationMetrics(RDD<scala.Tuple2<Object,Object>> scoreAndLabels, int numBins)  Method Summary Methods  Modifier and Type Method and Description double areaUnderPR() Computes the area under the precision-recall curve. double areaUnderROC() Computes the area under the receiver operating characteristic (ROC) curve. RDD<scala.Tuple2<Object,Object>> fMeasureByThreshold() Returns the (threshold, F-Measure) curve with beta = 1.0. RDD<scala.Tuple2<Object,Object>> fMeasureByThreshold(double beta) Returns the (threshold, F-Measure) curve. int numBins()  RDD<scala.Tuple2<Object,Object>> pr() Returns the precision-recall curve, which is an RDD of (recall, precision), NOT (precision, recall), with (0.0, 1.0) prepended to it. RDD<scala.Tuple2<Object,Object>> precisionByThreshold() Returns the (threshold, precision) curve. RDD<scala.Tuple2<Object,Object>> recallByThreshold() Returns the (threshold, recall) curve. RDD<scala.Tuple2<Object,Object>> roc() Returns the receiver operating characteristic (ROC) curve, which is an RDD of (false positive rate, true positive rate) with (0.0, 0.0) prepended and (1.0, 1.0) appended to it. RDD<scala.Tuple2<Object,Object>> scoreAndLabels()  RDD<Object> thresholds() Returns thresholds in descending order. void unpersist() Unpersist intermediate RDDs used in the computation. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BinaryClassificationMetrics public BinaryClassificationMetrics(RDD<scala.Tuple2<Object,Object>> scoreAndLabels, int numBins) BinaryClassificationMetrics public BinaryClassificationMetrics(RDD<scala.Tuple2<Object,Object>> scoreAndLabels) Defaults numBins to 0. Parameters:scoreAndLabels - (undocumented) Method Detail scoreAndLabels public RDD<scala.Tuple2<Object,Object>> scoreAndLabels() numBins public int numBins() unpersist public void unpersist() Unpersist intermediate RDDs used in the computation. thresholds public RDD<Object> thresholds() Returns thresholds in descending order. Returns:(undocumented) roc public RDD<scala.Tuple2<Object,Object>> roc() Returns the receiver operating characteristic (ROC) curve, which is an RDD of (false positive rate, true positive rate) with (0.0, 0.0) prepended and (1.0, 1.0) appended to it. Returns:(undocumented)See Also:http://en.wikipedia.org/wiki/Receiver_operating_characteristic areaUnderROC public double areaUnderROC() Computes the area under the receiver operating characteristic (ROC) curve. Returns:(undocumented) pr public RDD<scala.Tuple2<Object,Object>> pr() Returns the precision-recall curve, which is an RDD of (recall, precision), NOT (precision, recall), with (0.0, 1.0) prepended to it. Returns:(undocumented)See Also:http://en.wikipedia.org/wiki/Precision_and_recall areaUnderPR public double areaUnderPR() Computes the area under the precision-recall curve. Returns:(undocumented) fMeasureByThreshold public RDD<scala.Tuple2<Object,Object>> fMeasureByThreshold(double beta) Returns the (threshold, F-Measure) curve. Parameters:beta - the beta factor in F-Measure computation. Returns:an RDD of (threshold, F-Measure) pairs.See Also:http://en.wikipedia.org/wiki/F1_score fMeasureByThreshold public RDD<scala.Tuple2<Object,Object>> fMeasureByThreshold() Returns the (threshold, F-Measure) curve with beta = 1.0. Returns:(undocumented) precisionByThreshold public RDD<scala.Tuple2<Object,Object>> precisionByThreshold() Returns the (threshold, precision) curve. Returns:(undocumented) recallByThreshold public RDD<scala.Tuple2<Object,Object>> recallByThreshold() Returns the (threshold, recall) curve. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BinaryLogisticRegressionSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BinaryLogisticRegressionSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class BinaryLogisticRegressionSummary Object org.apache.spark.ml.classification.BinaryLogisticRegressionSummary All Implemented Interfaces: java.io.Serializable, LogisticRegressionSummary Direct Known Subclasses: BinaryLogisticRegressionTrainingSummary public class BinaryLogisticRegressionSummary extends Object implements LogisticRegressionSummary :: Experimental :: Binary Logistic regression results for a given model. param: predictions dataframe output by the model's transform method. param: probabilityCol field in "predictions" which gives the probability of each class as a vector. param: labelCol field in "predictions" which gives the true label of each instance. param: featuresCol field in "predictions" which gives the features of each instance as a vector. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description double areaUnderROC() Computes the area under the receiver operating characteristic (ROC) curve. String featuresCol() Field in "predictions" which gives the features of each instance as a vector. Dataset<Row> fMeasureByThreshold() Returns a dataframe with two fields (threshold, F-Measure) curve with beta = 1.0. String labelCol() Field in "predictions" which gives the true label of each instance (if available). Dataset<Row> pr() Returns the precision-recall curve, which is a Dataframe containing two fields recall, precision with (0.0, 1.0) prepended to it. Dataset<Row> precisionByThreshold() Returns a dataframe with two fields (threshold, precision) curve. Dataset<Row> predictions() Dataframe output by the model's `transform` method. String probabilityCol() Field in "predictions" which gives the probability of each class as a vector. Dataset<Row> recallByThreshold() Returns a dataframe with two fields (threshold, recall) curve. Dataset<Row> roc() Returns the receiver operating characteristic (ROC) curve, which is a Dataframe having two fields (FPR, TPR) with (0.0, 0.0) prepended and (1.0, 1.0) appended to it. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail predictions public Dataset<Row> predictions() Description copied from interface: LogisticRegressionSummary Dataframe output by the model's `transform` method. Specified by: predictions in interface LogisticRegressionSummary probabilityCol public String probabilityCol() Description copied from interface: LogisticRegressionSummary Field in "predictions" which gives the probability of each class as a vector. Specified by: probabilityCol in interface LogisticRegressionSummary labelCol public String labelCol() Description copied from interface: LogisticRegressionSummary Field in "predictions" which gives the true label of each instance (if available). Specified by: labelCol in interface LogisticRegressionSummary featuresCol public String featuresCol() Description copied from interface: LogisticRegressionSummary Field in "predictions" which gives the features of each instance as a vector. Specified by: featuresCol in interface LogisticRegressionSummary roc public Dataset<Row> roc() Returns the receiver operating characteristic (ROC) curve, which is a Dataframe having two fields (FPR, TPR) with (0.0, 0.0) prepended and (1.0, 1.0) appended to it. See http://en.wikipedia.org/wiki/Receiver_operating_characteristic Note: This ignores instance weights (setting all to 1.0) from LogisticRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) areaUnderROC public double areaUnderROC() Computes the area under the receiver operating characteristic (ROC) curve. Note: This ignores instance weights (setting all to 1.0) from LogisticRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) pr public Dataset<Row> pr() Returns the precision-recall curve, which is a Dataframe containing two fields recall, precision with (0.0, 1.0) prepended to it. Note: This ignores instance weights (setting all to 1.0) from LogisticRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) fMeasureByThreshold public Dataset<Row> fMeasureByThreshold() Returns a dataframe with two fields (threshold, F-Measure) curve with beta = 1.0. Note: This ignores instance weights (setting all to 1.0) from LogisticRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) precisionByThreshold public Dataset<Row> precisionByThreshold() Returns a dataframe with two fields (threshold, precision) curve. Every possible probability obtained in transforming the dataset are used as thresholds used in calculating the precision. Note: This ignores instance weights (setting all to 1.0) from LogisticRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) recallByThreshold public Dataset<Row> recallByThreshold() Returns a dataframe with two fields (threshold, recall) curve. Every possible probability obtained in transforming the dataset are used as thresholds used in calculating the recall. Note: This ignores instance weights (setting all to 1.0) from LogisticRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BinaryLogisticRegressionTrainingSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BinaryLogisticRegressionTrainingSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class BinaryLogisticRegressionTrainingSummary Object org.apache.spark.ml.classification.BinaryLogisticRegressionSummary org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary All Implemented Interfaces: java.io.Serializable, LogisticRegressionSummary, LogisticRegressionTrainingSummary public class BinaryLogisticRegressionTrainingSummary extends BinaryLogisticRegressionSummary implements LogisticRegressionTrainingSummary :: Experimental :: Logistic regression training results. param: predictions dataframe output by the model's transform method. param: probabilityCol field in "predictions" which gives the probability of each class as a vector. param: labelCol field in "predictions" which gives the true label of each instance. param: featuresCol field in "predictions" which gives the features of each instance as a vector. param: objectiveHistory objective function (scaled loss + regularization) at each iteration. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description double[] objectiveHistory() objective function (scaled loss + regularization) at each iteration. Methods inherited from class org.apache.spark.ml.classification.BinaryLogisticRegressionSummary areaUnderROC, featuresCol, fMeasureByThreshold, labelCol, pr, precisionByThreshold, predictions, probabilityCol, recallByThreshold, roc Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.classification.LogisticRegressionTrainingSummary totalIterations Methods inherited from interface org.apache.spark.ml.classification.LogisticRegressionSummary featuresCol, labelCol, predictions, probabilityCol Method Detail objectiveHistory public double[] objectiveHistory() Description copied from interface: LogisticRegressionTrainingSummary objective function (scaled loss + regularization) at each iteration. Specified by: objectiveHistory in interface LogisticRegressionTrainingSummary Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BinarySample (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BinarySample (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.test Class BinarySample Object org.apache.spark.mllib.stat.test.BinarySample All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class BinarySample extends Object implements scala.Product, scala.Serializable Class that represents the group and value of a sample. param: isExperiment if the sample is of the experiment group. param: value numeric value of the observation. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BinarySample(boolean isExperiment, double value)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  boolean isExperiment()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  String toString()  double value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BinarySample public BinarySample(boolean isExperiment, double value) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() isExperiment public boolean isExperiment() value public double value() toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BinaryType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BinaryType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class BinaryType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.BinaryType public class BinaryType extends DataType :: DeveloperApi :: The data type representing Array[Byte] values. Please use the singleton DataTypes.BinaryType. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of the BinaryType is 100 bytes. static String json()  static String prettyJson()  static String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, simpleString, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() simpleString public static String simpleString() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() The default size of a value of the BinaryType is 100 bytes. Specified by: defaultSize in class DataType Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BinomialBounds (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BinomialBounds (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.random Class BinomialBounds Object org.apache.spark.util.random.BinomialBounds public class BinomialBounds extends Object Utility functions that help us determine bounds on adjusted sampling rate to guarantee exact sample size with high confidence when sampling without replacement. Constructor Summary Constructors  Constructor and Description BinomialBounds()  Method Summary Methods  Modifier and Type Method and Description static double getLowerBound(double delta, long n, double fraction) Returns a threshold p such that if we conduct n Bernoulli trials with success rate = p, it is very unlikely to have more than fraction * n successes. static double getUpperBound(double delta, long n, double fraction) Returns a threshold p such that if we conduct n Bernoulli trials with success rate = p, it is very unlikely to have less than fraction * n successes. static double minSamplingRate()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BinomialBounds public BinomialBounds() Method Detail minSamplingRate public static double minSamplingRate() getLowerBound public static double getLowerBound(double delta, long n, double fraction) Returns a threshold p such that if we conduct n Bernoulli trials with success rate = p, it is very unlikely to have more than fraction * n successes. Parameters:delta - (undocumented)n - (undocumented)fraction - (undocumented) Returns:(undocumented) getUpperBound public static double getUpperBound(double delta, long n, double fraction) Returns a threshold p such that if we conduct n Bernoulli trials with success rate = p, it is very unlikely to have less than fraction * n successes. Parameters:delta - (undocumented)n - (undocumented)fraction - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BisectingKMeans (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BisectingKMeans (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class BisectingKMeans Object org.apache.spark.mllib.clustering.BisectingKMeans public class BisectingKMeans extends Object A bisecting k-means algorithm based on the paper "A comparison of document clustering techniques" by Steinbach, Karypis, and Kumar, with modification to fit Spark. The algorithm starts from a single cluster that contains all points. Iteratively it finds divisible clusters on the bottom level and bisects each of them using k-means, until there are k leaf clusters in total or no leaf clusters are divisible. The bisecting steps of clusters on the same level are grouped together to increase parallelism. If bisecting all divisible clusters on the bottom level would result more than k leaf clusters, larger clusters get higher priority. param: k the desired number of leaf clusters (default: 4). The actual number could be smaller if there are no divisible leaf clusters. param: maxIterations the max number of k-means iterations to split clusters (default: 20) param: minDivisibleClusterSize the minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster (default: 1) param: seed a random seed (default: hash value of the class name) See Also:http://glaros.dtc.umn.edu/gkhome/fetch/papers/docclusterKDDTMW00.pdf Steinbach, Karypis, and Kumar, A comparison of document clustering techniques, KDD Workshop on Text Mining, 2000.} Constructor Summary Constructors  Constructor and Description BisectingKMeans() Constructs with the default configuration Method Summary Methods  Modifier and Type Method and Description int getK() Gets the desired number of leaf clusters. int getMaxIterations() Gets the max number of k-means iterations to split clusters. double getMinDivisibleClusterSize() Gets the minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster. long getSeed() Gets the random seed. BisectingKMeansModel run(JavaRDD<Vector> data) Java-friendly version of run(). BisectingKMeansModel run(RDD<Vector> input) Runs the bisecting k-means algorithm. BisectingKMeans setK(int k) Sets the desired number of leaf clusters (default: 4). BisectingKMeans setMaxIterations(int maxIterations) Sets the max number of k-means iterations to split clusters (default: 20). BisectingKMeans setMinDivisibleClusterSize(double minDivisibleClusterSize) Sets the minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster (default: 1). BisectingKMeans setSeed(long seed) Sets the random seed (default: hash value of the class name). Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BisectingKMeans public BisectingKMeans() Constructs with the default configuration Method Detail setK public BisectingKMeans setK(int k) Sets the desired number of leaf clusters (default: 4). The actual number could be smaller if there are no divisible leaf clusters. Parameters:k - (undocumented) Returns:(undocumented) getK public int getK() Gets the desired number of leaf clusters. Returns:(undocumented) setMaxIterations public BisectingKMeans setMaxIterations(int maxIterations) Sets the max number of k-means iterations to split clusters (default: 20). Parameters:maxIterations - (undocumented) Returns:(undocumented) getMaxIterations public int getMaxIterations() Gets the max number of k-means iterations to split clusters. Returns:(undocumented) setMinDivisibleClusterSize public BisectingKMeans setMinDivisibleClusterSize(double minDivisibleClusterSize) Sets the minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster (default: 1). Parameters:minDivisibleClusterSize - (undocumented) Returns:(undocumented) getMinDivisibleClusterSize public double getMinDivisibleClusterSize() Gets the minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster. Returns:(undocumented) setSeed public BisectingKMeans setSeed(long seed) Sets the random seed (default: hash value of the class name). Parameters:seed - (undocumented) Returns:(undocumented) getSeed public long getSeed() Gets the random seed. Returns:(undocumented) run public BisectingKMeansModel run(RDD<Vector> input) Runs the bisecting k-means algorithm. Parameters:input - RDD of vectors Returns:model for the bisecting kmeans run public BisectingKMeansModel run(JavaRDD<Vector> data) Java-friendly version of run(). Parameters:data - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BisectingKMeansModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BisectingKMeansModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class BisectingKMeansModel.SaveLoadV1_0$ Object org.apache.spark.mllib.clustering.BisectingKMeansModel.SaveLoadV1_0$ Enclosing class: BisectingKMeansModel public static class BisectingKMeansModel.SaveLoadV1_0$ extends Object Field Summary Fields  Modifier and Type Field and Description static BisectingKMeansModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BisectingKMeansModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description BisectingKMeansModel load(SparkContext sc, String path, int rootId)  void save(SparkContext sc, BisectingKMeansModel model, String path)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final BisectingKMeansModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BisectingKMeansModel.SaveLoadV1_0$ public BisectingKMeansModel.SaveLoadV1_0$() Method Detail save public void save(SparkContext sc, BisectingKMeansModel model, String path) load public BisectingKMeansModel load(SparkContext sc, String path, int rootId) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BisectingKMeansModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BisectingKMeansModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class BisectingKMeansModel Object org.apache.spark.mllib.clustering.BisectingKMeansModel All Implemented Interfaces: java.io.Serializable, Saveable public class BisectingKMeansModel extends Object implements scala.Serializable, Saveable Clustering model produced by BisectingKMeans. The prediction is done level-by-level from the root node to a leaf node, and at each node among its children the closest to the input point is selected. param: root the root node of the clustering tree See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  BisectingKMeansModel.SaveLoadV1_0$  Method Summary Methods  Modifier and Type Method and Description Vector[] clusterCenters() Leaf cluster centers. double computeCost(JavaRDD<Vector> data) Java-friendly version of computeCost(). double computeCost(RDD<Vector> data) Computes the sum of squared distances between the input points and their corresponding cluster centers. double computeCost(Vector point) Computes the squared distance between the input point and the cluster center it belongs to. int k() Number of leaf clusters. static BisectingKMeansModel load(SparkContext sc, String path)  JavaRDD<Integer> predict(JavaRDD<Vector> points) Java-friendly version of predict(). RDD<Object> predict(RDD<Vector> points) Predicts the indices of the clusters that the input points belong to. int predict(Vector point) Predicts the index of the cluster that the input point belongs to. void save(SparkContext sc, String path) Save this model to the given path. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail load public static BisectingKMeansModel load(SparkContext sc, String path) clusterCenters public Vector[] clusterCenters() Leaf cluster centers. Returns:(undocumented) k public int k() Number of leaf clusters. Returns:(undocumented) predict public int predict(Vector point) Predicts the index of the cluster that the input point belongs to. Parameters:point - (undocumented) Returns:(undocumented) predict public RDD<Object> predict(RDD<Vector> points) Predicts the indices of the clusters that the input points belong to. Parameters:points - (undocumented) Returns:(undocumented) predict public JavaRDD<Integer> predict(JavaRDD<Vector> points) Java-friendly version of predict(). Parameters:points - (undocumented) Returns:(undocumented) computeCost public double computeCost(Vector point) Computes the squared distance between the input point and the cluster center it belongs to. Parameters:point - (undocumented) Returns:(undocumented) computeCost public double computeCost(RDD<Vector> data) Computes the sum of squared distances between the input points and their corresponding cluster centers. Parameters:data - (undocumented) Returns:(undocumented) computeCost public double computeCost(JavaRDD<Vector> data) Java-friendly version of computeCost(). Parameters:data - (undocumented) Returns:(undocumented) save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockId (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockId (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockId Object org.apache.spark.storage.BlockId Direct Known Subclasses: BroadcastBlockId, RDDBlockId, ShuffleBlockId, ShuffleDataBlockId, ShuffleIndexBlockId, StreamBlockId, TaskResultBlockId public abstract class BlockId extends Object :: DeveloperApi :: Identifies a particular Block of data, usually associated with a single file. A Block can be uniquely identified by its filename, but each type of Block has a different set of keys which produce its unique name. If your BlockId should be serializable, be sure to add it to the BlockId.apply() method. Constructor Summary Constructors  Constructor and Description BlockId()  Method Summary Methods  Modifier and Type Method and Description static BlockId apply(String id) Converts a BlockId "name" String back into a BlockId. scala.Option<RDDBlockId> asRDDId()  static scala.util.matching.Regex BROADCAST()  boolean equals(Object other)  int hashCode()  boolean isBroadcast()  boolean isRDD()  boolean isShuffle()  abstract String name() A globally unique identifier for this Block. static scala.util.matching.Regex RDD()  static scala.util.matching.Regex SHUFFLE_DATA()  static scala.util.matching.Regex SHUFFLE_INDEX()  static scala.util.matching.Regex SHUFFLE()  static scala.util.matching.Regex STREAM()  static scala.util.matching.Regex TASKRESULT()  static scala.util.matching.Regex TEST()  String toString()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail BlockId public BlockId() Method Detail RDD public static scala.util.matching.Regex RDD() SHUFFLE public static scala.util.matching.Regex SHUFFLE() SHUFFLE_DATA public static scala.util.matching.Regex SHUFFLE_DATA() SHUFFLE_INDEX public static scala.util.matching.Regex SHUFFLE_INDEX() BROADCAST public static scala.util.matching.Regex BROADCAST() TASKRESULT public static scala.util.matching.Regex TASKRESULT() STREAM public static scala.util.matching.Regex STREAM() TEST public static scala.util.matching.Regex TEST() apply public static BlockId apply(String id) Converts a BlockId "name" String back into a BlockId. name public abstract String name() A globally unique identifier for this Block. Can be used for ser/de. asRDDId public scala.Option<RDDBlockId> asRDDId() isRDD public boolean isRDD() isShuffle public boolean isShuffle() isBroadcast public boolean isBroadcast() toString public String toString() Overrides: toString in class Object hashCode public int hashCode() Overrides: hashCode in class Object equals public boolean equals(Object other) Overrides: equals in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerId (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerId (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerId Object org.apache.spark.storage.BlockManagerId All Implemented Interfaces: java.io.Externalizable, java.io.Serializable public class BlockManagerId extends Object implements java.io.Externalizable :: DeveloperApi :: This class represent an unique identifier for a BlockManager. The first 2 constructors of this class is made private to ensure that BlockManagerId objects can be created only using the apply method in the companion object. This allows de-duplication of ID objects. Also, constructor parameters are private to ensure that parameters cannot be modified from outside this class. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static BlockManagerId apply(java.io.ObjectInput in)  static BlockManagerId apply(String execId, String host, int port) Returns a BlockManagerId for the given configuration. static java.util.concurrent.ConcurrentHashMap<BlockManagerId,BlockManagerId> blockManagerIdCache()  boolean equals(Object that)  String executorId()  static BlockManagerId getCachedBlockManagerId(BlockManagerId id)  int hashCode()  String host()  String hostPort()  boolean isDriver()  int port()  void readExternal(java.io.ObjectInput in)  String toString()  void writeExternal(java.io.ObjectOutput out)  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Method Detail apply public static BlockManagerId apply(String execId, String host, int port) Returns a BlockManagerId for the given configuration. Parameters:execId - ID of the executor.host - Host name of the block manager.port - Port of the block manager. Returns:A new BlockManagerId. apply public static BlockManagerId apply(java.io.ObjectInput in) blockManagerIdCache public static java.util.concurrent.ConcurrentHashMap<BlockManagerId,BlockManagerId> blockManagerIdCache() getCachedBlockManagerId public static BlockManagerId getCachedBlockManagerId(BlockManagerId id) executorId public String executorId() hostPort public String hostPort() host public String host() port public int port() isDriver public boolean isDriver() writeExternal public void writeExternal(java.io.ObjectOutput out) Specified by: writeExternal in interface java.io.Externalizable readExternal public void readExternal(java.io.ObjectInput in) Specified by: readExternal in interface java.io.Externalizable toString public String toString() Overrides: toString in class Object hashCode public int hashCode() Overrides: hashCode in class Object equals public boolean equals(Object that) Overrides: equals in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.BlockManagerHeartbeat$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.BlockManagerHeartbeat$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.BlockManagerHeartbeat$ Object scala.runtime.AbstractFunction1<BlockManagerId,BlockManagerMessages.BlockManagerHeartbeat> org.apache.spark.storage.BlockManagerMessages.BlockManagerHeartbeat$ All Implemented Interfaces: java.io.Serializable, scala.Function1<BlockManagerId,BlockManagerMessages.BlockManagerHeartbeat> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.BlockManagerHeartbeat$ extends scala.runtime.AbstractFunction1<BlockManagerId,BlockManagerMessages.BlockManagerHeartbeat> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.BlockManagerHeartbeat$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.BlockManagerHeartbeat$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.BlockManagerHeartbeat$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.BlockManagerHeartbeat$ public BlockManagerMessages.BlockManagerHeartbeat$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.BlockManagerHeartbeat (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.BlockManagerHeartbeat (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.BlockManagerHeartbeat Object org.apache.spark.storage.BlockManagerMessages.BlockManagerHeartbeat All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.BlockManagerHeartbeat extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.BlockManagerHeartbeat(BlockManagerId blockManagerId)  Method Summary Methods  Modifier and Type Method and Description BlockManagerId blockManagerId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.BlockManagerHeartbeat public BlockManagerMessages.BlockManagerHeartbeat(BlockManagerId blockManagerId) Method Detail blockManagerId public BlockManagerId blockManagerId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetBlockStatus$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetBlockStatus$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetBlockStatus$ Object scala.runtime.AbstractFunction2<BlockId,Object,BlockManagerMessages.GetBlockStatus> org.apache.spark.storage.BlockManagerMessages.GetBlockStatus$ All Implemented Interfaces: java.io.Serializable, scala.Function2<BlockId,Object,BlockManagerMessages.GetBlockStatus> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetBlockStatus$ extends scala.runtime.AbstractFunction2<BlockId,Object,BlockManagerMessages.GetBlockStatus> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.GetBlockStatus$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetBlockStatus$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction2 apply$mcDDD$sp, apply$mcDDI$sp, apply$mcDDJ$sp, apply$mcDID$sp, apply$mcDII$sp, apply$mcDIJ$sp, apply$mcDJD$sp, apply$mcDJI$sp, apply$mcDJJ$sp, apply$mcFDD$sp, apply$mcFDI$sp, apply$mcFDJ$sp, apply$mcFID$sp, apply$mcFII$sp, apply$mcFIJ$sp, apply$mcFJD$sp, apply$mcFJI$sp, apply$mcFJJ$sp, apply$mcIDD$sp, apply$mcIDI$sp, apply$mcIDJ$sp, apply$mcIID$sp, apply$mcIII$sp, apply$mcIIJ$sp, apply$mcIJD$sp, apply$mcIJI$sp, apply$mcIJJ$sp, apply$mcJDD$sp, apply$mcJDI$sp, apply$mcJDJ$sp, apply$mcJID$sp, apply$mcJII$sp, apply$mcJIJ$sp, apply$mcJJD$sp, apply$mcJJI$sp, apply$mcJJJ$sp, apply$mcVDD$sp, apply$mcVDI$sp, apply$mcVDJ$sp, apply$mcVID$sp, apply$mcVII$sp, apply$mcVIJ$sp, apply$mcVJD$sp, apply$mcVJI$sp, apply$mcVJJ$sp, apply$mcZDD$sp, apply$mcZDI$sp, apply$mcZDJ$sp, apply$mcZID$sp, apply$mcZII$sp, apply$mcZIJ$sp, apply$mcZJD$sp, apply$mcZJI$sp, apply$mcZJJ$sp, curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function2 apply Field Detail MODULE$ public static final BlockManagerMessages.GetBlockStatus$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.GetBlockStatus$ public BlockManagerMessages.GetBlockStatus$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetBlockStatus (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetBlockStatus (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetBlockStatus Object org.apache.spark.storage.BlockManagerMessages.GetBlockStatus All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetBlockStatus extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetBlockStatus(BlockId blockId, boolean askSlaves)  Method Summary Methods  Modifier and Type Method and Description boolean askSlaves()  BlockId blockId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.GetBlockStatus public BlockManagerMessages.GetBlockStatus(BlockId blockId, boolean askSlaves) Method Detail blockId public BlockId blockId() askSlaves public boolean askSlaves() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetExecutorEndpointRef$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetExecutorEndpointRef$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetExecutorEndpointRef$ Object scala.runtime.AbstractFunction1<String,BlockManagerMessages.GetExecutorEndpointRef> org.apache.spark.storage.BlockManagerMessages.GetExecutorEndpointRef$ All Implemented Interfaces: java.io.Serializable, scala.Function1<String,BlockManagerMessages.GetExecutorEndpointRef> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetExecutorEndpointRef$ extends scala.runtime.AbstractFunction1<String,BlockManagerMessages.GetExecutorEndpointRef> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.GetExecutorEndpointRef$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetExecutorEndpointRef$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.GetExecutorEndpointRef$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.GetExecutorEndpointRef$ public BlockManagerMessages.GetExecutorEndpointRef$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetExecutorEndpointRef (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetExecutorEndpointRef (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetExecutorEndpointRef Object org.apache.spark.storage.BlockManagerMessages.GetExecutorEndpointRef All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetExecutorEndpointRef extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetExecutorEndpointRef(String executorId)  Method Summary Methods  Modifier and Type Method and Description String executorId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.GetExecutorEndpointRef public BlockManagerMessages.GetExecutorEndpointRef(String executorId) Method Detail executorId public String executorId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetLocations$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetLocations$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetLocations$ Object scala.runtime.AbstractFunction1<BlockId,BlockManagerMessages.GetLocations> org.apache.spark.storage.BlockManagerMessages.GetLocations$ All Implemented Interfaces: java.io.Serializable, scala.Function1<BlockId,BlockManagerMessages.GetLocations> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetLocations$ extends scala.runtime.AbstractFunction1<BlockId,BlockManagerMessages.GetLocations> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.GetLocations$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetLocations$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.GetLocations$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.GetLocations$ public BlockManagerMessages.GetLocations$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetLocations (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetLocations (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetLocations Object org.apache.spark.storage.BlockManagerMessages.GetLocations All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetLocations extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetLocations(BlockId blockId)  Method Summary Methods  Modifier and Type Method and Description BlockId blockId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.GetLocations public BlockManagerMessages.GetLocations(BlockId blockId) Method Detail blockId public BlockId blockId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetLocationsMultipleBlockIds$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetLocationsMultipleBlockIds$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetLocationsMultipleBlockIds$ Object scala.runtime.AbstractFunction1<BlockId[],BlockManagerMessages.GetLocationsMultipleBlockIds> org.apache.spark.storage.BlockManagerMessages.GetLocationsMultipleBlockIds$ All Implemented Interfaces: java.io.Serializable, scala.Function1<BlockId[],BlockManagerMessages.GetLocationsMultipleBlockIds> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetLocationsMultipleBlockIds$ extends scala.runtime.AbstractFunction1<BlockId[],BlockManagerMessages.GetLocationsMultipleBlockIds> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.GetLocationsMultipleBlockIds$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetLocationsMultipleBlockIds$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.GetLocationsMultipleBlockIds$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.GetLocationsMultipleBlockIds$ public BlockManagerMessages.GetLocationsMultipleBlockIds$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetLocationsMultipleBlockIds (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetLocationsMultipleBlockIds (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetLocationsMultipleBlockIds Object org.apache.spark.storage.BlockManagerMessages.GetLocationsMultipleBlockIds All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetLocationsMultipleBlockIds extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetLocationsMultipleBlockIds(BlockId[] blockIds)  Method Summary Methods  Modifier and Type Method and Description BlockId[] blockIds()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.GetLocationsMultipleBlockIds public BlockManagerMessages.GetLocationsMultipleBlockIds(BlockId[] blockIds) Method Detail blockIds public BlockId[] blockIds() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetMatchingBlockIds$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetMatchingBlockIds$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetMatchingBlockIds$ Object scala.runtime.AbstractFunction2<scala.Function1<BlockId,Object>,Object,BlockManagerMessages.GetMatchingBlockIds> org.apache.spark.storage.BlockManagerMessages.GetMatchingBlockIds$ All Implemented Interfaces: java.io.Serializable, scala.Function2<scala.Function1<BlockId,Object>,Object,BlockManagerMessages.GetMatchingBlockIds> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetMatchingBlockIds$ extends scala.runtime.AbstractFunction2<scala.Function1<BlockId,Object>,Object,BlockManagerMessages.GetMatchingBlockIds> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.GetMatchingBlockIds$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetMatchingBlockIds$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction2 apply$mcDDD$sp, apply$mcDDI$sp, apply$mcDDJ$sp, apply$mcDID$sp, apply$mcDII$sp, apply$mcDIJ$sp, apply$mcDJD$sp, apply$mcDJI$sp, apply$mcDJJ$sp, apply$mcFDD$sp, apply$mcFDI$sp, apply$mcFDJ$sp, apply$mcFID$sp, apply$mcFII$sp, apply$mcFIJ$sp, apply$mcFJD$sp, apply$mcFJI$sp, apply$mcFJJ$sp, apply$mcIDD$sp, apply$mcIDI$sp, apply$mcIDJ$sp, apply$mcIID$sp, apply$mcIII$sp, apply$mcIIJ$sp, apply$mcIJD$sp, apply$mcIJI$sp, apply$mcIJJ$sp, apply$mcJDD$sp, apply$mcJDI$sp, apply$mcJDJ$sp, apply$mcJID$sp, apply$mcJII$sp, apply$mcJIJ$sp, apply$mcJJD$sp, apply$mcJJI$sp, apply$mcJJJ$sp, apply$mcVDD$sp, apply$mcVDI$sp, apply$mcVDJ$sp, apply$mcVID$sp, apply$mcVII$sp, apply$mcVIJ$sp, apply$mcVJD$sp, apply$mcVJI$sp, apply$mcVJJ$sp, apply$mcZDD$sp, apply$mcZDI$sp, apply$mcZDJ$sp, apply$mcZID$sp, apply$mcZII$sp, apply$mcZIJ$sp, apply$mcZJD$sp, apply$mcZJI$sp, apply$mcZJJ$sp, curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function2 apply Field Detail MODULE$ public static final BlockManagerMessages.GetMatchingBlockIds$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.GetMatchingBlockIds$ public BlockManagerMessages.GetMatchingBlockIds$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetMatchingBlockIds (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetMatchingBlockIds (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetMatchingBlockIds Object org.apache.spark.storage.BlockManagerMessages.GetMatchingBlockIds All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetMatchingBlockIds extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetMatchingBlockIds(scala.Function1<BlockId,Object> filter, boolean askSlaves)  Method Summary Methods  Modifier and Type Method and Description boolean askSlaves()  scala.Function1<BlockId,Object> filter()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.GetMatchingBlockIds public BlockManagerMessages.GetMatchingBlockIds(scala.Function1<BlockId,Object> filter, boolean askSlaves) Method Detail filter public scala.Function1<BlockId,Object> filter() askSlaves public boolean askSlaves() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetMemoryStatus$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetMemoryStatus$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetMemoryStatus$ Object org.apache.spark.storage.BlockManagerMessages.GetMemoryStatus$ All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetMemoryStatus$ extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.GetMemoryStatus$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetMemoryStatus$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final BlockManagerMessages.GetMemoryStatus$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.GetMemoryStatus$ public BlockManagerMessages.GetMemoryStatus$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetPeers$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetPeers$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetPeers$ Object scala.runtime.AbstractFunction1<BlockManagerId,BlockManagerMessages.GetPeers> org.apache.spark.storage.BlockManagerMessages.GetPeers$ All Implemented Interfaces: java.io.Serializable, scala.Function1<BlockManagerId,BlockManagerMessages.GetPeers> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetPeers$ extends scala.runtime.AbstractFunction1<BlockManagerId,BlockManagerMessages.GetPeers> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.GetPeers$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetPeers$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.GetPeers$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.GetPeers$ public BlockManagerMessages.GetPeers$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetPeers (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetPeers (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetPeers Object org.apache.spark.storage.BlockManagerMessages.GetPeers All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetPeers extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetPeers(BlockManagerId blockManagerId)  Method Summary Methods  Modifier and Type Method and Description BlockManagerId blockManagerId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.GetPeers public BlockManagerMessages.GetPeers(BlockManagerId blockManagerId) Method Detail blockManagerId public BlockManagerId blockManagerId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.GetStorageStatus$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.GetStorageStatus$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.GetStorageStatus$ Object org.apache.spark.storage.BlockManagerMessages.GetStorageStatus$ All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.GetStorageStatus$ extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.GetStorageStatus$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.GetStorageStatus$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final BlockManagerMessages.GetStorageStatus$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.GetStorageStatus$ public BlockManagerMessages.GetStorageStatus$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.HasCachedBlocks$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.HasCachedBlocks$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.HasCachedBlocks$ Object scala.runtime.AbstractFunction1<String,BlockManagerMessages.HasCachedBlocks> org.apache.spark.storage.BlockManagerMessages.HasCachedBlocks$ All Implemented Interfaces: java.io.Serializable, scala.Function1<String,BlockManagerMessages.HasCachedBlocks> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.HasCachedBlocks$ extends scala.runtime.AbstractFunction1<String,BlockManagerMessages.HasCachedBlocks> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.HasCachedBlocks$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.HasCachedBlocks$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.HasCachedBlocks$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.HasCachedBlocks$ public BlockManagerMessages.HasCachedBlocks$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.HasCachedBlocks (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.HasCachedBlocks (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.HasCachedBlocks Object org.apache.spark.storage.BlockManagerMessages.HasCachedBlocks All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.HasCachedBlocks extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.HasCachedBlocks(String executorId)  Method Summary Methods  Modifier and Type Method and Description String executorId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.HasCachedBlocks public BlockManagerMessages.HasCachedBlocks(String executorId) Method Detail executorId public String executorId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RegisterBlockManager$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RegisterBlockManager$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RegisterBlockManager$ Object scala.runtime.AbstractFunction3<BlockManagerId,Object,org.apache.spark.rpc.RpcEndpointRef,BlockManagerMessages.RegisterBlockManager> org.apache.spark.storage.BlockManagerMessages.RegisterBlockManager$ All Implemented Interfaces: java.io.Serializable, scala.Function3<BlockManagerId,Object,org.apache.spark.rpc.RpcEndpointRef,BlockManagerMessages.RegisterBlockManager> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RegisterBlockManager$ extends scala.runtime.AbstractFunction3<BlockManagerId,Object,org.apache.spark.rpc.RpcEndpointRef,BlockManagerMessages.RegisterBlockManager> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.RegisterBlockManager$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RegisterBlockManager$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction3 curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function3 apply Field Detail MODULE$ public static final BlockManagerMessages.RegisterBlockManager$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.RegisterBlockManager$ public BlockManagerMessages.RegisterBlockManager$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RegisterBlockManager (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RegisterBlockManager (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RegisterBlockManager Object org.apache.spark.storage.BlockManagerMessages.RegisterBlockManager All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RegisterBlockManager extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RegisterBlockManager(BlockManagerId blockManagerId, long maxMemSize, org.apache.spark.rpc.RpcEndpointRef sender)  Method Summary Methods  Modifier and Type Method and Description BlockManagerId blockManagerId()  long maxMemSize()  org.apache.spark.rpc.RpcEndpointRef sender()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.RegisterBlockManager public BlockManagerMessages.RegisterBlockManager(BlockManagerId blockManagerId, long maxMemSize, org.apache.spark.rpc.RpcEndpointRef sender) Method Detail blockManagerId public BlockManagerId blockManagerId() maxMemSize public long maxMemSize() sender public org.apache.spark.rpc.RpcEndpointRef sender() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveBlock$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveBlock$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveBlock$ Object scala.runtime.AbstractFunction1<BlockId,BlockManagerMessages.RemoveBlock> org.apache.spark.storage.BlockManagerMessages.RemoveBlock$ All Implemented Interfaces: java.io.Serializable, scala.Function1<BlockId,BlockManagerMessages.RemoveBlock> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveBlock$ extends scala.runtime.AbstractFunction1<BlockId,BlockManagerMessages.RemoveBlock> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.RemoveBlock$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveBlock$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.RemoveBlock$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.RemoveBlock$ public BlockManagerMessages.RemoveBlock$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveBlock (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveBlock (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveBlock Object org.apache.spark.storage.BlockManagerMessages.RemoveBlock All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerSlave, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveBlock extends Object implements BlockManagerMessages.ToBlockManagerSlave, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveBlock(BlockId blockId)  Method Summary Methods  Modifier and Type Method and Description BlockId blockId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.RemoveBlock public BlockManagerMessages.RemoveBlock(BlockId blockId) Method Detail blockId public BlockId blockId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveBroadcast$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveBroadcast$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveBroadcast$ Object scala.runtime.AbstractFunction2<Object,Object,BlockManagerMessages.RemoveBroadcast> org.apache.spark.storage.BlockManagerMessages.RemoveBroadcast$ All Implemented Interfaces: java.io.Serializable, scala.Function2<Object,Object,BlockManagerMessages.RemoveBroadcast> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveBroadcast$ extends scala.runtime.AbstractFunction2<Object,Object,BlockManagerMessages.RemoveBroadcast> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.RemoveBroadcast$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveBroadcast$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction2 apply$mcDDD$sp, apply$mcDDI$sp, apply$mcDDJ$sp, apply$mcDID$sp, apply$mcDII$sp, apply$mcDIJ$sp, apply$mcDJD$sp, apply$mcDJI$sp, apply$mcDJJ$sp, apply$mcFDD$sp, apply$mcFDI$sp, apply$mcFDJ$sp, apply$mcFID$sp, apply$mcFII$sp, apply$mcFIJ$sp, apply$mcFJD$sp, apply$mcFJI$sp, apply$mcFJJ$sp, apply$mcIDD$sp, apply$mcIDI$sp, apply$mcIDJ$sp, apply$mcIID$sp, apply$mcIII$sp, apply$mcIIJ$sp, apply$mcIJD$sp, apply$mcIJI$sp, apply$mcIJJ$sp, apply$mcJDD$sp, apply$mcJDI$sp, apply$mcJDJ$sp, apply$mcJID$sp, apply$mcJII$sp, apply$mcJIJ$sp, apply$mcJJD$sp, apply$mcJJI$sp, apply$mcJJJ$sp, apply$mcVDD$sp, apply$mcVDI$sp, apply$mcVDJ$sp, apply$mcVID$sp, apply$mcVII$sp, apply$mcVIJ$sp, apply$mcVJD$sp, apply$mcVJI$sp, apply$mcVJJ$sp, apply$mcZDD$sp, apply$mcZDI$sp, apply$mcZDJ$sp, apply$mcZID$sp, apply$mcZII$sp, apply$mcZIJ$sp, apply$mcZJD$sp, apply$mcZJI$sp, apply$mcZJJ$sp, curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function2 apply Field Detail MODULE$ public static final BlockManagerMessages.RemoveBroadcast$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.RemoveBroadcast$ public BlockManagerMessages.RemoveBroadcast$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveBroadcast (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveBroadcast (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveBroadcast Object org.apache.spark.storage.BlockManagerMessages.RemoveBroadcast All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerSlave, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveBroadcast extends Object implements BlockManagerMessages.ToBlockManagerSlave, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveBroadcast(long broadcastId, boolean removeFromDriver)  Method Summary Methods  Modifier and Type Method and Description long broadcastId()  boolean removeFromDriver()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.RemoveBroadcast public BlockManagerMessages.RemoveBroadcast(long broadcastId, boolean removeFromDriver) Method Detail broadcastId public long broadcastId() removeFromDriver public boolean removeFromDriver() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveExecutor$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveExecutor$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveExecutor$ Object scala.runtime.AbstractFunction1<String,BlockManagerMessages.RemoveExecutor> org.apache.spark.storage.BlockManagerMessages.RemoveExecutor$ All Implemented Interfaces: java.io.Serializable, scala.Function1<String,BlockManagerMessages.RemoveExecutor> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveExecutor$ extends scala.runtime.AbstractFunction1<String,BlockManagerMessages.RemoveExecutor> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.RemoveExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveExecutor$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.RemoveExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.RemoveExecutor$ public BlockManagerMessages.RemoveExecutor$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveExecutor (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveExecutor (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveExecutor Object org.apache.spark.storage.BlockManagerMessages.RemoveExecutor All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveExecutor extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveExecutor(String execId)  Method Summary Methods  Modifier and Type Method and Description String execId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.RemoveExecutor public BlockManagerMessages.RemoveExecutor(String execId) Method Detail execId public String execId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveRdd$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveRdd$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveRdd$ Object scala.runtime.AbstractFunction1<Object,BlockManagerMessages.RemoveRdd> org.apache.spark.storage.BlockManagerMessages.RemoveRdd$ All Implemented Interfaces: java.io.Serializable, scala.Function1<Object,BlockManagerMessages.RemoveRdd> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveRdd$ extends scala.runtime.AbstractFunction1<Object,BlockManagerMessages.RemoveRdd> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.RemoveRdd$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveRdd$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.RemoveRdd$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.RemoveRdd$ public BlockManagerMessages.RemoveRdd$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveRdd (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveRdd (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveRdd Object org.apache.spark.storage.BlockManagerMessages.RemoveRdd All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerSlave, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveRdd extends Object implements BlockManagerMessages.ToBlockManagerSlave, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveRdd(int rddId)  Method Summary Methods  Modifier and Type Method and Description int rddId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.RemoveRdd public BlockManagerMessages.RemoveRdd(int rddId) Method Detail rddId public int rddId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveShuffle$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveShuffle$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveShuffle$ Object scala.runtime.AbstractFunction1<Object,BlockManagerMessages.RemoveShuffle> org.apache.spark.storage.BlockManagerMessages.RemoveShuffle$ All Implemented Interfaces: java.io.Serializable, scala.Function1<Object,BlockManagerMessages.RemoveShuffle> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveShuffle$ extends scala.runtime.AbstractFunction1<Object,BlockManagerMessages.RemoveShuffle> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.RemoveShuffle$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveShuffle$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final BlockManagerMessages.RemoveShuffle$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.RemoveShuffle$ public BlockManagerMessages.RemoveShuffle$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.RemoveShuffle (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.RemoveShuffle (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.RemoveShuffle Object org.apache.spark.storage.BlockManagerMessages.RemoveShuffle All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerSlave, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.RemoveShuffle extends Object implements BlockManagerMessages.ToBlockManagerSlave, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.RemoveShuffle(int shuffleId)  Method Summary Methods  Modifier and Type Method and Description int shuffleId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.RemoveShuffle public BlockManagerMessages.RemoveShuffle(int shuffleId) Method Detail shuffleId public int shuffleId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.StopBlockManagerMaster$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.StopBlockManagerMaster$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.StopBlockManagerMaster$ Object org.apache.spark.storage.BlockManagerMessages.StopBlockManagerMaster$ All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.StopBlockManagerMaster$ extends Object implements BlockManagerMessages.ToBlockManagerMaster, scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.StopBlockManagerMaster$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.StopBlockManagerMaster$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final BlockManagerMessages.StopBlockManagerMaster$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.StopBlockManagerMaster$ public BlockManagerMessages.StopBlockManagerMaster$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.ToBlockManagerMaster (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.ToBlockManagerMaster (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Interface BlockManagerMessages.ToBlockManagerMaster All Known Implementing Classes: BlockManagerMessages.BlockManagerHeartbeat, BlockManagerMessages.GetBlockStatus, BlockManagerMessages.GetExecutorEndpointRef, BlockManagerMessages.GetLocations, BlockManagerMessages.GetLocationsMultipleBlockIds, BlockManagerMessages.GetMatchingBlockIds, BlockManagerMessages.GetMemoryStatus$, BlockManagerMessages.GetPeers, BlockManagerMessages.GetStorageStatus$, BlockManagerMessages.HasCachedBlocks, BlockManagerMessages.RegisterBlockManager, BlockManagerMessages.RemoveExecutor, BlockManagerMessages.StopBlockManagerMaster$, BlockManagerMessages.UpdateBlockInfo Enclosing class: BlockManagerMessages public static interface BlockManagerMessages.ToBlockManagerMaster Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.ToBlockManagerSlave (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.ToBlockManagerSlave (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Interface BlockManagerMessages.ToBlockManagerSlave All Known Implementing Classes: BlockManagerMessages.RemoveBlock, BlockManagerMessages.RemoveBroadcast, BlockManagerMessages.RemoveRdd, BlockManagerMessages.RemoveShuffle, BlockManagerMessages.TriggerThreadDump$ Enclosing class: BlockManagerMessages public static interface BlockManagerMessages.ToBlockManagerSlave Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.TriggerThreadDump$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.TriggerThreadDump$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.TriggerThreadDump$ Object org.apache.spark.storage.BlockManagerMessages.TriggerThreadDump$ All Implemented Interfaces: java.io.Serializable, BlockManagerMessages.ToBlockManagerSlave, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.TriggerThreadDump$ extends Object implements BlockManagerMessages.ToBlockManagerSlave, scala.Product, scala.Serializable Driver -> Executor message to trigger a thread dump. See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.TriggerThreadDump$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.TriggerThreadDump$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final BlockManagerMessages.TriggerThreadDump$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.TriggerThreadDump$ public BlockManagerMessages.TriggerThreadDump$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.UpdateBlockInfo$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.UpdateBlockInfo$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.UpdateBlockInfo$ Object scala.runtime.AbstractFunction5<BlockManagerId,BlockId,StorageLevel,Object,Object,BlockManagerMessages.UpdateBlockInfo> org.apache.spark.storage.BlockManagerMessages.UpdateBlockInfo$ All Implemented Interfaces: java.io.Serializable, scala.Function5<BlockManagerId,BlockId,StorageLevel,Object,Object,BlockManagerMessages.UpdateBlockInfo> Enclosing class: BlockManagerMessages public static class BlockManagerMessages.UpdateBlockInfo$ extends scala.runtime.AbstractFunction5<BlockManagerId,BlockId,StorageLevel,Object,Object,BlockManagerMessages.UpdateBlockInfo> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static BlockManagerMessages.UpdateBlockInfo$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description BlockManagerMessages.UpdateBlockInfo$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction5 curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function5 apply Field Detail MODULE$ public static final BlockManagerMessages.UpdateBlockInfo$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail BlockManagerMessages.UpdateBlockInfo$ public BlockManagerMessages.UpdateBlockInfo$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages.UpdateBlockInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages.UpdateBlockInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages.UpdateBlockInfo Object org.apache.spark.storage.BlockManagerMessages.UpdateBlockInfo All Implemented Interfaces: java.io.Externalizable, java.io.Serializable, BlockManagerMessages.ToBlockManagerMaster, scala.Equals, scala.Product Enclosing class: BlockManagerMessages public static class BlockManagerMessages.UpdateBlockInfo extends Object implements BlockManagerMessages.ToBlockManagerMaster, java.io.Externalizable, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockManagerMessages.UpdateBlockInfo()  BlockManagerMessages.UpdateBlockInfo(BlockManagerId blockManagerId, BlockId blockId, StorageLevel storageLevel, long memSize, long diskSize)  Method Summary Methods  Modifier and Type Method and Description BlockId blockId()  BlockManagerId blockManagerId()  long diskSize()  long memSize()  void readExternal(java.io.ObjectInput in)  StorageLevel storageLevel()  void writeExternal(java.io.ObjectOutput out)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockManagerMessages.UpdateBlockInfo public BlockManagerMessages.UpdateBlockInfo(BlockManagerId blockManagerId, BlockId blockId, StorageLevel storageLevel, long memSize, long diskSize) BlockManagerMessages.UpdateBlockInfo public BlockManagerMessages.UpdateBlockInfo() Method Detail blockManagerId public BlockManagerId blockManagerId() blockId public BlockId blockId() storageLevel public StorageLevel storageLevel() memSize public long memSize() diskSize public long diskSize() writeExternal public void writeExternal(java.io.ObjectOutput out) Specified by: writeExternal in interface java.io.Externalizable readExternal public void readExternal(java.io.ObjectInput in) Specified by: readExternal in interface java.io.Externalizable Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockManagerMessages (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockManagerMessages (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockManagerMessages Object org.apache.spark.storage.BlockManagerMessages public class BlockManagerMessages extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  BlockManagerMessages.BlockManagerHeartbeat  static class  BlockManagerMessages.BlockManagerHeartbeat$  static class  BlockManagerMessages.GetBlockStatus  static class  BlockManagerMessages.GetBlockStatus$  static class  BlockManagerMessages.GetExecutorEndpointRef  static class  BlockManagerMessages.GetExecutorEndpointRef$  static class  BlockManagerMessages.GetLocations  static class  BlockManagerMessages.GetLocations$  static class  BlockManagerMessages.GetLocationsMultipleBlockIds  static class  BlockManagerMessages.GetLocationsMultipleBlockIds$  static class  BlockManagerMessages.GetMatchingBlockIds  static class  BlockManagerMessages.GetMatchingBlockIds$  static class  BlockManagerMessages.GetMemoryStatus$  static class  BlockManagerMessages.GetPeers  static class  BlockManagerMessages.GetPeers$  static class  BlockManagerMessages.GetStorageStatus$  static class  BlockManagerMessages.HasCachedBlocks  static class  BlockManagerMessages.HasCachedBlocks$  static class  BlockManagerMessages.RegisterBlockManager  static class  BlockManagerMessages.RegisterBlockManager$  static class  BlockManagerMessages.RemoveBlock  static class  BlockManagerMessages.RemoveBlock$  static class  BlockManagerMessages.RemoveBroadcast  static class  BlockManagerMessages.RemoveBroadcast$  static class  BlockManagerMessages.RemoveExecutor  static class  BlockManagerMessages.RemoveExecutor$  static class  BlockManagerMessages.RemoveRdd  static class  BlockManagerMessages.RemoveRdd$  static class  BlockManagerMessages.RemoveShuffle  static class  BlockManagerMessages.RemoveShuffle$  static class  BlockManagerMessages.StopBlockManagerMaster$  static interface  BlockManagerMessages.ToBlockManagerMaster  static interface  BlockManagerMessages.ToBlockManagerSlave  static class  BlockManagerMessages.TriggerThreadDump$ Driver -> Executor message to trigger a thread dump. static class  BlockManagerMessages.UpdateBlockInfo  static class  BlockManagerMessages.UpdateBlockInfo$  Constructor Summary Constructors  Constructor and Description BlockManagerMessages()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BlockManagerMessages public BlockManagerMessages() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockMatrix (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockMatrix (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg.distributed Class BlockMatrix Object org.apache.spark.mllib.linalg.distributed.BlockMatrix All Implemented Interfaces: java.io.Serializable, DistributedMatrix public class BlockMatrix extends Object implements DistributedMatrix Represents a distributed matrix in blocks of local matrices. param: blocks The RDD of sub-matrix blocks ((blockRowIndex, blockColIndex), sub-matrix) that form this distributed matrix. If multiple blocks with the same index exist, the results for operations like add and multiply will be unpredictable. param: rowsPerBlock Number of rows that make up each block. The blocks forming the final rows are not required to have the given number of rows param: colsPerBlock Number of columns that make up each block. The blocks forming the final columns are not required to have the given number of columns param: nRows Number of rows of this matrix. If the supplied value is less than or equal to zero, the number of rows will be calculated when numRows is invoked. param: nCols Number of columns of this matrix. If the supplied value is less than or equal to zero, the number of columns will be calculated when numCols is invoked. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockMatrix(RDD<scala.Tuple2<scala.Tuple2<Object,Object>,Matrix>> blocks, int rowsPerBlock, int colsPerBlock) Alternate constructor for BlockMatrix without the input of the number of rows and columns. BlockMatrix(RDD<scala.Tuple2<scala.Tuple2<Object,Object>,Matrix>> blocks, int rowsPerBlock, int colsPerBlock, long nRows, long nCols)  Method Summary Methods  Modifier and Type Method and Description BlockMatrix add(BlockMatrix other) Adds the given block matrix other to this block matrix: this + other. RDD<scala.Tuple2<scala.Tuple2<Object,Object>,Matrix>> blocks()  BlockMatrix cache() Caches the underlying RDD. int colsPerBlock()  BlockMatrix multiply(BlockMatrix other) Left multiplies this BlockMatrix to other, another BlockMatrix. int numColBlocks()  long numCols() Gets or computes the number of columns. int numRowBlocks()  long numRows() Gets or computes the number of rows. BlockMatrix persist(StorageLevel storageLevel) Persists the underlying RDD with the specified storage level. int rowsPerBlock()  BlockMatrix subtract(BlockMatrix other) Subtracts the given block matrix other from this block matrix: this - other. CoordinateMatrix toCoordinateMatrix() Converts to CoordinateMatrix. IndexedRowMatrix toIndexedRowMatrix() Converts to IndexedRowMatrix. Matrix toLocalMatrix() Collect the distributed matrix on the driver as a `DenseMatrix`. BlockMatrix transpose() Transpose this BlockMatrix. void validate() Validates the block matrix info against the matrix data (blocks) and throws an exception if any error is found. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BlockMatrix public BlockMatrix(RDD<scala.Tuple2<scala.Tuple2<Object,Object>,Matrix>> blocks, int rowsPerBlock, int colsPerBlock, long nRows, long nCols) BlockMatrix public BlockMatrix(RDD<scala.Tuple2<scala.Tuple2<Object,Object>,Matrix>> blocks, int rowsPerBlock, int colsPerBlock) Alternate constructor for BlockMatrix without the input of the number of rows and columns. Parameters:blocks - The RDD of sub-matrix blocks ((blockRowIndex, blockColIndex), sub-matrix) that form this distributed matrix. If multiple blocks with the same index exist, the results for operations like add and multiply will be unpredictable.rowsPerBlock - Number of rows that make up each block. The blocks forming the final rows are not required to have the given number of rowscolsPerBlock - Number of columns that make up each block. The blocks forming the final columns are not required to have the given number of columns Method Detail blocks public RDD<scala.Tuple2<scala.Tuple2<Object,Object>,Matrix>> blocks() rowsPerBlock public int rowsPerBlock() colsPerBlock public int colsPerBlock() numRows public long numRows() Description copied from interface: DistributedMatrix Gets or computes the number of rows. Specified by: numRows in interface DistributedMatrix numCols public long numCols() Description copied from interface: DistributedMatrix Gets or computes the number of columns. Specified by: numCols in interface DistributedMatrix numRowBlocks public int numRowBlocks() numColBlocks public int numColBlocks() validate public void validate() Validates the block matrix info against the matrix data (blocks) and throws an exception if any error is found. cache public BlockMatrix cache() Caches the underlying RDD. persist public BlockMatrix persist(StorageLevel storageLevel) Persists the underlying RDD with the specified storage level. toCoordinateMatrix public CoordinateMatrix toCoordinateMatrix() Converts to CoordinateMatrix. toIndexedRowMatrix public IndexedRowMatrix toIndexedRowMatrix() Converts to IndexedRowMatrix. The number of columns must be within the integer range. toLocalMatrix public Matrix toLocalMatrix() Collect the distributed matrix on the driver as a `DenseMatrix`. transpose public BlockMatrix transpose() Transpose this BlockMatrix. Returns a new BlockMatrix instance sharing the same underlying data. Is a lazy operation. Returns:(undocumented) add public BlockMatrix add(BlockMatrix other) Adds the given block matrix other to this block matrix: this + other. The matrices must have the same size and matching rowsPerBlock and colsPerBlock values. If one of the blocks that are being added are instances of SparseMatrix, the resulting sub matrix will also be a SparseMatrix, even if it is being added to a DenseMatrix. If two dense matrices are added, the output will also be a DenseMatrix. Parameters:other - (undocumented) Returns:(undocumented) subtract public BlockMatrix subtract(BlockMatrix other) Subtracts the given block matrix other from this block matrix: this - other. The matrices must have the same size and matching rowsPerBlock and colsPerBlock values. If one of the blocks that are being subtracted are instances of SparseMatrix, the resulting sub matrix will also be a SparseMatrix, even if it is being subtracted from a DenseMatrix. If two dense matrices are subtracted, the output will also be a DenseMatrix. Parameters:other - (undocumented) Returns:(undocumented) multiply public BlockMatrix multiply(BlockMatrix other) Left multiplies this BlockMatrix to other, another BlockMatrix. The colsPerBlock of this matrix must equal the rowsPerBlock of other. If other contains SparseMatrix, they will have to be converted to a DenseMatrix. The output BlockMatrix will only consist of blocks of DenseMatrix. This may cause some performance issues until support for multiplying two sparse matrices is added. Note: The behavior of multiply has changed in 1.6.0. multiply used to throw an error when there were blocks with duplicate indices. Now, the blocks with duplicate indices will be added with each other. Parameters:other - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockNotFoundException (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockNotFoundException (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockNotFoundException Object Throwable Exception org.apache.spark.storage.BlockNotFoundException All Implemented Interfaces: java.io.Serializable public class BlockNotFoundException extends Exception See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockNotFoundException(String blockId)  Method Summary Methods inherited from class Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail BlockNotFoundException public BlockNotFoundException(String blockId) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockStatus (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockStatus (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockStatus Object org.apache.spark.storage.BlockStatus All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class BlockStatus extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockStatus(StorageLevel storageLevel, long memSize, long diskSize)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  long diskSize()  static BlockStatus empty()  abstract static boolean equals(Object that)  boolean isCached()  long memSize()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  StorageLevel storageLevel()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockStatus public BlockStatus(StorageLevel storageLevel, long memSize, long diskSize) Method Detail empty public static BlockStatus empty() canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() storageLevel public StorageLevel storageLevel() memSize public long memSize() diskSize public long diskSize() isCached public boolean isCached() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BlockUpdatedInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BlockUpdatedInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BlockUpdatedInfo Object org.apache.spark.storage.BlockUpdatedInfo All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class BlockUpdatedInfo extends Object implements scala.Product, scala.Serializable :: DeveloperApi :: Stores information about a block status in a block manager. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BlockUpdatedInfo(BlockManagerId blockManagerId, BlockId blockId, StorageLevel storageLevel, long memSize, long diskSize)  Method Summary Methods  Modifier and Type Method and Description BlockId blockId()  BlockManagerId blockManagerId()  abstract static boolean canEqual(Object that)  long diskSize()  abstract static boolean equals(Object that)  long memSize()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  StorageLevel storageLevel()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BlockUpdatedInfo public BlockUpdatedInfo(BlockManagerId blockManagerId, BlockId blockId, StorageLevel storageLevel, long memSize, long diskSize) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() blockManagerId public BlockManagerId blockManagerId() blockId public BlockId blockId() storageLevel public StorageLevel storageLevel() memSize public long memSize() diskSize public long diskSize() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BloomFilter.Version (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BloomFilter.Version (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method org.apache.spark.util.sketch Enum BloomFilter.Version Object Enum<BloomFilter.Version> org.apache.spark.util.sketch.BloomFilter.Version All Implemented Interfaces: java.io.Serializable, Comparable<BloomFilter.Version> Enclosing class: BloomFilter public static enum BloomFilter.Version extends Enum<BloomFilter.Version> Enum Constant Summary Enum Constants  Enum Constant and Description V1 BloomFilter binary format version 1. Method Summary Methods  Modifier and Type Method and Description static BloomFilter.Version valueOf(String name) Returns the enum constant of this type with the specified name. static BloomFilter.Version[] values() Returns an array containing the constants of this enum type, in the order they are declared. Methods inherited from class Enum compareTo, equals, getDeclaringClass, hashCode, name, ordinal, toString, valueOf Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Enum Constant Detail V1 public static final BloomFilter.Version V1 BloomFilter binary format version 1. All values written in big-endian order: Version number, always 1 (32 bit) Number of hash functions (32 bit) Total number of words of the underlying bit array (32 bit) The words/longs (numWords * 64 bit) Method Detail values public static BloomFilter.Version[] values() Returns an array containing the constants of this enum type, in the order they are declared. This method may be used to iterate over the constants as follows: for (BloomFilter.Version c : BloomFilter.Version.values())   System.out.println(c); Returns:an array containing the constants of this enum type, in the order they are declared valueOf public static BloomFilter.Version valueOf(String name) Returns the enum constant of this type with the specified name. The string must match exactly an identifier used to declare an enum constant in this type. (Extraneous whitespace characters are not permitted.) Parameters:name - the name of the enum constant to be returned. Returns:the enum constant with the specified name Throws: IllegalArgumentException - if this enum type has no constant with the specified name NullPointerException - if the argument is null Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method BloomFilter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BloomFilter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.sketch Class BloomFilter Object org.apache.spark.util.sketch.BloomFilter public abstract class BloomFilter extends Object A Bloom filter is a space-efficient probabilistic data structure that offers an approximate containment test with one-sided error: if it claims that an item is contained in it, this might be in error, but if it claims that an item is not contained in it, then this is definitely true. Currently supported data types include: Byte Short Integer Long String The false positive probability (FPP) of a Bloom filter is defined as the probability that mightContain(Object) will erroneously return true for an object that hasu not actually been put in the BloomFilter. The implementation is largely based on the BloomFilter class from Guava. Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  BloomFilter.Version  Constructor Summary Constructors  Constructor and Description BloomFilter()  Method Summary Methods  Modifier and Type Method and Description abstract long bitSize() Returns the number of bits in the underlying bit array. static BloomFilter create(long expectedNumItems) Creates a BloomFilter with the expected number of insertions and a default expected false positive probability of 3%. static BloomFilter create(long expectedNumItems, double fpp) Creates a BloomFilter with the expected number of insertions and expected false positive probability. static BloomFilter create(long expectedNumItems, long numBits) Creates a BloomFilter with given expectedNumItems and numBits, it will pick an optimal numHashFunctions which can minimize fpp for the bloom filter. abstract double expectedFpp() Returns the probability that mightContain(Object) erroneously return true for an object that has not actually been put in the BloomFilter. abstract boolean isCompatible(BloomFilter other) Determines whether a given bloom filter is compatible with this bloom filter. abstract BloomFilter mergeInPlace(BloomFilter other) Combines this bloom filter with another bloom filter by performing a bitwise OR of the underlying data. abstract boolean mightContain(Object item) Returns true if the element might have been put in this Bloom filter, false if this is definitely not the case. abstract boolean mightContainBinary(byte[] item) A specialized variant of mightContain(Object) that only tests byte array items. abstract boolean mightContainLong(long item) A specialized variant of mightContain(Object) that only tests long items. abstract boolean mightContainString(String item) A specialized variant of mightContain(Object) that only tests String items. abstract boolean put(Object item) Puts an item into this BloomFilter. abstract boolean putBinary(byte[] item) A specialized variant of put(Object) that only supports byte array items. abstract boolean putLong(long item) A specialized variant of put(Object) that only supports long items. abstract boolean putString(String item) A specialized variant of put(Object) that only supports String items. static BloomFilter readFrom(java.io.InputStream in) Reads in a BloomFilter from an input stream. abstract void writeTo(java.io.OutputStream out) Writes out this BloomFilter to an output stream in binary format. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BloomFilter public BloomFilter() Method Detail expectedFpp public abstract double expectedFpp() Returns the probability that mightContain(Object) erroneously return true for an object that has not actually been put in the BloomFilter. Ideally, this number should be close to the fpp parameter passed in create(long, double), or smaller. If it is significantly higher, it is usually the case that too many items (more than expected) have been put in the BloomFilter, degenerating it. bitSize public abstract long bitSize() Returns the number of bits in the underlying bit array. put public abstract boolean put(Object item) Puts an item into this BloomFilter. Ensures that subsequent invocations of mightContain(Object) with the same item will always return true. Returns:true if the bloom filter's bits changed as a result of this operation. If the bits changed, this is definitely the first time object has been added to the filter. If the bits haven't changed, this might be the first time object has been added to the filter. Note that put(t) always returns the opposite result to what mightContain(t) would have returned at the time it is called. putString public abstract boolean putString(String item) A specialized variant of put(Object) that only supports String items. putLong public abstract boolean putLong(long item) A specialized variant of put(Object) that only supports long items. putBinary public abstract boolean putBinary(byte[] item) A specialized variant of put(Object) that only supports byte array items. isCompatible public abstract boolean isCompatible(BloomFilter other) Determines whether a given bloom filter is compatible with this bloom filter. For two bloom filters to be compatible, they must have the same bit size. Parameters:other - The bloom filter to check for compatibility. mergeInPlace public abstract BloomFilter mergeInPlace(BloomFilter other) throws IncompatibleMergeException Combines this bloom filter with another bloom filter by performing a bitwise OR of the underlying data. The mutations happen to this instance. Callers must ensure the bloom filters are appropriately sized to avoid saturating them. Parameters:other - The bloom filter to combine this bloom filter with. It is not mutated. Throws: IncompatibleMergeException - if isCompatible(other) == false mightContain public abstract boolean mightContain(Object item) Returns true if the element might have been put in this Bloom filter, false if this is definitely not the case. mightContainString public abstract boolean mightContainString(String item) A specialized variant of mightContain(Object) that only tests String items. mightContainLong public abstract boolean mightContainLong(long item) A specialized variant of mightContain(Object) that only tests long items. mightContainBinary public abstract boolean mightContainBinary(byte[] item) A specialized variant of mightContain(Object) that only tests byte array items. writeTo public abstract void writeTo(java.io.OutputStream out) throws java.io.IOException Writes out this BloomFilter to an output stream in binary format. It is the caller's responsibility to close the stream. Throws: java.io.IOException readFrom public static BloomFilter readFrom(java.io.InputStream in) throws java.io.IOException Reads in a BloomFilter from an input stream. It is the caller's responsibility to close the stream. Throws: java.io.IOException create public static BloomFilter create(long expectedNumItems) Creates a BloomFilter with the expected number of insertions and a default expected false positive probability of 3%. Note that overflowing a BloomFilter with significantly more elements than specified, will result in its saturation, and a sharp deterioration of its false positive probability. create public static BloomFilter create(long expectedNumItems, double fpp) Creates a BloomFilter with the expected number of insertions and expected false positive probability. Note that overflowing a BloomFilter with significantly more elements than specified, will result in its saturation, and a sharp deterioration of its false positive probability. create public static BloomFilter create(long expectedNumItems, long numBits) Creates a BloomFilter with given expectedNumItems and numBits, it will pick an optimal numHashFunctions which can minimize fpp for the bloom filter. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BooleanParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BooleanParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class BooleanParam Object org.apache.spark.ml.param.Param<Object> org.apache.spark.ml.param.BooleanParam All Implemented Interfaces: java.io.Serializable public class BooleanParam extends Param<Object> :: DeveloperApi :: Specialized version of Param[Boolean] for Java. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BooleanParam(Identifiable parent, String name, String doc)  BooleanParam(String parent, String name, String doc)  Method Summary Methods  Modifier and Type Method and Description boolean jsonDecode(String json)  String jsonEncode(boolean value)  ParamPair<Object> w(boolean value) Creates a param pair with the given value (for Java). Methods inherited from class org.apache.spark.ml.param.Param doc, equals, hashCode, isValid, jsonEncode, name, parent, toString, w Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail BooleanParam public BooleanParam(String parent, String name, String doc) BooleanParam public BooleanParam(Identifiable parent, String name, String doc) Method Detail w public ParamPair<Object> w(boolean value) Creates a param pair with the given value (for Java). jsonEncode public String jsonEncode(boolean value) jsonDecode public boolean jsonDecode(String json) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BooleanType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BooleanType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class BooleanType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.BooleanType public class BooleanType extends DataType :: DeveloperApi :: The data type representing Boolean values. Please use the singleton DataTypes.BooleanType. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of the BooleanType is 1 byte. static String json()  static String prettyJson()  static String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, simpleString, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() simpleString public static String simpleString() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() The default size of a value of the BooleanType is 1 byte. Specified by: defaultSize in class DataType Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BoostingStrategy (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BoostingStrategy (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.configuration Class BoostingStrategy Object org.apache.spark.mllib.tree.configuration.BoostingStrategy All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class BoostingStrategy extends Object implements scala.Serializable, scala.Product Configuration options for GradientBoostedTrees. param: treeStrategy Parameters for the tree algorithm. We support regression and binary classification for boosting. Impurity setting will be ignored. param: loss Loss function used for minimization during gradient boosting. param: numIterations Number of iterations of boosting. In other words, the number of weak hypotheses used in the final model. param: learningRate Learning rate for shrinking the contribution of each estimator. The learning rate should be between in the interval (0, 1] param: validationTol validationTol is a condition which decides iteration termination when runWithValidation is used. The end of iteration is decided based on below logic: If the current loss on the validation set is > 0.01, the diff of validation error is compared to relative tolerance which is validationTol * (current loss on the validation set). If the current loss on the validation set is <= 0.01, the diff of validation error is compared to absolute tolerance which is validationTol * 0.01. Ignored when org.apache.spark.mllib.tree.GradientBoostedTrees.run() is used. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BoostingStrategy(Strategy treeStrategy, Loss loss, int numIterations, double learningRate, double validationTol)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  static BoostingStrategy defaultParams(scala.Enumeration.Value algo) Returns default configuration for the boosting algorithm static BoostingStrategy defaultParams(String algo) Returns default configuration for the boosting algorithm abstract static boolean equals(Object that)  double getLearningRate()  Loss getLoss()  int getNumIterations()  Strategy getTreeStrategy()  double getValidationTol()  double learningRate()  Loss loss()  int numIterations()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  void setLearningRate(double x$1)  void setLoss(Loss x$1)  void setNumIterations(int x$1)  void setTreeStrategy(Strategy x$1)  void setValidationTol(double x$1)  Strategy treeStrategy()  double validationTol()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BoostingStrategy public BoostingStrategy(Strategy treeStrategy, Loss loss, int numIterations, double learningRate, double validationTol) Method Detail defaultParams public static BoostingStrategy defaultParams(String algo) Returns default configuration for the boosting algorithm Parameters:algo - Learning goal. Supported: "Classification" or "Regression" Returns:Configuration for boosting algorithm defaultParams public static BoostingStrategy defaultParams(scala.Enumeration.Value algo) Returns default configuration for the boosting algorithm Parameters:algo - Learning goal. Supported: org.apache.spark.mllib.tree.configuration.Algo.Classification, org.apache.spark.mllib.tree.configuration.Algo.Regression Returns:Configuration for boosting algorithm canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() treeStrategy public Strategy treeStrategy() setTreeStrategy public void setTreeStrategy(Strategy x$1) loss public Loss loss() setLoss public void setLoss(Loss x$1) numIterations public int numIterations() setNumIterations public void setNumIterations(int x$1) learningRate public double learningRate() setLearningRate public void setLearningRate(double x$1) validationTol public double validationTol() setValidationTol public void setValidationTol(double x$1) getTreeStrategy public Strategy getTreeStrategy() getLoss public Loss getLoss() getNumIterations public int getNumIterations() getLearningRate public double getLearningRate() getValidationTol public double getValidationTol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BoundedDouble (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BoundedDouble (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.partial Class BoundedDouble Object org.apache.spark.partial.BoundedDouble public class BoundedDouble extends Object A Double value with error bars and associated confidence. Constructor Summary Constructors  Constructor and Description BoundedDouble(double mean, double confidence, double low, double high)  Method Summary Methods  Modifier and Type Method and Description double confidence()  boolean equals(Object that) Note that consistent with Double, any NaN value will make equality false int hashCode()  double high()  double low()  double mean()  String toString()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail BoundedDouble public BoundedDouble(double mean, double confidence, double low, double high) Method Detail mean public double mean() confidence public double confidence() low public double low() high public double high() toString public String toString() Overrides: toString in class Object hashCode public int hashCode() Overrides: hashCode in class Object equals public boolean equals(Object that) Note that consistent with Double, any NaN value will make equality false Overrides: equals in class Object Parameters:that - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BreezeUtil (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BreezeUtil (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.ann Class BreezeUtil Object org.apache.spark.ml.ann.BreezeUtil public class BreezeUtil extends Object In-place DGEMM and DGEMV for Breeze Constructor Summary Constructors  Constructor and Description BreezeUtil()  Method Summary Methods  Modifier and Type Method and Description static void dgemm(double alpha, breeze.linalg.DenseMatrix<Object> A, breeze.linalg.DenseMatrix<Object> B, double beta, breeze.linalg.DenseMatrix<Object> C) DGEMM: C := alpha * A * B + beta * C static void dgemv(double alpha, breeze.linalg.DenseMatrix<Object> A, breeze.linalg.DenseVector<Object> x, double beta, breeze.linalg.DenseVector<Object> y) DGEMV: y := alpha * A * x + beta * y Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BreezeUtil public BreezeUtil() Method Detail dgemm public static void dgemm(double alpha, breeze.linalg.DenseMatrix<Object> A, breeze.linalg.DenseMatrix<Object> B, double beta, breeze.linalg.DenseMatrix<Object> C) DGEMM: C := alpha * A * B + beta * C Parameters:alpha - alphaA - AB - Bbeta - betaC - C dgemv public static void dgemv(double alpha, breeze.linalg.DenseMatrix<Object> A, breeze.linalg.DenseVector<Object> x, double beta, breeze.linalg.DenseVector<Object> y) DGEMV: y := alpha * A * x + beta * y Parameters:alpha - alphaA - Ax - xbeta - betay - y Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Broadcast (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Broadcast (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.broadcast Class Broadcast<T> Object org.apache.spark.broadcast.Broadcast<T> All Implemented Interfaces: java.io.Serializable public abstract class Broadcast<T> extends Object implements java.io.Serializable A broadcast variable. Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost. Broadcast variables are created from a variable v by calling SparkContext.broadcast(T, scala.reflect.ClassTag<T>). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. The interpreter session below shows this: scala> val broadcastVar = sc.broadcast(Array(1, 2, 3)) broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int} = Broadcast(0) scala> broadcastVar.value res0: Array[Int] = Array(1, 2, 3) After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later). param: id A unique identifier for the broadcast variable. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Broadcast(long id, scala.reflect.ClassTag<T> evidence$1)  Method Summary Methods  Modifier and Type Method and Description void destroy() Destroy all data and metadata related to this broadcast variable. long id()  String toString()  void unpersist() Asynchronously delete cached copies of this broadcast on the executors. void unpersist(boolean blocking) Delete cached copies of this broadcast on the executors. T value() Get the broadcasted value. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail Broadcast public Broadcast(long id, scala.reflect.ClassTag<T> evidence$1) Method Detail id public long id() value public T value() Get the broadcasted value. unpersist public void unpersist() Asynchronously delete cached copies of this broadcast on the executors. If the broadcast is used after this is called, it will need to be re-sent to each executor. unpersist public void unpersist(boolean blocking) Delete cached copies of this broadcast on the executors. If the broadcast is used after this is called, it will need to be re-sent to each executor. Parameters:blocking - Whether to block until unpersisting has completed destroy public void destroy() Destroy all data and metadata related to this broadcast variable. Use this with caution; once a broadcast variable has been destroyed, it cannot be used again. This method blocks until destroy has completed toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BroadcastBlockId (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BroadcastBlockId (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BroadcastBlockId Object org.apache.spark.storage.BlockId org.apache.spark.storage.BroadcastBlockId All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class BroadcastBlockId extends BlockId implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description BroadcastBlockId(long broadcastId, String field)  Method Summary Methods  Modifier and Type Method and Description static scala.Option<RDDBlockId> asRDDId()  long broadcastId()  abstract static boolean canEqual(Object that)  static boolean equals(Object other)  String field()  static int hashCode()  static boolean isBroadcast()  static boolean isRDD()  static boolean isShuffle()  String name() A globally unique identifier for this Block. abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  static String toString()  Methods inherited from class org.apache.spark.storage.BlockId apply, asRDDId, BROADCAST, equals, hashCode, isBroadcast, isRDD, isShuffle, RDD, SHUFFLE_DATA, SHUFFLE_INDEX, SHUFFLE, STREAM, TASKRESULT, TEST, toString Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail BroadcastBlockId public BroadcastBlockId(long broadcastId, String field) Method Detail asRDDId public static scala.Option<RDDBlockId> asRDDId() isRDD public static boolean isRDD() isShuffle public static boolean isShuffle() isBroadcast public static boolean isBroadcast() toString public static String toString() hashCode public static int hashCode() equals public static boolean equals(Object other) canEqual public abstract static boolean canEqual(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() broadcastId public long broadcastId() field public String field() name public String name() Description copied from class: BlockId A globally unique identifier for this Block. Can be used for ser/de. Specified by: name in class BlockId Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Broker (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Broker (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kafka Class Broker Object org.apache.spark.streaming.kafka.Broker All Implemented Interfaces: java.io.Serializable public final class Broker extends Object implements scala.Serializable Represents the host and port info for a Kafka broker. Differs from the Kafka project's internal kafka.cluster.Broker, which contains a server ID. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static Broker apply(String host, int port)  static Broker create(String host, int port)  boolean equals(Object obj) Broker's port int hashCode()  String host() Broker's hostname int port() Broker's port String toString()  static scala.Option<scala.Tuple2<String,Object>> unapply(Broker broker)  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Method Detail create public static Broker create(String host, int port) apply public static Broker apply(String host, int port) unapply public static scala.Option<scala.Tuple2<String,Object>> unapply(Broker broker) host public String host() Broker's hostname port public int port() Broker's port equals public boolean equals(Object obj) Broker's port Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Bucketizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Bucketizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class Bucketizer Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<Bucketizer> org.apache.spark.ml.feature.Bucketizer All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public final class Bucketizer extends Model<Bucketizer> implements DefaultParamsWritable Bucketizer maps a column of continuous features to a column of feature buckets. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Bucketizer()  Bucketizer(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  Bucketizer copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  double[] getSplits()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Bucketizer load(String path)  static Param<String> outputCol()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static void save(String path)  static <T> Params set(Param<T> param, T value)  Bucketizer setInputCol(String value)  Bucketizer setOutputCol(String value)  static M setParent(Estimator<M> parent)  Bucketizer setSplits(double[] value)  DoubleArrayParam splits() Parameter for mapping continuous features into buckets. static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail Bucketizer public Bucketizer(String uid) Bucketizer public Bucketizer() Method Detail load public static Bucketizer load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) splits public DoubleArrayParam splits() Parameter for mapping continuous features into buckets. With n+1 splits, there are n buckets. A bucket defined by splits x,y holds values in the range [x,y) except the last bucket, which also includes y. Splits should be of length >= 3 and strictly increasing. Values at -inf, inf must be explicitly provided to cover all Double values; otherwise, values outside the splits specified will be treated as errors. Returns:(undocumented) getSplits public double[] getSplits() setSplits public Bucketizer setSplits(double[] value) setInputCol public Bucketizer setInputCol(String value) setOutputCol public Bucketizer setOutputCol(String value) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public Bucketizer copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<Bucketizer> Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BufferReleasingInputStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BufferReleasingInputStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage Class BufferReleasingInputStream Object java.io.InputStream org.apache.spark.storage.BufferReleasingInputStream All Implemented Interfaces: java.io.Closeable, AutoCloseable public class BufferReleasingInputStream extends java.io.InputStream Helper class that ensures a ManagedBuffer is released upon InputStream.close() Constructor Summary Constructors  Constructor and Description BufferReleasingInputStream(java.io.InputStream delegate, org.apache.spark.storage.ShuffleBlockFetcherIterator iterator)  Method Summary Methods  Modifier and Type Method and Description int available()  void close()  void mark(int readlimit)  boolean markSupported()  int read()  int read(byte[] b)  int read(byte[] b, int off, int len)  void reset()  long skip(long n)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BufferReleasingInputStream public BufferReleasingInputStream(java.io.InputStream delegate, org.apache.spark.storage.ShuffleBlockFetcherIterator iterator) Method Detail read public int read() Specified by: read in class java.io.InputStream close public void close() Specified by: close in interface java.io.Closeable Specified by: close in interface AutoCloseable Overrides: close in class java.io.InputStream available public int available() Overrides: available in class java.io.InputStream mark public void mark(int readlimit) Overrides: mark in class java.io.InputStream skip public long skip(long n) Overrides: skip in class java.io.InputStream markSupported public boolean markSupported() Overrides: markSupported in class java.io.InputStream read public int read(byte[] b) Overrides: read in class java.io.InputStream read public int read(byte[] b, int off, int len) Overrides: read in class java.io.InputStream reset public void reset() Overrides: reset in class java.io.InputStream Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ByteType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ByteType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class ByteType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.NumericType org.apache.spark.sql.types.ByteType public class ByteType extends NumericType :: DeveloperApi :: The data type representing Byte values. Please use the singleton DataTypes.ByteType. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of the ByteType is 1 byte. static String json()  static String prettyJson()  String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() The default size of a value of the ByteType is 1 byte. Returns:(undocumented) simpleString public String simpleString() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method BytecodeUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="BytecodeUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx.util Class BytecodeUtils Object org.apache.spark.graphx.util.BytecodeUtils public class BytecodeUtils extends Object Includes an utility function to test whether a function accesses a specific attribute of an object. Constructor Summary Constructors  Constructor and Description BytecodeUtils()  Method Summary Methods  Modifier and Type Method and Description static boolean invokedMethod(Object closure, Class<?> targetClass, String targetMethod) Test whether the given closure invokes the specified method in the specified class. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail BytecodeUtils public BytecodeUtils() Method Detail invokedMethod public static boolean invokedMethod(Object closure, Class<?> targetClass, String targetMethod) Test whether the given closure invokes the specified method in the specified class. Parameters:closure - (undocumented)targetClass - (undocumented)targetMethod - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CalendarIntervalType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CalendarIntervalType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class CalendarIntervalType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.CalendarIntervalType public class CalendarIntervalType extends DataType :: DeveloperApi :: The data type representing calendar time intervals. The calendar time interval is stored internally in two components: number of months the number of microseconds. Note that calendar intervals are not comparable. Please use the singleton DataTypes.CalendarIntervalType. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of this data type, used internally for size estimation. static String json()  static String prettyJson()  static String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, simpleString, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() simpleString public static String simpleString() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() Description copied from class: DataType The default size of a value of this data type, used internally for size estimation. Specified by: defaultSize in class DataType Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Catalog (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Catalog (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.catalog Class Catalog Object org.apache.spark.sql.catalog.Catalog Direct Known Subclasses: CatalogImpl public abstract class Catalog extends Object Catalog interface for Spark. To access this, use SparkSession.catalog. Since: 2.0.0 Constructor Summary Constructors  Constructor and Description Catalog()  Method Summary Methods  Modifier and Type Method and Description abstract void cacheTable(String tableName) Caches the specified table in-memory. abstract void clearCache() Removes all cached tables from the in-memory cache. abstract Dataset<Row> createExternalTable(String tableName, String path) :: Experimental :: Creates an external table from the given path and returns the corresponding DataFrame. abstract Dataset<Row> createExternalTable(String tableName, String source, java.util.Map<String,String> options) :: Experimental :: Creates an external table from the given path based on a data source and a set of options. abstract Dataset<Row> createExternalTable(String tableName, String source, scala.collection.immutable.Map<String,String> options) :: Experimental :: (Scala-specific) Creates an external table from the given path based on a data source and a set of options. abstract Dataset<Row> createExternalTable(String tableName, String path, String source) :: Experimental :: Creates an external table from the given path based on a data source and returns the corresponding DataFrame. abstract Dataset<Row> createExternalTable(String tableName, String source, StructType schema, java.util.Map<String,String> options) :: Experimental :: Create an external table from the given path based on a data source, a schema and a set of options. abstract Dataset<Row> createExternalTable(String tableName, String source, StructType schema, scala.collection.immutable.Map<String,String> options) :: Experimental :: (Scala-specific) Create an external table from the given path based on a data source, a schema and a set of options. abstract String currentDatabase() Returns the current default database in this session. abstract void dropTempView(String viewName) Drops the temporary view with the given view name in the catalog. abstract boolean isCached(String tableName) Returns true if the table is currently cached in-memory. abstract Dataset<Column> listColumns(String tableName) Returns a list of columns for the given table in the current database or the given temporary table. abstract Dataset<Column> listColumns(String dbName, String tableName) Returns a list of columns for the given table in the specified database. abstract Dataset<Database> listDatabases() Returns a list of databases available across all sessions. abstract Dataset<Function> listFunctions() Returns a list of functions registered in the current database. abstract Dataset<Function> listFunctions(String dbName) Returns a list of functions registered in the specified database. abstract Dataset<Table> listTables() Returns a list of tables in the current database. abstract Dataset<Table> listTables(String dbName) Returns a list of tables in the specified database. abstract void refreshByPath(String path) Invalidate and refresh all the cached data (and the associated metadata) for any dataframe that contains the given data source path. abstract void refreshTable(String tableName) Invalidate and refresh all the cached metadata of the given table. abstract void setCurrentDatabase(String dbName) Sets the current default database in this session. abstract void uncacheTable(String tableName) Removes the specified table from the in-memory cache. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Catalog public Catalog() Method Detail currentDatabase public abstract String currentDatabase() Returns the current default database in this session. Returns:(undocumented)Since: 2.0.0 setCurrentDatabase public abstract void setCurrentDatabase(String dbName) Sets the current default database in this session. Parameters:dbName - (undocumented)Since: 2.0.0 listDatabases public abstract Dataset<Database> listDatabases() Returns a list of databases available across all sessions. Returns:(undocumented)Since: 2.0.0 listTables public abstract Dataset<Table> listTables() Returns a list of tables in the current database. This includes all temporary tables. Returns:(undocumented)Since: 2.0.0 listTables public abstract Dataset<Table> listTables(String dbName) throws AnalysisException Returns a list of tables in the specified database. This includes all temporary tables. Parameters:dbName - (undocumented) Returns:(undocumented) Throws: AnalysisExceptionSince: 2.0.0 listFunctions public abstract Dataset<Function> listFunctions() Returns a list of functions registered in the current database. This includes all temporary functions Returns:(undocumented)Since: 2.0.0 listFunctions public abstract Dataset<Function> listFunctions(String dbName) throws AnalysisException Returns a list of functions registered in the specified database. This includes all temporary functions Parameters:dbName - (undocumented) Returns:(undocumented) Throws: AnalysisExceptionSince: 2.0.0 listColumns public abstract Dataset<Column> listColumns(String tableName) throws AnalysisException Returns a list of columns for the given table in the current database or the given temporary table. Parameters:tableName - (undocumented) Returns:(undocumented) Throws: AnalysisExceptionSince: 2.0.0 listColumns public abstract Dataset<Column> listColumns(String dbName, String tableName) throws AnalysisException Returns a list of columns for the given table in the specified database. Parameters:dbName - (undocumented)tableName - (undocumented) Returns:(undocumented) Throws: AnalysisExceptionSince: 2.0.0 createExternalTable public abstract Dataset<Row> createExternalTable(String tableName, String path) :: Experimental :: Creates an external table from the given path and returns the corresponding DataFrame. It will use the default data source configured by spark.sql.sources.default. Parameters:tableName - (undocumented)path - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public abstract Dataset<Row> createExternalTable(String tableName, String path, String source) :: Experimental :: Creates an external table from the given path based on a data source and returns the corresponding DataFrame. Parameters:tableName - (undocumented)path - (undocumented)source - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public abstract Dataset<Row> createExternalTable(String tableName, String source, java.util.Map<String,String> options) :: Experimental :: Creates an external table from the given path based on a data source and a set of options. Then, returns the corresponding DataFrame. Parameters:tableName - (undocumented)source - (undocumented)options - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public abstract Dataset<Row> createExternalTable(String tableName, String source, scala.collection.immutable.Map<String,String> options) :: Experimental :: (Scala-specific) Creates an external table from the given path based on a data source and a set of options. Then, returns the corresponding DataFrame. Parameters:tableName - (undocumented)source - (undocumented)options - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public abstract Dataset<Row> createExternalTable(String tableName, String source, StructType schema, java.util.Map<String,String> options) :: Experimental :: Create an external table from the given path based on a data source, a schema and a set of options. Then, returns the corresponding DataFrame. Parameters:tableName - (undocumented)source - (undocumented)schema - (undocumented)options - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public abstract Dataset<Row> createExternalTable(String tableName, String source, StructType schema, scala.collection.immutable.Map<String,String> options) :: Experimental :: (Scala-specific) Create an external table from the given path based on a data source, a schema and a set of options. Then, returns the corresponding DataFrame. Parameters:tableName - (undocumented)source - (undocumented)schema - (undocumented)options - (undocumented) Returns:(undocumented)Since: 2.0.0 dropTempView public abstract void dropTempView(String viewName) Drops the temporary view with the given view name in the catalog. If the view has been cached before, then it will also be uncached. Parameters:viewName - the name of the view to be dropped.Since: 2.0.0 isCached public abstract boolean isCached(String tableName) Returns true if the table is currently cached in-memory. Parameters:tableName - (undocumented) Returns:(undocumented)Since: 2.0.0 cacheTable public abstract void cacheTable(String tableName) Caches the specified table in-memory. Parameters:tableName - (undocumented)Since: 2.0.0 uncacheTable public abstract void uncacheTable(String tableName) Removes the specified table from the in-memory cache. Parameters:tableName - (undocumented)Since: 2.0.0 clearCache public abstract void clearCache() Removes all cached tables from the in-memory cache. Since: 2.0.0 refreshTable public abstract void refreshTable(String tableName) Invalidate and refresh all the cached metadata of the given table. For performance reasons, Spark SQL or the external data source library it uses might cache certain metadata about a table, such as the location of blocks. When those change outside of Spark SQL, users should call this function to invalidate the cache. If this table is cached as an InMemoryRelation, drop the original cached version and make the new version cached lazily. Parameters:tableName - (undocumented)Since: 2.0.0 refreshByPath public abstract void refreshByPath(String path) Invalidate and refresh all the cached data (and the associated metadata) for any dataframe that contains the given data source path. Parameters:path - (undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CatalogImpl (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CatalogImpl (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.internal Class CatalogImpl Object org.apache.spark.sql.catalog.Catalog org.apache.spark.sql.internal.CatalogImpl public class CatalogImpl extends Catalog Internal implementation of the user-facing Catalog. Constructor Summary Constructors  Constructor and Description CatalogImpl(SparkSession sparkSession)  Method Summary Methods  Modifier and Type Method and Description void cacheTable(String tableName) Caches the specified table in-memory. void clearCache() Removes all cached tables from the in-memory cache. Dataset<Row> createExternalTable(String tableName, String path) :: Experimental :: Creates an external table from the given path and returns the corresponding DataFrame. Dataset<Row> createExternalTable(String tableName, String source, java.util.Map<String,String> options) :: Experimental :: Creates an external table from the given path based on a data source and a set of options. Dataset<Row> createExternalTable(String tableName, String source, scala.collection.immutable.Map<String,String> options) :: Experimental :: (Scala-specific) Creates an external table from the given path based on a data source and a set of options. Dataset<Row> createExternalTable(String tableName, String path, String source) :: Experimental :: Creates an external table from the given path based on a data source and returns the corresponding DataFrame. Dataset<Row> createExternalTable(String tableName, String source, StructType schema, java.util.Map<String,String> options) :: Experimental :: Create an external table from the given path based on a data source, a schema and a set of options. Dataset<Row> createExternalTable(String tableName, String source, StructType schema, scala.collection.immutable.Map<String,String> options) :: Experimental :: (Scala-specific) Create an external table from the given path based on a data source, a schema and a set of options. String currentDatabase() Returns the current default database in this session. void dropTempView(String viewName) Drops the temporary view with the given view name in the catalog. boolean isCached(String tableName) Returns true if the table is currently cached in-memory. Dataset<Column> listColumns(String tableName) Returns a list of columns for the given table in the current database. Dataset<Column> listColumns(String dbName, String tableName) Returns a list of columns for the given table in the specified database. Dataset<Database> listDatabases() Returns a list of databases available across all sessions. Dataset<Function> listFunctions() Returns a list of functions registered in the current database. Dataset<Function> listFunctions(String dbName) Returns a list of functions registered in the specified database. Dataset<Table> listTables() Returns a list of tables in the current database. Dataset<Table> listTables(String dbName) Returns a list of tables in the specified database. static <T extends org.apache.spark.sql.catalyst.DefinedByConstructorParams> Dataset<T> makeDataset(scala.collection.Seq<T> data, SparkSession sparkSession, scala.reflect.api.TypeTags.TypeTag<T> evidence$1)  void refreshByPath(String resourcePath) Refresh the cache entry and the associated metadata for all dataframes (if any), that contain the given data source path. void refreshTable(String tableName) Refresh the cache entry for a table, if any. void setCurrentDatabase(String dbName) Sets the current default database in this session. void uncacheTable(String tableName) Removes the specified table from the in-memory cache. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CatalogImpl public CatalogImpl(SparkSession sparkSession) Method Detail makeDataset public static <T extends org.apache.spark.sql.catalyst.DefinedByConstructorParams> Dataset<T> makeDataset(scala.collection.Seq<T> data, SparkSession sparkSession, scala.reflect.api.TypeTags.TypeTag<T> evidence$1) currentDatabase public String currentDatabase() Returns the current default database in this session. Specified by: currentDatabase in class Catalog Returns:(undocumented) setCurrentDatabase public void setCurrentDatabase(String dbName) throws AnalysisException Sets the current default database in this session. Specified by: setCurrentDatabase in class Catalog Parameters:dbName - (undocumented) Throws: AnalysisException listDatabases public Dataset<Database> listDatabases() Returns a list of databases available across all sessions. Specified by: listDatabases in class Catalog Returns:(undocumented) listTables public Dataset<Table> listTables() Returns a list of tables in the current database. This includes all temporary tables. Specified by: listTables in class Catalog Returns:(undocumented) listTables public Dataset<Table> listTables(String dbName) throws AnalysisException Returns a list of tables in the specified database. This includes all temporary tables. Specified by: listTables in class Catalog Parameters:dbName - (undocumented) Returns:(undocumented) Throws: AnalysisException listFunctions public Dataset<Function> listFunctions() Returns a list of functions registered in the current database. This includes all temporary functions Specified by: listFunctions in class Catalog Returns:(undocumented) listFunctions public Dataset<Function> listFunctions(String dbName) throws AnalysisException Returns a list of functions registered in the specified database. This includes all temporary functions Specified by: listFunctions in class Catalog Parameters:dbName - (undocumented) Returns:(undocumented) Throws: AnalysisException listColumns public Dataset<Column> listColumns(String tableName) throws AnalysisException Returns a list of columns for the given table in the current database. Specified by: listColumns in class Catalog Parameters:tableName - (undocumented) Returns:(undocumented) Throws: AnalysisException listColumns public Dataset<Column> listColumns(String dbName, String tableName) throws AnalysisException Returns a list of columns for the given table in the specified database. Specified by: listColumns in class Catalog Parameters:dbName - (undocumented)tableName - (undocumented) Returns:(undocumented) Throws: AnalysisException createExternalTable public Dataset<Row> createExternalTable(String tableName, String path) :: Experimental :: Creates an external table from the given path and returns the corresponding DataFrame. It will use the default data source configured by spark.sql.sources.default. Specified by: createExternalTable in class Catalog Parameters:tableName - (undocumented)path - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public Dataset<Row> createExternalTable(String tableName, String path, String source) :: Experimental :: Creates an external table from the given path based on a data source and returns the corresponding DataFrame. Specified by: createExternalTable in class Catalog Parameters:tableName - (undocumented)path - (undocumented)source - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public Dataset<Row> createExternalTable(String tableName, String source, java.util.Map<String,String> options) :: Experimental :: Creates an external table from the given path based on a data source and a set of options. Then, returns the corresponding DataFrame. Specified by: createExternalTable in class Catalog Parameters:tableName - (undocumented)source - (undocumented)options - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public Dataset<Row> createExternalTable(String tableName, String source, scala.collection.immutable.Map<String,String> options) :: Experimental :: (Scala-specific) Creates an external table from the given path based on a data source and a set of options. Then, returns the corresponding DataFrame. Specified by: createExternalTable in class Catalog Parameters:tableName - (undocumented)source - (undocumented)options - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public Dataset<Row> createExternalTable(String tableName, String source, StructType schema, java.util.Map<String,String> options) :: Experimental :: Create an external table from the given path based on a data source, a schema and a set of options. Then, returns the corresponding DataFrame. Specified by: createExternalTable in class Catalog Parameters:tableName - (undocumented)source - (undocumented)schema - (undocumented)options - (undocumented) Returns:(undocumented)Since: 2.0.0 createExternalTable public Dataset<Row> createExternalTable(String tableName, String source, StructType schema, scala.collection.immutable.Map<String,String> options) :: Experimental :: (Scala-specific) Create an external table from the given path based on a data source, a schema and a set of options. Then, returns the corresponding DataFrame. Specified by: createExternalTable in class Catalog Parameters:tableName - (undocumented)source - (undocumented)schema - (undocumented)options - (undocumented) Returns:(undocumented)Since: 2.0.0 dropTempView public void dropTempView(String viewName) Drops the temporary view with the given view name in the catalog. If the view has been cached/persisted before, it's also unpersisted. Specified by: dropTempView in class Catalog Parameters:viewName - the name of the view to be dropped.Since: 2.0.0 isCached public boolean isCached(String tableName) Returns true if the table is currently cached in-memory. Specified by: isCached in class Catalog Parameters:tableName - (undocumented) Returns:(undocumented)Since: 2.0.0 cacheTable public void cacheTable(String tableName) Caches the specified table in-memory. Specified by: cacheTable in class Catalog Parameters:tableName - (undocumented)Since: 2.0.0 uncacheTable public void uncacheTable(String tableName) Removes the specified table from the in-memory cache. Specified by: uncacheTable in class Catalog Parameters:tableName - (undocumented)Since: 2.0.0 clearCache public void clearCache() Removes all cached tables from the in-memory cache. Specified by: clearCache in class Catalog Since: 2.0.0 refreshTable public void refreshTable(String tableName) Refresh the cache entry for a table, if any. For Hive metastore table, the metadata is refreshed. Specified by: refreshTable in class Catalog Parameters:tableName - (undocumented)Since: 2.0.0 refreshByPath public void refreshByPath(String resourcePath) Refresh the cache entry and the associated metadata for all dataframes (if any), that contain the given data source path. Specified by: refreshByPath in class Catalog Parameters:resourcePath - (undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CatalystScan (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CatalystScan (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Interface CatalystScan public interface CatalystScan ::Experimental:: An interface for experimenting with a more direct connection to the query planner. Compared to PrunedFilteredScan, this operator receives the raw expressions from the LogicalPlan. Unlike the other APIs this interface is NOT designed to be binary compatible across releases and thus should only be used for experimentation. Since: 1.3.0 Method Summary Methods  Modifier and Type Method and Description RDD<Row> buildScan(scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> requiredColumns, scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> filters)  Method Detail buildScan RDD<Row> buildScan(scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> requiredColumns, scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> filters) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CategoricalSplit (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CategoricalSplit (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class CategoricalSplit Object org.apache.spark.ml.tree.CategoricalSplit All Implemented Interfaces: java.io.Serializable, Split public class CategoricalSplit extends Object implements Split Split which tests a categorical feature. param: featureIndex Index of the feature to test param: _leftCategories If the feature value is in this set of categories, then the split goes left. Otherwise, it goes right. param: numCategories Number of categories for this feature. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description boolean equals(Object o)  int featureIndex() Index of feature which this split tests int hashCode()  double[] leftCategories() Get sorted categories which split to the left int numCategories()  double[] rightCategories() Get sorted categories which split to the right Methods inherited from class Object getClass, notify, notifyAll, toString, wait, wait, wait Method Detail featureIndex public int featureIndex() Description copied from interface: Split Index of feature which this split tests Specified by: featureIndex in interface Split numCategories public int numCategories() hashCode public int hashCode() Overrides: hashCode in class Object equals public boolean equals(Object o) Overrides: equals in class Object leftCategories public double[] leftCategories() Get sorted categories which split to the left rightCategories public double[] rightCategories() Get sorted categories which split to the right Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CausedBy (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CausedBy (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class CausedBy Object org.apache.spark.util.CausedBy public class CausedBy extends Object Extractor Object for pulling out the root cause of an error. If the error contains no cause, it will return the error itself. Usage: try { ... } catch { case CausedBy(ex: CommitDeniedException) => ... } Constructor Summary Constructors  Constructor and Description CausedBy()  Method Summary Methods  Modifier and Type Method and Description static scala.Option<Throwable> unapply(Throwable e)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CausedBy public CausedBy() Method Detail unapply public static scala.Option<Throwable> unapply(Throwable e) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CheckpointReader (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CheckpointReader (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming Class CheckpointReader Object org.apache.spark.streaming.CheckpointReader public class CheckpointReader extends Object Constructor Summary Constructors  Constructor and Description CheckpointReader()  Method Summary Methods  Modifier and Type Method and Description static scala.Option<org.apache.spark.streaming.Checkpoint> read(String checkpointDir) Read checkpoint files present in the given checkpoint directory. static scala.Option<org.apache.spark.streaming.Checkpoint> read(String checkpointDir, SparkConf conf, org.apache.hadoop.conf.Configuration hadoopConf, boolean ignoreReadError) Read checkpoint files present in the given checkpoint directory. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CheckpointReader public CheckpointReader() Method Detail read public static scala.Option<org.apache.spark.streaming.Checkpoint> read(String checkpointDir) Read checkpoint files present in the given checkpoint directory. If there are no checkpoint files, then return None, else try to return the latest valid checkpoint object. If no checkpoint files could be read correctly, then return None. Parameters:checkpointDir - (undocumented) Returns:(undocumented) read public static scala.Option<org.apache.spark.streaming.Checkpoint> read(String checkpointDir, SparkConf conf, org.apache.hadoop.conf.Configuration hadoopConf, boolean ignoreReadError) Read checkpoint files present in the given checkpoint directory. If there are no checkpoint files, then return None, else try to return the latest valid checkpoint object. If no checkpoint files could be read correctly, then return None (if ignoreReadError = true), or throw exception (if ignoreReadError = false). Parameters:checkpointDir - (undocumented)conf - (undocumented)hadoopConf - (undocumented)ignoreReadError - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CheckpointState (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CheckpointState (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class CheckpointState Object org.apache.spark.rdd.CheckpointState public class CheckpointState extends Object Enumeration to manage state transitions of an RDD through checkpointing [ Initialized --> checkpointing in progress --> checkpointed ]. Constructor Summary Constructors  Constructor and Description CheckpointState()  Method Summary Methods  Modifier and Type Method and Description static scala.Enumeration.Value apply(int x)  static scala.Enumeration.Value Checkpointed()  static scala.Enumeration.Value CheckpointingInProgress()  static scala.Enumeration.Value Initialized()  static int maxId()  static String toString()  static scala.Enumeration.ValueSet values()  static scala.Enumeration.Value withName(String s)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CheckpointState public CheckpointState() Method Detail Initialized public static scala.Enumeration.Value Initialized() CheckpointingInProgress public static scala.Enumeration.Value CheckpointingInProgress() Checkpointed public static scala.Enumeration.Value Checkpointed() toString public static String toString() values public static scala.Enumeration.ValueSet values() maxId public static final int maxId() apply public static final scala.Enumeration.Value apply(int x) withName public static final scala.Enumeration.Value withName(String s) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ChiSqSelector (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ChiSqSelector (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class ChiSqSelector Object org.apache.spark.mllib.feature.ChiSqSelector All Implemented Interfaces: java.io.Serializable public class ChiSqSelector extends Object implements scala.Serializable Creates a ChiSquared feature selector. param: numTopFeatures number of features that selector will select (ordered by statistic value descending) Note that if the number of features is less than numTopFeatures, then this will select all features. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ChiSqSelector(int numTopFeatures)  Method Summary Methods  Modifier and Type Method and Description ChiSqSelectorModel fit(RDD<LabeledPoint> data) Returns a ChiSquared feature selector. int numTopFeatures()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ChiSqSelector public ChiSqSelector(int numTopFeatures) Method Detail numTopFeatures public int numTopFeatures() fit public ChiSqSelectorModel fit(RDD<LabeledPoint> data) Returns a ChiSquared feature selector. Parameters:data - an RDD[LabeledPoint] containing the labeled dataset with categorical features. Real-valued features will be treated as categorical for each distinct value. Apply feature discretizer before using this function. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ChiSqSelectorModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ChiSqSelectorModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class ChiSqSelectorModel.SaveLoadV1_0$ Object org.apache.spark.mllib.feature.ChiSqSelectorModel.SaveLoadV1_0$ Enclosing class: ChiSqSelectorModel public static class ChiSqSelectorModel.SaveLoadV1_0$ extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description class  ChiSqSelectorModel.SaveLoadV1_0$.Data Model data for import/export Field Summary Fields  Modifier and Type Field and Description static ChiSqSelectorModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description ChiSqSelectorModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description ChiSqSelectorModel load(SparkContext sc, String path)  void save(SparkContext sc, ChiSqSelectorModel model, String path)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final ChiSqSelectorModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail ChiSqSelectorModel.SaveLoadV1_0$ public ChiSqSelectorModel.SaveLoadV1_0$() Method Detail save public void save(SparkContext sc, ChiSqSelectorModel model, String path) load public ChiSqSelectorModel load(SparkContext sc, String path) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ChiSqSelectorModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ChiSqSelectorModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class ChiSqSelectorModel Object org.apache.spark.mllib.feature.ChiSqSelectorModel All Implemented Interfaces: java.io.Serializable, VectorTransformer, Saveable public class ChiSqSelectorModel extends Object implements VectorTransformer, Saveable Chi Squared selector model. param: selectedFeatures list of indices to select (filter). Must be ordered asc See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  ChiSqSelectorModel.SaveLoadV1_0$  Constructor Summary Constructors  Constructor and Description ChiSqSelectorModel(int[] selectedFeatures)  Method Summary Methods  Modifier and Type Method and Description static ChiSqSelectorModel load(SparkContext sc, String path)  void save(SparkContext sc, String path) Save this model to the given path. int[] selectedFeatures()  Vector transform(Vector vector) Applies transformation on a vector. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.feature.VectorTransformer transform, transform Constructor Detail ChiSqSelectorModel public ChiSqSelectorModel(int[] selectedFeatures) Method Detail load public static ChiSqSelectorModel load(SparkContext sc, String path) selectedFeatures public int[] selectedFeatures() transform public Vector transform(Vector vector) Applies transformation on a vector. Specified by: transform in interface VectorTransformer Parameters:vector - vector to be transformed. Returns:transformed vector. save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ChiSqTest.Method$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ChiSqTest.Method$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.test Class ChiSqTest.Method$ Object scala.runtime.AbstractFunction2<String,scala.Function2<Object,Object,Object>,ChiSqTest.Method> org.apache.spark.mllib.stat.test.ChiSqTest.Method$ All Implemented Interfaces: java.io.Serializable, scala.Function2<String,scala.Function2<Object,Object,Object>,ChiSqTest.Method> Enclosing class: ChiSqTest public static class ChiSqTest.Method$ extends scala.runtime.AbstractFunction2<String,scala.Function2<Object,Object,Object>,ChiSqTest.Method> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static ChiSqTest.Method$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description ChiSqTest.Method$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction2 apply$mcDDD$sp, apply$mcDDI$sp, apply$mcDDJ$sp, apply$mcDID$sp, apply$mcDII$sp, apply$mcDIJ$sp, apply$mcDJD$sp, apply$mcDJI$sp, apply$mcDJJ$sp, apply$mcFDD$sp, apply$mcFDI$sp, apply$mcFDJ$sp, apply$mcFID$sp, apply$mcFII$sp, apply$mcFIJ$sp, apply$mcFJD$sp, apply$mcFJI$sp, apply$mcFJJ$sp, apply$mcIDD$sp, apply$mcIDI$sp, apply$mcIDJ$sp, apply$mcIID$sp, apply$mcIII$sp, apply$mcIIJ$sp, apply$mcIJD$sp, apply$mcIJI$sp, apply$mcIJJ$sp, apply$mcJDD$sp, apply$mcJDI$sp, apply$mcJDJ$sp, apply$mcJID$sp, apply$mcJII$sp, apply$mcJIJ$sp, apply$mcJJD$sp, apply$mcJJI$sp, apply$mcJJJ$sp, apply$mcVDD$sp, apply$mcVDI$sp, apply$mcVDJ$sp, apply$mcVID$sp, apply$mcVII$sp, apply$mcVIJ$sp, apply$mcVJD$sp, apply$mcVJI$sp, apply$mcVJJ$sp, apply$mcZDD$sp, apply$mcZDI$sp, apply$mcZDJ$sp, apply$mcZID$sp, apply$mcZII$sp, apply$mcZIJ$sp, apply$mcZJD$sp, apply$mcZJI$sp, apply$mcZJJ$sp, curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function2 apply Field Detail MODULE$ public static final ChiSqTest.Method$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail ChiSqTest.Method$ public ChiSqTest.Method$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ChiSqTest.Method (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ChiSqTest.Method (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.test Class ChiSqTest.Method Object org.apache.spark.mllib.stat.test.ChiSqTest.Method All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: ChiSqTest public static class ChiSqTest.Method extends Object implements scala.Product, scala.Serializable param: name String name for the method. param: chiSqFunc Function for computing the statistic given the observed and expected counts. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ChiSqTest.Method(String name, scala.Function2<Object,Object,Object> chiSqFunc)  Method Summary Methods  Modifier and Type Method and Description scala.Function2<Object,Object,Object> chiSqFunc()  String name()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail ChiSqTest.Method public ChiSqTest.Method(String name, scala.Function2<Object,Object,Object> chiSqFunc) Method Detail name public String name() chiSqFunc public scala.Function2<Object,Object,Object> chiSqFunc() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ChiSqTest.NullHypothesis$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ChiSqTest.NullHypothesis$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.test Class ChiSqTest.NullHypothesis$ Object scala.Enumeration org.apache.spark.mllib.stat.test.ChiSqTest.NullHypothesis$ All Implemented Interfaces: java.io.Serializable Enclosing class: ChiSqTest public static class ChiSqTest.NullHypothesis$ extends scala.Enumeration See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from class scala.Enumeration scala.Enumeration.Val, scala.Enumeration.Value, scala.Enumeration.ValueOrdering$, scala.Enumeration.ValueSet, scala.Enumeration.ValueSet$ Field Summary Fields  Modifier and Type Field and Description static ChiSqTest.NullHypothesis$ MODULE$ Static reference to the singleton instance of this Scala object. Fields inherited from class scala.Enumeration serialVersionUID Constructor Summary Constructors  Constructor and Description ChiSqTest.NullHypothesis$()  Method Summary Methods  Modifier and Type Method and Description scala.Enumeration.Value goodnessOfFit()  scala.Enumeration.Value independence()  Methods inherited from class scala.Enumeration apply, maxId, nextId_$eq, nextId, nextName_$eq, nextName, readResolve, scala$Enumeration$$bottomId_$eq, scala$Enumeration$$bottomId, scala$Enumeration$$isValDef$1, scala$Enumeration$$nameOf, scala$Enumeration$$nextNameOrNull, scala$Enumeration$$nmap, scala$Enumeration$$populateNameMap, scala$Enumeration$$topId_$eq, scala$Enumeration$$topId, scala$Enumeration$$vmap, scala$Enumeration$$vsetDefined_$eq, toString, Value, Value, Value, Value, ValueOrdering, values, ValueSet, withName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Field Detail MODULE$ public static final ChiSqTest.NullHypothesis$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail ChiSqTest.NullHypothesis$ public ChiSqTest.NullHypothesis$() Method Detail goodnessOfFit public scala.Enumeration.Value goodnessOfFit() independence public scala.Enumeration.Value independence() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ChiSqTest (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ChiSqTest (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.test Class ChiSqTest Object org.apache.spark.mllib.stat.test.ChiSqTest public class ChiSqTest extends Object Conduct the chi-squared test for the input RDDs using the specified method. Goodness-of-fit test is conducted on two Vectors, whereas test of independence is conducted on an input of type Matrix in which independence between columns is assessed. We also provide a method for computing the chi-squared statistic between each feature and the label for an input RDD[LabeledPoint], return an Array[ChiSquaredTestResult] of size = number of features in the input RDD. Supported methods for goodness of fit: pearson (default) Supported methods for independence: pearson (default) More information on Chi-squared test: http://en.wikipedia.org/wiki/Chi-squared_test Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  ChiSqTest.Method param: name String name for the method. static class  ChiSqTest.Method$  static class  ChiSqTest.NullHypothesis$  Constructor Summary Constructors  Constructor and Description ChiSqTest()  Method Summary Methods  Modifier and Type Method and Description static ChiSqTestResult chiSquared(Vector observed, Vector expected, String methodName)  static ChiSqTestResult[] chiSquaredFeatures(RDD<LabeledPoint> data, String methodName) Conduct Pearson's independence test for each feature against the label across the input RDD. static ChiSqTestResult chiSquaredMatrix(Matrix counts, String methodName)  static ChiSqTest.Method PEARSON()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ChiSqTest public ChiSqTest() Method Detail PEARSON public static ChiSqTest.Method PEARSON() chiSquaredFeatures public static ChiSqTestResult[] chiSquaredFeatures(RDD<LabeledPoint> data, String methodName) Conduct Pearson's independence test for each feature against the label across the input RDD. The contingency table is constructed from the raw (feature, label) pairs and used to conduct the independence test. Returns an array containing the ChiSquaredTestResult for every feature against the label. Parameters:data - (undocumented)methodName - (undocumented) Returns:(undocumented) chiSquared public static ChiSqTestResult chiSquared(Vector observed, Vector expected, String methodName) chiSquaredMatrix public static ChiSqTestResult chiSquaredMatrix(Matrix counts, String methodName) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ChiSqTestResult (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ChiSqTestResult (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.test Class ChiSqTestResult Object org.apache.spark.mllib.stat.test.ChiSqTestResult All Implemented Interfaces: TestResult<Object> public class ChiSqTestResult extends Object implements TestResult<Object> Object containing the test results for the chi-squared hypothesis test. Method Summary Methods  Modifier and Type Method and Description int degreesOfFreedom() Returns the degree(s) of freedom of the hypothesis test. String method()  String nullHypothesis() Null hypothesis of the test. double pValue() The probability of obtaining a test statistic result at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. double statistic() Test statistic. String toString() String explaining the hypothesis test result. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Method Detail pValue public double pValue() Description copied from interface: TestResult The probability of obtaining a test statistic result at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. Specified by: pValue in interface TestResult<Object> Returns:(undocumented) degreesOfFreedom public int degreesOfFreedom() Description copied from interface: TestResult Returns the degree(s) of freedom of the hypothesis test. Return type should be Number(e.g. Int, Double) or tuples of Numbers for toString compatibility. Specified by: degreesOfFreedom in interface TestResult<Object> Returns:(undocumented) statistic public double statistic() Description copied from interface: TestResult Test statistic. Specified by: statistic in interface TestResult<Object> Returns:(undocumented) method public String method() nullHypothesis public String nullHypothesis() Description copied from interface: TestResult Null hypothesis of the test. Specified by: nullHypothesis in interface TestResult<Object> Returns:(undocumented) toString public String toString() Description copied from interface: TestResult String explaining the hypothesis test result. Specific classes implementing this trait should override this method to output test-specific information. Specified by: toString in interface TestResult<Object> Overrides: toString in class Object Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CholeskyDecomposition (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CholeskyDecomposition (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg Class CholeskyDecomposition Object org.apache.spark.mllib.linalg.CholeskyDecomposition public class CholeskyDecomposition extends Object Compute Cholesky decomposition. Constructor Summary Constructors  Constructor and Description CholeskyDecomposition()  Method Summary Methods  Modifier and Type Method and Description static double[] inverse(double[] UAi, int k) Computes the inverse of a real symmetric positive definite matrix A using the Cholesky factorization A = U**T*U. static double[] solve(double[] A, double[] bx) Solves a symmetric positive definite linear system via Cholesky factorization. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CholeskyDecomposition public CholeskyDecomposition() Method Detail solve public static double[] solve(double[] A, double[] bx) Solves a symmetric positive definite linear system via Cholesky factorization. The input arguments are modified in-place to store the factorization and the solution. Parameters:A - the upper triangular part of Abx - right-hand side Returns:the solution array inverse public static double[] inverse(double[] UAi, int k) Computes the inverse of a real symmetric positive definite matrix A using the Cholesky factorization A = U**T*U. The input arguments are modified in-place to store the inverse matrix. Parameters:UAi - the upper triangular factor U from the Cholesky factorization A = U**T*Uk - the dimension of A Returns:the upper triangle of the (symmetric) inverse of A Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ChunkedByteBufferInputStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ChunkedByteBufferInputStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.io Class ChunkedByteBufferInputStream Object java.io.InputStream org.apache.spark.util.io.ChunkedByteBufferInputStream All Implemented Interfaces: java.io.Closeable, AutoCloseable public class ChunkedByteBufferInputStream extends java.io.InputStream Reads data from a ChunkedByteBuffer. param: dispose if true, ChunkedByteBuffer.dispose() will be called at the end of the stream in order to close any memory-mapped files which back the buffer. Constructor Summary Constructors  Constructor and Description ChunkedByteBufferInputStream(org.apache.spark.util.io.ChunkedByteBuffer chunkedByteBuffer, boolean dispose)  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.util.io.ChunkedByteBuffer chunkedByteBuffer()  void close()  int read()  int read(byte[] dest, int offset, int length)  long skip(long bytes)  Methods inherited from class java.io.InputStream available, mark, markSupported, read, reset Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ChunkedByteBufferInputStream public ChunkedByteBufferInputStream(org.apache.spark.util.io.ChunkedByteBuffer chunkedByteBuffer, boolean dispose) Method Detail chunkedByteBuffer public org.apache.spark.util.io.ChunkedByteBuffer chunkedByteBuffer() read public int read() Specified by: read in class java.io.InputStream read public int read(byte[] dest, int offset, int length) Overrides: read in class java.io.InputStream skip public long skip(long bytes) Overrides: skip in class java.io.InputStream close public void close() Specified by: close in interface java.io.Closeable Specified by: close in interface AutoCloseable Overrides: close in class java.io.InputStream Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ClassificationModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ClassificationModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification Interface ClassificationModel All Superinterfaces: java.io.Serializable All Known Implementing Classes: LogisticRegressionModel, NaiveBayesModel, SVMModel public interface ClassificationModel extends scala.Serializable Represents a classification model that predicts to which of a set of categories an example belongs. The categories are represented by double values: 0.0, 1.0, 2.0, etc. Method Summary Methods  Modifier and Type Method and Description JavaRDD<Double> predict(JavaRDD<Vector> testData) Predict values for examples stored in a JavaRDD. RDD<Object> predict(RDD<Vector> testData) Predict values for the given data set using the model trained. double predict(Vector testData) Predict values for a single data point using the model trained. Method Detail predict RDD<Object> predict(RDD<Vector> testData) Predict values for the given data set using the model trained. Parameters:testData - RDD representing data points to be predicted Returns:an RDD[Double] where each entry contains the corresponding prediction predict double predict(Vector testData) Predict values for a single data point using the model trained. Parameters:testData - array representing a single data point Returns:predicted category from the trained model predict JavaRDD<Double> predict(JavaRDD<Vector> testData) Predict values for examples stored in a JavaRDD. Parameters:testData - JavaRDD representing data points to be predicted Returns:a JavaRDD[java.lang.Double] where each entry contains the corresponding prediction Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Classifier (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Classifier (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class Classifier<FeaturesType,E extends Classifier<FeaturesType,E,M>,M extends ClassificationModel<FeaturesType,M>> Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<FeaturesType,E,M> org.apache.spark.ml.classification.Classifier<FeaturesType,E,M> All Implemented Interfaces: java.io.Serializable, Params, Identifiable Direct Known Subclasses: ProbabilisticClassifier public abstract class Classifier<FeaturesType,E extends Classifier<FeaturesType,E,M>,M extends ClassificationModel<FeaturesType,M>> extends Predictor<FeaturesType,E,M> :: DeveloperApi :: Single-label binary or multiclass classification. Classes are indexed {0, 1, ..., numClasses - 1}. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Classifier()  Method Summary Methods  Modifier and Type Method and Description Param<String> featuresCol() Param for features column name. String getFeaturesCol()  String getLabelCol()  String getPredictionCol()  String getRawPredictionCol()  Param<String> labelCol() Param for label column name. Param<String> predictionCol() Param for prediction column name. Param<String> rawPredictionCol() Param for raw prediction (a.k.a. E setRawPredictionCol(String value)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Methods inherited from class org.apache.spark.ml.Predictor copy, fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copy, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail Classifier public Classifier() Method Detail setRawPredictionCol public E setRawPredictionCol(String value) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) rawPredictionCol public Param<String> rawPredictionCol() Param for raw prediction (a.k.a. confidence) column name. Returns:(undocumented) getRawPredictionCol public String getRawPredictionCol() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CleanAccum (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CleanAccum (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class CleanAccum Object org.apache.spark.CleanAccum All Implemented Interfaces: java.io.Serializable, CleanupTask, scala.Equals, scala.Product public class CleanAccum extends Object implements CleanupTask, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CleanAccum(long accId)  Method Summary Methods  Modifier and Type Method and Description long accId()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CleanAccum public CleanAccum(long accId) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() accId public long accId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CleanBroadcast (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CleanBroadcast (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class CleanBroadcast Object org.apache.spark.CleanBroadcast All Implemented Interfaces: java.io.Serializable, CleanupTask, scala.Equals, scala.Product public class CleanBroadcast extends Object implements CleanupTask, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CleanBroadcast(long broadcastId)  Method Summary Methods  Modifier and Type Method and Description long broadcastId()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CleanBroadcast public CleanBroadcast(long broadcastId) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() broadcastId public long broadcastId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CleanCheckpoint (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CleanCheckpoint (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class CleanCheckpoint Object org.apache.spark.CleanCheckpoint All Implemented Interfaces: java.io.Serializable, CleanupTask, scala.Equals, scala.Product public class CleanCheckpoint extends Object implements CleanupTask, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CleanCheckpoint(int rddId)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  int rddId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CleanCheckpoint public CleanCheckpoint(int rddId) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() rddId public int rddId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CleanRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CleanRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class CleanRDD Object org.apache.spark.CleanRDD All Implemented Interfaces: java.io.Serializable, CleanupTask, scala.Equals, scala.Product public class CleanRDD extends Object implements CleanupTask, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CleanRDD(int rddId)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  int rddId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CleanRDD public CleanRDD(int rddId) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() rddId public int rddId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CleanShuffle (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CleanShuffle (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class CleanShuffle Object org.apache.spark.CleanShuffle All Implemented Interfaces: java.io.Serializable, CleanupTask, scala.Equals, scala.Product public class CleanShuffle extends Object implements CleanupTask, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CleanShuffle(int shuffleId)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  int shuffleId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CleanShuffle public CleanShuffle(int shuffleId) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() shuffleId public int shuffleId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CleanupTask (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CleanupTask (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Interface CleanupTask All Known Implementing Classes: CleanAccum, CleanBroadcast, CleanCheckpoint, CleanRDD, CleanShuffle public interface CleanupTask Classes that represent cleaning tasks. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CleanupTaskWeakReference (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CleanupTaskWeakReference (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class CleanupTaskWeakReference Object java.lang.ref.Reference<T> java.lang.ref.WeakReference<Object> org.apache.spark.CleanupTaskWeakReference public class CleanupTaskWeakReference extends java.lang.ref.WeakReference<Object> A WeakReference associated with a CleanupTask. When the referent object becomes only weakly reachable, the corresponding CleanupTaskWeakReference is automatically added to the given reference queue. Constructor Summary Constructors  Constructor and Description CleanupTaskWeakReference(CleanupTask task, Object referent, java.lang.ref.ReferenceQueue<Object> referenceQueue)  Method Summary Methods  Modifier and Type Method and Description CleanupTask task()  Methods inherited from class java.lang.ref.Reference clear, enqueue, get, isEnqueued Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CleanupTaskWeakReference public CleanupTaskWeakReference(CleanupTask task, Object referent, java.lang.ref.ReferenceQueue<Object> referenceQueue) Method Detail task public CleanupTask task() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ClosureCleaner (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ClosureCleaner (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class ClosureCleaner Object org.apache.spark.util.ClosureCleaner public class ClosureCleaner extends Object A cleaner that renders closures serializable if they can be done so safely. Constructor Summary Constructors  Constructor and Description ClosureCleaner()  Method Summary Methods  Modifier and Type Method and Description static void clean(Object closure, boolean checkSerializable, boolean cleanTransitively) Clean the given closure in place. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ClosureCleaner public ClosureCleaner() Method Detail clean public static void clean(Object closure, boolean checkSerializable, boolean cleanTransitively) Clean the given closure in place. More specifically, this renders the given closure serializable as long as it does not explicitly reference unserializable objects. Parameters:closure - the closure to cleancheckSerializable - whether to verify that the closure is serializable after cleaningcleanTransitively - whether to clean enclosing closures transitively Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoGroupFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoGroupFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface CoGroupFunction<K,V1,V2,R> All Superinterfaces: java.io.Serializable public interface CoGroupFunction<K,V1,V2,R> extends java.io.Serializable A function that returns zero or more output records from each grouping key and its values from 2 Datasets. Method Summary Methods  Modifier and Type Method and Description java.util.Iterator<R> call(K key, java.util.Iterator<V1> left, java.util.Iterator<V2> right)  Method Detail call java.util.Iterator<R> call(K key, java.util.Iterator<V1> left, java.util.Iterator<V2> right) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoGroupedRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoGroupedRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class CoGroupedRDD<K> Object org.apache.spark.rdd.RDD<scala.Tuple2<K,scala.collection.Iterable<?>[]>> org.apache.spark.rdd.CoGroupedRDD<K> All Implemented Interfaces: java.io.Serializable public class CoGroupedRDD<K> extends RDD<scala.Tuple2<K,scala.collection.Iterable<?>[]>> :: DeveloperApi :: A RDD that cogroups its parents. For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key. Note: This is an internal API. We recommend users use RDD.cogroup(...) instead of instantiating this directly. param: rdds parent RDDs. param: part partitioner used to partition the shuffle output See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoGroupedRDD(scala.collection.Seq<RDD<? extends scala.Product2<K,?>>> rdds, Partitioner part, scala.reflect.ClassTag<K> evidence$1)  Method Summary Methods  Modifier and Type Method and Description void clearDependencies() Clears the dependencies of this RDD. scala.collection.Iterator<scala.Tuple2<K,scala.collection.Iterable<?>[]>> compute(Partition s, TaskContext context) :: DeveloperApi :: Implemented by subclasses to compute a given partition. scala.collection.Seq<Dependency<?>> getDependencies() Implemented by subclasses to return how this RDD depends on parent RDDs. Partition[] getPartitions() Implemented by subclasses to return the set of partitions in this RDD. scala.Some<Partitioner> partitioner() Optionally overridden by subclasses to specify how they are partitioned. scala.collection.Seq<RDD<? extends scala.Product2<K,?>>> rdds()  CoGroupedRDD<K> setSerializer(Serializer serializer) Set a serializer for this RDD's shuffle, or null to use the default (spark.serializer) Methods inherited from class org.apache.spark.rdd.RDD aggregate, cache, cartesian, checkpoint, coalesce, collect, collect, context, count, countApprox, countApproxDistinct, countApproxDistinct, countByValue, countByValueApprox, dependencies, distinct, distinct, doubleRDDToDoubleRDDFunctions, filter, first, flatMap, fold, foreach, foreachPartition, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupBy, id, intersection, intersection, intersection, isCheckpointed, isEmpty, iterator, keyBy, localCheckpoint, map, mapPartitions, mapPartitionsWithIndex, max, min, name, numericRDDToDoubleRDDFunctions, partitions, persist, persist, pipe, pipe, pipe, preferredLocations, randomSplit, rddToAsyncRDDActions, rddToOrderedRDDFunctions, rddToPairRDDFunctions, rddToSequenceFileRDDFunctions, reduce, repartition, sample, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortBy, sparkContext, subtract, subtract, subtract, take, takeOrdered, takeSample, toDebugString, toJavaRDD, toLocalIterator, top, toString, treeAggregate, treeReduce, union, unpersist, zip, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail CoGroupedRDD public CoGroupedRDD(scala.collection.Seq<RDD<? extends scala.Product2<K,?>>> rdds, Partitioner part, scala.reflect.ClassTag<K> evidence$1) Method Detail rdds public scala.collection.Seq<RDD<? extends scala.Product2<K,?>>> rdds() setSerializer public CoGroupedRDD<K> setSerializer(Serializer serializer) Set a serializer for this RDD's shuffle, or null to use the default (spark.serializer) getDependencies public scala.collection.Seq<Dependency<?>> getDependencies() Description copied from class: RDD Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only be called once, so it is safe to implement a time-consuming computation in it. Returns:(undocumented) getPartitions public Partition[] getPartitions() Description copied from class: RDD Implemented by subclasses to return the set of partitions in this RDD. This method will only be called once, so it is safe to implement a time-consuming computation in it. The partitions in this array must satisfy the following property: rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index } Returns:(undocumented) partitioner public scala.Some<Partitioner> partitioner() Description copied from class: RDD Optionally overridden by subclasses to specify how they are partitioned. Overrides: partitioner in class RDD<scala.Tuple2<K,scala.collection.Iterable<?>[]>> compute public scala.collection.Iterator<scala.Tuple2<K,scala.collection.Iterable<?>[]>> compute(Partition s, TaskContext context) Description copied from class: RDD :: DeveloperApi :: Implemented by subclasses to compute a given partition. Specified by: compute in class RDD<scala.Tuple2<K,scala.collection.Iterable<?>[]>> Parameters:s - (undocumented)context - (undocumented) Returns:(undocumented) clearDependencies public void clearDependencies() Description copied from class: RDD Clears the dependencies of this RDD. This method must ensure that all references to the original parent RDDs is removed to enable the parent RDDs to be garbage collected. Subclasses of RDD may override this method for implementing their own cleaning logic. See UnionRDD for an example. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.AddWebUIFilter$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.AddWebUIFilter$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.AddWebUIFilter$ Object scala.runtime.AbstractFunction3<String,scala.collection.immutable.Map<String,String>,String,CoarseGrainedClusterMessages.AddWebUIFilter> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.AddWebUIFilter$ All Implemented Interfaces: java.io.Serializable, scala.Function3<String,scala.collection.immutable.Map<String,String>,String,CoarseGrainedClusterMessages.AddWebUIFilter> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.AddWebUIFilter$ extends scala.runtime.AbstractFunction3<String,scala.collection.immutable.Map<String,String>,String,CoarseGrainedClusterMessages.AddWebUIFilter> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.AddWebUIFilter$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.AddWebUIFilter$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction3 curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function3 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.AddWebUIFilter$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.AddWebUIFilter$ public CoarseGrainedClusterMessages.AddWebUIFilter$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.AddWebUIFilter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.AddWebUIFilter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.AddWebUIFilter Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.AddWebUIFilter All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.AddWebUIFilter extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.AddWebUIFilter(String filterName, scala.collection.immutable.Map<String,String> filterParams, String proxyBase)  Method Summary Methods  Modifier and Type Method and Description String filterName()  scala.collection.immutable.Map<String,String> filterParams()  String proxyBase()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.AddWebUIFilter public CoarseGrainedClusterMessages.AddWebUIFilter(String filterName, scala.collection.immutable.Map<String,String> filterParams, String proxyBase) Method Detail filterName public String filterName() filterParams public scala.collection.immutable.Map<String,String> filterParams() proxyBase public String proxyBase() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.GetExecutorLossReason$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.GetExecutorLossReason$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.GetExecutorLossReason$ Object scala.runtime.AbstractFunction1<String,CoarseGrainedClusterMessages.GetExecutorLossReason> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.GetExecutorLossReason$ All Implemented Interfaces: java.io.Serializable, scala.Function1<String,CoarseGrainedClusterMessages.GetExecutorLossReason> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.GetExecutorLossReason$ extends scala.runtime.AbstractFunction1<String,CoarseGrainedClusterMessages.GetExecutorLossReason> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.GetExecutorLossReason$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.GetExecutorLossReason$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.GetExecutorLossReason$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.GetExecutorLossReason$ public CoarseGrainedClusterMessages.GetExecutorLossReason$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.GetExecutorLossReason (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.GetExecutorLossReason (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.GetExecutorLossReason Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.GetExecutorLossReason All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.GetExecutorLossReason extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.GetExecutorLossReason(String executorId)  Method Summary Methods  Modifier and Type Method and Description String executorId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.GetExecutorLossReason public CoarseGrainedClusterMessages.GetExecutorLossReason(String executorId) Method Detail executorId public String executorId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.KillExecutors$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.KillExecutors$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.KillExecutors$ Object scala.runtime.AbstractFunction1<scala.collection.Seq<String>,CoarseGrainedClusterMessages.KillExecutors> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.KillExecutors$ All Implemented Interfaces: java.io.Serializable, scala.Function1<scala.collection.Seq<String>,CoarseGrainedClusterMessages.KillExecutors> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.KillExecutors$ extends scala.runtime.AbstractFunction1<scala.collection.Seq<String>,CoarseGrainedClusterMessages.KillExecutors> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.KillExecutors$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.KillExecutors$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.KillExecutors$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.KillExecutors$ public CoarseGrainedClusterMessages.KillExecutors$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.KillExecutors (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.KillExecutors (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.KillExecutors Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.KillExecutors All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.KillExecutors extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.KillExecutors(scala.collection.Seq<String> executorIds)  Method Summary Methods  Modifier and Type Method and Description scala.collection.Seq<String> executorIds()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.KillExecutors public CoarseGrainedClusterMessages.KillExecutors(scala.collection.Seq<String> executorIds) Method Detail executorIds public scala.collection.Seq<String> executorIds() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.KillTask$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.KillTask$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.KillTask$ Object scala.runtime.AbstractFunction3<Object,String,Object,CoarseGrainedClusterMessages.KillTask> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.KillTask$ All Implemented Interfaces: java.io.Serializable, scala.Function3<Object,String,Object,CoarseGrainedClusterMessages.KillTask> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.KillTask$ extends scala.runtime.AbstractFunction3<Object,String,Object,CoarseGrainedClusterMessages.KillTask> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.KillTask$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.KillTask$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction3 curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function3 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.KillTask$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.KillTask$ public CoarseGrainedClusterMessages.KillTask$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.KillTask (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.KillTask (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.KillTask Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.KillTask All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.KillTask extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.KillTask(long taskId, String executor, boolean interruptThread)  Method Summary Methods  Modifier and Type Method and Description String executor()  boolean interruptThread()  long taskId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.KillTask public CoarseGrainedClusterMessages.KillTask(long taskId, String executor, boolean interruptThread) Method Detail taskId public long taskId() executor public String executor() interruptThread public boolean interruptThread() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.LaunchTask$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.LaunchTask$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.LaunchTask$ Object scala.runtime.AbstractFunction1<org.apache.spark.util.SerializableBuffer,CoarseGrainedClusterMessages.LaunchTask> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.LaunchTask$ All Implemented Interfaces: java.io.Serializable, scala.Function1<org.apache.spark.util.SerializableBuffer,CoarseGrainedClusterMessages.LaunchTask> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.LaunchTask$ extends scala.runtime.AbstractFunction1<org.apache.spark.util.SerializableBuffer,CoarseGrainedClusterMessages.LaunchTask> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.LaunchTask$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.LaunchTask$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.LaunchTask$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.LaunchTask$ public CoarseGrainedClusterMessages.LaunchTask$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.LaunchTask (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.LaunchTask (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.LaunchTask Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.LaunchTask All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.LaunchTask extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.LaunchTask(org.apache.spark.util.SerializableBuffer data)  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.util.SerializableBuffer data()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.LaunchTask public CoarseGrainedClusterMessages.LaunchTask(org.apache.spark.util.SerializableBuffer data) Method Detail data public org.apache.spark.util.SerializableBuffer data() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RegisterClusterManager$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RegisterClusterManager$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RegisterClusterManager$ Object scala.runtime.AbstractFunction1<org.apache.spark.rpc.RpcEndpointRef,CoarseGrainedClusterMessages.RegisterClusterManager> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RegisterClusterManager$ All Implemented Interfaces: java.io.Serializable, scala.Function1<org.apache.spark.rpc.RpcEndpointRef,CoarseGrainedClusterMessages.RegisterClusterManager> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RegisterClusterManager$ extends scala.runtime.AbstractFunction1<org.apache.spark.rpc.RpcEndpointRef,CoarseGrainedClusterMessages.RegisterClusterManager> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.RegisterClusterManager$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RegisterClusterManager$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.RegisterClusterManager$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.RegisterClusterManager$ public CoarseGrainedClusterMessages.RegisterClusterManager$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RegisterClusterManager (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RegisterClusterManager (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RegisterClusterManager Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RegisterClusterManager All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RegisterClusterManager extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RegisterClusterManager(org.apache.spark.rpc.RpcEndpointRef am)  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.rpc.RpcEndpointRef am()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.RegisterClusterManager public CoarseGrainedClusterMessages.RegisterClusterManager(org.apache.spark.rpc.RpcEndpointRef am) Method Detail am public org.apache.spark.rpc.RpcEndpointRef am() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RegisterExecutor$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RegisterExecutor$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RegisterExecutor$ Object scala.runtime.AbstractFunction5<String,org.apache.spark.rpc.RpcEndpointRef,String,Object,scala.collection.immutable.Map<String,String>,CoarseGrainedClusterMessages.RegisterExecutor> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RegisterExecutor$ All Implemented Interfaces: java.io.Serializable, scala.Function5<String,org.apache.spark.rpc.RpcEndpointRef,String,Object,scala.collection.immutable.Map<String,String>,CoarseGrainedClusterMessages.RegisterExecutor> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RegisterExecutor$ extends scala.runtime.AbstractFunction5<String,org.apache.spark.rpc.RpcEndpointRef,String,Object,scala.collection.immutable.Map<String,String>,CoarseGrainedClusterMessages.RegisterExecutor> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.RegisterExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RegisterExecutor$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction5 curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function5 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.RegisterExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.RegisterExecutor$ public CoarseGrainedClusterMessages.RegisterExecutor$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RegisterExecutor (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RegisterExecutor (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RegisterExecutor Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RegisterExecutor All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RegisterExecutor extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RegisterExecutor(String executorId, org.apache.spark.rpc.RpcEndpointRef executorRef, String hostname, int cores, scala.collection.immutable.Map<String,String> logUrls)  Method Summary Methods  Modifier and Type Method and Description int cores()  String executorId()  org.apache.spark.rpc.RpcEndpointRef executorRef()  String hostname()  scala.collection.immutable.Map<String,String> logUrls()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.RegisterExecutor public CoarseGrainedClusterMessages.RegisterExecutor(String executorId, org.apache.spark.rpc.RpcEndpointRef executorRef, String hostname, int cores, scala.collection.immutable.Map<String,String> logUrls) Method Detail executorId public String executorId() executorRef public org.apache.spark.rpc.RpcEndpointRef executorRef() hostname public String hostname() cores public int cores() logUrls public scala.collection.immutable.Map<String,String> logUrls() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RegisterExecutorFailed$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RegisterExecutorFailed$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RegisterExecutorFailed$ Object scala.runtime.AbstractFunction1<String,CoarseGrainedClusterMessages.RegisterExecutorFailed> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RegisterExecutorFailed$ All Implemented Interfaces: java.io.Serializable, scala.Function1<String,CoarseGrainedClusterMessages.RegisterExecutorFailed> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RegisterExecutorFailed$ extends scala.runtime.AbstractFunction1<String,CoarseGrainedClusterMessages.RegisterExecutorFailed> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.RegisterExecutorFailed$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RegisterExecutorFailed$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.RegisterExecutorFailed$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.RegisterExecutorFailed$ public CoarseGrainedClusterMessages.RegisterExecutorFailed$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RegisterExecutorFailed (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RegisterExecutorFailed (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RegisterExecutorFailed Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RegisterExecutorFailed All Implemented Interfaces: java.io.Serializable, CoarseGrainedClusterMessages.RegisterExecutorResponse, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RegisterExecutorFailed extends Object implements CoarseGrainedClusterMessages.RegisterExecutorResponse, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RegisterExecutorFailed(String message)  Method Summary Methods  Modifier and Type Method and Description String message()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.RegisterExecutorFailed public CoarseGrainedClusterMessages.RegisterExecutorFailed(String message) Method Detail message public String message() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RegisterExecutorResponse (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RegisterExecutorResponse (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Interface CoarseGrainedClusterMessages.RegisterExecutorResponse All Known Implementing Classes: CoarseGrainedClusterMessages.RegisteredExecutor$, CoarseGrainedClusterMessages.RegisterExecutorFailed Enclosing class: CoarseGrainedClusterMessages public static interface CoarseGrainedClusterMessages.RegisterExecutorResponse Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RegisteredExecutor$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RegisteredExecutor$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RegisteredExecutor$ Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RegisteredExecutor$ All Implemented Interfaces: java.io.Serializable, CoarseGrainedClusterMessages.RegisterExecutorResponse, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RegisteredExecutor$ extends Object implements CoarseGrainedClusterMessages.RegisterExecutorResponse, scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.RegisteredExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RegisteredExecutor$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final CoarseGrainedClusterMessages.RegisteredExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.RegisteredExecutor$ public CoarseGrainedClusterMessages.RegisteredExecutor$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RemoveExecutor$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RemoveExecutor$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RemoveExecutor$ Object scala.runtime.AbstractFunction2<String,org.apache.spark.scheduler.ExecutorLossReason,CoarseGrainedClusterMessages.RemoveExecutor> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RemoveExecutor$ All Implemented Interfaces: java.io.Serializable, scala.Function2<String,org.apache.spark.scheduler.ExecutorLossReason,CoarseGrainedClusterMessages.RemoveExecutor> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RemoveExecutor$ extends scala.runtime.AbstractFunction2<String,org.apache.spark.scheduler.ExecutorLossReason,CoarseGrainedClusterMessages.RemoveExecutor> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.RemoveExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RemoveExecutor$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction2 apply$mcDDD$sp, apply$mcDDI$sp, apply$mcDDJ$sp, apply$mcDID$sp, apply$mcDII$sp, apply$mcDIJ$sp, apply$mcDJD$sp, apply$mcDJI$sp, apply$mcDJJ$sp, apply$mcFDD$sp, apply$mcFDI$sp, apply$mcFDJ$sp, apply$mcFID$sp, apply$mcFII$sp, apply$mcFIJ$sp, apply$mcFJD$sp, apply$mcFJI$sp, apply$mcFJJ$sp, apply$mcIDD$sp, apply$mcIDI$sp, apply$mcIDJ$sp, apply$mcIID$sp, apply$mcIII$sp, apply$mcIIJ$sp, apply$mcIJD$sp, apply$mcIJI$sp, apply$mcIJJ$sp, apply$mcJDD$sp, apply$mcJDI$sp, apply$mcJDJ$sp, apply$mcJID$sp, apply$mcJII$sp, apply$mcJIJ$sp, apply$mcJJD$sp, apply$mcJJI$sp, apply$mcJJJ$sp, apply$mcVDD$sp, apply$mcVDI$sp, apply$mcVDJ$sp, apply$mcVID$sp, apply$mcVII$sp, apply$mcVIJ$sp, apply$mcVJD$sp, apply$mcVJI$sp, apply$mcVJJ$sp, apply$mcZDD$sp, apply$mcZDI$sp, apply$mcZDJ$sp, apply$mcZID$sp, apply$mcZII$sp, apply$mcZIJ$sp, apply$mcZJD$sp, apply$mcZJI$sp, apply$mcZJJ$sp, curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function2 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.RemoveExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.RemoveExecutor$ public CoarseGrainedClusterMessages.RemoveExecutor$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RemoveExecutor (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RemoveExecutor (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RemoveExecutor Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RemoveExecutor All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RemoveExecutor extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RemoveExecutor(String executorId, org.apache.spark.scheduler.ExecutorLossReason reason)  Method Summary Methods  Modifier and Type Method and Description String executorId()  org.apache.spark.scheduler.ExecutorLossReason reason()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.RemoveExecutor public CoarseGrainedClusterMessages.RemoveExecutor(String executorId, org.apache.spark.scheduler.ExecutorLossReason reason) Method Detail executorId public String executorId() reason public org.apache.spark.scheduler.ExecutorLossReason reason() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RequestExecutors$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RequestExecutors$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RequestExecutors$ Object scala.runtime.AbstractFunction3<Object,Object,scala.collection.immutable.Map<String,Object>,CoarseGrainedClusterMessages.RequestExecutors> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RequestExecutors$ All Implemented Interfaces: java.io.Serializable, scala.Function3<Object,Object,scala.collection.immutable.Map<String,Object>,CoarseGrainedClusterMessages.RequestExecutors> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RequestExecutors$ extends scala.runtime.AbstractFunction3<Object,Object,scala.collection.immutable.Map<String,Object>,CoarseGrainedClusterMessages.RequestExecutors> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.RequestExecutors$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RequestExecutors$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction3 curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function3 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.RequestExecutors$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.RequestExecutors$ public CoarseGrainedClusterMessages.RequestExecutors$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RequestExecutors (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RequestExecutors (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RequestExecutors Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RequestExecutors All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RequestExecutors extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RequestExecutors(int requestedTotal, int localityAwareTasks, scala.collection.immutable.Map<String,Object> hostToLocalTaskCount)  Method Summary Methods  Modifier and Type Method and Description scala.collection.immutable.Map<String,Object> hostToLocalTaskCount()  int localityAwareTasks()  int requestedTotal()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.RequestExecutors public CoarseGrainedClusterMessages.RequestExecutors(int requestedTotal, int localityAwareTasks, scala.collection.immutable.Map<String,Object> hostToLocalTaskCount) Method Detail requestedTotal public int requestedTotal() localityAwareTasks public int localityAwareTasks() hostToLocalTaskCount public scala.collection.immutable.Map<String,Object> hostToLocalTaskCount() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$ Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$ extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$ public CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.RetrieveSparkProps$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.RetrieveSparkProps$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.RetrieveSparkProps$ Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.RetrieveSparkProps$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.RetrieveSparkProps$ extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.RetrieveSparkProps$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.RetrieveSparkProps$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final CoarseGrainedClusterMessages.RetrieveSparkProps$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.RetrieveSparkProps$ public CoarseGrainedClusterMessages.RetrieveSparkProps$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.ReviveOffers$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.ReviveOffers$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.ReviveOffers$ Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.ReviveOffers$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.ReviveOffers$ extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.ReviveOffers$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.ReviveOffers$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final CoarseGrainedClusterMessages.ReviveOffers$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.ReviveOffers$ public CoarseGrainedClusterMessages.ReviveOffers$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.SetupDriver$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.SetupDriver$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.SetupDriver$ Object scala.runtime.AbstractFunction1<org.apache.spark.rpc.RpcEndpointRef,CoarseGrainedClusterMessages.SetupDriver> org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.SetupDriver$ All Implemented Interfaces: java.io.Serializable, scala.Function1<org.apache.spark.rpc.RpcEndpointRef,CoarseGrainedClusterMessages.SetupDriver> Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.SetupDriver$ extends scala.runtime.AbstractFunction1<org.apache.spark.rpc.RpcEndpointRef,CoarseGrainedClusterMessages.SetupDriver> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.SetupDriver$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.SetupDriver$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function1 apply Field Detail MODULE$ public static final CoarseGrainedClusterMessages.SetupDriver$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.SetupDriver$ public CoarseGrainedClusterMessages.SetupDriver$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.SetupDriver (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.SetupDriver (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.SetupDriver Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.SetupDriver All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.SetupDriver extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.SetupDriver(org.apache.spark.rpc.RpcEndpointRef driver)  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.rpc.RpcEndpointRef driver()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.SetupDriver public CoarseGrainedClusterMessages.SetupDriver(org.apache.spark.rpc.RpcEndpointRef driver) Method Detail driver public org.apache.spark.rpc.RpcEndpointRef driver() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.Shutdown$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.Shutdown$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.Shutdown$ Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.Shutdown$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.Shutdown$ extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.Shutdown$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.Shutdown$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final CoarseGrainedClusterMessages.Shutdown$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.Shutdown$ public CoarseGrainedClusterMessages.Shutdown$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.StatusUpdate$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.StatusUpdate$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.StatusUpdate$ Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.StatusUpdate$ All Implemented Interfaces: java.io.Serializable Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.StatusUpdate$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.StatusUpdate$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.StatusUpdate$()  Method Summary Methods  Modifier and Type Method and Description CoarseGrainedClusterMessages.StatusUpdate apply(String executorId, long taskId, scala.Enumeration.Value state, java.nio.ByteBuffer data) Alternate factory method that takes a ByteBuffer directly for the data field Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final CoarseGrainedClusterMessages.StatusUpdate$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.StatusUpdate$ public CoarseGrainedClusterMessages.StatusUpdate$() Method Detail apply public CoarseGrainedClusterMessages.StatusUpdate apply(String executorId, long taskId, scala.Enumeration.Value state, java.nio.ByteBuffer data) Alternate factory method that takes a ByteBuffer directly for the data field Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.StatusUpdate (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.StatusUpdate (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.StatusUpdate Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.StatusUpdate All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.StatusUpdate extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.StatusUpdate(String executorId, long taskId, scala.Enumeration.Value state, org.apache.spark.util.SerializableBuffer data)  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.util.SerializableBuffer data()  String executorId()  scala.Enumeration.Value state()  long taskId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CoarseGrainedClusterMessages.StatusUpdate public CoarseGrainedClusterMessages.StatusUpdate(String executorId, long taskId, scala.Enumeration.Value state, org.apache.spark.util.SerializableBuffer data) Method Detail executorId public String executorId() taskId public long taskId() state public scala.Enumeration.Value state() data public org.apache.spark.util.SerializableBuffer data() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.StopDriver$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.StopDriver$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.StopDriver$ Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.StopDriver$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.StopDriver$ extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.StopDriver$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.StopDriver$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final CoarseGrainedClusterMessages.StopDriver$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.StopDriver$ public CoarseGrainedClusterMessages.StopDriver$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.StopExecutor$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.StopExecutor$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.StopExecutor$ Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.StopExecutor$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.StopExecutor$ extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.StopExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.StopExecutor$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final CoarseGrainedClusterMessages.StopExecutor$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.StopExecutor$ public CoarseGrainedClusterMessages.StopExecutor$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages.StopExecutors$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages.StopExecutors$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages.StopExecutors$ Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.StopExecutors$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: CoarseGrainedClusterMessages public static class CoarseGrainedClusterMessages.StopExecutors$ extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static CoarseGrainedClusterMessages.StopExecutors$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages.StopExecutors$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final CoarseGrainedClusterMessages.StopExecutors$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail CoarseGrainedClusterMessages.StopExecutors$ public CoarseGrainedClusterMessages.StopExecutors$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoarseGrainedClusterMessages (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoarseGrainedClusterMessages (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class CoarseGrainedClusterMessages Object org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages public class CoarseGrainedClusterMessages extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  CoarseGrainedClusterMessages.AddWebUIFilter  static class  CoarseGrainedClusterMessages.AddWebUIFilter$  static class  CoarseGrainedClusterMessages.GetExecutorLossReason  static class  CoarseGrainedClusterMessages.GetExecutorLossReason$  static class  CoarseGrainedClusterMessages.KillExecutors  static class  CoarseGrainedClusterMessages.KillExecutors$  static class  CoarseGrainedClusterMessages.KillTask  static class  CoarseGrainedClusterMessages.KillTask$  static class  CoarseGrainedClusterMessages.LaunchTask  static class  CoarseGrainedClusterMessages.LaunchTask$  static class  CoarseGrainedClusterMessages.RegisterClusterManager  static class  CoarseGrainedClusterMessages.RegisterClusterManager$  static class  CoarseGrainedClusterMessages.RegisteredExecutor$  static class  CoarseGrainedClusterMessages.RegisterExecutor  static class  CoarseGrainedClusterMessages.RegisterExecutor$  static class  CoarseGrainedClusterMessages.RegisterExecutorFailed  static class  CoarseGrainedClusterMessages.RegisterExecutorFailed$  static interface  CoarseGrainedClusterMessages.RegisterExecutorResponse  static class  CoarseGrainedClusterMessages.RemoveExecutor  static class  CoarseGrainedClusterMessages.RemoveExecutor$  static class  CoarseGrainedClusterMessages.RequestExecutors  static class  CoarseGrainedClusterMessages.RequestExecutors$  static class  CoarseGrainedClusterMessages.RetrieveLastAllocatedExecutorId$  static class  CoarseGrainedClusterMessages.RetrieveSparkProps$  static class  CoarseGrainedClusterMessages.ReviveOffers$  static class  CoarseGrainedClusterMessages.SetupDriver  static class  CoarseGrainedClusterMessages.SetupDriver$  static class  CoarseGrainedClusterMessages.Shutdown$  static class  CoarseGrainedClusterMessages.StatusUpdate  static class  CoarseGrainedClusterMessages.StatusUpdate$  static class  CoarseGrainedClusterMessages.StopDriver$  static class  CoarseGrainedClusterMessages.StopExecutor$  static class  CoarseGrainedClusterMessages.StopExecutors$  Constructor Summary Constructors  Constructor and Description CoarseGrainedClusterMessages()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CoarseGrainedClusterMessages public CoarseGrainedClusterMessages() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CodegenMetrics (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CodegenMetrics (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.metrics.source Class CodegenMetrics Object org.apache.spark.metrics.source.CodegenMetrics public class CodegenMetrics extends Object :: Experimental :: Metrics for code generation. Constructor Summary Constructors  Constructor and Description CodegenMetrics()  Method Summary Methods  Modifier and Type Method and Description static com.codahale.metrics.Histogram METRIC_COMPILATION_TIME() Histogram of the time it took to compile source code text (in milliseconds). static com.codahale.metrics.Histogram METRIC_GENERATED_CLASS_BYTECODE_SIZE() Histogram of the bytecode size of each class generated by CodeGenerator. static com.codahale.metrics.Histogram METRIC_GENERATED_METHOD_BYTECODE_SIZE() Histogram of the bytecode size of each method in classes generated by CodeGenerator. static com.codahale.metrics.Histogram METRIC_SOURCE_CODE_SIZE() Histogram of the length of source code text compiled by CodeGenerator (in characters). static com.codahale.metrics.MetricRegistry metricRegistry()  static String sourceName()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CodegenMetrics public CodegenMetrics() Method Detail sourceName public static String sourceName() metricRegistry public static com.codahale.metrics.MetricRegistry metricRegistry() METRIC_SOURCE_CODE_SIZE public static com.codahale.metrics.Histogram METRIC_SOURCE_CODE_SIZE() Histogram of the length of source code text compiled by CodeGenerator (in characters). Returns:(undocumented) METRIC_COMPILATION_TIME public static com.codahale.metrics.Histogram METRIC_COMPILATION_TIME() Histogram of the time it took to compile source code text (in milliseconds). Returns:(undocumented) METRIC_GENERATED_CLASS_BYTECODE_SIZE public static com.codahale.metrics.Histogram METRIC_GENERATED_CLASS_BYTECODE_SIZE() Histogram of the bytecode size of each class generated by CodeGenerator. Returns:(undocumented) METRIC_GENERATED_METHOD_BYTECODE_SIZE public static com.codahale.metrics.Histogram METRIC_GENERATED_METHOD_BYTECODE_SIZE() Histogram of the bytecode size of each method in classes generated by CodeGenerator. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CollectionAccumulator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CollectionAccumulator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class CollectionAccumulator<T> Object org.apache.spark.util.AccumulatorV2<T,java.util.List<T>> org.apache.spark.util.CollectionAccumulator<T> All Implemented Interfaces: java.io.Serializable public class CollectionAccumulator<T> extends AccumulatorV2<T,java.util.List<T>> An accumulator for collecting a list of elements. Since: 2.0.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CollectionAccumulator()  Method Summary Methods  Modifier and Type Method and Description void add(T v) Takes the inputs and accumulates. CollectionAccumulator<T> copy() Creates a new copy of this accumulator. CollectionAccumulator<T> copyAndReset() Creates a new copy of this accumulator, which is zero value. boolean isZero() Returns if this accumulator is zero value or not. void merge(AccumulatorV2<T,java.util.List<T>> other) Merges another same-type accumulator into this one and update its state, i.e. void reset() Resets this accumulator, which is zero value. java.util.List<T> value() Defines the current value of this accumulator Methods inherited from class org.apache.spark.util.AccumulatorV2 id, isRegistered, name, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail CollectionAccumulator public CollectionAccumulator() Method Detail isZero public boolean isZero() Description copied from class: AccumulatorV2 Returns if this accumulator is zero value or not. e.g. for a counter accumulator, 0 is zero value; for a list accumulator, Nil is zero value. Specified by: isZero in class AccumulatorV2<T,java.util.List<T>> Returns:(undocumented) copyAndReset public CollectionAccumulator<T> copyAndReset() Description copied from class: AccumulatorV2 Creates a new copy of this accumulator, which is zero value. i.e. call isZero on the copy must return true. Overrides: copyAndReset in class AccumulatorV2<T,java.util.List<T>> Returns:(undocumented) copy public CollectionAccumulator<T> copy() Description copied from class: AccumulatorV2 Creates a new copy of this accumulator. Specified by: copy in class AccumulatorV2<T,java.util.List<T>> Returns:(undocumented) reset public void reset() Description copied from class: AccumulatorV2 Resets this accumulator, which is zero value. i.e. call isZero must return true. Specified by: reset in class AccumulatorV2<T,java.util.List<T>> add public void add(T v) Description copied from class: AccumulatorV2 Takes the inputs and accumulates. Specified by: add in class AccumulatorV2<T,java.util.List<T>> Parameters:v - (undocumented) merge public void merge(AccumulatorV2<T,java.util.List<T>> other) Description copied from class: AccumulatorV2 Merges another same-type accumulator into this one and update its state, i.e. this should be merge-in-place. Specified by: merge in class AccumulatorV2<T,java.util.List<T>> Parameters:other - (undocumented) value public java.util.List<T> value() Description copied from class: AccumulatorV2 Defines the current value of this accumulator Specified by: value in class AccumulatorV2<T,java.util.List<T>> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CollectionsUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CollectionsUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class CollectionsUtils Object org.apache.spark.util.CollectionsUtils public class CollectionsUtils extends Object Constructor Summary Constructors  Constructor and Description CollectionsUtils()  Method Summary Methods  Modifier and Type Method and Description static <K> scala.Function2<Object,K,Object> makeBinarySearch(scala.math.Ordering<K> evidence$1, scala.reflect.ClassTag<K> evidence$2)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CollectionsUtils public CollectionsUtils() Method Detail makeBinarySearch public static <K> scala.Function2<Object,K,Object> makeBinarySearch(scala.math.Ordering<K> evidence$1, scala.reflect.ClassTag<K> evidence$2) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Column (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Column (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class Column Object org.apache.spark.sql.Column Direct Known Subclasses: ColumnName, TypedColumn public class Column extends Object A column that will be computed based on the data in a DataFrame. A new column is constructed based on the input columns present in a dataframe: df("columnName") // On a specific DataFrame. col("columnName") // A generic column no yet associated with a DataFrame. col("columnName.field") // Extracting a struct field col("`a.column.with.dots`") // Escape `.` in column names. $"columnName" // Scala short hand for a named column. expr("a + 1") // A column that is constructed from a parsed SQL Expression. lit("abc") // A column that produces a literal (constant) value. Column objects can be composed to form complex expressions: $"a" + 1 $"a" === $"b" Since: 1.3.0 Constructor Summary Constructors  Constructor and Description Column(org.apache.spark.sql.catalyst.expressions.Expression expr)  Column(String name)  Method Summary Methods  Modifier and Type Method and Description Column alias(String alias) Gives the column an alias. Column and(Column other) Boolean AND. Column apply(Object extraction) Extracts a value or values from a complex type. <U> TypedColumn<Object,U> as(Encoder<U> evidence$1) Provides a type hint about the expected return value of this column. Column as(scala.collection.Seq<String> aliases) (Scala-specific) Assigns the given aliases to the results of a table generating function. Column as(String alias) Gives the column an alias. Column as(String[] aliases) Assigns the given aliases to the results of a table generating function. Column as(String alias, Metadata metadata) Gives the column an alias with metadata. Column as(scala.Symbol alias) Gives the column an alias. Column asc() Returns an ordering used in sorting. Column between(Object lowerBound, Object upperBound) True if the current column is between the lower bound and upper bound, inclusive. Column bitwiseAND(Object other) Compute bitwise AND of this expression with another expression. Column bitwiseOR(Object other) Compute bitwise OR of this expression with another expression. Column bitwiseXOR(Object other) Compute bitwise XOR of this expression with another expression. Column cast(DataType to) Casts the column to a different data type. Column cast(String to) Casts the column to a different data type, using the canonical string representation of the type. Column contains(Object other) Contains the other element. Column desc() Returns an ordering used in sorting. Column divide(Object other) Division this expression by another expression. Column endsWith(Column other) String ends with. Column endsWith(String literal) String ends with another string literal. Column eqNullSafe(Object other) Equality test that is safe for null values. boolean equals(Object that)  Column equalTo(Object other) Equality test. void explain(boolean extended) Prints the expression to the console for debugging purpose. Column geq(Object other) Greater than or equal to an expression. Column getField(String fieldName) An expression that gets a field by name in a StructType. Column getItem(Object key) An expression that gets an item at position ordinal out of an array, or gets a value by key key in a MapType. Column gt(Object other) Greater than. int hashCode()  Column isin(Object... list) A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments. Column isin(scala.collection.Seq<Object> list) A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments. Column isNaN() True if the current expression is NaN. Column isNotNull() True if the current expression is NOT null. Column isNull() True if the current expression is null. Column leq(Object other) Less than or equal to. Column like(String literal) SQL like expression. Column lt(Object other) Less than. Column minus(Object other) Subtraction. Column mod(Object other) Modulo (a.k.a. Column multiply(Object other) Multiplication of this expression and another expression. Column name(String alias) Gives the column a name (alias). Column notEqual(Object other) Inequality test. Column or(Column other) Boolean OR. Column otherwise(Object value) Evaluates a list of conditions and returns one of multiple possible result expressions. Column over() Define a empty analytic clause. Column over(WindowSpec window) Define a windowing column. Column plus(Object other) Sum of this expression and another expression. Column rlike(String literal) SQL RLIKE expression (LIKE with Regex). Column startsWith(Column other) String starts with. Column startsWith(String literal) String starts with another string literal. Column substr(Column startPos, Column len) An expression that returns a substring. Column substr(int startPos, int len) An expression that returns a substring. String toString()  static scala.Option<org.apache.spark.sql.catalyst.expressions.Expression> unapply(Column col)  Column when(Column condition, Object value) Evaluates a list of conditions and returns one of multiple possible result expressions. Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail Column public Column(org.apache.spark.sql.catalyst.expressions.Expression expr) Column public Column(String name) Method Detail unapply public static scala.Option<org.apache.spark.sql.catalyst.expressions.Expression> unapply(Column col) isin public Column isin(Object... list) A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments. Parameters:list - (undocumented) Returns:(undocumented)Since: 1.5.0 toString public String toString() Overrides: toString in class Object equals public boolean equals(Object that) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object as public <U> TypedColumn<Object,U> as(Encoder<U> evidence$1) Provides a type hint about the expected return value of this column. This information can be used by operations such as select on a Dataset to automatically convert the results into the correct JVM types. Parameters:evidence$1 - (undocumented) Returns:(undocumented)Since: 1.6.0 apply public Column apply(Object extraction) Extracts a value or values from a complex type. The following types of extraction are supported: - Given an Array, an integer ordinal can be used to retrieve a single value. - Given a Map, a key of the correct type can be used to retrieve an individual value. - Given a Struct, a string fieldName can be used to extract that field. - Given an Array of Structs, a string fieldName can be used to extract filed of every struct in that array, and return an Array of fields Parameters:extraction - (undocumented) Returns:(undocumented)Since: 1.4.0 equalTo public Column equalTo(Object other) Equality test. // Scala: df.filter( df("colA") === df("colB") ) // Java import static org.apache.spark.sql.functions.*; df.filter( col("colA").equalTo(col("colB")) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 notEqual public Column notEqual(Object other) Inequality test. // Scala: df.select( df("colA") !== df("colB") ) df.select( !(df("colA") === df("colB")) ) // Java: import static org.apache.spark.sql.functions.*; df.filter( col("colA").notEqual(col("colB")) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 gt public Column gt(Object other) Greater than. // Scala: The following selects people older than 21. people.select( people("age") > lit(21) ) // Java: import static org.apache.spark.sql.functions.*; people.select( people("age").gt(21) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 lt public Column lt(Object other) Less than. // Scala: The following selects people younger than 21. people.select( people("age") < 21 ) // Java: people.select( people("age").lt(21) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 leq public Column leq(Object other) Less than or equal to. // Scala: The following selects people age 21 or younger than 21. people.select( people("age") <= 21 ) // Java: people.select( people("age").leq(21) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 geq public Column geq(Object other) Greater than or equal to an expression. // Scala: The following selects people age 21 or older than 21. people.select( people("age") >= 21 ) // Java: people.select( people("age").geq(21) ) Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 eqNullSafe public Column eqNullSafe(Object other) Equality test that is safe for null values. Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 when public Column when(Column condition, Object value) Evaluates a list of conditions and returns one of multiple possible result expressions. If otherwise is not defined at the end, null is returned for unmatched conditions. // Example: encoding gender string column into integer. // Scala: people.select(when(people("gender") === "male", 0) .when(people("gender") === "female", 1) .otherwise(2)) // Java: people.select(when(col("gender").equalTo("male"), 0) .when(col("gender").equalTo("female"), 1) .otherwise(2)) Parameters:condition - (undocumented)value - (undocumented) Returns:(undocumented)Since: 1.4.0 otherwise public Column otherwise(Object value) Evaluates a list of conditions and returns one of multiple possible result expressions. If otherwise is not defined at the end, null is returned for unmatched conditions. // Example: encoding gender string column into integer. // Scala: people.select(when(people("gender") === "male", 0) .when(people("gender") === "female", 1) .otherwise(2)) // Java: people.select(when(col("gender").equalTo("male"), 0) .when(col("gender").equalTo("female"), 1) .otherwise(2)) Parameters:value - (undocumented) Returns:(undocumented)Since: 1.4.0 between public Column between(Object lowerBound, Object upperBound) True if the current column is between the lower bound and upper bound, inclusive. Parameters:lowerBound - (undocumented)upperBound - (undocumented) Returns:(undocumented)Since: 1.4.0 isNaN public Column isNaN() True if the current expression is NaN. Returns:(undocumented)Since: 1.5.0 isNull public Column isNull() True if the current expression is null. Returns:(undocumented)Since: 1.3.0 isNotNull public Column isNotNull() True if the current expression is NOT null. Returns:(undocumented)Since: 1.3.0 or public Column or(Column other) Boolean OR. // Scala: The following selects people that are in school or employed. people.filter( people("inSchool") || people("isEmployed") ) // Java: people.filter( people("inSchool").or(people("isEmployed")) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 and public Column and(Column other) Boolean AND. // Scala: The following selects people that are in school and employed at the same time. people.select( people("inSchool") && people("isEmployed") ) // Java: people.select( people("inSchool").and(people("isEmployed")) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 plus public Column plus(Object other) Sum of this expression and another expression. // Scala: The following selects the sum of a person's height and weight. people.select( people("height") + people("weight") ) // Java: people.select( people("height").plus(people("weight")) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 minus public Column minus(Object other) Subtraction. Subtract the other expression from this expression. // Scala: The following selects the difference between people's height and their weight. people.select( people("height") - people("weight") ) // Java: people.select( people("height").minus(people("weight")) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 multiply public Column multiply(Object other) Multiplication of this expression and another expression. // Scala: The following multiplies a person's height by their weight. people.select( people("height") * people("weight") ) // Java: people.select( people("height").multiply(people("weight")) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 divide public Column divide(Object other) Division this expression by another expression. // Scala: The following divides a person's height by their weight. people.select( people("height") / people("weight") ) // Java: people.select( people("height").divide(people("weight")) ); Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 mod public Column mod(Object other) Modulo (a.k.a. remainder) expression. Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 isin public Column isin(scala.collection.Seq<Object> list) A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments. Parameters:list - (undocumented) Returns:(undocumented)Since: 1.5.0 like public Column like(String literal) SQL like expression. Parameters:literal - (undocumented) Returns:(undocumented)Since: 1.3.0 rlike public Column rlike(String literal) SQL RLIKE expression (LIKE with Regex). Parameters:literal - (undocumented) Returns:(undocumented)Since: 1.3.0 getItem public Column getItem(Object key) An expression that gets an item at position ordinal out of an array, or gets a value by key key in a MapType. Parameters:key - (undocumented) Returns:(undocumented)Since: 1.3.0 getField public Column getField(String fieldName) An expression that gets a field by name in a StructType. Parameters:fieldName - (undocumented) Returns:(undocumented)Since: 1.3.0 substr public Column substr(Column startPos, Column len) An expression that returns a substring. Parameters:startPos - expression for the starting position.len - expression for the length of the substring. Returns:(undocumented)Since: 1.3.0 substr public Column substr(int startPos, int len) An expression that returns a substring. Parameters:startPos - starting position.len - length of the substring. Returns:(undocumented)Since: 1.3.0 contains public Column contains(Object other) Contains the other element. Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 startsWith public Column startsWith(Column other) String starts with. Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 startsWith public Column startsWith(String literal) String starts with another string literal. Parameters:literal - (undocumented) Returns:(undocumented)Since: 1.3.0 endsWith public Column endsWith(Column other) String ends with. Parameters:other - (undocumented) Returns:(undocumented)Since: 1.3.0 endsWith public Column endsWith(String literal) String ends with another string literal. Parameters:literal - (undocumented) Returns:(undocumented)Since: 1.3.0 alias public Column alias(String alias) Gives the column an alias. Same as as. // Renames colA to colB in select output. df.select($"colA".alias("colB")) Parameters:alias - (undocumented) Returns:(undocumented)Since: 1.4.0 as public Column as(String alias) Gives the column an alias. // Renames colA to colB in select output. df.select($"colA".as("colB")) If the current column has metadata associated with it, this metadata will be propagated to the new column. If this not desired, use as with explicitly empty metadata. Parameters:alias - (undocumented) Returns:(undocumented)Since: 1.3.0 as public Column as(scala.collection.Seq<String> aliases) (Scala-specific) Assigns the given aliases to the results of a table generating function. // Renames colA to colB in select output. df.select(explode($"myMap").as("key" :: "value" :: Nil)) Parameters:aliases - (undocumented) Returns:(undocumented)Since: 1.4.0 as public Column as(String[] aliases) Assigns the given aliases to the results of a table generating function. // Renames colA to colB in select output. df.select(explode($"myMap").as("key" :: "value" :: Nil)) Parameters:aliases - (undocumented) Returns:(undocumented)Since: 1.4.0 as public Column as(scala.Symbol alias) Gives the column an alias. // Renames colA to colB in select output. df.select($"colA".as('colB)) If the current column has metadata associated with it, this metadata will be propagated to the new column. If this not desired, use as with explicitly empty metadata. Parameters:alias - (undocumented) Returns:(undocumented)Since: 1.3.0 as public Column as(String alias, Metadata metadata) Gives the column an alias with metadata. val metadata: Metadata = ... df.select($"colA".as("colB", metadata)) Parameters:alias - (undocumented)metadata - (undocumented) Returns:(undocumented)Since: 1.3.0 name public Column name(String alias) Gives the column a name (alias). // Renames colA to colB in select output. df.select($"colA".name("colB")) If the current column has metadata associated with it, this metadata will be propagated to the new column. If this not desired, use as with explicitly empty metadata. Parameters:alias - (undocumented) Returns:(undocumented)Since: 2.0.0 cast public Column cast(DataType to) Casts the column to a different data type. // Casts colA to IntegerType. import org.apache.spark.sql.types.IntegerType df.select(df("colA").cast(IntegerType)) // equivalent to df.select(df("colA").cast("int")) Parameters:to - (undocumented) Returns:(undocumented)Since: 1.3.0 cast public Column cast(String to) Casts the column to a different data type, using the canonical string representation of the type. The supported types are: string, boolean, byte, short, int, long, float, double, decimal, date, timestamp. // Casts colA to integer. df.select(df("colA").cast("int")) Parameters:to - (undocumented) Returns:(undocumented)Since: 1.3.0 desc public Column desc() Returns an ordering used in sorting. // Scala: sort a DataFrame by age column in descending order. df.sort(df("age").desc) // Java df.sort(df.col("age").desc()); Returns:(undocumented)Since: 1.3.0 asc public Column asc() Returns an ordering used in sorting. // Scala: sort a DataFrame by age column in ascending order. df.sort(df("age").asc) // Java df.sort(df.col("age").asc()); Returns:(undocumented)Since: 1.3.0 explain public void explain(boolean extended) Prints the expression to the console for debugging purpose. Parameters:extended - (undocumented)Since: 1.3.0 bitwiseOR public Column bitwiseOR(Object other) Compute bitwise OR of this expression with another expression. df.select($"colA".bitwiseOR($"colB")) Parameters:other - (undocumented) Returns:(undocumented)Since: 1.4.0 bitwiseAND public Column bitwiseAND(Object other) Compute bitwise AND of this expression with another expression. df.select($"colA".bitwiseAND($"colB")) Parameters:other - (undocumented) Returns:(undocumented)Since: 1.4.0 bitwiseXOR public Column bitwiseXOR(Object other) Compute bitwise XOR of this expression with another expression. df.select($"colA".bitwiseXOR($"colB")) Parameters:other - (undocumented) Returns:(undocumented)Since: 1.4.0 over public Column over(WindowSpec window) Define a windowing column. val w = Window.partitionBy("name").orderBy("id") df.select( sum("price").over(w.rangeBetween(Long.MinValue, 2)), avg("price").over(w.rowsBetween(0, 4)) ) Parameters:window - (undocumented) Returns:(undocumented)Since: 1.4.0 over public Column over() Define a empty analytic clause. In this case the analytic function is applied and presented for all rows in the result set. df.select( sum("price").over(), avg("price").over() ) Returns:(undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ColumnName (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ColumnName (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class ColumnName Object org.apache.spark.sql.Column org.apache.spark.sql.ColumnName public class ColumnName extends Column :: Experimental :: A convenient class used for constructing schema. Since: 1.3.0 Constructor Summary Constructors  Constructor and Description ColumnName(String name)  Method Summary Methods  Modifier and Type Method and Description StructField array(DataType dataType) Creates a new StructField of type array. StructField binary() Creates a new StructField of type binary. StructField date() Creates a new StructField of type date. StructField decimal() Creates a new StructField of type decimal. StructField decimal(int precision, int scale) Creates a new StructField of type decimal. StructField map(DataType keyType, DataType valueType) Creates a new StructField of type map. StructField map(MapType mapType)  StructField string() Creates a new StructField of type string. StructField struct(scala.collection.Seq<StructField> fields) Creates a new StructField of type struct. StructField struct(StructType structType) Creates a new StructField of type struct. StructField timestamp() Creates a new StructField of type timestamp. Methods inherited from class org.apache.spark.sql.Column alias, and, apply, as, as, as, as, as, as, asc, between, bitwiseAND, bitwiseOR, bitwiseXOR, cast, cast, contains, desc, divide, endsWith, endsWith, eqNullSafe, equals, equalTo, explain, geq, getField, getItem, gt, hashCode, isin, isin, isNaN, isNotNull, isNull, leq, like, lt, minus, mod, multiply, name, notEqual, or, otherwise, over, over, plus, rlike, startsWith, startsWith, substr, substr, toString, unapply, when Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail ColumnName public ColumnName(String name) Method Detail string public StructField string() Creates a new StructField of type string. Returns:(undocumented)Since: 1.3.0 date public StructField date() Creates a new StructField of type date. Returns:(undocumented)Since: 1.3.0 decimal public StructField decimal() Creates a new StructField of type decimal. Returns:(undocumented)Since: 1.3.0 decimal public StructField decimal(int precision, int scale) Creates a new StructField of type decimal. Parameters:precision - (undocumented)scale - (undocumented) Returns:(undocumented)Since: 1.3.0 timestamp public StructField timestamp() Creates a new StructField of type timestamp. Returns:(undocumented)Since: 1.3.0 binary public StructField binary() Creates a new StructField of type binary. Returns:(undocumented)Since: 1.3.0 array public StructField array(DataType dataType) Creates a new StructField of type array. Parameters:dataType - (undocumented) Returns:(undocumented)Since: 1.3.0 map public StructField map(DataType keyType, DataType valueType) Creates a new StructField of type map. Parameters:keyType - (undocumented)valueType - (undocumented) Returns:(undocumented)Since: 1.3.0 map public StructField map(MapType mapType) struct public StructField struct(scala.collection.Seq<StructField> fields) Creates a new StructField of type struct. Parameters:fields - (undocumented) Returns:(undocumented)Since: 1.3.0 struct public StructField struct(StructType structType) Creates a new StructField of type struct. Parameters:structType - (undocumented) Returns:(undocumented)Since: 1.3.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ColumnPruner (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ColumnPruner (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class ColumnPruner Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.feature.ColumnPruner All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class ColumnPruner extends Transformer implements MLWritable Utility transformer for removing temporary columns from a DataFrame. TODO(ekl) make this a public transformer See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ColumnPruner(scala.collection.immutable.Set<String> columnsToPrune)  ColumnPruner(String uid, scala.collection.immutable.Set<String> columnsToPrune)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  scala.collection.immutable.Set<String> columnsToPrune()  ColumnPruner copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static ColumnPruner load(String path)  static Param<?>[] params()  static MLReader<ColumnPruner> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail ColumnPruner public ColumnPruner(String uid, scala.collection.immutable.Set<String> columnsToPrune) ColumnPruner public ColumnPruner(scala.collection.immutable.Set<String> columnsToPrune) Method Detail read public static MLReader<ColumnPruner> read() load public static ColumnPruner load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) columnsToPrune public scala.collection.immutable.Set<String> columnsToPrune() transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public ColumnPruner copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Transformer Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ComplexFutureAction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ComplexFutureAction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class ComplexFutureAction<T> Object org.apache.spark.ComplexFutureAction<T> All Implemented Interfaces: FutureAction<T>, scala.concurrent.Awaitable<T>, scala.concurrent.Future<T> public class ComplexFutureAction<T> extends Object implements FutureAction<T> A FutureAction for actions that could trigger multiple Spark jobs. Examples include take, takeSample. Cancellation works by setting the cancelled flag to true and cancelling any pending jobs. Nested Class Summary Nested classes/interfaces inherited from interface scala.concurrent.Future scala.concurrent.Future.InternalCallbackExecutor$ Constructor Summary Constructors  Constructor and Description ComplexFutureAction(scala.Function1<JobSubmitter,scala.concurrent.Future<T>> run)  Method Summary Methods  Modifier and Type Method and Description void cancel() Cancels the execution of this action. boolean isCancelled() Returns whether the action has been cancelled. boolean isCompleted() Returns whether the action has already been completed with a value or an exception. scala.collection.Seq<Object> jobIds() Returns the job IDs run by the underlying async operation. <U> void onComplete(scala.Function1<scala.util.Try<T>,U> func, scala.concurrent.ExecutionContext executor) When this action is completed, either through an exception, or a value, applies the provided function. ComplexFutureAction<T> ready(scala.concurrent.duration.Duration atMost, scala.concurrent.CanAwait permit) Blocks until this action completes. T result(scala.concurrent.duration.Duration atMost, scala.concurrent.CanAwait permit) Awaits and returns the result (of type T) of this action. scala.Option<scala.util.Try<T>> value() The value of this Future. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.FutureAction get Methods inherited from interface scala.concurrent.Future andThen, collect, failed, fallbackTo, filter, flatMap, foreach, map, mapTo, onFailure, onSuccess, recover, recoverWith, transform, withFilter, zip Constructor Detail ComplexFutureAction public ComplexFutureAction(scala.Function1<JobSubmitter,scala.concurrent.Future<T>> run) Method Detail cancel public void cancel() Description copied from interface: FutureAction Cancels the execution of this action. Specified by: cancel in interface FutureAction<T> isCancelled public boolean isCancelled() Description copied from interface: FutureAction Returns whether the action has been cancelled. Specified by: isCancelled in interface FutureAction<T> Returns:(undocumented) ready public ComplexFutureAction<T> ready(scala.concurrent.duration.Duration atMost, scala.concurrent.CanAwait permit) throws InterruptedException, java.util.concurrent.TimeoutException Description copied from interface: FutureAction Blocks until this action completes. Specified by: ready in interface FutureAction<T> Specified by: ready in interface scala.concurrent.Awaitable<T> Parameters:atMost - maximum wait time, which may be negative (no waiting is done), Duration.Inf for unbounded waiting, or a finite positive durationpermit - (undocumented) Returns:this FutureAction Throws: InterruptedException java.util.concurrent.TimeoutException result public T result(scala.concurrent.duration.Duration atMost, scala.concurrent.CanAwait permit) throws Exception Description copied from interface: FutureAction Awaits and returns the result (of type T) of this action. Specified by: result in interface FutureAction<T> Specified by: result in interface scala.concurrent.Awaitable<T> Parameters:atMost - maximum wait time, which may be negative (no waiting is done), Duration.Inf for unbounded waiting, or a finite positive durationpermit - (undocumented) Returns:the result value if the action is completed within the specific maximum wait time Throws: Exception - exception during action execution onComplete public <U> void onComplete(scala.Function1<scala.util.Try<T>,U> func, scala.concurrent.ExecutionContext executor) Description copied from interface: FutureAction When this action is completed, either through an exception, or a value, applies the provided function. Specified by: onComplete in interface FutureAction<T> Specified by: onComplete in interface scala.concurrent.Future<T> Parameters:func - (undocumented)executor - (undocumented) isCompleted public boolean isCompleted() Description copied from interface: FutureAction Returns whether the action has already been completed with a value or an exception. Specified by: isCompleted in interface FutureAction<T> Specified by: isCompleted in interface scala.concurrent.Future<T> Returns:(undocumented) value public scala.Option<scala.util.Try<T>> value() Description copied from interface: FutureAction The value of this Future. If the future is not completed the returned value will be None. If the future is completed the value will be Some(Success(t)) if it contains a valid result, or Some(Failure(error)) if it contains an exception. Specified by: value in interface FutureAction<T> Specified by: value in interface scala.concurrent.Future<T> Returns:(undocumented) jobIds public scala.collection.Seq<Object> jobIds() Description copied from interface: FutureAction Returns the job IDs run by the underlying async operation. This returns the current snapshot of the job list. Certain operations may run multiple jobs, so multiple calls to this method may return different lists. Specified by: jobIds in interface FutureAction<T> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CompressionCodec (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CompressionCodec (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.io Interface CompressionCodec All Known Implementing Classes: LZ4CompressionCodec, LZFCompressionCodec, SnappyCompressionCodec public interface CompressionCodec :: DeveloperApi :: CompressionCodec allows the customization of choosing different compression implementations to be used in block storage. Note: The wire protocol for a codec is not guaranteed compatible across versions of Spark. This is intended for use as an internal compression utility within a single Spark application. Method Summary Methods  Modifier and Type Method and Description java.io.InputStream compressedInputStream(java.io.InputStream s)  java.io.OutputStream compressedOutputStream(java.io.OutputStream s)  Method Detail compressedOutputStream java.io.OutputStream compressedOutputStream(java.io.OutputStream s) compressedInputStream java.io.InputStream compressedInputStream(java.io.InputStream s) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ConfigEntryWithDefault (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ConfigEntryWithDefault (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.internal.config Class ConfigEntryWithDefault<T> Object org.apache.spark.internal.config.ConfigEntryWithDefault<T> public class ConfigEntryWithDefault<T> extends Object Constructor Summary Constructors  Constructor and Description ConfigEntryWithDefault(String key, T _defaultValue, scala.Function1<String,T> valueConverter, scala.Function1<T,String> stringConverter, String doc, boolean isPublic)  Method Summary Methods  Modifier and Type Method and Description scala.Option<T> defaultValue()  String defaultValueString()  String doc()  boolean isPublic()  String key()  T readFrom(SparkConf conf)  scala.Function1<T,String> stringConverter()  String toString()  scala.Function1<String,T> valueConverter()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail ConfigEntryWithDefault public ConfigEntryWithDefault(String key, T _defaultValue, scala.Function1<String,T> valueConverter, scala.Function1<T,String> stringConverter, String doc, boolean isPublic) Method Detail defaultValue public scala.Option<T> defaultValue() defaultValueString public String defaultValueString() readFrom public T readFrom(SparkConf conf) key public String key() valueConverter public scala.Function1<String,T> valueConverter() stringConverter public scala.Function1<T,String> stringConverter() doc public String doc() isPublic public boolean isPublic() toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ConfigHelpers (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ConfigHelpers (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.internal.config Class ConfigHelpers Object org.apache.spark.internal.config.ConfigHelpers public class ConfigHelpers extends Object Constructor Summary Constructors  Constructor and Description ConfigHelpers()  Method Summary Methods  Modifier and Type Method and Description static long byteFromString(String str, org.apache.spark.network.util.ByteUnit unit)  static String byteToString(long v, org.apache.spark.network.util.ByteUnit unit)  static <T> String seqToString(scala.collection.Seq<T> v, scala.Function1<T,String> stringConverter)  static <T> scala.collection.Seq<T> stringToSeq(String str, scala.Function1<String,T> converter)  static long timeFromString(String str, java.util.concurrent.TimeUnit unit)  static String timeToString(long v, java.util.concurrent.TimeUnit unit)  static boolean toBoolean(String s, String key)  static <T> T toNumber(String s, scala.Function1<String,T> converter, String key, String configType)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ConfigHelpers public ConfigHelpers() Method Detail toNumber public static <T> T toNumber(String s, scala.Function1<String,T> converter, String key, String configType) toBoolean public static boolean toBoolean(String s, String key) stringToSeq public static <T> scala.collection.Seq<T> stringToSeq(String str, scala.Function1<String,T> converter) seqToString public static <T> String seqToString(scala.collection.Seq<T> v, scala.Function1<T,String> stringConverter) timeFromString public static long timeFromString(String str, java.util.concurrent.TimeUnit unit) timeToString public static String timeToString(long v, java.util.concurrent.TimeUnit unit) byteFromString public static long byteFromString(String str, org.apache.spark.network.util.ByteUnit unit) byteToString public static String byteToString(long v, org.apache.spark.network.util.ByteUnit unit) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ConnectedComponents (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ConnectedComponents (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx.lib Class ConnectedComponents Object org.apache.spark.graphx.lib.ConnectedComponents public class ConnectedComponents extends Object Connected components algorithm. Constructor Summary Constructors  Constructor and Description ConnectedComponents()  Method Summary Methods  Modifier and Type Method and Description static <VD,ED> Graph<Object,ED> run(Graph<VD,ED> graph, scala.reflect.ClassTag<VD> evidence$3, scala.reflect.ClassTag<ED> evidence$4) Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. static <VD,ED> Graph<Object,ED> run(Graph<VD,ED> graph, int maxIterations, scala.reflect.ClassTag<VD> evidence$1, scala.reflect.ClassTag<ED> evidence$2) Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ConnectedComponents public ConnectedComponents() Method Detail run public static <VD,ED> Graph<Object,ED> run(Graph<VD,ED> graph, int maxIterations, scala.reflect.ClassTag<VD> evidence$1, scala.reflect.ClassTag<ED> evidence$2) Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. Parameters:graph - the graph for which to compute the connected componentsmaxIterations - the maximum number of iterations to run forevidence$1 - (undocumented)evidence$2 - (undocumented) Returns:a graph with vertex attributes containing the smallest vertex in each connected component run public static <VD,ED> Graph<Object,ED> run(Graph<VD,ED> graph, scala.reflect.ClassTag<VD> evidence$3, scala.reflect.ClassTag<ED> evidence$4) Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. Parameters:graph - the graph for which to compute the connected componentsevidence$3 - (undocumented)evidence$4 - (undocumented) Returns:a graph with vertex attributes containing the smallest vertex in each connected component Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ConstantInputDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ConstantInputDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.dstream Class ConstantInputDStream<T> Object org.apache.spark.streaming.dstream.DStream<T> org.apache.spark.streaming.dstream.InputDStream<T> org.apache.spark.streaming.dstream.ConstantInputDStream<T> All Implemented Interfaces: java.io.Serializable public class ConstantInputDStream<T> extends InputDStream<T> An input stream that always returns the same RDD on each time step. Useful for testing. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ConstantInputDStream(StreamingContext _ssc, RDD<T> rdd, scala.reflect.ClassTag<T> evidence$1)  Method Summary Methods  Modifier and Type Method and Description scala.Option<RDD<T>> compute(Time validTime) Method that generates a RDD for the given time void start() Method called to start receiving data. void stop() Method called to stop receiving data. Methods inherited from class org.apache.spark.streaming.dstream.InputDStream dependencies, id, slideDuration Methods inherited from class org.apache.spark.streaming.dstream.DStream cache, checkpoint, context, count, countByValue, countByValueAndWindow, countByWindow, filter, flatMap, foreachRDD, foreachRDD, glom, map, mapPartitions, persist, persist, print, print, reduce, reduceByWindow, reduceByWindow, repartition, saveAsObjectFiles, saveAsTextFiles, slice, slice, toPairDStreamFunctions, transform, transform, transformWith, transformWith, union, window, window Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ConstantInputDStream public ConstantInputDStream(StreamingContext _ssc, RDD<T> rdd, scala.reflect.ClassTag<T> evidence$1) Method Detail start public void start() Description copied from class: InputDStream Method called to start receiving data. Subclasses must implement this method. Specified by: start in class InputDStream<T> stop public void stop() Description copied from class: InputDStream Method called to stop receiving data. Subclasses must implement this method. Specified by: stop in class InputDStream<T> compute public scala.Option<RDD<T>> compute(Time validTime) Description copied from class: DStream Method that generates a RDD for the given time Specified by: compute in class DStream<T> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ContinuousSplit (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ContinuousSplit (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class ContinuousSplit Object org.apache.spark.ml.tree.ContinuousSplit All Implemented Interfaces: java.io.Serializable, Split public class ContinuousSplit extends Object implements Split Split which tests a continuous feature. param: featureIndex Index of the feature to test param: threshold If the feature value is less than or equal to this threshold, then the split goes left. Otherwise, it goes right. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description boolean equals(Object o)  int featureIndex() Index of feature which this split tests int hashCode()  double threshold()  Methods inherited from class Object getClass, notify, notifyAll, toString, wait, wait, wait Method Detail featureIndex public int featureIndex() Description copied from interface: Split Index of feature which this split tests Specified by: featureIndex in interface Split threshold public double threshold() equals public boolean equals(Object o) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CoordinateMatrix (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CoordinateMatrix (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg.distributed Class CoordinateMatrix Object org.apache.spark.mllib.linalg.distributed.CoordinateMatrix All Implemented Interfaces: java.io.Serializable, DistributedMatrix public class CoordinateMatrix extends Object implements DistributedMatrix Represents a matrix in coordinate format. param: entries matrix entries param: nRows number of rows. A non-positive value means unknown, and then the number of rows will be determined by the max row index plus one. param: nCols number of columns. A non-positive value means unknown, and then the number of columns will be determined by the max column index plus one. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CoordinateMatrix(RDD<MatrixEntry> entries) Alternative constructor leaving matrix dimensions to be determined automatically. CoordinateMatrix(RDD<MatrixEntry> entries, long nRows, long nCols)  Method Summary Methods  Modifier and Type Method and Description RDD<MatrixEntry> entries()  long numCols() Gets or computes the number of columns. long numRows() Gets or computes the number of rows. BlockMatrix toBlockMatrix() Converts to BlockMatrix. BlockMatrix toBlockMatrix(int rowsPerBlock, int colsPerBlock) Converts to BlockMatrix. IndexedRowMatrix toIndexedRowMatrix() Converts to IndexedRowMatrix. RowMatrix toRowMatrix() Converts to RowMatrix, dropping row indices after grouping by row index. CoordinateMatrix transpose() Transposes this CoordinateMatrix. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CoordinateMatrix public CoordinateMatrix(RDD<MatrixEntry> entries, long nRows, long nCols) CoordinateMatrix public CoordinateMatrix(RDD<MatrixEntry> entries) Alternative constructor leaving matrix dimensions to be determined automatically. Method Detail entries public RDD<MatrixEntry> entries() numCols public long numCols() Gets or computes the number of columns. Specified by: numCols in interface DistributedMatrix numRows public long numRows() Gets or computes the number of rows. Specified by: numRows in interface DistributedMatrix transpose public CoordinateMatrix transpose() Transposes this CoordinateMatrix. toIndexedRowMatrix public IndexedRowMatrix toIndexedRowMatrix() Converts to IndexedRowMatrix. The number of columns must be within the integer range. toRowMatrix public RowMatrix toRowMatrix() Converts to RowMatrix, dropping row indices after grouping by row index. The number of columns must be within the integer range. Returns:(undocumented) toBlockMatrix public BlockMatrix toBlockMatrix() Converts to BlockMatrix. Creates blocks of SparseMatrix with size 1024 x 1024. toBlockMatrix public BlockMatrix toBlockMatrix(int rowsPerBlock, int colsPerBlock) Converts to BlockMatrix. Creates blocks of SparseMatrix. Parameters:rowsPerBlock - The number of rows of each block. The blocks at the bottom edge may have a smaller value. Must be an integer value greater than 0.colsPerBlock - The number of columns of each block. The blocks at the right edge may have a smaller value. Must be an integer value greater than 0. Returns:a BlockMatrix Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CorrelationNames (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CorrelationNames (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.correlation Class CorrelationNames Object org.apache.spark.mllib.stat.correlation.CorrelationNames public class CorrelationNames extends Object Maintains supported and default correlation names. Currently supported correlations: pearson, spearman. Current default correlation: pearson. After new correlation algorithms are added, please update the documentation here and in Statistics.scala for the correlation APIs. Constructor Summary Constructors  Constructor and Description CorrelationNames()  Method Summary Methods  Modifier and Type Method and Description static String defaultCorrName()  static scala.collection.immutable.Map<String,org.apache.spark.mllib.stat.correlation.Correlation> nameToObjectMap()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CorrelationNames public CorrelationNames() Method Detail nameToObjectMap public static scala.collection.immutable.Map<String,org.apache.spark.mllib.stat.correlation.Correlation> nameToObjectMap() defaultCorrName public static String defaultCorrName() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Correlations (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Correlations (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.correlation Class Correlations Object org.apache.spark.mllib.stat.correlation.Correlations public class Correlations extends Object Delegates computation to the specific correlation object based on the input method name. Constructor Summary Constructors  Constructor and Description Correlations()  Method Summary Methods  Modifier and Type Method and Description static double corr(RDD<Object> x, RDD<Object> y, String method)  static Matrix corrMatrix(RDD<Vector> X, String method)  static org.apache.spark.mllib.stat.correlation.Correlation getCorrelationFromName(String method)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Correlations public Correlations() Method Detail corr public static double corr(RDD<Object> x, RDD<Object> y, String method) corrMatrix public static Matrix corrMatrix(RDD<Vector> X, String method) getCorrelationFromName public static org.apache.spark.mllib.stat.correlation.Correlation getCorrelationFromName(String method) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CountMinSketch.Version (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CountMinSketch.Version (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method org.apache.spark.util.sketch Enum CountMinSketch.Version Object Enum<CountMinSketch.Version> org.apache.spark.util.sketch.CountMinSketch.Version All Implemented Interfaces: java.io.Serializable, Comparable<CountMinSketch.Version> Enclosing class: CountMinSketch public static enum CountMinSketch.Version extends Enum<CountMinSketch.Version> Enum Constant Summary Enum Constants  Enum Constant and Description V1 CountMinSketch binary format version 1. Method Summary Methods  Modifier and Type Method and Description static CountMinSketch.Version valueOf(String name) Returns the enum constant of this type with the specified name. static CountMinSketch.Version[] values() Returns an array containing the constants of this enum type, in the order they are declared. Methods inherited from class Enum compareTo, equals, getDeclaringClass, hashCode, name, ordinal, toString, valueOf Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Enum Constant Detail V1 public static final CountMinSketch.Version V1 CountMinSketch binary format version 1. All values written in big-endian order: Version number, always 1 (32 bit) Total count of added items (64 bit) Depth (32 bit) Width (32 bit) Hash functions (depth * 64 bit) Count table Row 0 (width * 64 bit) Row 1 (width * 64 bit) ... Row depth - 1 (width * 64 bit) Method Detail values public static CountMinSketch.Version[] values() Returns an array containing the constants of this enum type, in the order they are declared. This method may be used to iterate over the constants as follows: for (CountMinSketch.Version c : CountMinSketch.Version.values())   System.out.println(c); Returns:an array containing the constants of this enum type, in the order they are declared valueOf public static CountMinSketch.Version valueOf(String name) Returns the enum constant of this type with the specified name. The string must match exactly an identifier used to declare an enum constant in this type. (Extraneous whitespace characters are not permitted.) Parameters:name - the name of the enum constant to be returned. Returns:the enum constant with the specified name Throws: IllegalArgumentException - if this enum type has no constant with the specified name NullPointerException - if the argument is null Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method CountMinSketch (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CountMinSketch (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.sketch Class CountMinSketch Object org.apache.spark.util.sketch.CountMinSketch public abstract class CountMinSketch extends Object A Count-min sketch is a probabilistic data structure used for summarizing streams of data in sub-linear space. Currently, supported data types include: Byte Short Integer Long String A CountMinSketch is initialized with a random seed, and a pair of parameters: relative error (or eps), and confidence (or delta) Suppose you want to estimate the number of times an element x has appeared in a data stream so far. With probability delta, the estimate of this frequency is within the range true frequency <= estimate <= true frequency + eps * N, where N is the total count of items have appeared the data stream so far. Under the cover, a CountMinSketch is essentially a two-dimensional long array with depth d and width w, where d = ceil(2 / eps) w = ceil(-log(1 - confidence) / log(2)) This implementation is largely based on the CountMinSketch class from stream-lib. Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  CountMinSketch.Version  Constructor Summary Constructors  Constructor and Description CountMinSketch()  Method Summary Methods  Modifier and Type Method and Description abstract void add(Object item) Increments item's count by one. abstract void add(Object item, long count) Increments item's count by count. abstract void addBinary(byte[] item) Increments item's count by one. abstract void addBinary(byte[] item, long count) Increments item's count by count. abstract void addLong(long item) Increments item's count by one. abstract void addLong(long item, long count) Increments item's count by count. abstract void addString(String item) Increments item's count by one. abstract void addString(String item, long count) Increments item's count by count. abstract double confidence() Returns the confidence (or delta) of this CountMinSketch. static CountMinSketch create(double eps, double confidence, int seed) Creates a CountMinSketch with given relative error (eps), confidence, and random seed. static CountMinSketch create(int depth, int width, int seed) Creates a CountMinSketch with given depth, width, and random seed. abstract int depth() Depth of this CountMinSketch. abstract long estimateCount(Object item) Returns the estimated frequency of item. abstract CountMinSketch mergeInPlace(CountMinSketch other) Merges another CountMinSketch with this one in place. static CountMinSketch readFrom(java.io.InputStream in) Reads in a CountMinSketch from an input stream. abstract double relativeError() Returns the relative error (or eps) of this CountMinSketch. abstract long totalCount() Total count of items added to this CountMinSketch so far. abstract int width() Width of this CountMinSketch. abstract void writeTo(java.io.OutputStream out) Writes out this CountMinSketch to an output stream in binary format. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail CountMinSketch public CountMinSketch() Method Detail relativeError public abstract double relativeError() Returns the relative error (or eps) of this CountMinSketch. confidence public abstract double confidence() Returns the confidence (or delta) of this CountMinSketch. depth public abstract int depth() Depth of this CountMinSketch. width public abstract int width() Width of this CountMinSketch. totalCount public abstract long totalCount() Total count of items added to this CountMinSketch so far. add public abstract void add(Object item) Increments item's count by one. add public abstract void add(Object item, long count) Increments item's count by count. addLong public abstract void addLong(long item) Increments item's count by one. addLong public abstract void addLong(long item, long count) Increments item's count by count. addString public abstract void addString(String item) Increments item's count by one. addString public abstract void addString(String item, long count) Increments item's count by count. addBinary public abstract void addBinary(byte[] item) Increments item's count by one. addBinary public abstract void addBinary(byte[] item, long count) Increments item's count by count. estimateCount public abstract long estimateCount(Object item) Returns the estimated frequency of item. mergeInPlace public abstract CountMinSketch mergeInPlace(CountMinSketch other) throws IncompatibleMergeException Merges another CountMinSketch with this one in place. Note that only Count-Min sketches with the same depth, width, and random seed can be merged. Throws: IncompatibleMergeException - if the other CountMinSketch has incompatible depth, width, relative-error, confidence, or random seed. writeTo public abstract void writeTo(java.io.OutputStream out) throws java.io.IOException Writes out this CountMinSketch to an output stream in binary format. It is the caller's responsibility to close the stream. Throws: java.io.IOException readFrom public static CountMinSketch readFrom(java.io.InputStream in) throws java.io.IOException Reads in a CountMinSketch from an input stream. It is the caller's responsibility to close the stream. Throws: java.io.IOException create public static CountMinSketch create(int depth, int width, int seed) Creates a CountMinSketch with given depth, width, and random seed. Parameters:depth - depth of the Count-min Sketch, must be positivewidth - width of the Count-min Sketch, must be positiveseed - random seed create public static CountMinSketch create(double eps, double confidence, int seed) Creates a CountMinSketch with given relative error (eps), confidence, and random seed. Parameters:eps - relative error, must be positiveconfidence - confidence, must be positive and less than 1.0seed - random seed Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CountVectorizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CountVectorizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class CountVectorizer Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<CountVectorizerModel> org.apache.spark.ml.feature.CountVectorizer All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class CountVectorizer extends Estimator<CountVectorizerModel> implements DefaultParamsWritable Extracts a vocabulary from document collections and generates a CountVectorizerModel. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CountVectorizer()  CountVectorizer(String uid)  Method Summary Methods  Modifier and Type Method and Description static BooleanParam binary()  BooleanParam binary() Binary toggle to control the output vector values. static Params clear(Param<?> param)  CountVectorizer copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  CountVectorizerModel fit(Dataset<?> dataset) Fits a model to the input data. static <T> scala.Option<T> get(Param<T> param)  static boolean getBinary()  boolean getBinary()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  static double getMinDF()  double getMinDF()  static double getMinTF()  double getMinTF()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static int getVocabSize()  int getVocabSize()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static CountVectorizer load(String path)  static DoubleParam minDF()  DoubleParam minDF() Specifies the minimum number of different documents a term must appear in to be included in the vocabulary. static DoubleParam minTF()  DoubleParam minTF() Filter to ignore rare words in a document. static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  CountVectorizer setBinary(boolean value)  CountVectorizer setInputCol(String value)  CountVectorizer setMinDF(double value)  CountVectorizer setMinTF(double value)  CountVectorizer setOutputCol(String value)  CountVectorizer setVocabSize(int value)  static String toString()  StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. static void validateParams()  static IntParam vocabSize()  IntParam vocabSize() Max size of the vocabulary. static MLWriter write()  Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Constructor Detail CountVectorizer public CountVectorizer(String uid) CountVectorizer public CountVectorizer() Method Detail load public static CountVectorizer load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() vocabSize public static IntParam vocabSize() getVocabSize public static int getVocabSize() minDF public static DoubleParam minDF() getMinDF public static double getMinDF() minTF public static DoubleParam minTF() getMinTF public static double getMinTF() binary public static BooleanParam binary() getBinary public static boolean getBinary() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setInputCol public CountVectorizer setInputCol(String value) setOutputCol public CountVectorizer setOutputCol(String value) setVocabSize public CountVectorizer setVocabSize(int value) setMinDF public CountVectorizer setMinDF(double value) setMinTF public CountVectorizer setMinTF(double value) setBinary public CountVectorizer setBinary(boolean value) fit public CountVectorizerModel fit(Dataset<?> dataset) Description copied from class: Estimator Fits a model to the input data. Specified by: fit in class Estimator<CountVectorizerModel> Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public CountVectorizer copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Estimator<CountVectorizerModel> Parameters:extra - (undocumented) Returns:(undocumented) vocabSize public IntParam vocabSize() Max size of the vocabulary. CountVectorizer will build a vocabulary that only considers the top vocabSize terms ordered by term frequency across the corpus. Default: 2^18^ Returns:(undocumented) getVocabSize public int getVocabSize() minDF public DoubleParam minDF() Specifies the minimum number of different documents a term must appear in to be included in the vocabulary. If this is an integer >= 1, this specifies the number of documents the term must appear in; if this is a double in [0,1), then this specifies the fraction of documents. Default: 1.0 Returns:(undocumented) getMinDF public double getMinDF() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. minTF public DoubleParam minTF() Filter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer >= 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document's token count). Note that the parameter is only used in transform of CountVectorizerModel and does not affect fitting. Default: 1.0 Returns:(undocumented) getMinTF public double getMinTF() binary public BooleanParam binary() Binary toggle to control the output vector values. If True, all nonzero counts (after minTF filter applied) are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default: false Returns:(undocumented) getBinary public boolean getBinary() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CountVectorizerModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CountVectorizerModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class CountVectorizerModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<CountVectorizerModel> org.apache.spark.ml.feature.CountVectorizerModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class CountVectorizerModel extends Model<CountVectorizerModel> implements MLWritable Converts a text document to a sparse vector of token counts. param: vocabulary An Array over terms. Only the terms in the vocabulary will be counted. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CountVectorizerModel(String[] vocabulary)  CountVectorizerModel(String uid, String[] vocabulary)  Method Summary Methods  Modifier and Type Method and Description static BooleanParam binary()  BooleanParam binary() Binary toggle to control the output vector values. static Params clear(Param<?> param)  CountVectorizerModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static boolean getBinary()  boolean getBinary()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  static double getMinDF()  double getMinDF()  static double getMinTF()  double getMinTF()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static int getVocabSize()  int getVocabSize()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static CountVectorizerModel load(String path)  static DoubleParam minDF()  DoubleParam minDF() Specifies the minimum number of different documents a term must appear in to be included in the vocabulary. static DoubleParam minTF()  DoubleParam minTF() Filter to ignore rare words in a document. static Param<String> outputCol()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static MLReader<CountVectorizerModel> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  CountVectorizerModel setBinary(boolean value)  CountVectorizerModel setInputCol(String value)  CountVectorizerModel setMinTF(double value)  CountVectorizerModel setOutputCol(String value)  static M setParent(Estimator<M> parent)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. static void validateParams()  static IntParam vocabSize()  IntParam vocabSize() Max size of the vocabulary. String[] vocabulary()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.MLWritable save Constructor Detail CountVectorizerModel public CountVectorizerModel(String uid, String[] vocabulary) CountVectorizerModel public CountVectorizerModel(String[] vocabulary) Method Detail read public static MLReader<CountVectorizerModel> read() load public static CountVectorizerModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() vocabSize public static IntParam vocabSize() getVocabSize public static int getVocabSize() minDF public static DoubleParam minDF() getMinDF public static double getMinDF() minTF public static DoubleParam minTF() getMinTF public static double getMinTF() binary public static BooleanParam binary() getBinary public static boolean getBinary() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) vocabulary public String[] vocabulary() setInputCol public CountVectorizerModel setInputCol(String value) setOutputCol public CountVectorizerModel setOutputCol(String value) setMinTF public CountVectorizerModel setMinTF(double value) setBinary public CountVectorizerModel setBinary(boolean value) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public CountVectorizerModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<CountVectorizerModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) vocabSize public IntParam vocabSize() Max size of the vocabulary. CountVectorizer will build a vocabulary that only considers the top vocabSize terms ordered by term frequency across the corpus. Default: 2^18^ Returns:(undocumented) getVocabSize public int getVocabSize() minDF public DoubleParam minDF() Specifies the minimum number of different documents a term must appear in to be included in the vocabulary. If this is an integer >= 1, this specifies the number of documents the term must appear in; if this is a double in [0,1), then this specifies the fraction of documents. Default: 1.0 Returns:(undocumented) getMinDF public double getMinDF() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. minTF public DoubleParam minTF() Filter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer >= 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document's token count). Note that the parameter is only used in transform of CountVectorizerModel and does not affect fitting. Default: 1.0 Returns:(undocumented) getMinTF public double getMinTF() binary public BooleanParam binary() Binary toggle to control the output vector values. If True, all nonzero counts (after minTF filter applied) are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default: false Returns:(undocumented) getBinary public boolean getBinary() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CreatableRelationProvider (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CreatableRelationProvider (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Interface CreatableRelationProvider public interface CreatableRelationProvider Since: 1.3.0 Method Summary Methods  Modifier and Type Method and Description BaseRelation createRelation(SQLContext sqlContext, SaveMode mode, scala.collection.immutable.Map<String,String> parameters, Dataset<Row> data) Creates a relation with the given parameters based on the contents of the given DataFrame. Method Detail createRelation BaseRelation createRelation(SQLContext sqlContext, SaveMode mode, scala.collection.immutable.Map<String,String> parameters, Dataset<Row> data) Creates a relation with the given parameters based on the contents of the given DataFrame. The mode specifies the expected behavior of createRelation when data already exists. Right now, there are three modes, Append, Overwrite, and ErrorIfExists. Append mode means that when saving a DataFrame to a data source, if data already exists, contents of the DataFrame are expected to be appended to existing data. Overwrite mode means that when saving a DataFrame to a data source, if data already exists, existing data is expected to be overwritten by the contents of the DataFrame. ErrorIfExists mode means that when saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown. Parameters:sqlContext - (undocumented)mode - (undocumented)parameters - (undocumented)data - (undocumented) Returns:(undocumented)Since: 1.3.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CreateHiveTableAsSelectCommand (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CreateHiveTableAsSelectCommand (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive.execution Class CreateHiveTableAsSelectCommand Object org.apache.spark.sql.catalyst.trees.TreeNode<PlanType> org.apache.spark.sql.catalyst.plans.QueryPlan<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> org.apache.spark.sql.catalyst.plans.logical.LogicalPlan org.apache.spark.sql.catalyst.plans.logical.LeafNode org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand All Implemented Interfaces: java.io.Serializable, org.apache.spark.sql.catalyst.plans.logical.Command, org.apache.spark.sql.execution.command.RunnableCommand, scala.Equals, scala.Product public class CreateHiveTableAsSelectCommand extends org.apache.spark.sql.catalyst.plans.logical.LeafNode implements org.apache.spark.sql.execution.command.RunnableCommand, scala.Product, scala.Serializable Create table and insert the query result into it. param: tableDesc the Table Describe, which may contains serde, storage handler etc. param: query the query whose result will be insert into the new relation param: ignoreIfExists allow continue working if it's already exists, otherwise raise exception See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CreateHiveTableAsSelectCommand(org.apache.spark.sql.catalyst.catalog.CatalogTable tableDesc, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan query, boolean ignoreIfExists)  Method Summary Methods  Modifier and Type Method and Description static org.apache.spark.sql.catalyst.expressions.AttributeSeq allAttributes()  static boolean analyzed()  static BaseType apply(int number)  String argString()  static String asCode()  abstract static boolean canEqual(Object that)  static org.apache.spark.sql.catalyst.plans.logical.LogicalPlan canonicalized()  static scala.collection.Seq<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> children()  static boolean childrenResolved()  static <B> scala.collection.Seq<B> collect(scala.PartialFunction<BaseType,B> pf)  static <B> scala.Option<B> collectFirst(scala.PartialFunction<BaseType,B> pf)  static scala.collection.Seq<BaseType> collectLeaves()  static org.apache.spark.sql.catalyst.expressions.ExpressionSet constraints()  static scala.collection.immutable.Set<org.apache.spark.sql.catalyst.trees.TreeNode<?>> containsChild()  abstract static boolean equals(Object that)  static scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> expressions()  static boolean fastEquals(org.apache.spark.sql.catalyst.trees.TreeNode<?> other)  static scala.Option<BaseType> find(scala.Function1<BaseType,Object> f)  static <A> scala.collection.Seq<A> flatMap(scala.Function1<BaseType,scala.collection.TraversableOnce<A>> f)  static void foreach(scala.Function1<BaseType,scala.runtime.BoxedUnit> f)  static void foreachUp(scala.Function1<BaseType,scala.runtime.BoxedUnit> f)  static scala.collection.mutable.StringBuilder generateTreeString(int depth, scala.collection.Seq<Object> lastChildren, scala.collection.mutable.StringBuilder builder, boolean verbose, String prefix)  static String generateTreeString$default$5()  static int hashCode()  boolean ignoreIfExists()  void initializeLogging(boolean isInterpreter)  scala.collection.Seq<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> innerChildren()  static org.apache.spark.sql.catalyst.expressions.AttributeSet inputSet()  static boolean isStreaming()  org.slf4j.Logger log_()  static BaseType makeCopy(Object[] newArgs)  static <A> scala.collection.Seq<A> map(scala.Function1<BaseType,A> f)  static BaseType mapChildren(scala.Function1<BaseType,BaseType> f)  static scala.Option<Object> maxRows()  static org.apache.spark.sql.catalyst.expressions.AttributeSet missingInput()  static String nodeName()  static String numberedTreeString()  static org.apache.spark.sql.catalyst.trees.Origin origin()  static scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> output()  static org.apache.spark.sql.catalyst.expressions.AttributeSet outputSet()  static String prettyJson()  static void printSchema()  static org.apache.spark.sql.catalyst.expressions.AttributeSet producedAttributes()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan query()  static org.apache.spark.sql.catalyst.expressions.AttributeSet references()  static void refresh()  static scala.Option<org.apache.spark.sql.catalyst.expressions.NamedExpression> resolve(scala.collection.Seq<String> nameParts, scala.Function2<String,String,Object> resolver)  static scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> resolve(StructType schema, scala.Function2<String,String,Object> resolver)  static scala.Option<org.apache.spark.sql.catalyst.expressions.NamedExpression> resolveChildren(scala.collection.Seq<String> nameParts, scala.Function2<String,String,Object> resolver)  static boolean resolved()  static org.apache.spark.sql.catalyst.plans.logical.LogicalPlan resolveExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> r)  static org.apache.spark.sql.catalyst.plans.logical.LogicalPlan resolveOperators(scala.PartialFunction<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> rule)  static scala.Option<org.apache.spark.sql.catalyst.expressions.NamedExpression> resolveQuoted(String name, scala.Function2<String,String,Object> resolver)  scala.collection.Seq<Row> run(SparkSession sparkSession)  static boolean sameResult(PlanType plan)  static StructType schema()  static String schemaString()  static String simpleString()  static org.apache.spark.sql.catalyst.plans.logical.Statistics statistics()  static scala.collection.Seq<PlanType> subqueries()  org.apache.spark.sql.catalyst.catalog.CatalogTable tableDesc()  static String toJSON()  static String toString()  static BaseType transform(scala.PartialFunction<BaseType,BaseType> rule)  static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformAllExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule)  static BaseType transformDown(scala.PartialFunction<BaseType,BaseType> rule)  static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule)  static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressionsDown(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule)  static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressionsUp(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule)  static BaseType transformUp(scala.PartialFunction<BaseType,BaseType> rule)  static String treeString()  static String treeString(boolean verbose)  static String verboseString()  static BaseType withNewChildren(scala.collection.Seq<BaseType> newChildren)  Methods inherited from class org.apache.spark.sql.catalyst.plans.logical.LeafNode children, producedAttributes Methods inherited from class org.apache.spark.sql.catalyst.plans.logical.LogicalPlan analyzed, canonicalized, childrenResolved, initializeLogIfNecessary, isStreaming, isTraceEnabled, log, logDebug, logDebug, logError, logError, logInfo, logInfo, logName, logTrace, logTrace, logWarning, logWarning, maxRows, org$apache$spark$internal$Logging$$log__$eq, org$apache$spark$internal$Logging$$log_, org$apache$spark$sql$catalyst$plans$logical$LogicalPlan$$name$1, org$apache$spark$sql$catalyst$plans$logical$LogicalPlan$$resolveAsColumn, org$apache$spark$sql$catalyst$plans$logical$LogicalPlan$$resolveAsTableColumn, refresh, resolve, resolve, resolve, resolveChildren, resolved, resolveExpressions, resolveOperators, resolveQuoted, setAnalyzed, statePrefix, statistics Methods inherited from class org.apache.spark.sql.catalyst.plans.QueryPlan allAttributes, cleanArgs, constraints, expressions, getRelevantConstraints, inputSet, missingInput, org$apache$spark$sql$catalyst$plans$QueryPlan$$aliasMap, org$apache$spark$sql$catalyst$plans$QueryPlan$$cleanArg$1, org$apache$spark$sql$catalyst$plans$QueryPlan$$getConstraintClass, org$apache$spark$sql$catalyst$plans$QueryPlan$$isRecursiveDeduction, org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1, org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2, org$apache$spark$sql$catalyst$plans$QueryPlan$$scanNullIntolerantExpr, org$apache$spark$sql$catalyst$plans$QueryPlan$$seqToExpressions$1, output, outputSet, printSchema, references, sameResult, schema, schemaString, simpleString, subqueries, transformAllExpressions, transformExpressions, transformExpressionsDown, transformExpressionsUp, validConstraints, verboseString Methods inherited from class org.apache.spark.sql.catalyst.trees.TreeNode apply, asCode, collect, collectFirst, collectLeaves, containsChild, fastEquals, find, flatMap, foreach, foreachUp, fromJSON, generateTreeString, generateTreeString$default$5, getNodeNumbered, hashCode, jsonFields, makeCopy, map, mapChildren, mapProductIterator, nodeName, numberedTreeString, org$apache$spark$sql$catalyst$trees$TreeNode$$allChildren, org$apache$spark$sql$catalyst$trees$TreeNode$$collectJsonValue$1, org$apache$spark$sql$catalyst$trees$TreeNode$$parseToJson, origin, otherCopyArgs, prettyJson, productIterator, productPrefix, stringArgs, toJSON, toString, transform, transformChildren, transformDown, transformUp, treeString, treeString, withNewChildren Methods inherited from class Object equals, getClass, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.sql.catalyst.plans.logical.Command children, output Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail CreateHiveTableAsSelectCommand public CreateHiveTableAsSelectCommand(org.apache.spark.sql.catalyst.catalog.CatalogTable tableDesc, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan query, boolean ignoreIfExists) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() origin public static org.apache.spark.sql.catalyst.trees.Origin origin() containsChild public static scala.collection.immutable.Set<org.apache.spark.sql.catalyst.trees.TreeNode<?>> containsChild() hashCode public static int hashCode() fastEquals public static boolean fastEquals(org.apache.spark.sql.catalyst.trees.TreeNode<?> other) find public static scala.Option<BaseType> find(scala.Function1<BaseType,Object> f) foreach public static void foreach(scala.Function1<BaseType,scala.runtime.BoxedUnit> f) foreachUp public static void foreachUp(scala.Function1<BaseType,scala.runtime.BoxedUnit> f) map public static <A> scala.collection.Seq<A> map(scala.Function1<BaseType,A> f) flatMap public static <A> scala.collection.Seq<A> flatMap(scala.Function1<BaseType,scala.collection.TraversableOnce<A>> f) collect public static <B> scala.collection.Seq<B> collect(scala.PartialFunction<BaseType,B> pf) collectLeaves public static scala.collection.Seq<BaseType> collectLeaves() collectFirst public static <B> scala.Option<B> collectFirst(scala.PartialFunction<BaseType,B> pf) mapChildren public static BaseType mapChildren(scala.Function1<BaseType,BaseType> f) withNewChildren public static BaseType withNewChildren(scala.collection.Seq<BaseType> newChildren) transform public static BaseType transform(scala.PartialFunction<BaseType,BaseType> rule) transformDown public static BaseType transformDown(scala.PartialFunction<BaseType,BaseType> rule) transformUp public static BaseType transformUp(scala.PartialFunction<BaseType,BaseType> rule) makeCopy public static BaseType makeCopy(Object[] newArgs) nodeName public static String nodeName() toString public static String toString() treeString public static String treeString() treeString public static String treeString(boolean verbose) numberedTreeString public static String numberedTreeString() apply public static BaseType apply(int number) generateTreeString public static scala.collection.mutable.StringBuilder generateTreeString(int depth, scala.collection.Seq<Object> lastChildren, scala.collection.mutable.StringBuilder builder, boolean verbose, String prefix) asCode public static String asCode() toJSON public static String toJSON() prettyJson public static String prettyJson() generateTreeString$default$5 public static String generateTreeString$default$5() constraints public static org.apache.spark.sql.catalyst.expressions.ExpressionSet constraints() outputSet public static org.apache.spark.sql.catalyst.expressions.AttributeSet outputSet() references public static org.apache.spark.sql.catalyst.expressions.AttributeSet references() inputSet public static org.apache.spark.sql.catalyst.expressions.AttributeSet inputSet() missingInput public static org.apache.spark.sql.catalyst.expressions.AttributeSet missingInput() transformExpressions public static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule) transformExpressionsDown public static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressionsDown(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule) transformExpressionsUp public static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressionsUp(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule) transformAllExpressions public static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformAllExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule) expressions public static final scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> expressions() schema public static StructType schema() schemaString public static String schemaString() printSchema public static void printSchema() simpleString public static String simpleString() verboseString public static String verboseString() subqueries public static scala.collection.Seq<PlanType> subqueries() sameResult public static boolean sameResult(PlanType plan) allAttributes public static org.apache.spark.sql.catalyst.expressions.AttributeSeq allAttributes() analyzed public static boolean analyzed() isStreaming public static boolean isStreaming() resolveOperators public static org.apache.spark.sql.catalyst.plans.logical.LogicalPlan resolveOperators(scala.PartialFunction<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> rule) resolveExpressions public static org.apache.spark.sql.catalyst.plans.logical.LogicalPlan resolveExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> r) statistics public static org.apache.spark.sql.catalyst.plans.logical.Statistics statistics() maxRows public static scala.Option<Object> maxRows() resolved public static boolean resolved() childrenResolved public static boolean childrenResolved() canonicalized public static org.apache.spark.sql.catalyst.plans.logical.LogicalPlan canonicalized() resolve public static scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> resolve(StructType schema, scala.Function2<String,String,Object> resolver) resolveChildren public static scala.Option<org.apache.spark.sql.catalyst.expressions.NamedExpression> resolveChildren(scala.collection.Seq<String> nameParts, scala.Function2<String,String,Object> resolver) resolve public static scala.Option<org.apache.spark.sql.catalyst.expressions.NamedExpression> resolve(scala.collection.Seq<String> nameParts, scala.Function2<String,String,Object> resolver) resolveQuoted public static scala.Option<org.apache.spark.sql.catalyst.expressions.NamedExpression> resolveQuoted(String name, scala.Function2<String,String,Object> resolver) refresh public static void refresh() producedAttributes public static org.apache.spark.sql.catalyst.expressions.AttributeSet producedAttributes() children public static final scala.collection.Seq<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> children() output public static scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> output() tableDesc public org.apache.spark.sql.catalyst.catalog.CatalogTable tableDesc() query public org.apache.spark.sql.catalyst.plans.logical.LogicalPlan query() ignoreIfExists public boolean ignoreIfExists() innerChildren public scala.collection.Seq<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> innerChildren() run public scala.collection.Seq<Row> run(SparkSession sparkSession) Specified by: run in interface org.apache.spark.sql.execution.command.RunnableCommand argString public String argString() Overrides: argString in class org.apache.spark.sql.catalyst.trees.TreeNode<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan> log_ public org.slf4j.Logger log_() initializeLogging public void initializeLogging(boolean isInterpreter) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CrossValidator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CrossValidator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tuning Class CrossValidator Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<CrossValidatorModel> org.apache.spark.ml.tuning.CrossValidator All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class CrossValidator extends Estimator<CrossValidatorModel> implements MLWritable K-fold cross validation. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description CrossValidator()  CrossValidator(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  CrossValidator copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static Param<Estimator<?>> estimator()  Param<Estimator<?>> estimator() param for the estimator to be validated static Param<ParamMap[]> estimatorParamMaps()  Param<ParamMap[]> estimatorParamMaps() param for estimator param maps static Param<Evaluator> evaluator()  Param<Evaluator> evaluator() param for the evaluator used to select hyper-parameters that maximize the validated metric static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  CrossValidatorModel fit(Dataset<?> dataset) Fits a model to the input data. static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static Estimator<?> getEstimator()  Estimator<?> getEstimator()  static ParamMap[] getEstimatorParamMaps()  ParamMap[] getEstimatorParamMaps()  static Evaluator getEvaluator()  Evaluator getEvaluator()  static int getNumFolds()  int getNumFolds()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static long getSeed()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static CrossValidator load(String path)  static IntParam numFolds()  IntParam numFolds() Param for number of folds for cross validation. static Param<?>[] params()  static MLReader<CrossValidator> read()  static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  CrossValidator setEstimator(Estimator<?> value)  CrossValidator setEstimatorParamMaps(ParamMap[] value)  CrossValidator setEvaluator(Evaluator value)  CrossValidator setNumFolds(int value)  CrossValidator setSeed(long value)  static String toString()  StructType transformSchema(StructType schema) :: DeveloperApi :: StructType transformSchemaImpl(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.MLWritable save Constructor Detail CrossValidator public CrossValidator(String uid) CrossValidator public CrossValidator() Method Detail read public static MLReader<CrossValidator> read() load public static CrossValidator load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() seed public static final LongParam seed() getSeed public static final long getSeed() estimator public static Param<Estimator<?>> estimator() getEstimator public static Estimator<?> getEstimator() estimatorParamMaps public static Param<ParamMap[]> estimatorParamMaps() getEstimatorParamMaps public static ParamMap[] getEstimatorParamMaps() evaluator public static Param<Evaluator> evaluator() getEvaluator public static Evaluator getEvaluator() numFolds public static IntParam numFolds() getNumFolds public static int getNumFolds() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setEstimator public CrossValidator setEstimator(Estimator<?> value) setEstimatorParamMaps public CrossValidator setEstimatorParamMaps(ParamMap[] value) setEvaluator public CrossValidator setEvaluator(Evaluator value) setNumFolds public CrossValidator setNumFolds(int value) setSeed public CrossValidator setSeed(long value) fit public CrossValidatorModel fit(Dataset<?> dataset) Description copied from class: Estimator Fits a model to the input data. Specified by: fit in class Estimator<CrossValidatorModel> Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public CrossValidator copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Estimator<CrossValidatorModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) numFolds public IntParam numFolds() Param for number of folds for cross validation. Must be >= 2. Default: 3 Returns:(undocumented) getNumFolds public int getNumFolds() estimator public Param<Estimator<?>> estimator() param for the estimator to be validated Returns:(undocumented) getEstimator public Estimator<?> getEstimator() estimatorParamMaps public Param<ParamMap[]> estimatorParamMaps() param for estimator param maps Returns:(undocumented) getEstimatorParamMaps public ParamMap[] getEstimatorParamMaps() evaluator public Param<Evaluator> evaluator() param for the evaluator used to select hyper-parameters that maximize the validated metric Returns:(undocumented) getEvaluator public Evaluator getEvaluator() transformSchemaImpl public StructType transformSchemaImpl(StructType schema) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method CrossValidatorModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="CrossValidatorModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tuning Class CrossValidatorModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<CrossValidatorModel> org.apache.spark.ml.tuning.CrossValidatorModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class CrossValidatorModel extends Model<CrossValidatorModel> implements MLWritable Model from k-fold cross validation. param: bestModel The best model selected from k-fold cross validation. param: avgMetrics Average cross-validation metrics for each paramMap in CrossValidator.estimatorParamMaps, in the corresponding order. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description double[] avgMetrics()  Model<?> bestModel()  static Params clear(Param<?> param)  CrossValidatorModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static Param<Estimator<?>> estimator()  Param<Estimator<?>> estimator() param for the estimator to be validated static Param<ParamMap[]> estimatorParamMaps()  Param<ParamMap[]> estimatorParamMaps() param for estimator param maps static Param<Evaluator> evaluator()  Param<Evaluator> evaluator() param for the evaluator used to select hyper-parameters that maximize the validated metric static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static Estimator<?> getEstimator()  Estimator<?> getEstimator()  static ParamMap[] getEstimatorParamMaps()  ParamMap[] getEstimatorParamMaps()  static Evaluator getEvaluator()  Evaluator getEvaluator()  static int getNumFolds()  int getNumFolds()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static long getSeed()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static CrossValidatorModel load(String path)  static IntParam numFolds()  IntParam numFolds() Param for number of folds for cross validation. static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static MLReader<CrossValidatorModel> read()  static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  static M setParent(Estimator<M> parent)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: StructType transformSchemaImpl(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.MLWritable save Method Detail read public static MLReader<CrossValidatorModel> read() load public static CrossValidatorModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() seed public static final LongParam seed() getSeed public static final long getSeed() estimator public static Param<Estimator<?>> estimator() getEstimator public static Estimator<?> getEstimator() estimatorParamMaps public static Param<ParamMap[]> estimatorParamMaps() getEstimatorParamMaps public static ParamMap[] getEstimatorParamMaps() evaluator public static Param<Evaluator> evaluator() getEvaluator public static Evaluator getEvaluator() numFolds public static IntParam numFolds() getNumFolds public static int getNumFolds() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) bestModel public Model<?> bestModel() avgMetrics public double[] avgMetrics() transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public CrossValidatorModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<CrossValidatorModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) numFolds public IntParam numFolds() Param for number of folds for cross validation. Must be >= 2. Default: 3 Returns:(undocumented) getNumFolds public int getNumFolds() estimator public Param<Estimator<?>> estimator() param for the estimator to be validated Returns:(undocumented) getEstimator public Estimator<?> getEstimator() estimatorParamMaps public Param<ParamMap[]> estimatorParamMaps() param for estimator param maps Returns:(undocumented) getEstimatorParamMaps public ParamMap[] getEstimatorParamMaps() evaluator public Param<Evaluator> evaluator() param for the evaluator used to select hyper-parameters that maximize the validated metric Returns:(undocumented) getEvaluator public Evaluator getEvaluator() transformSchemaImpl public StructType transformSchemaImpl(StructType schema) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DB2Dialect (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DB2Dialect (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class DB2Dialect Object org.apache.spark.sql.jdbc.DB2Dialect public class DB2Dialect extends Object Constructor Summary Constructors  Constructor and Description DB2Dialect()  Method Summary Methods  Modifier and Type Method and Description static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties)  static boolean canHandle(String url)  static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md)  static scala.Option<JdbcType> getJDBCType(DataType dt)  static String getTableExistsQuery(String table)  static String quoteIdentifier(String colName)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DB2Dialect public DB2Dialect() Method Detail canHandle public static boolean canHandle(String url) getJDBCType public static scala.Option<JdbcType> getJDBCType(DataType dt) getCatalystType public static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) quoteIdentifier public static String quoteIdentifier(String colName) getTableExistsQuery public static String getTableExistsQuery(String table) beforeFetch public static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DCT (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DCT (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class DCT Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.UnaryTransformer<Vector,Vector,DCT> org.apache.spark.ml.feature.DCT All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class DCT extends UnaryTransformer<Vector,Vector,DCT> implements DefaultParamsWritable A feature transformer that takes the 1D discrete cosine transform of a real vector. No zero padding is performed on the input vector. It returns a real vector of the same length representing the DCT. The return vector is scaled such that the transform matrix is unitary (aka scaled DCT-II). More information on Wikipedia. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DCT()  DCT(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  static T copy(ParamMap extra)  static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  boolean getInverse()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  BooleanParam inverse() Indicates whether to perform the inverse DCT (true) or forward DCT (false). static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static DCT load(String path)  static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  static T setInputCol(String value)  DCT setInverse(boolean value)  static T setOutputCol(String value)  static String toString()  static Dataset<Row> transform(Dataset<?> dataset)  static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.UnaryTransformer copy, setInputCol, setOutputCol, transform, transformSchema Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail DCT public DCT(String uid) DCT public DCT() Method Detail load public static DCT load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() setInputCol public static T setInputCol(String value) setOutputCol public static T setOutputCol(String value) transformSchema public static StructType transformSchema(StructType schema) transform public static Dataset<Row> transform(Dataset<?> dataset) copy public static T copy(ParamMap extra) save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) inverse public BooleanParam inverse() Indicates whether to perform the inverse DCT (true) or forward DCT (false). Default: false Returns:(undocumented) setInverse public DCT setInverse(boolean value) getInverse public boolean getInverse() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.dstream Class DStream<T> Object org.apache.spark.streaming.dstream.DStream<T> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: InputDStream, MapWithStateDStream public abstract class DStream<T> extends Object implements scala.Serializable A Discretized Stream (DStream), the basic abstraction in Spark Streaming, is a continuous sequence of RDDs (of the same type) representing a continuous stream of data (see org.apache.spark.rdd.RDD in the Spark core documentation for more details on RDDs). DStreams can either be created from live data (such as, data from TCP sockets, Kafka, Flume, etc.) using a StreamingContext or it can be generated by transforming existing DStreams using operations such as map, window and reduceByKeyAndWindow. While a Spark Streaming program is running, each DStream periodically generates a RDD, either from live data or by transforming the RDD generated by a parent DStream. This class contains the basic operations available on all DStreams, such as map, filter and window. In addition, PairDStreamFunctions contains operations available only on DStreams of key-value pairs, such as groupByKeyAndWindow and join. These operations are automatically available on any DStream of pairs (e.g., DStream[(Int, Int)] through implicit conversions. A DStream internally is characterized by a few basic properties: - A list of other DStreams that the DStream depends on - A time interval at which the DStream generates an RDD - A function that is used to generate an RDD after each time interval See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DStream(StreamingContext ssc, scala.reflect.ClassTag<T> evidence$1)  Method Summary Methods  Modifier and Type Method and Description DStream<T> cache() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) DStream<T> checkpoint(Duration interval) Enable periodic checkpointing of RDDs of this DStream abstract scala.Option<RDD<T>> compute(Time validTime) Method that generates a RDD for the given time StreamingContext context() Return the StreamingContext associated with this DStream DStream<Object> count() Return a new DStream in which each RDD has a single element generated by counting each RDD of this DStream. DStream<scala.Tuple2<T,Object>> countByValue(int numPartitions, scala.math.Ordering<T> ord) Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream. DStream<scala.Tuple2<T,Object>> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions, scala.math.Ordering<T> ord) Return a new DStream in which each RDD contains the count of distinct elements in RDDs in a sliding window over this DStream. DStream<Object> countByWindow(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by counting the number of elements in a sliding window over this DStream. abstract scala.collection.immutable.List<DStream<?>> dependencies() List of parent DStreams on which this DStream depends on DStream<T> filter(scala.Function1<T,Object> filterFunc) Return a new DStream containing only the elements that satisfy a predicate. <U> DStream<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> flatMapFunc, scala.reflect.ClassTag<U> evidence$3) Return a new DStream by applying a function to all elements of this DStream, and then flattening the results void foreachRDD(scala.Function1<RDD<T>,scala.runtime.BoxedUnit> foreachFunc) Apply a function to each RDD in this DStream. void foreachRDD(scala.Function2<RDD<T>,Time,scala.runtime.BoxedUnit> foreachFunc) Apply a function to each RDD in this DStream. DStream<Object> glom() Return a new DStream in which each RDD is generated by applying glom() to each RDD of this DStream. <U> DStream<U> map(scala.Function1<T,U> mapFunc, scala.reflect.ClassTag<U> evidence$2) Return a new DStream by applying a function to all elements of this DStream. <U> DStream<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> mapPartFunc, boolean preservePartitioning, scala.reflect.ClassTag<U> evidence$4) Return a new DStream in which each RDD is generated by applying mapPartitions() to each RDDs of this DStream. DStream<T> persist() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) DStream<T> persist(StorageLevel level) Persist the RDDs of this DStream with the given storage level void print() Print the first ten elements of each RDD generated in this DStream. void print(int num) Print the first num elements of each RDD generated in this DStream. DStream<T> reduce(scala.Function2<T,T,T> reduceFunc) Return a new DStream in which each RDD has a single element generated by reducing each RDD of this DStream. DStream<T> reduceByWindow(scala.Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream. DStream<T> reduceByWindow(scala.Function2<T,T,T> reduceFunc, scala.Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream. DStream<T> repartition(int numPartitions) Return a new DStream with an increased or decreased level of parallelism. void saveAsObjectFiles(String prefix, String suffix) Save each RDD in this DStream as a Sequence file of serialized objects. void saveAsTextFiles(String prefix, String suffix) Save each RDD in this DStream as at text file, using string representation of elements. scala.collection.Seq<RDD<T>> slice(org.apache.spark.streaming.Interval interval) Return all the RDDs defined by the Interval object (both end times included) scala.collection.Seq<RDD<T>> slice(Time fromTime, Time toTime) Return all the RDDs between 'fromTime' to 'toTime' (both included) abstract Duration slideDuration() Time interval after which the DStream generates a RDD static <K,V> PairDStreamFunctions<K,V> toPairDStreamFunctions(DStream<scala.Tuple2<K,V>> stream, scala.reflect.ClassTag<K> kt, scala.reflect.ClassTag<V> vt, scala.math.Ordering<K> ord)  <U> DStream<U> transform(scala.Function1<RDD<T>,RDD<U>> transformFunc, scala.reflect.ClassTag<U> evidence$5) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. <U> DStream<U> transform(scala.Function2<RDD<T>,Time,RDD<U>> transformFunc, scala.reflect.ClassTag<U> evidence$6) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. <U,V> DStream<V> transformWith(DStream<U> other, scala.Function2<RDD<T>,RDD<U>,RDD<V>> transformFunc, scala.reflect.ClassTag<U> evidence$7, scala.reflect.ClassTag<V> evidence$8) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. <U,V> DStream<V> transformWith(DStream<U> other, scala.Function3<RDD<T>,RDD<U>,Time,RDD<V>> transformFunc, scala.reflect.ClassTag<U> evidence$9, scala.reflect.ClassTag<V> evidence$10) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. DStream<T> union(DStream<T> that) Return a new DStream by unifying data of another DStream with this DStream. DStream<T> window(Duration windowDuration) Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream. DStream<T> window(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DStream public DStream(StreamingContext ssc, scala.reflect.ClassTag<T> evidence$1) Method Detail toPairDStreamFunctions public static <K,V> PairDStreamFunctions<K,V> toPairDStreamFunctions(DStream<scala.Tuple2<K,V>> stream, scala.reflect.ClassTag<K> kt, scala.reflect.ClassTag<V> vt, scala.math.Ordering<K> ord) slideDuration public abstract Duration slideDuration() Time interval after which the DStream generates a RDD dependencies public abstract scala.collection.immutable.List<DStream<?>> dependencies() List of parent DStreams on which this DStream depends on compute public abstract scala.Option<RDD<T>> compute(Time validTime) Method that generates a RDD for the given time context public StreamingContext context() Return the StreamingContext associated with this DStream persist public DStream<T> persist(StorageLevel level) Persist the RDDs of this DStream with the given storage level persist public DStream<T> persist() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) cache public DStream<T> cache() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) checkpoint public DStream<T> checkpoint(Duration interval) Enable periodic checkpointing of RDDs of this DStream Parameters:interval - Time interval after which generated RDD will be checkpointed Returns:(undocumented) map public <U> DStream<U> map(scala.Function1<T,U> mapFunc, scala.reflect.ClassTag<U> evidence$2) Return a new DStream by applying a function to all elements of this DStream. flatMap public <U> DStream<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> flatMapFunc, scala.reflect.ClassTag<U> evidence$3) Return a new DStream by applying a function to all elements of this DStream, and then flattening the results Parameters:flatMapFunc - (undocumented)evidence$3 - (undocumented) Returns:(undocumented) filter public DStream<T> filter(scala.Function1<T,Object> filterFunc) Return a new DStream containing only the elements that satisfy a predicate. glom public DStream<Object> glom() Return a new DStream in which each RDD is generated by applying glom() to each RDD of this DStream. Applying glom() to an RDD coalesces all elements within each partition into an array. Returns:(undocumented) repartition public DStream<T> repartition(int numPartitions) Return a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented) mapPartitions public <U> DStream<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> mapPartFunc, boolean preservePartitioning, scala.reflect.ClassTag<U> evidence$4) Return a new DStream in which each RDD is generated by applying mapPartitions() to each RDDs of this DStream. Applying mapPartitions() to an RDD applies a function to each partition of the RDD. Parameters:mapPartFunc - (undocumented)preservePartitioning - (undocumented)evidence$4 - (undocumented) Returns:(undocumented) reduce public DStream<T> reduce(scala.Function2<T,T,T> reduceFunc) Return a new DStream in which each RDD has a single element generated by reducing each RDD of this DStream. Parameters:reduceFunc - (undocumented) Returns:(undocumented) count public DStream<Object> count() Return a new DStream in which each RDD has a single element generated by counting each RDD of this DStream. Returns:(undocumented) countByValue public DStream<scala.Tuple2<T,Object>> countByValue(int numPartitions, scala.math.Ordering<T> ord) Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions (Spark's default number of partitions if numPartitions not specified). Parameters:numPartitions - (undocumented)ord - (undocumented) Returns:(undocumented) foreachRDD public void foreachRDD(scala.Function1<RDD<T>,scala.runtime.BoxedUnit> foreachFunc) Apply a function to each RDD in this DStream. This is an output operator, so 'this' DStream will be registered as an output stream and therefore materialized. Parameters:foreachFunc - (undocumented) foreachRDD public void foreachRDD(scala.Function2<RDD<T>,Time,scala.runtime.BoxedUnit> foreachFunc) Apply a function to each RDD in this DStream. This is an output operator, so 'this' DStream will be registered as an output stream and therefore materialized. Parameters:foreachFunc - (undocumented) transform public <U> DStream<U> transform(scala.Function1<RDD<T>,RDD<U>> transformFunc, scala.reflect.ClassTag<U> evidence$5) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. Parameters:transformFunc - (undocumented)evidence$5 - (undocumented) Returns:(undocumented) transform public <U> DStream<U> transform(scala.Function2<RDD<T>,Time,RDD<U>> transformFunc, scala.reflect.ClassTag<U> evidence$6) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. Parameters:transformFunc - (undocumented)evidence$6 - (undocumented) Returns:(undocumented) transformWith public <U,V> DStream<V> transformWith(DStream<U> other, scala.Function2<RDD<T>,RDD<U>,RDD<V>> transformFunc, scala.reflect.ClassTag<U> evidence$7, scala.reflect.ClassTag<V> evidence$8) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. Parameters:other - (undocumented)transformFunc - (undocumented)evidence$7 - (undocumented)evidence$8 - (undocumented) Returns:(undocumented) transformWith public <U,V> DStream<V> transformWith(DStream<U> other, scala.Function3<RDD<T>,RDD<U>,Time,RDD<V>> transformFunc, scala.reflect.ClassTag<U> evidence$9, scala.reflect.ClassTag<V> evidence$10) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. Parameters:other - (undocumented)transformFunc - (undocumented)evidence$9 - (undocumented)evidence$10 - (undocumented) Returns:(undocumented) print public void print() Print the first ten elements of each RDD generated in this DStream. This is an output operator, so this DStream will be registered as an output stream and there materialized. print public void print(int num) Print the first num elements of each RDD generated in this DStream. This is an output operator, so this DStream will be registered as an output stream and there materialized. Parameters:num - (undocumented) window public DStream<T> window(Duration windowDuration) Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream. The new DStream generates RDDs with the same interval as this DStream. Parameters:windowDuration - width of the window; must be a multiple of this DStream's interval. Returns:(undocumented) window public DStream<T> window(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) reduceByWindow public DStream<T> reduceByWindow(scala.Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream. Parameters:reduceFunc - associative and commutative reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) reduceByWindow public DStream<T> reduceByWindow(scala.Function2<T,T,T> reduceFunc, scala.Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream. However, the reduction is done incrementally using the old window's reduced value : 1. reduce the new values that entered the window (e.g., adding new counts) 2. "inverse reduce" the old values that left the window (e.g., subtracting old counts) This is more efficient than reduceByWindow without "inverse reduce" function. However, it is applicable to only "invertible reduce functions". Parameters:reduceFunc - associative and commutative reduce functioninvReduceFunc - inverse reduce function; such that for all y, invertible x: invReduceFunc(reduceFunc(x, y), x) = ywindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) countByWindow public DStream<Object> countByWindow(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by counting the number of elements in a sliding window over this DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) countByValueAndWindow public DStream<scala.Tuple2<T,Object>> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions, scala.math.Ordering<T> ord) Return a new DStream in which each RDD contains the count of distinct elements in RDDs in a sliding window over this DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions (Spark's default number of partitions if numPartitions not specified). Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalnumPartitions - number of partitions of each RDD in the new DStream.ord - (undocumented) Returns:(undocumented) union public DStream<T> union(DStream<T> that) Return a new DStream by unifying data of another DStream with this DStream. Parameters:that - Another DStream having the same slideDuration as this DStream. Returns:(undocumented) slice public scala.collection.Seq<RDD<T>> slice(org.apache.spark.streaming.Interval interval) Return all the RDDs defined by the Interval object (both end times included) Parameters:interval - (undocumented) Returns:(undocumented) slice public scala.collection.Seq<RDD<T>> slice(Time fromTime, Time toTime) Return all the RDDs between 'fromTime' to 'toTime' (both included) Parameters:fromTime - (undocumented)toTime - (undocumented) Returns:(undocumented) saveAsObjectFiles public void saveAsObjectFiles(String prefix, String suffix) Save each RDD in this DStream as a Sequence file of serialized objects. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented) saveAsTextFiles public void saveAsTextFiles(String prefix, String suffix) Save each RDD in this DStream as at text file, using string representation of elements. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataFrameNaFunctions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataFrameNaFunctions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class DataFrameNaFunctions Object org.apache.spark.sql.DataFrameNaFunctions public final class DataFrameNaFunctions extends Object :: Experimental :: Functionality for working with missing data in DataFrames. Since: 1.3.1 Method Summary Methods  Modifier and Type Method and Description Dataset<Row> drop() Returns a new DataFrame that drops rows containing any null or NaN values. Dataset<Row> drop(int minNonNulls) Returns a new DataFrame that drops rows containing less than minNonNulls non-null and non-NaN values. Dataset<Row> drop(int minNonNulls, scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that drops rows containing less than minNonNulls non-null and non-NaN values in the specified columns. Dataset<Row> drop(int minNonNulls, String[] cols) Returns a new DataFrame that drops rows containing less than minNonNulls non-null and non-NaN values in the specified columns. Dataset<Row> drop(scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that drops rows containing any null or NaN values in the specified columns. Dataset<Row> drop(String how) Returns a new DataFrame that drops rows containing null or NaN values. Dataset<Row> drop(String[] cols) Returns a new DataFrame that drops rows containing any null or NaN values in the specified columns. Dataset<Row> drop(String how, scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that drops rows containing null or NaN values in the specified columns. Dataset<Row> drop(String how, String[] cols) Returns a new DataFrame that drops rows containing null or NaN values in the specified columns. Dataset<Row> fill(double value) Returns a new DataFrame that replaces null or NaN values in numeric columns with value. Dataset<Row> fill(double value, scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that replaces null or NaN values in specified numeric columns. Dataset<Row> fill(double value, String[] cols) Returns a new DataFrame that replaces null or NaN values in specified numeric columns. Dataset<Row> fill(java.util.Map<String,Object> valueMap) Returns a new DataFrame that replaces null values. Dataset<Row> fill(scala.collection.immutable.Map<String,Object> valueMap) (Scala-specific) Returns a new DataFrame that replaces null values. Dataset<Row> fill(String value) Returns a new DataFrame that replaces null values in string columns with value. Dataset<Row> fill(String value, scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that replaces null values in specified string columns. Dataset<Row> fill(String value, String[] cols) Returns a new DataFrame that replaces null values in specified string columns. <T> Dataset<Row> replace(scala.collection.Seq<String> cols, scala.collection.immutable.Map<T,T> replacement) (Scala-specific) Replaces values matching keys in replacement map. <T> Dataset<Row> replace(String[] cols, java.util.Map<T,T> replacement) Replaces values matching keys in replacement map with the corresponding values. <T> Dataset<Row> replace(String col, java.util.Map<T,T> replacement) Replaces values matching keys in replacement map with the corresponding values. <T> Dataset<Row> replace(String col, scala.collection.immutable.Map<T,T> replacement) (Scala-specific) Replaces values matching keys in replacement map. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail drop public Dataset<Row> drop() Returns a new DataFrame that drops rows containing any null or NaN values. Returns:(undocumented)Since: 1.3.1 drop public Dataset<Row> drop(String how) Returns a new DataFrame that drops rows containing null or NaN values. If how is "any", then drop rows containing any null or NaN values. If how is "all", then drop rows only if every column is null or NaN for that row. Parameters:how - (undocumented) Returns:(undocumented)Since: 1.3.1 drop public Dataset<Row> drop(String[] cols) Returns a new DataFrame that drops rows containing any null or NaN values in the specified columns. Parameters:cols - (undocumented) Returns:(undocumented)Since: 1.3.1 drop public Dataset<Row> drop(scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that drops rows containing any null or NaN values in the specified columns. Parameters:cols - (undocumented) Returns:(undocumented)Since: 1.3.1 drop public Dataset<Row> drop(String how, String[] cols) Returns a new DataFrame that drops rows containing null or NaN values in the specified columns. If how is "any", then drop rows containing any null or NaN values in the specified columns. If how is "all", then drop rows only if every specified column is null or NaN for that row. Parameters:how - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.3.1 drop public Dataset<Row> drop(String how, scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that drops rows containing null or NaN values in the specified columns. If how is "any", then drop rows containing any null or NaN values in the specified columns. If how is "all", then drop rows only if every specified column is null or NaN for that row. Parameters:how - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.3.1 drop public Dataset<Row> drop(int minNonNulls) Returns a new DataFrame that drops rows containing less than minNonNulls non-null and non-NaN values. Parameters:minNonNulls - (undocumented) Returns:(undocumented)Since: 1.3.1 drop public Dataset<Row> drop(int minNonNulls, String[] cols) Returns a new DataFrame that drops rows containing less than minNonNulls non-null and non-NaN values in the specified columns. Parameters:minNonNulls - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.3.1 drop public Dataset<Row> drop(int minNonNulls, scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that drops rows containing less than minNonNulls non-null and non-NaN values in the specified columns. Parameters:minNonNulls - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.3.1 fill public Dataset<Row> fill(double value) Returns a new DataFrame that replaces null or NaN values in numeric columns with value. Parameters:value - (undocumented) Returns:(undocumented)Since: 1.3.1 fill public Dataset<Row> fill(String value) Returns a new DataFrame that replaces null values in string columns with value. Parameters:value - (undocumented) Returns:(undocumented)Since: 1.3.1 fill public Dataset<Row> fill(double value, String[] cols) Returns a new DataFrame that replaces null or NaN values in specified numeric columns. If a specified column is not a numeric column, it is ignored. Parameters:value - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.3.1 fill public Dataset<Row> fill(double value, scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that replaces null or NaN values in specified numeric columns. If a specified column is not a numeric column, it is ignored. Parameters:value - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.3.1 fill public Dataset<Row> fill(String value, String[] cols) Returns a new DataFrame that replaces null values in specified string columns. If a specified column is not a string column, it is ignored. Parameters:value - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.3.1 fill public Dataset<Row> fill(String value, scala.collection.Seq<String> cols) (Scala-specific) Returns a new DataFrame that replaces null values in specified string columns. If a specified column is not a string column, it is ignored. Parameters:value - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.3.1 fill public Dataset<Row> fill(java.util.Map<String,Object> valueMap) Returns a new DataFrame that replaces null values. The key of the map is the column name, and the value of the map is the replacement value. The value must be of the following type: Integer, Long, Float, Double, String, Boolean. Replacement values are cast to the column data type. For example, the following replaces null values in column "A" with string "unknown", and null values in column "B" with numeric value 1.0. import com.google.common.collect.ImmutableMap; df.na.fill(ImmutableMap.of("A", "unknown", "B", 1.0)); Parameters:valueMap - (undocumented) Returns:(undocumented)Since: 1.3.1 fill public Dataset<Row> fill(scala.collection.immutable.Map<String,Object> valueMap) (Scala-specific) Returns a new DataFrame that replaces null values. The key of the map is the column name, and the value of the map is the replacement value. The value must be of the following type: Int, Long, Float, Double, String, Boolean. Replacement values are cast to the column data type. For example, the following replaces null values in column "A" with string "unknown", and null values in column "B" with numeric value 1.0. df.na.fill(Map( "A" -> "unknown", "B" -> 1.0 )) Parameters:valueMap - (undocumented) Returns:(undocumented)Since: 1.3.1 replace public <T> Dataset<Row> replace(String col, java.util.Map<T,T> replacement) Replaces values matching keys in replacement map with the corresponding values. Key and value of replacement map must have the same type, and can only be doubles, strings or booleans. If col is "*", then the replacement is applied on all string columns or numeric columns. import com.google.common.collect.ImmutableMap; // Replaces all occurrences of 1.0 with 2.0 in column "height". df.replace("height", ImmutableMap.of(1.0, 2.0)); // Replaces all occurrences of "UNKNOWN" with "unnamed" in column "name". df.replace("name", ImmutableMap.of("UNKNOWN", "unnamed")); // Replaces all occurrences of "UNKNOWN" with "unnamed" in all string columns. df.replace("*", ImmutableMap.of("UNKNOWN", "unnamed")); Parameters:col - name of the column to apply the value replacementreplacement - value replacement map, as explained above Returns:(undocumented)Since: 1.3.1 replace public <T> Dataset<Row> replace(String[] cols, java.util.Map<T,T> replacement) Replaces values matching keys in replacement map with the corresponding values. Key and value of replacement map must have the same type, and can only be doubles, strings or booleans. import com.google.common.collect.ImmutableMap; // Replaces all occurrences of 1.0 with 2.0 in column "height" and "weight". df.replace(new String[] {"height", "weight"}, ImmutableMap.of(1.0, 2.0)); // Replaces all occurrences of "UNKNOWN" with "unnamed" in column "firstname" and "lastname". df.replace(new String[] {"firstname", "lastname"}, ImmutableMap.of("UNKNOWN", "unnamed")); Parameters:cols - list of columns to apply the value replacementreplacement - value replacement map, as explained above Returns:(undocumented)Since: 1.3.1 replace public <T> Dataset<Row> replace(String col, scala.collection.immutable.Map<T,T> replacement) (Scala-specific) Replaces values matching keys in replacement map. Key and value of replacement map must have the same type, and can only be doubles, strings or booleans. If col is "*", then the replacement is applied on all string columns , numeric columns or boolean columns. // Replaces all occurrences of 1.0 with 2.0 in column "height". df.replace("height", Map(1.0 -> 2.0)) // Replaces all occurrences of "UNKNOWN" with "unnamed" in column "name". df.replace("name", Map("UNKNOWN" -> "unnamed") // Replaces all occurrences of "UNKNOWN" with "unnamed" in all string columns. df.replace("*", Map("UNKNOWN" -> "unnamed") Parameters:col - name of the column to apply the value replacementreplacement - value replacement map, as explained above Returns:(undocumented)Since: 1.3.1 replace public <T> Dataset<Row> replace(scala.collection.Seq<String> cols, scala.collection.immutable.Map<T,T> replacement) (Scala-specific) Replaces values matching keys in replacement map. Key and value of replacement map must have the same type, and can only be doubles , strings or booleans. // Replaces all occurrences of 1.0 with 2.0 in column "height" and "weight". df.replace("height" :: "weight" :: Nil, Map(1.0 -> 2.0)); // Replaces all occurrences of "UNKNOWN" with "unnamed" in column "firstname" and "lastname". df.replace("firstname" :: "lastname" :: Nil, Map("UNKNOWN" -> "unnamed"); Parameters:cols - list of columns to apply the value replacementreplacement - value replacement map, as explained above Returns:(undocumented)Since: 1.3.1 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataFrameReader (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataFrameReader (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class DataFrameReader Object org.apache.spark.sql.DataFrameReader public class DataFrameReader extends Object Interface used to load a Dataset from external storage systems (e.g. file systems, key-value stores, etc). Use SparkSession.read to access this. Since: 1.4.0 Method Summary Methods  Modifier and Type Method and Description Dataset<Row> csv(scala.collection.Seq<String> paths) Loads a CSV file and returns the result as a DataFrame. Dataset<Row> csv(String... paths) Loads a CSV file and returns the result as a DataFrame. Dataset<Row> csv(String path) Loads a CSV file and returns the result as a DataFrame. DataFrameReader format(String source) Specifies the input data source format. Dataset<Row> jdbc(String url, String table, java.util.Properties properties) Construct a DataFrame representing the database table accessible via JDBC URL url named table and connection properties. Dataset<Row> jdbc(String url, String table, String[] predicates, java.util.Properties connectionProperties) Construct a DataFrame representing the database table accessible via JDBC URL url named table using connection properties. Dataset<Row> jdbc(String url, String table, String columnName, long lowerBound, long upperBound, int numPartitions, java.util.Properties connectionProperties) Construct a DataFrame representing the database table accessible via JDBC URL url named table. Dataset<Row> json(JavaRDD<String> jsonRDD) Loads a JavaRDD[String] storing JSON objects (one object per record) and returns the result as a DataFrame. Dataset<Row> json(RDD<String> jsonRDD) Loads an RDD[String] storing JSON objects (one object per record) and returns the result as a DataFrame. Dataset<Row> json(scala.collection.Seq<String> paths) Loads a JSON file (one object per line) and returns the result as a DataFrame. Dataset<Row> json(String... paths) Loads a JSON file (one object per line) and returns the result as a DataFrame. Dataset<Row> json(String path) Loads a JSON file (one object per line) and returns the result as a DataFrame. Dataset<Row> load() Loads input in as a DataFrame, for data sources that don't require a path (e.g. Dataset<Row> load(scala.collection.Seq<String> paths) Loads input in as a DataFrame, for data sources that support multiple paths. Dataset<Row> load(String... paths) Loads input in as a DataFrame, for data sources that support multiple paths. Dataset<Row> load(String path) Loads input in as a DataFrame, for data sources that require a path (e.g. DataFrameReader option(String key, boolean value) Adds an input option for the underlying data source. DataFrameReader option(String key, double value) Adds an input option for the underlying data source. DataFrameReader option(String key, long value) Adds an input option for the underlying data source. DataFrameReader option(String key, String value) Adds an input option for the underlying data source. DataFrameReader options(scala.collection.Map<String,String> options) (Scala-specific) Adds input options for the underlying data source. DataFrameReader options(java.util.Map<String,String> options) Adds input options for the underlying data source. Dataset<Row> orc(scala.collection.Seq<String> paths) Loads an ORC file and returns the result as a DataFrame. Dataset<Row> orc(String... paths) Loads an ORC file and returns the result as a DataFrame. Dataset<Row> orc(String path) Loads an ORC file and returns the result as a DataFrame. Dataset<Row> parquet(scala.collection.Seq<String> paths) Loads a Parquet file, returning the result as a DataFrame. Dataset<Row> parquet(String... paths) Loads a Parquet file, returning the result as a DataFrame. Dataset<Row> parquet(String path) Loads a Parquet file, returning the result as a DataFrame. DataFrameReader schema(StructType schema) Specifies the input schema. Dataset<Row> table(String tableName) Returns the specified table as a DataFrame. Dataset<Row> text(scala.collection.Seq<String> paths) Loads text files and returns a DataFrame whose schema starts with a string column named "value", and followed by partitioned columns if there are any. Dataset<Row> text(String... paths) Loads text files and returns a DataFrame whose schema starts with a string column named "value", and followed by partitioned columns if there are any. Dataset<Row> text(String path) Loads text files and returns a DataFrame whose schema starts with a string column named "value", and followed by partitioned columns if there are any. Dataset<String> textFile(scala.collection.Seq<String> paths) Loads text files and returns a Dataset of String. Dataset<String> textFile(String... paths) Loads text files and returns a Dataset of String. Dataset<String> textFile(String path) Loads text files and returns a Dataset of String. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail load public Dataset<Row> load(String... paths) Loads input in as a DataFrame, for data sources that support multiple paths. Only works if the source is a HadoopFsRelationProvider. Parameters:paths - (undocumented) Returns:(undocumented)Since: 1.6.0 json public Dataset<Row> json(String... paths) Loads a JSON file (one object per line) and returns the result as a DataFrame. This function goes through the input once to determine the input schema. If you know the schema in advance, use the version that specifies the schema to avoid the extra scan. You can set the following JSON-specific options to deal with non-standard JSON files: primitivesAsString (default false): infers all primitive values as a string type prefersDecimal (default false): infers all floating-point values as a decimal type. If the values do not fit in decimal, then it infers them as doubles. allowComments (default false): ignores Java/C++ style comment in JSON records allowUnquotedFieldNames (default false): allows unquoted JSON field names allowSingleQuotes (default true): allows single quotes in addition to double quotes allowNumericLeadingZeros (default false): allows leading zeros in numbers (e.g. 00012) allowBackslashEscapingAnyCharacter (default false): allows accepting quoting of all character using backslash quoting mechanism mode (default PERMISSIVE): allows a mode for dealing with corrupt records during parsing. PERMISSIVE : sets other fields to null when it meets a corrupted record, and puts the malformed string into a new field configured by columnNameOfCorruptRecord. When a schema is set by user, it sets null for extra fields. DROPMALFORMED : ignores the whole corrupted records. FAILFAST : throws an exception when it meets corrupted records. columnNameOfCorruptRecord (default is the value specified in spark.sql.columnNameOfCorruptRecord): allows renaming the new field having malformed string created by PERMISSIVE mode. This overrides spark.sql.columnNameOfCorruptRecord. dateFormat (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. timestampFormat (default yyyy-MM-dd'T'HH:mm:ss.SSSZZ): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. Parameters:paths - (undocumented) Returns:(undocumented)Since: 2.0.0 csv public Dataset<Row> csv(String... paths) Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema if inferSchema is enabled. To avoid going through the entire data once, disable inferSchema option or specify the schema explicitly using schema. You can set the following CSV-specific options to deal with CSV files: sep (default ,): sets the single character as a separator for each field and value. encoding (default UTF-8): decodes the CSV files by the given encoding type. quote (default "): sets the single character used for escaping quoted values where the separator can be part of the value. If you would like to turn off quotations, you need to set not null but an empty string. This behaviour is different form com.databricks.spark.csv. escape (default \): sets the single character used for escaping quotes inside an already quoted value. comment (default empty string): sets the single character used for skipping lines beginning with this character. By default, it is disabled. header (default false): uses the first line as names of columns. inferSchema (default false): infers the input schema automatically from data. It requires one extra pass over the data. ignoreLeadingWhiteSpace (default false): defines whether or not leading whitespaces from values being read should be skipped. ignoreTrailingWhiteSpace (default false): defines whether or not trailing whitespaces from values being read should be skipped. nullValue (default empty string): sets the string representation of a null value. Since 2.0.1, this applies to all supported types including the string type. nanValue (default NaN): sets the string representation of a non-number" value. positiveInf (default Inf): sets the string representation of a positive infinity value. negativeInf (default -Inf): sets the string representation of a negative infinity value. dateFormat (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. timestampFormat (default yyyy-MM-dd'T'HH:mm:ss.SSSZZ): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. java.sql.Timestamp.valueOf() and java.sql.Date.valueOf() or ISO 8601 format. maxColumns (default 20480): defines a hard limit of how many columns a record can have. maxCharsPerColumn (default 1000000): defines the maximum number of characters allowed for any given value being read. maxMalformedLogPerPartition (default 10): sets the maximum number of malformed rows Spark will log for each partition. Malformed records beyond this number will be ignored. mode (default PERMISSIVE): allows a mode for dealing with corrupt records during parsing. PERMISSIVE : sets other fields to null when it meets a corrupted record. When a schema is set by user, it sets null for extra fields. DROPMALFORMED : ignores the whole corrupted records. FAILFAST : throws an exception when it meets corrupted records. Parameters:paths - (undocumented) Returns:(undocumented)Since: 2.0.0 parquet public Dataset<Row> parquet(String... paths) Loads a Parquet file, returning the result as a DataFrame. You can set the following Parquet-specific option(s) for reading Parquet files: mergeSchema (default is the value specified in spark.sql.parquet.mergeSchema): sets whether we should merge schemas collected from all Parquet part-files. This will override spark.sql.parquet.mergeSchema. Parameters:paths - (undocumented) Returns:(undocumented)Since: 1.4.0 orc public Dataset<Row> orc(String... paths) Loads an ORC file and returns the result as a DataFrame. Parameters:paths - input paths Returns:(undocumented)Since: 2.0.0 text public Dataset<Row> text(String... paths) Loads text files and returns a DataFrame whose schema starts with a string column named "value", and followed by partitioned columns if there are any. Each line in the text files is a new row in the resulting DataFrame. For example: // Scala: spark.read.text("/path/to/spark/README.md") // Java: spark.read().text("/path/to/spark/README.md") Parameters:paths - input paths Returns:(undocumented)Since: 1.6.0 textFile public Dataset<String> textFile(String... paths) Loads text files and returns a Dataset of String. The underlying schema of the Dataset contains a single string column named "value". If the directory structure of the text files contains partitioning information, those are ignored in the resulting Dataset. To include partitioning information as columns, use text. Each line in the text files is a new element in the resulting Dataset. For example: // Scala: spark.read.textFile("/path/to/spark/README.md") // Java: spark.read().textFile("/path/to/spark/README.md") Parameters:paths - input path Returns:(undocumented)Since: 2.0.0 format public DataFrameReader format(String source) Specifies the input data source format. Parameters:source - (undocumented) Returns:(undocumented)Since: 1.4.0 schema public DataFrameReader schema(StructType schema) Specifies the input schema. Some data sources (e.g. JSON) can infer the input schema automatically from data. By specifying the schema here, the underlying data source can skip the schema inference step, and thus speed up data loading. Parameters:schema - (undocumented) Returns:(undocumented)Since: 1.4.0 option public DataFrameReader option(String key, String value) Adds an input option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 1.4.0 option public DataFrameReader option(String key, boolean value) Adds an input option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataFrameReader option(String key, long value) Adds an input option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataFrameReader option(String key, double value) Adds an input option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 options public DataFrameReader options(scala.collection.Map<String,String> options) (Scala-specific) Adds input options for the underlying data source. Parameters:options - (undocumented) Returns:(undocumented)Since: 1.4.0 options public DataFrameReader options(java.util.Map<String,String> options) Adds input options for the underlying data source. Parameters:options - (undocumented) Returns:(undocumented)Since: 1.4.0 load public Dataset<Row> load() Loads input in as a DataFrame, for data sources that don't require a path (e.g. external key-value stores). Returns:(undocumented)Since: 1.4.0 load public Dataset<Row> load(String path) Loads input in as a DataFrame, for data sources that require a path (e.g. data backed by a local or distributed file system). Parameters:path - (undocumented) Returns:(undocumented)Since: 1.4.0 load public Dataset<Row> load(scala.collection.Seq<String> paths) Loads input in as a DataFrame, for data sources that support multiple paths. Only works if the source is a HadoopFsRelationProvider. Parameters:paths - (undocumented) Returns:(undocumented)Since: 1.6.0 jdbc public Dataset<Row> jdbc(String url, String table, java.util.Properties properties) Construct a DataFrame representing the database table accessible via JDBC URL url named table and connection properties. Parameters:url - (undocumented)table - (undocumented)properties - (undocumented) Returns:(undocumented)Since: 1.4.0 jdbc public Dataset<Row> jdbc(String url, String table, String columnName, long lowerBound, long upperBound, int numPartitions, java.util.Properties connectionProperties) Construct a DataFrame representing the database table accessible via JDBC URL url named table. Partitions of the table will be retrieved in parallel based on the parameters passed to this function. Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash your external database systems. Parameters:url - JDBC database url of the form jdbc:subprotocol:subname.table - Name of the table in the external database.columnName - the name of a column of integral type that will be used for partitioning.lowerBound - the minimum value of columnName used to decide partition stride.upperBound - the maximum value of columnName used to decide partition stride.numPartitions - the number of partitions. This, along with lowerBound (inclusive), upperBound (exclusive), form partition strides for generated WHERE clause expressions used to split the column columnName evenly.connectionProperties - JDBC database connection arguments, a list of arbitrary string tag/value. Normally at least a "user" and "password" property should be included. "fetchsize" can be used to control the number of rows per fetch. Returns:(undocumented)Since: 1.4.0 jdbc public Dataset<Row> jdbc(String url, String table, String[] predicates, java.util.Properties connectionProperties) Construct a DataFrame representing the database table accessible via JDBC URL url named table using connection properties. The predicates parameter gives a list expressions suitable for inclusion in WHERE clauses; each one defines one partition of the DataFrame. Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash your external database systems. Parameters:url - JDBC database url of the form jdbc:subprotocol:subnametable - Name of the table in the external database.predicates - Condition in the where clause for each partition.connectionProperties - JDBC database connection arguments, a list of arbitrary string tag/value. Normally at least a "user" and "password" property should be included. "fetchsize" can be used to control the number of rows per fetch. Returns:(undocumented)Since: 1.4.0 json public Dataset<Row> json(String path) Loads a JSON file (one object per line) and returns the result as a DataFrame. See the documentation on the overloaded json() method with varargs for more details. Parameters:path - (undocumented) Returns:(undocumented)Since: 1.4.0 json public Dataset<Row> json(scala.collection.Seq<String> paths) Loads a JSON file (one object per line) and returns the result as a DataFrame. This function goes through the input once to determine the input schema. If you know the schema in advance, use the version that specifies the schema to avoid the extra scan. You can set the following JSON-specific options to deal with non-standard JSON files: primitivesAsString (default false): infers all primitive values as a string type prefersDecimal (default false): infers all floating-point values as a decimal type. If the values do not fit in decimal, then it infers them as doubles. allowComments (default false): ignores Java/C++ style comment in JSON records allowUnquotedFieldNames (default false): allows unquoted JSON field names allowSingleQuotes (default true): allows single quotes in addition to double quotes allowNumericLeadingZeros (default false): allows leading zeros in numbers (e.g. 00012) allowBackslashEscapingAnyCharacter (default false): allows accepting quoting of all character using backslash quoting mechanism mode (default PERMISSIVE): allows a mode for dealing with corrupt records during parsing. PERMISSIVE : sets other fields to null when it meets a corrupted record, and puts the malformed string into a new field configured by columnNameOfCorruptRecord. When a schema is set by user, it sets null for extra fields. DROPMALFORMED : ignores the whole corrupted records. FAILFAST : throws an exception when it meets corrupted records. columnNameOfCorruptRecord (default is the value specified in spark.sql.columnNameOfCorruptRecord): allows renaming the new field having malformed string created by PERMISSIVE mode. This overrides spark.sql.columnNameOfCorruptRecord. dateFormat (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. timestampFormat (default yyyy-MM-dd'T'HH:mm:ss.SSSZZ): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. Parameters:paths - (undocumented) Returns:(undocumented)Since: 2.0.0 json public Dataset<Row> json(JavaRDD<String> jsonRDD) Loads a JavaRDD[String] storing JSON objects (one object per record) and returns the result as a DataFrame. Unless the schema is specified using schema function, this function goes through the input once to determine the input schema. Parameters:jsonRDD - input RDD with one JSON object per record Returns:(undocumented)Since: 1.4.0 json public Dataset<Row> json(RDD<String> jsonRDD) Loads an RDD[String] storing JSON objects (one object per record) and returns the result as a DataFrame. Unless the schema is specified using schema function, this function goes through the input once to determine the input schema. Parameters:jsonRDD - input RDD with one JSON object per record Returns:(undocumented)Since: 1.4.0 csv public Dataset<Row> csv(String path) Loads a CSV file and returns the result as a DataFrame. See the documentation on the other overloaded csv() method for more details. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 csv public Dataset<Row> csv(scala.collection.Seq<String> paths) Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema if inferSchema is enabled. To avoid going through the entire data once, disable inferSchema option or specify the schema explicitly using schema. You can set the following CSV-specific options to deal with CSV files: sep (default ,): sets the single character as a separator for each field and value. encoding (default UTF-8): decodes the CSV files by the given encoding type. quote (default "): sets the single character used for escaping quoted values where the separator can be part of the value. If you would like to turn off quotations, you need to set not null but an empty string. This behaviour is different form com.databricks.spark.csv. escape (default \): sets the single character used for escaping quotes inside an already quoted value. comment (default empty string): sets the single character used for skipping lines beginning with this character. By default, it is disabled. header (default false): uses the first line as names of columns. inferSchema (default false): infers the input schema automatically from data. It requires one extra pass over the data. ignoreLeadingWhiteSpace (default false): defines whether or not leading whitespaces from values being read should be skipped. ignoreTrailingWhiteSpace (default false): defines whether or not trailing whitespaces from values being read should be skipped. nullValue (default empty string): sets the string representation of a null value. Since 2.0.1, this applies to all supported types including the string type. nanValue (default NaN): sets the string representation of a non-number" value. positiveInf (default Inf): sets the string representation of a positive infinity value. negativeInf (default -Inf): sets the string representation of a negative infinity value. dateFormat (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. timestampFormat (default yyyy-MM-dd'T'HH:mm:ss.SSSZZ): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. java.sql.Timestamp.valueOf() and java.sql.Date.valueOf() or ISO 8601 format. maxColumns (default 20480): defines a hard limit of how many columns a record can have. maxCharsPerColumn (default 1000000): defines the maximum number of characters allowed for any given value being read. maxMalformedLogPerPartition (default 10): sets the maximum number of malformed rows Spark will log for each partition. Malformed records beyond this number will be ignored. mode (default PERMISSIVE): allows a mode for dealing with corrupt records during parsing. PERMISSIVE : sets other fields to null when it meets a corrupted record. When a schema is set by user, it sets null for extra fields. DROPMALFORMED : ignores the whole corrupted records. FAILFAST : throws an exception when it meets corrupted records. Parameters:paths - (undocumented) Returns:(undocumented)Since: 2.0.0 parquet public Dataset<Row> parquet(String path) Loads a Parquet file, returning the result as a DataFrame. See the documentation on the other overloaded parquet() method for more details. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 parquet public Dataset<Row> parquet(scala.collection.Seq<String> paths) Loads a Parquet file, returning the result as a DataFrame. You can set the following Parquet-specific option(s) for reading Parquet files: mergeSchema (default is the value specified in spark.sql.parquet.mergeSchema): sets whether we should merge schemas collected from all Parquet part-files. This will override spark.sql.parquet.mergeSchema. Parameters:paths - (undocumented) Returns:(undocumented)Since: 1.4.0 orc public Dataset<Row> orc(String path) Loads an ORC file and returns the result as a DataFrame. Parameters:path - input path Returns:(undocumented)Since: 1.5.0 orc public Dataset<Row> orc(scala.collection.Seq<String> paths) Loads an ORC file and returns the result as a DataFrame. Parameters:paths - input paths Returns:(undocumented)Since: 2.0.0 table public Dataset<Row> table(String tableName) Returns the specified table as a DataFrame. Parameters:tableName - (undocumented) Returns:(undocumented)Since: 1.4.0 text public Dataset<Row> text(String path) Loads text files and returns a DataFrame whose schema starts with a string column named "value", and followed by partitioned columns if there are any. See the documentation on the other overloaded text() method for more details. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 text public Dataset<Row> text(scala.collection.Seq<String> paths) Loads text files and returns a DataFrame whose schema starts with a string column named "value", and followed by partitioned columns if there are any. Each line in the text files is a new row in the resulting DataFrame. For example: // Scala: spark.read.text("/path/to/spark/README.md") // Java: spark.read().text("/path/to/spark/README.md") Parameters:paths - input paths Returns:(undocumented)Since: 1.6.0 textFile public Dataset<String> textFile(String path) Loads text files and returns a Dataset of String. See the documentation on the other overloaded textFile() method for more details. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 textFile public Dataset<String> textFile(scala.collection.Seq<String> paths) Loads text files and returns a Dataset of String. The underlying schema of the Dataset contains a single string column named "value". If the directory structure of the text files contains partitioning information, those are ignored in the resulting Dataset. To include partitioning information as columns, use text. Each line in the text files is a new element in the resulting Dataset. For example: // Scala: spark.read.textFile("/path/to/spark/README.md") // Java: spark.read().textFile("/path/to/spark/README.md") Parameters:paths - input path Returns:(undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataFrameStatFunctions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataFrameStatFunctions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class DataFrameStatFunctions Object org.apache.spark.sql.DataFrameStatFunctions public final class DataFrameStatFunctions extends Object :: Experimental :: Statistic functions for DataFrames. Since: 1.4.0 Method Summary Methods  Modifier and Type Method and Description double[] approxQuantile(String col, double[] probabilities, double relativeError) Calculates the approximate quantiles of a numerical column of a DataFrame. BloomFilter bloomFilter(Column col, long expectedNumItems, double fpp) Builds a Bloom filter over a specified column. BloomFilter bloomFilter(Column col, long expectedNumItems, long numBits) Builds a Bloom filter over a specified column. BloomFilter bloomFilter(String colName, long expectedNumItems, double fpp) Builds a Bloom filter over a specified column. BloomFilter bloomFilter(String colName, long expectedNumItems, long numBits) Builds a Bloom filter over a specified column. double corr(String col1, String col2) Calculates the Pearson Correlation Coefficient of two columns of a DataFrame. double corr(String col1, String col2, String method) Calculates the correlation of two columns of a DataFrame. CountMinSketch countMinSketch(Column col, double eps, double confidence, int seed) Builds a Count-min Sketch over a specified column. CountMinSketch countMinSketch(Column col, int depth, int width, int seed) Builds a Count-min Sketch over a specified column. CountMinSketch countMinSketch(String colName, double eps, double confidence, int seed) Builds a Count-min Sketch over a specified column. CountMinSketch countMinSketch(String colName, int depth, int width, int seed) Builds a Count-min Sketch over a specified column. double cov(String col1, String col2) Calculate the sample covariance of two numerical columns of a DataFrame. Dataset<Row> crosstab(String col1, String col2) Computes a pair-wise frequency table of the given columns. Dataset<Row> freqItems(scala.collection.Seq<String> cols) (Scala-specific) Finding frequent items for columns, possibly with false positives. Dataset<Row> freqItems(scala.collection.Seq<String> cols, double support) (Scala-specific) Finding frequent items for columns, possibly with false positives. Dataset<Row> freqItems(String[] cols) Finding frequent items for columns, possibly with false positives. Dataset<Row> freqItems(String[] cols, double support) Finding frequent items for columns, possibly with false positives. <T> Dataset<Row> sampleBy(String col, java.util.Map<T,Double> fractions, long seed) Returns a stratified sample without replacement based on the fraction given on each stratum. <T> Dataset<Row> sampleBy(String col, scala.collection.immutable.Map<T,Object> fractions, long seed) Returns a stratified sample without replacement based on the fraction given on each stratum. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail approxQuantile public double[] approxQuantile(String col, double[] probabilities, double relativeError) Calculates the approximate quantiles of a numerical column of a DataFrame. The result of this algorithm has the following deterministic bound: If the DataFrame has N elements and if we request the quantile at probability p up to error err, then the algorithm will return a sample x from the DataFrame so that the *exact* rank of x is close to (p * N). More precisely, floor((p - err) * N) <= rank(x) <= ceil((p + err) * N). This method implements a variation of the Greenwald-Khanna algorithm (with some speed optimizations). The algorithm was first present in Space-efficient Online Computation of Quantile Summaries by Greenwald and Khanna. Parameters:col - the name of the numerical columnprobabilities - a list of quantile probabilities Each number must belong to [0, 1]. For example 0 is the minimum, 0.5 is the median, 1 is the maximum.relativeError - The relative target precision to achieve (>= 0). If set to zero, the exact quantiles are computed, which could be very expensive. Note that values greater than 1 are accepted but give the same result as 1. Returns:the approximate quantiles at the given probabilities Since: 2.0.0 cov public double cov(String col1, String col2) Calculate the sample covariance of two numerical columns of a DataFrame. Parameters:col1 - the name of the first columncol2 - the name of the second column Returns:the covariance of the two columns. val df = sc.parallelize(0 until 10).toDF("id").withColumn("rand1", rand(seed=10)) .withColumn("rand2", rand(seed=27)) df.stat.cov("rand1", "rand2") res1: Double = 0.065... Since: 1.4.0 corr public double corr(String col1, String col2, String method) Calculates the correlation of two columns of a DataFrame. Currently only supports the Pearson Correlation Coefficient. For Spearman Correlation, consider using RDD methods found in MLlib's Statistics. Parameters:col1 - the name of the columncol2 - the name of the column to calculate the correlation againstmethod - (undocumented) Returns:The Pearson Correlation Coefficient as a Double. val df = sc.parallelize(0 until 10).toDF("id").withColumn("rand1", rand(seed=10)) .withColumn("rand2", rand(seed=27)) df.stat.corr("rand1", "rand2") res1: Double = 0.613... Since: 1.4.0 corr public double corr(String col1, String col2) Calculates the Pearson Correlation Coefficient of two columns of a DataFrame. Parameters:col1 - the name of the columncol2 - the name of the column to calculate the correlation against Returns:The Pearson Correlation Coefficient as a Double. val df = sc.parallelize(0 until 10).toDF("id").withColumn("rand1", rand(seed=10)) .withColumn("rand2", rand(seed=27)) df.stat.corr("rand1", "rand2", "pearson") res1: Double = 0.613... Since: 1.4.0 crosstab public Dataset<Row> crosstab(String col1, String col2) Computes a pair-wise frequency table of the given columns. Also known as a contingency table. The number of distinct values for each column should be less than 1e4. At most 1e6 non-zero pair frequencies will be returned. The first column of each row will be the distinct values of col1 and the column names will be the distinct values of col2. The name of the first column will be $col1_$col2. Counts will be returned as Longs. Pairs that have no occurrences will have zero as their counts. Null elements will be replaced by "null", and back ticks will be dropped from elements if they exist. Parameters:col1 - The name of the first column. Distinct items will make the first item of each row.col2 - The name of the second column. Distinct items will make the column names of the DataFrame. Returns:A DataFrame containing for the contingency table. val df = spark.createDataFrame(Seq((1, 1), (1, 2), (2, 1), (2, 1), (2, 3), (3, 2), (3, 3))) .toDF("key", "value") val ct = df.stat.crosstab("key", "value") ct.show() +---------+---+---+---+ |key_value| 1| 2| 3| +---------+---+---+---+ | 2| 2| 0| 1| | 1| 1| 1| 0| | 3| 0| 1| 1| +---------+---+---+---+ Since: 1.4.0 freqItems public Dataset<Row> freqItems(String[] cols, double support) Finding frequent items for columns, possibly with false positives. Using the frequent element count algorithm described in http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou. The support should be greater than 1e-4. This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting DataFrame. Parameters:cols - the names of the columns to search frequent items in.support - The minimum frequency for an item to be considered frequent. Should be greater than 1e-4. Returns:A Local DataFrame with the Array of frequent items for each column. val rows = Seq.tabulate(100) { i => if (i % 2 == 0) (1, -1.0) else (i, i * -1.0) } val df = spark.createDataFrame(rows).toDF("a", "b") // find the items with a frequency greater than 0.4 (observed 40% of the time) for columns // "a" and "b" val freqSingles = df.stat.freqItems(Array("a", "b"), 0.4) freqSingles.show() +-----------+-------------+ |a_freqItems| b_freqItems| +-----------+-------------+ | [1, 99]|[-1.0, -99.0]| +-----------+-------------+ // find the pair of items with a frequency greater than 0.1 in columns "a" and "b" val pairDf = df.select(struct("a", "b").as("a-b")) val freqPairs = pairDf.stat.freqItems(Array("a-b"), 0.1) freqPairs.select(explode($"a-b_freqItems").as("freq_ab")).show() +----------+ | freq_ab| +----------+ | [1,-1.0]| | ... | +----------+ Since: 1.4.0 freqItems public Dataset<Row> freqItems(String[] cols) Finding frequent items for columns, possibly with false positives. Using the frequent element count algorithm described in http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou. Uses a default support of 1%. This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting DataFrame. Parameters:cols - the names of the columns to search frequent items in. Returns:A Local DataFrame with the Array of frequent items for each column. Since: 1.4.0 freqItems public Dataset<Row> freqItems(scala.collection.Seq<String> cols, double support) (Scala-specific) Finding frequent items for columns, possibly with false positives. Using the frequent element count algorithm described in http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou. This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting DataFrame. Parameters:cols - the names of the columns to search frequent items in.support - (undocumented) Returns:A Local DataFrame with the Array of frequent items for each column. val rows = Seq.tabulate(100) { i => if (i % 2 == 0) (1, -1.0) else (i, i * -1.0) } val df = spark.createDataFrame(rows).toDF("a", "b") // find the items with a frequency greater than 0.4 (observed 40% of the time) for columns // "a" and "b" val freqSingles = df.stat.freqItems(Seq("a", "b"), 0.4) freqSingles.show() +-----------+-------------+ |a_freqItems| b_freqItems| +-----------+-------------+ | [1, 99]|[-1.0, -99.0]| +-----------+-------------+ // find the pair of items with a frequency greater than 0.1 in columns "a" and "b" val pairDf = df.select(struct("a", "b").as("a-b")) val freqPairs = pairDf.stat.freqItems(Seq("a-b"), 0.1) freqPairs.select(explode($"a-b_freqItems").as("freq_ab")).show() +----------+ | freq_ab| +----------+ | [1,-1.0]| | ... | +----------+ Since: 1.4.0 freqItems public Dataset<Row> freqItems(scala.collection.Seq<String> cols) (Scala-specific) Finding frequent items for columns, possibly with false positives. Using the frequent element count algorithm described in http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou. Uses a default support of 1%. This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting DataFrame. Parameters:cols - the names of the columns to search frequent items in. Returns:A Local DataFrame with the Array of frequent items for each column. Since: 1.4.0 sampleBy public <T> Dataset<Row> sampleBy(String col, scala.collection.immutable.Map<T,Object> fractions, long seed) Returns a stratified sample without replacement based on the fraction given on each stratum. Parameters:col - column that defines stratafractions - sampling fraction for each stratum. If a stratum is not specified, we treat its fraction as zero.seed - random seed Returns:a new DataFrame that represents the stratified sample val df = spark.createDataFrame(Seq((1, 1), (1, 2), (2, 1), (2, 1), (2, 3), (3, 2), (3, 3))).toDF("key", "value") val fractions = Map(1 -> 1.0, 3 -> 0.5) df.stat.sampleBy("key", fractions, 36L).show() +---+-----+ |key|value| +---+-----+ | 1| 1| | 1| 2| | 3| 2| +---+-----+ Since: 1.5.0 sampleBy public <T> Dataset<Row> sampleBy(String col, java.util.Map<T,Double> fractions, long seed) Returns a stratified sample without replacement based on the fraction given on each stratum. Parameters:col - column that defines stratafractions - sampling fraction for each stratum. If a stratum is not specified, we treat its fraction as zero.seed - random seed Returns:a new DataFrame that represents the stratified sample Since: 1.5.0 countMinSketch public CountMinSketch countMinSketch(String colName, int depth, int width, int seed) Builds a Count-min Sketch over a specified column. Parameters:colName - name of the column over which the sketch is builtdepth - depth of the sketchwidth - width of the sketchseed - random seed Returns:a CountMinSketch over column colNameSince: 2.0.0 countMinSketch public CountMinSketch countMinSketch(String colName, double eps, double confidence, int seed) Builds a Count-min Sketch over a specified column. Parameters:colName - name of the column over which the sketch is builteps - relative error of the sketchconfidence - confidence of the sketchseed - random seed Returns:a CountMinSketch over column colNameSince: 2.0.0 countMinSketch public CountMinSketch countMinSketch(Column col, int depth, int width, int seed) Builds a Count-min Sketch over a specified column. Parameters:col - the column over which the sketch is builtdepth - depth of the sketchwidth - width of the sketchseed - random seed Returns:a CountMinSketch over column colNameSince: 2.0.0 countMinSketch public CountMinSketch countMinSketch(Column col, double eps, double confidence, int seed) Builds a Count-min Sketch over a specified column. Parameters:col - the column over which the sketch is builteps - relative error of the sketchconfidence - confidence of the sketchseed - random seed Returns:a CountMinSketch over column colNameSince: 2.0.0 bloomFilter public BloomFilter bloomFilter(String colName, long expectedNumItems, double fpp) Builds a Bloom filter over a specified column. Parameters:colName - name of the column over which the filter is builtexpectedNumItems - expected number of items which will be put into the filter.fpp - expected false positive probability of the filter. Returns:(undocumented)Since: 2.0.0 bloomFilter public BloomFilter bloomFilter(Column col, long expectedNumItems, double fpp) Builds a Bloom filter over a specified column. Parameters:col - the column over which the filter is builtexpectedNumItems - expected number of items which will be put into the filter.fpp - expected false positive probability of the filter. Returns:(undocumented)Since: 2.0.0 bloomFilter public BloomFilter bloomFilter(String colName, long expectedNumItems, long numBits) Builds a Bloom filter over a specified column. Parameters:colName - name of the column over which the filter is builtexpectedNumItems - expected number of items which will be put into the filter.numBits - expected number of bits of the filter. Returns:(undocumented)Since: 2.0.0 bloomFilter public BloomFilter bloomFilter(Column col, long expectedNumItems, long numBits) Builds a Bloom filter over a specified column. Parameters:col - the column over which the filter is builtexpectedNumItems - expected number of items which will be put into the filter.numBits - expected number of bits of the filter. Returns:(undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataFrameWriter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataFrameWriter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class DataFrameWriter<T> Object org.apache.spark.sql.DataFrameWriter<T> public final class DataFrameWriter<T> extends Object Interface used to write a Dataset to external storage systems (e.g. file systems, key-value stores, etc). Use Dataset.write to access this. Since: 1.4.0 Method Summary Methods  Modifier and Type Method and Description DataFrameWriter<T> bucketBy(int numBuckets, String colName, scala.collection.Seq<String> colNames) Buckets the output by the given columns. DataFrameWriter<T> bucketBy(int numBuckets, String colName, String... colNames) Buckets the output by the given columns. void csv(String path) Saves the content of the DataFrame in CSV format at the specified path. DataFrameWriter<T> format(String source) Specifies the underlying output data source. void insertInto(String tableName) Inserts the content of the DataFrame to the specified table. void jdbc(String url, String table, java.util.Properties connectionProperties) Saves the content of the DataFrame to an external database table via JDBC. void json(String path) Saves the content of the DataFrame in JSON format at the specified path. DataFrameWriter<T> mode(SaveMode saveMode) Specifies the behavior when data or table already exists. DataFrameWriter<T> mode(String saveMode) Specifies the behavior when data or table already exists. DataFrameWriter<T> option(String key, boolean value) Adds an output option for the underlying data source. DataFrameWriter<T> option(String key, double value) Adds an output option for the underlying data source. DataFrameWriter<T> option(String key, long value) Adds an output option for the underlying data source. DataFrameWriter<T> option(String key, String value) Adds an output option for the underlying data source. DataFrameWriter<T> options(scala.collection.Map<String,String> options) (Scala-specific) Adds output options for the underlying data source. DataFrameWriter<T> options(java.util.Map<String,String> options) Adds output options for the underlying data source. void orc(String path) Saves the content of the DataFrame in ORC format at the specified path. void parquet(String path) Saves the content of the DataFrame in Parquet format at the specified path. DataFrameWriter<T> partitionBy(scala.collection.Seq<String> colNames) Partitions the output by the given columns on the file system. DataFrameWriter<T> partitionBy(String... colNames) Partitions the output by the given columns on the file system. void save() Saves the content of the DataFrame as the specified table. void save(String path) Saves the content of the DataFrame at the specified path. void saveAsTable(String tableName) Saves the content of the DataFrame as the specified table. DataFrameWriter<T> sortBy(String colName, scala.collection.Seq<String> colNames) Sorts the output in each bucket by the given columns. DataFrameWriter<T> sortBy(String colName, String... colNames) Sorts the output in each bucket by the given columns. void text(String path) Saves the content of the DataFrame in a text file at the specified path. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail partitionBy public DataFrameWriter<T> partitionBy(String... colNames) Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like: - year=2016/month=01/ - year=2016/month=02/ Partitioning is one of the most widely used techniques to optimize physical data layout. It provides a coarse-grained index for skipping unnecessary data reads when queries have predicates on the partitioned columns. In order for partitioning to work well, the number of distinct values in each column should typically be less than tens of thousands. This was initially applicable for Parquet but in 1.5+ covers JSON, text, ORC and avro as well. Parameters:colNames - (undocumented) Returns:(undocumented)Since: 1.4.0 bucketBy public DataFrameWriter<T> bucketBy(int numBuckets, String colName, String... colNames) Buckets the output by the given columns. If specified, the output is laid out on the file system similar to Hive's bucketing scheme. This is applicable for Parquet, JSON and ORC. Parameters:numBuckets - (undocumented)colName - (undocumented)colNames - (undocumented) Returns:(undocumented)Since: 2.0 sortBy public DataFrameWriter<T> sortBy(String colName, String... colNames) Sorts the output in each bucket by the given columns. This is applicable for Parquet, JSON and ORC. Parameters:colName - (undocumented)colNames - (undocumented) Returns:(undocumented)Since: 2.0 mode public DataFrameWriter<T> mode(SaveMode saveMode) Specifies the behavior when data or table already exists. Options include: - SaveMode.Overwrite: overwrite the existing data. - SaveMode.Append: append the data. - SaveMode.Ignore: ignore the operation (i.e. no-op). - SaveMode.ErrorIfExists: default option, throw an exception at runtime. Parameters:saveMode - (undocumented) Returns:(undocumented)Since: 1.4.0 mode public DataFrameWriter<T> mode(String saveMode) Specifies the behavior when data or table already exists. Options include: - overwrite: overwrite the existing data. - append: append the data. - ignore: ignore the operation (i.e. no-op). - error: default option, throw an exception at runtime. Parameters:saveMode - (undocumented) Returns:(undocumented)Since: 1.4.0 format public DataFrameWriter<T> format(String source) Specifies the underlying output data source. Built-in options include "parquet", "json", etc. Parameters:source - (undocumented) Returns:(undocumented)Since: 1.4.0 option public DataFrameWriter<T> option(String key, String value) Adds an output option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 1.4.0 option public DataFrameWriter<T> option(String key, boolean value) Adds an output option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataFrameWriter<T> option(String key, long value) Adds an output option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataFrameWriter<T> option(String key, double value) Adds an output option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 options public DataFrameWriter<T> options(scala.collection.Map<String,String> options) (Scala-specific) Adds output options for the underlying data source. Parameters:options - (undocumented) Returns:(undocumented)Since: 1.4.0 options public DataFrameWriter<T> options(java.util.Map<String,String> options) Adds output options for the underlying data source. Parameters:options - (undocumented) Returns:(undocumented)Since: 1.4.0 partitionBy public DataFrameWriter<T> partitionBy(scala.collection.Seq<String> colNames) Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like: - year=2016/month=01/ - year=2016/month=02/ Partitioning is one of the most widely used techniques to optimize physical data layout. It provides a coarse-grained index for skipping unnecessary data reads when queries have predicates on the partitioned columns. In order for partitioning to work well, the number of distinct values in each column should typically be less than tens of thousands. This was initially applicable for Parquet but in 1.5+ covers JSON, text, ORC and avro as well. Parameters:colNames - (undocumented) Returns:(undocumented)Since: 1.4.0 bucketBy public DataFrameWriter<T> bucketBy(int numBuckets, String colName, scala.collection.Seq<String> colNames) Buckets the output by the given columns. If specified, the output is laid out on the file system similar to Hive's bucketing scheme. This is applicable for Parquet, JSON and ORC. Parameters:numBuckets - (undocumented)colName - (undocumented)colNames - (undocumented) Returns:(undocumented)Since: 2.0 sortBy public DataFrameWriter<T> sortBy(String colName, scala.collection.Seq<String> colNames) Sorts the output in each bucket by the given columns. This is applicable for Parquet, JSON and ORC. Parameters:colName - (undocumented)colNames - (undocumented) Returns:(undocumented)Since: 2.0 save public void save(String path) Saves the content of the DataFrame at the specified path. Parameters:path - (undocumented)Since: 1.4.0 save public void save() Saves the content of the DataFrame as the specified table. Since: 1.4.0 insertInto public void insertInto(String tableName) Inserts the content of the DataFrame to the specified table. It requires that the schema of the DataFrame is the same as the schema of the table. Note: Unlike saveAsTable, insertInto ignores the column names and just uses position-based resolution. For example: scala> Seq((1, 2)).toDF("i", "j").write.mode("overwrite").saveAsTable("t1") scala> Seq((3, 4)).toDF("j", "i").write.insertInto("t1") scala> Seq((5, 6)).toDF("a", "b").write.insertInto("t1") scala> sql("select * from t1").show +---+---+ | i| j| +---+---+ | 5| 6| | 3| 4| | 1| 2| +---+---+ Because it inserts data to an existing table, format or options will be ignored. Parameters:tableName - (undocumented)Since: 1.4.0 saveAsTable public void saveAsTable(String tableName) Saves the content of the DataFrame as the specified table. In the case the table already exists, behavior of this function depends on the save mode, specified by the mode function (default to throwing an exception). When mode is Overwrite, the schema of the DataFrame does not need to be the same as that of the existing table. When mode is Append, if there is an existing table, we will use the format and options of the existing table. The column order in the schema of the DataFrame doesn't need to be same as that of the existing table. Unlike insertInto, saveAsTable will use the column names to find the correct column positions. For example: scala> Seq((1, 2)).toDF("i", "j").write.mode("overwrite").saveAsTable("t1") scala> Seq((3, 4)).toDF("j", "i").write.mode("append").saveAsTable("t1") scala> sql("select * from t1").show +---+---+ | i| j| +---+---+ | 1| 2| | 4| 3| +---+---+ When the DataFrame is created from a non-partitioned HadoopFsRelation with a single input path, and the data source provider can be mapped to an existing Hive builtin SerDe (i.e. ORC and Parquet), the table is persisted in a Hive compatible format, which means other systems like Hive will be able to read this table. Otherwise, the table is persisted in a Spark SQL specific format. Parameters:tableName - (undocumented)Since: 1.4.0 jdbc public void jdbc(String url, String table, java.util.Properties connectionProperties) Saves the content of the DataFrame to an external database table via JDBC. In the case the table already exists in the external database, behavior of this function depends on the save mode, specified by the mode function (default to throwing an exception). Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash your external database systems. Parameters:url - JDBC database url of the form jdbc:subprotocol:subnametable - Name of the table in the external database.connectionProperties - JDBC database connection arguments, a list of arbitrary string tag/value. Normally at least a "user" and "password" property should be included. "batchsize" can be used to control the number of rows per insert.Since: 1.4.0 json public void json(String path) Saves the content of the DataFrame in JSON format at the specified path. This is equivalent to: format("json").save(path) You can set the following JSON-specific option(s) for writing JSON files: compression (default null): compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate). dateFormat (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. timestampFormat (default yyyy-MM-dd'T'HH:mm:ss.SSSZZ): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. Parameters:path - (undocumented)Since: 1.4.0 parquet public void parquet(String path) Saves the content of the DataFrame in Parquet format at the specified path. This is equivalent to: format("parquet").save(path) You can set the following Parquet-specific option(s) for writing Parquet files: compression (default is the value specified in spark.sql.parquet.compression.codec): compression codec to use when saving to file. This can be one of the known case-insensitive shorten names(none, snappy, gzip, and lzo). This will override spark.sql.parquet.compression.codec. Parameters:path - (undocumented)Since: 1.4.0 orc public void orc(String path) Saves the content of the DataFrame in ORC format at the specified path. This is equivalent to: format("orc").save(path) You can set the following ORC-specific option(s) for writing ORC files: compression (default snappy): compression codec to use when saving to file. This can be one of the known case-insensitive shorten names(none, snappy, zlib, and lzo). This will override orc.compress. Parameters:path - (undocumented)Since: 1.5.0 text public void text(String path) Saves the content of the DataFrame in a text file at the specified path. The DataFrame must have only one column that is of string type. Each row becomes a new line in the output file. For example: // Scala: df.write.text("/path/to/output") // Java: df.write().text("/path/to/output") You can set the following option(s) for writing text files: compression (default null): compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate). Parameters:path - (undocumented)Since: 1.6.0 csv public void csv(String path) Saves the content of the DataFrame in CSV format at the specified path. This is equivalent to: format("csv").save(path) You can set the following CSV-specific option(s) for writing CSV files: sep (default ,): sets the single character as a separator for each field and value. quote (default "): sets the single character used for escaping quoted values where the separator can be part of the value. escape (default \): sets the single character used for escaping quotes inside an already quoted value. escapeQuotes (default true): a flag indicating whether values containing quotes should always be enclosed in quotes. Default is to escape all values containing a quote character. quoteAll (default false): A flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character. header (default false): writes the names of columns as the first line. nullValue (default empty string): sets the string representation of a null value. compression (default null): compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate). dateFormat (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. timestampFormat (default yyyy-MM-dd'T'HH:mm:ss.SSSZZ): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. Parameters:path - (undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataSourceRegister (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataSourceRegister (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Interface DataSourceRegister All Known Implementing Classes: OrcFileFormat public interface DataSourceRegister ::DeveloperApi:: Data sources should implement this trait so that they can register an alias to their data source. This allows users to give the data source alias as the format type over the fully qualified class name. A new instance of this class will be instantiated each time a DDL call is made. Since: 1.5.0 Method Summary Methods  Modifier and Type Method and Description String shortName() The string that represents the format that this data source provider uses. Method Detail shortName String shortName() The string that represents the format that this data source provider uses. This is overridden by children to provide a nice alias for the data source. For example: override def shortName(): String = "parquet" Returns:(undocumented)Since: 1.5.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataStreamReader (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataStreamReader (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.streaming Class DataStreamReader Object org.apache.spark.sql.streaming.DataStreamReader public final class DataStreamReader extends Object Interface used to load a streaming Dataset from external storage systems (e.g. file systems, key-value stores, etc). Use SparkSession.readStream to access this. Since: 2.0.0 Method Summary Methods  Modifier and Type Method and Description Dataset<Row> csv(String path) Loads a CSV file stream and returns the result as a DataFrame. DataStreamReader format(String source) Specifies the input data source format. Dataset<Row> json(String path) Loads a JSON file stream (one object per line) and returns the result as a DataFrame. Dataset<Row> load() Loads input data stream in as a DataFrame, for data streams that don't require a path (e.g. Dataset<Row> load(String path) Loads input in as a DataFrame, for data streams that read from some path. DataStreamReader option(String key, boolean value) Adds an input option for the underlying data source. DataStreamReader option(String key, double value) Adds an input option for the underlying data source. DataStreamReader option(String key, long value) Adds an input option for the underlying data source. DataStreamReader option(String key, String value) Adds an input option for the underlying data source. DataStreamReader options(scala.collection.Map<String,String> options) (Scala-specific) Adds input options for the underlying data source. DataStreamReader options(java.util.Map<String,String> options) Adds input options for the underlying data source. Dataset<Row> parquet(String path) Loads a Parquet file stream, returning the result as a DataFrame. DataStreamReader schema(StructType schema) Specifies the input schema. Dataset<Row> text(String path) Loads text files and returns a DataFrame whose schema starts with a string column named "value", and followed by partitioned columns if there are any. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail format public DataStreamReader format(String source) Specifies the input data source format. Parameters:source - (undocumented) Returns:(undocumented)Since: 2.0.0 schema public DataStreamReader schema(StructType schema) Specifies the input schema. Some data sources (e.g. JSON) can infer the input schema automatically from data. By specifying the schema here, the underlying data source can skip the schema inference step, and thus speed up data loading. Parameters:schema - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataStreamReader option(String key, String value) Adds an input option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataStreamReader option(String key, boolean value) Adds an input option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataStreamReader option(String key, long value) Adds an input option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataStreamReader option(String key, double value) Adds an input option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 options public DataStreamReader options(scala.collection.Map<String,String> options) (Scala-specific) Adds input options for the underlying data source. Parameters:options - (undocumented) Returns:(undocumented)Since: 2.0.0 options public DataStreamReader options(java.util.Map<String,String> options) Adds input options for the underlying data source. Parameters:options - (undocumented) Returns:(undocumented)Since: 2.0.0 load public Dataset<Row> load() Loads input data stream in as a DataFrame, for data streams that don't require a path (e.g. external key-value stores). Returns:(undocumented)Since: 2.0.0 load public Dataset<Row> load(String path) Loads input in as a DataFrame, for data streams that read from some path. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 json public Dataset<Row> json(String path) Loads a JSON file stream (one object per line) and returns the result as a DataFrame. This function goes through the input once to determine the input schema. If you know the schema in advance, use the version that specifies the schema to avoid the extra scan. You can set the following JSON-specific options to deal with non-standard JSON files: maxFilesPerTrigger (default: no max limit): sets the maximum number of new files to be considered in every trigger. primitivesAsString (default false): infers all primitive values as a string type prefersDecimal (default false): infers all floating-point values as a decimal type. If the values do not fit in decimal, then it infers them as doubles. allowComments (default false): ignores Java/C++ style comment in JSON records allowUnquotedFieldNames (default false): allows unquoted JSON field names allowSingleQuotes (default true): allows single quotes in addition to double quotes allowNumericLeadingZeros (default false): allows leading zeros in numbers (e.g. 00012) allowBackslashEscapingAnyCharacter (default false): allows accepting quoting of all character using backslash quoting mechanism mode (default PERMISSIVE): allows a mode for dealing with corrupt records during parsing. PERMISSIVE : sets other fields to null when it meets a corrupted record, and puts the malformed string into a new field configured by columnNameOfCorruptRecord. When a schema is set by user, it sets null for extra fields. DROPMALFORMED : ignores the whole corrupted records. FAILFAST : throws an exception when it meets corrupted records. columnNameOfCorruptRecord (default is the value specified in spark.sql.columnNameOfCorruptRecord): allows renaming the new field having malformed string created by PERMISSIVE mode. This overrides spark.sql.columnNameOfCorruptRecord. dateFormat (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. timestampFormat (default yyyy-MM-dd'T'HH:mm:ss.SSSZZ): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 csv public Dataset<Row> csv(String path) Loads a CSV file stream and returns the result as a DataFrame. This function will go through the input once to determine the input schema if inferSchema is enabled. To avoid going through the entire data once, disable inferSchema option or specify the schema explicitly using schema. You can set the following CSV-specific options to deal with CSV files: maxFilesPerTrigger (default: no max limit): sets the maximum number of new files to be considered in every trigger. sep (default ,): sets the single character as a separator for each field and value. encoding (default UTF-8): decodes the CSV files by the given encoding type. quote (default "): sets the single character used for escaping quoted values where the separator can be part of the value. If you would like to turn off quotations, you need to set not null but an empty string. This behaviour is different form com.databricks.spark.csv. escape (default \): sets the single character used for escaping quotes inside an already quoted value. comment (default empty string): sets the single character used for skipping lines beginning with this character. By default, it is disabled. header (default false): uses the first line as names of columns. inferSchema (default false): infers the input schema automatically from data. It requires one extra pass over the data. ignoreLeadingWhiteSpace (default false): defines whether or not leading whitespaces from values being read should be skipped. ignoreTrailingWhiteSpace (default false): defines whether or not trailing whitespaces from values being read should be skipped. nullValue (default empty string): sets the string representation of a null value. Since 2.0.1, this applies to all supported types including the string type. nanValue (default NaN): sets the string representation of a non-number" value. positiveInf (default Inf): sets the string representation of a positive infinity value. negativeInf (default -Inf): sets the string representation of a negative infinity value. dateFormat (default yyyy-MM-dd): sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. timestampFormat (default yyyy-MM-dd'T'HH:mm:ss.SSSZZ): sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. maxColumns (default 20480): defines a hard limit of how many columns a record can have. maxCharsPerColumn (default 1000000): defines the maximum number of characters allowed for any given value being read. mode (default PERMISSIVE): allows a mode for dealing with corrupt records during parsing. PERMISSIVE : sets other fields to null when it meets a corrupted record. When a schema is set by user, it sets null for extra fields. DROPMALFORMED : ignores the whole corrupted records. FAILFAST : throws an exception when it meets corrupted records. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 parquet public Dataset<Row> parquet(String path) Loads a Parquet file stream, returning the result as a DataFrame. You can set the following Parquet-specific option(s) for reading Parquet files: maxFilesPerTrigger (default: no max limit): sets the maximum number of new files to be considered in every trigger. mergeSchema (default is the value specified in spark.sql.parquet.mergeSchema): sets whether we should merge schemas collected from all Parquet part-files. This will override spark.sql.parquet.mergeSchema. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 text public Dataset<Row> text(String path) Loads text files and returns a DataFrame whose schema starts with a string column named "value", and followed by partitioned columns if there are any. Each line in the text files is a new row in the resulting DataFrame. For example: // Scala: spark.readStream.text("/path/to/directory/") // Java: spark.readStream().text("/path/to/directory/") You can set the following text-specific options to deal with text files: maxFilesPerTrigger (default: no max limit): sets the maximum number of new files to be considered in every trigger. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataStreamWriter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataStreamWriter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.streaming Class DataStreamWriter<T> Object org.apache.spark.sql.streaming.DataStreamWriter<T> public final class DataStreamWriter<T> extends Object :: Experimental :: Interface used to write a streaming Dataset to external storage systems (e.g. file systems, key-value stores, etc). Use Dataset.writeStream to access this. Since: 2.0.0 Method Summary Methods  Modifier and Type Method and Description DataStreamWriter<T> foreach(ForeachWriter<T> writer) Starts the execution of the streaming query, which will continually send results to the given ForeachWriter as as new data arrives. DataStreamWriter<T> format(String source) Specifies the underlying output data source. DataStreamWriter<T> option(String key, boolean value) Adds an output option for the underlying data source. DataStreamWriter<T> option(String key, double value) Adds an output option for the underlying data source. DataStreamWriter<T> option(String key, long value) Adds an output option for the underlying data source. DataStreamWriter<T> option(String key, String value) Adds an output option for the underlying data source. DataStreamWriter<T> options(scala.collection.Map<String,String> options) (Scala-specific) Adds output options for the underlying data source. DataStreamWriter<T> options(java.util.Map<String,String> options) Adds output options for the underlying data source. DataStreamWriter<T> outputMode(OutputMode outputMode) Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. DataStreamWriter<T> outputMode(String outputMode) Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. DataStreamWriter<T> partitionBy(scala.collection.Seq<String> colNames) Partitions the output by the given columns on the file system. DataStreamWriter<T> partitionBy(String... colNames) Partitions the output by the given columns on the file system. DataStreamWriter<T> queryName(String queryName) Specifies the name of the StreamingQuery that can be started with start(). StreamingQuery start() Starts the execution of the streaming query, which will continually output results to the given path as new data arrives. StreamingQuery start(String path) Starts the execution of the streaming query, which will continually output results to the given path as new data arrives. DataStreamWriter<T> trigger(Trigger trigger) Set the trigger for the stream query. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail partitionBy public DataStreamWriter<T> partitionBy(String... colNames) Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like: - year=2016/month=01/ - year=2016/month=02/ Partitioning is one of the most widely used techniques to optimize physical data layout. It provides a coarse-grained index for skipping unnecessary data reads when queries have predicates on the partitioned columns. In order for partitioning to work well, the number of distinct values in each column should typically be less than tens of thousands. This was initially applicable for Parquet but in 1.5+ covers JSON, text, ORC and avro as well. Parameters:colNames - (undocumented) Returns:(undocumented)Since: 1.4.0 outputMode public DataStreamWriter<T> outputMode(OutputMode outputMode) Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. - OutputMode.Append(): only the new rows in the streaming DataFrame/Dataset will be written to the sink - OutputMode.Complete(): all the rows in the streaming DataFrame/Dataset will be written to the sink every time these is some updates Parameters:outputMode - (undocumented) Returns:(undocumented)Since: 2.0.0 outputMode public DataStreamWriter<T> outputMode(String outputMode) Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. - append: only the new rows in the streaming DataFrame/Dataset will be written to the sink - complete: all the rows in the streaming DataFrame/Dataset will be written to the sink every time these is some updates Parameters:outputMode - (undocumented) Returns:(undocumented)Since: 2.0.0 trigger public DataStreamWriter<T> trigger(Trigger trigger) Set the trigger for the stream query. The default value is ProcessingTime(0) and it will run the query as fast as possible. Scala Example: df.writeStream.trigger(ProcessingTime("10 seconds")) import scala.concurrent.duration._ df.writeStream.trigger(ProcessingTime(10.seconds)) Java Example: df.writeStream().trigger(ProcessingTime.create("10 seconds")) import java.util.concurrent.TimeUnit df.writeStream().trigger(ProcessingTime.create(10, TimeUnit.SECONDS)) Parameters:trigger - (undocumented) Returns:(undocumented)Since: 2.0.0 queryName public DataStreamWriter<T> queryName(String queryName) Specifies the name of the StreamingQuery that can be started with start(). This name must be unique among all the currently active queries in the associated SQLContext. Parameters:queryName - (undocumented) Returns:(undocumented)Since: 2.0.0 format public DataStreamWriter<T> format(String source) Specifies the underlying output data source. Built-in options include "parquet" for now. Parameters:source - (undocumented) Returns:(undocumented)Since: 2.0.0 partitionBy public DataStreamWriter<T> partitionBy(scala.collection.Seq<String> colNames) Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like: - year=2016/month=01/ - year=2016/month=02/ Partitioning is one of the most widely used techniques to optimize physical data layout. It provides a coarse-grained index for skipping unnecessary data reads when queries have predicates on the partitioned columns. In order for partitioning to work well, the number of distinct values in each column should typically be less than tens of thousands. This was initially applicable for Parquet but in 1.5+ covers JSON, text, ORC and avro as well. Parameters:colNames - (undocumented) Returns:(undocumented)Since: 1.4.0 option public DataStreamWriter<T> option(String key, String value) Adds an output option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataStreamWriter<T> option(String key, boolean value) Adds an output option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataStreamWriter<T> option(String key, long value) Adds an output option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 option public DataStreamWriter<T> option(String key, double value) Adds an output option for the underlying data source. Parameters:key - (undocumented)value - (undocumented) Returns:(undocumented)Since: 2.0.0 options public DataStreamWriter<T> options(scala.collection.Map<String,String> options) (Scala-specific) Adds output options for the underlying data source. Parameters:options - (undocumented) Returns:(undocumented)Since: 2.0.0 options public DataStreamWriter<T> options(java.util.Map<String,String> options) Adds output options for the underlying data source. Parameters:options - (undocumented) Returns:(undocumented)Since: 2.0.0 start public StreamingQuery start(String path) Starts the execution of the streaming query, which will continually output results to the given path as new data arrives. The returned StreamingQuery object can be used to interact with the stream. Parameters:path - (undocumented) Returns:(undocumented)Since: 2.0.0 start public StreamingQuery start() Starts the execution of the streaming query, which will continually output results to the given path as new data arrives. The returned StreamingQuery object can be used to interact with the stream. Returns:(undocumented)Since: 2.0.0 foreach public DataStreamWriter<T> foreach(ForeachWriter<T> writer) Starts the execution of the streaming query, which will continually send results to the given ForeachWriter as as new data arrives. The ForeachWriter can be used to send the data generated by the DataFrame/Dataset to an external system. Scala example: datasetOfString.writeStream.foreach(new ForeachWriter[String] { def open(partitionId: Long, version: Long): Boolean = { // open connection } def process(record: String) = { // write string to connection } def close(errorOrNull: Throwable): Unit = { // close the connection } }).start() Java example: datasetOfString.writeStream().foreach(new ForeachWriter<String>() { @Override public boolean open(long partitionId, long version) { // open connection } @Override public void process(String value) { // write string to connection } @Override public void close(Throwable errorOrNull) { // close the connection } }).start(); Parameters:writer - (undocumented) Returns:(undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class DataType Object org.apache.spark.sql.types.DataType Direct Known Subclasses: ArrayType, BinaryType, BooleanType, CalendarIntervalType, DateType, MapType, NullType, NumericType, StringType, StructType, TimestampType public abstract class DataType extends Object :: DeveloperApi :: The base type of all Spark SQL data types. Constructor Summary Constructors  Constructor and Description DataType()  Method Summary Methods  Modifier and Type Method and Description String catalogString() String representation for the type saved in external catalogs. abstract int defaultSize() The default size of a value of this data type, used internally for size estimation. static DataType fromJson(String json)  String json() The compact JSON representation of this data type. String prettyJson() The pretty (i.e. String simpleString() Readable string representation for the type. String sql()  String typeName() Name of the type used in JSON serialization. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DataType public DataType() Method Detail fromJson public static DataType fromJson(String json) defaultSize public abstract int defaultSize() The default size of a value of this data type, used internally for size estimation. Returns:(undocumented) typeName public String typeName() Name of the type used in JSON serialization. json public String json() The compact JSON representation of this data type. prettyJson public String prettyJson() The pretty (i.e. indented) JSON representation of this data type. simpleString public String simpleString() Readable string representation for the type. catalogString public String catalogString() String representation for the type saved in external catalogs. sql public String sql() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataTypes (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataTypes (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class DataTypes Object org.apache.spark.sql.types.DataTypes public class DataTypes extends Object To get/create specific data type, users should use singleton objects and factory methods provided by this class. Field Summary Fields  Modifier and Type Field and Description static DataType BinaryType Gets the BinaryType object. static DataType BooleanType Gets the BooleanType object. static DataType ByteType Gets the ByteType object. static DataType CalendarIntervalType Gets the CalendarIntervalType object. static DataType DateType Gets the DateType object. static DataType DoubleType Gets the DoubleType object. static DataType FloatType Gets the FloatType object. static DataType IntegerType Gets the IntegerType object. static DataType LongType Gets the LongType object. static DataType NullType Gets the NullType object. static DataType ShortType Gets the ShortType object. static DataType StringType Gets the StringType object. static DataType TimestampType Gets the TimestampType object. Constructor Summary Constructors  Constructor and Description DataTypes()  Method Summary Methods  Modifier and Type Method and Description static ArrayType createArrayType(DataType elementType) Creates an ArrayType by specifying the data type of elements (elementType). static ArrayType createArrayType(DataType elementType, boolean containsNull) Creates an ArrayType by specifying the data type of elements (elementType) and whether the array contains null values (containsNull). static DecimalType createDecimalType() Creates a DecimalType with default precision and scale, which are 10 and 0. static DecimalType createDecimalType(int precision, int scale) Creates a DecimalType by specifying the precision and scale. static MapType createMapType(DataType keyType, DataType valueType) Creates a MapType by specifying the data type of keys (keyType) and values (keyType). static MapType createMapType(DataType keyType, DataType valueType, boolean valueContainsNull) Creates a MapType by specifying the data type of keys (keyType), the data type of values (keyType), and whether values contain any null value (valueContainsNull). static StructField createStructField(String name, DataType dataType, boolean nullable) Creates a StructField with empty metadata. static StructField createStructField(String name, DataType dataType, boolean nullable, Metadata metadata) Creates a StructField by specifying the name (name), data type (dataType) and whether values of this field can be null values (nullable). static StructType createStructType(java.util.List<StructField> fields) Creates a StructType with the given list of StructFields (fields). static StructType createStructType(StructField[] fields) Creates a StructType with the given StructField array (fields). Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail StringType public static final DataType StringType Gets the StringType object. BinaryType public static final DataType BinaryType Gets the BinaryType object. BooleanType public static final DataType BooleanType Gets the BooleanType object. DateType public static final DataType DateType Gets the DateType object. TimestampType public static final DataType TimestampType Gets the TimestampType object. CalendarIntervalType public static final DataType CalendarIntervalType Gets the CalendarIntervalType object. DoubleType public static final DataType DoubleType Gets the DoubleType object. FloatType public static final DataType FloatType Gets the FloatType object. ByteType public static final DataType ByteType Gets the ByteType object. IntegerType public static final DataType IntegerType Gets the IntegerType object. LongType public static final DataType LongType Gets the LongType object. ShortType public static final DataType ShortType Gets the ShortType object. NullType public static final DataType NullType Gets the NullType object. Constructor Detail DataTypes public DataTypes() Method Detail createArrayType public static ArrayType createArrayType(DataType elementType) Creates an ArrayType by specifying the data type of elements (elementType). The field of containsNull is set to true. createArrayType public static ArrayType createArrayType(DataType elementType, boolean containsNull) Creates an ArrayType by specifying the data type of elements (elementType) and whether the array contains null values (containsNull). createDecimalType public static DecimalType createDecimalType(int precision, int scale) Creates a DecimalType by specifying the precision and scale. createDecimalType public static DecimalType createDecimalType() Creates a DecimalType with default precision and scale, which are 10 and 0. createMapType public static MapType createMapType(DataType keyType, DataType valueType) Creates a MapType by specifying the data type of keys (keyType) and values (keyType). The field of valueContainsNull is set to true. createMapType public static MapType createMapType(DataType keyType, DataType valueType, boolean valueContainsNull) Creates a MapType by specifying the data type of keys (keyType), the data type of values (keyType), and whether values contain any null value (valueContainsNull). createStructField public static StructField createStructField(String name, DataType dataType, boolean nullable, Metadata metadata) Creates a StructField by specifying the name (name), data type (dataType) and whether values of this field can be null values (nullable). createStructField public static StructField createStructField(String name, DataType dataType, boolean nullable) Creates a StructField with empty metadata. See Also:createStructField(String, DataType, boolean, Metadata) createStructType public static StructType createStructType(java.util.List<StructField> fields) Creates a StructType with the given list of StructFields (fields). createStructType public static StructType createStructType(StructField[] fields) Creates a StructType with the given StructField array (fields). Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DataValidators (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DataValidators (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.util Class DataValidators Object org.apache.spark.mllib.util.DataValidators public class DataValidators extends Object :: DeveloperApi :: A collection of methods used to validate data before applying ML algorithms. Constructor Summary Constructors  Constructor and Description DataValidators()  Method Summary Methods  Modifier and Type Method and Description static scala.Function1<RDD<LabeledPoint>,Object> binaryLabelValidator() Function to check if labels used for classification are either zero or one. static scala.Function1<RDD<LabeledPoint>,Object> multiLabelValidator(int k) Function to check if labels used for k class multi-label classification are in the range of {0, 1, ..., k - 1}. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DataValidators public DataValidators() Method Detail binaryLabelValidator public static scala.Function1<RDD<LabeledPoint>,Object> binaryLabelValidator() Function to check if labels used for classification are either zero or one. Returns:True if labels are all zero or one, false otherwise. multiLabelValidator public static scala.Function1<RDD<LabeledPoint>,Object> multiLabelValidator(int k) Function to check if labels used for k class multi-label classification are in the range of {0, 1, ..., k - 1}. Parameters:k - (undocumented) Returns:True if labels are all in the range of {0, 1, ..., k-1}, false otherwise. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Database (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Database (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.catalog Class Database Object org.apache.spark.sql.catalog.Database All Implemented Interfaces: org.apache.spark.sql.catalyst.DefinedByConstructorParams public class Database extends Object implements org.apache.spark.sql.catalyst.DefinedByConstructorParams A database in Spark, as returned by the listDatabases method defined in Catalog. param: name name of the database. param: description description of the database. param: locationUri path (in the form of a uri) to data files. Since: 2.0.0 Constructor Summary Constructors  Constructor and Description Database(String name, String description, String locationUri)  Method Summary Methods  Modifier and Type Method and Description String description()  String locationUri()  String name()  String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail Database public Database(String name, String description, String locationUri) Method Detail name public String name() description public String description() locationUri public String locationUri() toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Dataset (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Dataset (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class Dataset<T> Object org.apache.spark.sql.Dataset<T> All Implemented Interfaces: java.io.Serializable public class Dataset<T> extends Object implements scala.Serializable A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations. Each Dataset also has an untyped view called a DataFrame, which is a Dataset of Row. Operations available on Datasets are divided into transformations and actions. Transformations are the ones that produce new Datasets, and actions are the ones that trigger computation and return results. Example transformations include map, filter, select, and aggregate (groupBy). Example actions count, show, or writing data out to file systems. Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally, a Dataset represents a logical plan that describes the computation required to produce the data. When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a physical plan for efficient execution in a parallel and distributed manner. To explore the logical plan as well as optimized physical plan, use the explain function. To efficiently support domain-specific objects, an Encoder is required. The encoder maps the domain specific type T to Spark's internal type system. For example, given a class Person with two fields, name (string) and age (int), an encoder is used to tell Spark to generate code at runtime to serialize the Person object into a binary structure. This binary structure often has much lower memory footprint as well as are optimized for efficiency in data processing (e.g. in a columnar format). To understand the internal binary representation for data, use the schema function. There are typically two ways to create a Dataset. The most common way is by pointing Spark to some files on storage systems, using the read function available on a SparkSession. val people = spark.read.parquet("...").as[Person] // Scala Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java Datasets can also be created through transformations available on existing Datasets. For example, the following creates a new Dataset by applying a filter on the existing one: val names = people.map(_.name) // in Scala; names is a Dataset[String] Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING)); // in Java 8 Dataset operations can also be untyped, through various domain-specific-language (DSL) functions defined in: Dataset (this class), Column, and functions. These operations are very similar to the operations available in the data frame abstraction in R or Python. To select a column from the Dataset, use apply method in Scala and col in Java. val ageCol = people("age") // in Scala Column ageCol = people.col("age"); // in Java Note that the Column type can also be manipulated through its various functions. // The following creates a new column that increases everybody's age by 10. people("age") + 10 // in Scala people.col("age").plus(10); // in Java A more concrete example in Scala: // To create Dataset[Row] using SparkSession val people = spark.read.parquet("...") val department = spark.read.parquet("...") people.filter("age > 30") .join(department, people("deptId") === department("id")) .groupBy(department("name"), "gender") .agg(avg(people("salary")), max(people("age"))) and in Java: // To create Dataset<Row> using SparkSession Dataset<Row> people = spark.read().parquet("..."); Dataset<Row> department = spark.read().parquet("..."); people.filter("age".gt(30)) .join(department, people.col("deptId").equalTo(department("id"))) .groupBy(department.col("name"), "gender") .agg(avg(people.col("salary")), max(people.col("age"))); Since: 1.6.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Dataset(SparkSession sparkSession, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan logicalPlan, Encoder<T> encoder)  Dataset(SQLContext sqlContext, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan logicalPlan, Encoder<T> encoder)  Method Summary Methods  Modifier and Type Method and Description Dataset<Row> agg(Column expr, Column... exprs) Aggregates on the entire Dataset without groups. Dataset<Row> agg(Column expr, scala.collection.Seq<Column> exprs) Aggregates on the entire Dataset without groups. Dataset<Row> agg(scala.collection.immutable.Map<String,String> exprs) (Scala-specific) Aggregates on the entire Dataset without groups. Dataset<Row> agg(java.util.Map<String,String> exprs) (Java-specific) Aggregates on the entire Dataset without groups. Dataset<Row> agg(scala.Tuple2<String,String> aggExpr, scala.collection.Seq<scala.Tuple2<String,String>> aggExprs) (Scala-specific) Aggregates on the entire Dataset without groups. Dataset<T> alias(String alias) Returns a new Dataset with an alias set. Dataset<T> alias(scala.Symbol alias) (Scala-specific) Returns a new Dataset with an alias set. Column apply(String colName) Selects column based on the column name and return it as a Column. <U> Dataset<U> as(Encoder<U> evidence$2) :: Experimental :: Returns a new Dataset where each record has been mapped on to the specified type. Dataset<T> as(String alias) Returns a new Dataset with an alias set. Dataset<T> as(scala.Symbol alias) (Scala-specific) Returns a new Dataset with an alias set. Dataset<T> cache() Persist this Dataset with the default storage level (MEMORY_AND_DISK). scala.reflect.ClassTag<T> classTag()  Dataset<T> coalesce(int numPartitions) Returns a new Dataset that has exactly numPartitions partitions. Column col(String colName) Selects column based on the column name and return it as a Column. Object collect() Returns an array that contains all of Rows in this Dataset. java.util.List<T> collectAsList() Returns a Java list that contains all of Rows in this Dataset. String[] columns() Returns all column names as an array. long count() Returns the number of rows in the Dataset. void createOrReplaceTempView(String viewName) Creates a temporary view using the given name. void createTempView(String viewName) Creates a temporary view using the given name. RelationalGroupedDataset cube(Column... cols) Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them. RelationalGroupedDataset cube(scala.collection.Seq<Column> cols) Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them. RelationalGroupedDataset cube(String col1, scala.collection.Seq<String> cols) Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them. RelationalGroupedDataset cube(String col1, String... cols) Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them. Dataset<Row> describe(scala.collection.Seq<String> cols) Computes statistics for numeric columns, including count, mean, stddev, min, and max. Dataset<Row> describe(String... cols) Computes statistics for numeric columns, including count, mean, stddev, min, and max. Dataset<T> distinct() Returns a new Dataset that contains only the unique rows from this Dataset. Dataset<Row> drop(Column col) Returns a new Dataset with a column dropped. Dataset<Row> drop(scala.collection.Seq<String> colNames) Returns a new Dataset with columns dropped. Dataset<Row> drop(String... colNames) Returns a new Dataset with columns dropped. Dataset<Row> drop(String colName) Returns a new Dataset with a column dropped. Dataset<T> dropDuplicates() Returns a new Dataset that contains only the unique rows from this Dataset. Dataset<T> dropDuplicates(scala.collection.Seq<String> colNames) (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only the subset of columns. Dataset<T> dropDuplicates(String[] colNames) Returns a new Dataset with duplicate rows removed, considering only the subset of columns. Dataset<T> dropDuplicates(String col1, scala.collection.Seq<String> cols) Returns a new Dataset with duplicate rows removed, considering only the subset of columns. Dataset<T> dropDuplicates(String col1, String... cols) Returns a new Dataset with duplicate rows removed, considering only the subset of columns. scala.Tuple2<String,String>[] dtypes() Returns all column names and their data types as an array. Dataset<T> except(Dataset<T> other) Returns a new Dataset containing rows in this Dataset but not in another Dataset. void explain() Prints the physical plan to the console for debugging purposes. void explain(boolean extended) Prints the plans (logical and physical) to the console for debugging purposes. <A extends scala.Product> Dataset<Row> explode(scala.collection.Seq<Column> input, scala.Function1<Row,scala.collection.TraversableOnce<A>> f, scala.reflect.api.TypeTags.TypeTag<A> evidence$5) Deprecated.  use flatMap() or select() with functions.explode() instead. Since 2.0.0. <A,B> Dataset<Row> explode(String inputColumn, String outputColumn, scala.Function1<A,scala.collection.TraversableOnce<B>> f, scala.reflect.api.TypeTags.TypeTag<B> evidence$6) Deprecated.  use flatMap() or select() with functions.explode() instead. Since 2.0.0. Dataset<T> filter(Column condition) Filters rows using the given condition. Dataset<T> filter(FilterFunction<T> func) :: Experimental :: (Java-specific) Returns a new Dataset that only contains elements where func returns true. Dataset<T> filter(scala.Function1<T,Object> func) :: Experimental :: (Scala-specific) Returns a new Dataset that only contains elements where func returns true. Dataset<T> filter(String conditionExpr) Filters rows using the given SQL expression. T first() Returns the first row. <U> Dataset<U> flatMap(FlatMapFunction<T,U> f, Encoder<U> encoder) :: Experimental :: (Java-specific) Returns a new Dataset by first applying a function to all elements of this Dataset, and then flattening the results. <U> Dataset<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> func, Encoder<U> evidence$9) :: Experimental :: (Scala-specific) Returns a new Dataset by first applying a function to all elements of this Dataset, and then flattening the results. void foreach(ForeachFunction<T> func) (Java-specific) Runs func on each element of this Dataset. void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f) Applies a function f to all rows. void foreachPartition(ForeachPartitionFunction<T> func) (Java-specific) Runs func on each partition of this Dataset. void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) Applies a function f to each partition of this Dataset. RelationalGroupedDataset groupBy(Column... cols) Groups the Dataset using the specified columns, so we can run aggregation on them. RelationalGroupedDataset groupBy(scala.collection.Seq<Column> cols) Groups the Dataset using the specified columns, so we can run aggregation on them. RelationalGroupedDataset groupBy(String col1, scala.collection.Seq<String> cols) Groups the Dataset using the specified columns, so that we can run aggregation on them. RelationalGroupedDataset groupBy(String col1, String... cols) Groups the Dataset using the specified columns, so that we can run aggregation on them. <K> KeyValueGroupedDataset<K,T> groupByKey(scala.Function1<T,K> func, Encoder<K> evidence$4) :: Experimental :: (Scala-specific) Returns a KeyValueGroupedDataset where the data is grouped by the given key func. <K> KeyValueGroupedDataset<K,T> groupByKey(MapFunction<T,K> func, Encoder<K> encoder) :: Experimental :: (Java-specific) Returns a KeyValueGroupedDataset where the data is grouped by the given key func. T head() Returns the first row. Object head(int n) Returns the first n rows. String[] inputFiles() Returns a best-effort snapshot of the files that compose this Dataset. Dataset<T> intersect(Dataset<T> other) Returns a new Dataset containing rows only in both this Dataset and another Dataset. boolean isLocal() Returns true if the collect and take methods can be run locally (without any Spark executors). boolean isStreaming() Returns true if this Dataset contains one or more sources that continuously return data as it arrives. JavaRDD<T> javaRDD() Returns the content of the Dataset as a JavaRDD of Ts. Dataset<Row> join(Dataset<?> right) Cartesian join with another DataFrame. Dataset<Row> join(Dataset<?> right, Column joinExprs) Inner join with another DataFrame, using the given join expression. Dataset<Row> join(Dataset<?> right, Column joinExprs, String joinType) Join with another DataFrame, using the given join expression. Dataset<Row> join(Dataset<?> right, scala.collection.Seq<String> usingColumns) Inner equi-join with another DataFrame using the given columns. Dataset<Row> join(Dataset<?> right, scala.collection.Seq<String> usingColumns, String joinType) Equi-join with another DataFrame using the given columns. Dataset<Row> join(Dataset<?> right, String usingColumn) Inner equi-join with another DataFrame using the given column. <U> Dataset<scala.Tuple2<T,U>> joinWith(Dataset<U> other, Column condition) :: Experimental :: Using inner equi-join to join this Dataset returning a Tuple2 for each pair where condition evaluates to true. <U> Dataset<scala.Tuple2<T,U>> joinWith(Dataset<U> other, Column condition, String joinType) :: Experimental :: Joins this Dataset returning a Tuple2 for each pair where condition evaluates to true. Dataset<T> limit(int n) Returns a new Dataset by taking the first n rows. <U> Dataset<U> map(scala.Function1<T,U> func, Encoder<U> evidence$7) :: Experimental :: (Scala-specific) Returns a new Dataset that contains the result of applying func to each element. <U> Dataset<U> map(MapFunction<T,U> func, Encoder<U> encoder) :: Experimental :: (Java-specific) Returns a new Dataset that contains the result of applying func to each element. <U> Dataset<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> func, Encoder<U> evidence$8) :: Experimental :: (Scala-specific) Returns a new Dataset that contains the result of applying func to each partition. <U> Dataset<U> mapPartitions(MapPartitionsFunction<T,U> f, Encoder<U> encoder) :: Experimental :: (Java-specific) Returns a new Dataset that contains the result of applying f to each partition. DataFrameNaFunctions na() Returns a DataFrameNaFunctions for working with missing data. static Dataset<Row> ofRows(SparkSession sparkSession, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan logicalPlan)  Dataset<T> orderBy(Column... sortExprs) Returns a new Dataset sorted by the given expressions. Dataset<T> orderBy(scala.collection.Seq<Column> sortExprs) Returns a new Dataset sorted by the given expressions. Dataset<T> orderBy(String sortCol, scala.collection.Seq<String> sortCols) Returns a new Dataset sorted by the given expressions. Dataset<T> orderBy(String sortCol, String... sortCols) Returns a new Dataset sorted by the given expressions. Dataset<T> persist() Persist this Dataset with the default storage level (MEMORY_AND_DISK). Dataset<T> persist(StorageLevel newLevel) Persist this Dataset with the given storage level. void printSchema() Prints the schema to the console in a nice tree format. org.apache.spark.sql.execution.QueryExecution queryExecution()  Dataset<T>[] randomSplit(double[] weights) Randomly splits this Dataset with the provided weights. Dataset<T>[] randomSplit(double[] weights, long seed) Randomly splits this Dataset with the provided weights. java.util.List<Dataset<T>> randomSplitAsList(double[] weights, long seed) Returns a Java list that contains randomly split Dataset with the provided weights. RDD<T> rdd() Represents the content of the Dataset as an RDD of T. T reduce(scala.Function2<T,T,T> func) :: Experimental :: (Scala-specific) Reduces the elements of this Dataset using the specified binary function. T reduce(ReduceFunction<T> func) :: Experimental :: (Java-specific) Reduces the elements of this Dataset using the specified binary function. void registerTempTable(String tableName) Deprecated.  Use createOrReplaceTempView(viewName) instead. Since 2.0.0. Dataset<T> repartition(Column... partitionExprs) Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions. Dataset<T> repartition(int numPartitions) Returns a new Dataset that has exactly numPartitions partitions. Dataset<T> repartition(int numPartitions, Column... partitionExprs) Returns a new Dataset partitioned by the given partitioning expressions into numPartitions. Dataset<T> repartition(int numPartitions, scala.collection.Seq<Column> partitionExprs) Returns a new Dataset partitioned by the given partitioning expressions into numPartitions. Dataset<T> repartition(scala.collection.Seq<Column> partitionExprs) Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions. RelationalGroupedDataset rollup(Column... cols) Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them. RelationalGroupedDataset rollup(scala.collection.Seq<Column> cols) Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them. RelationalGroupedDataset rollup(String col1, scala.collection.Seq<String> cols) Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them. RelationalGroupedDataset rollup(String col1, String... cols) Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them. Dataset<T> sample(boolean withReplacement, double fraction) Returns a new Dataset by sampling a fraction of rows, using a random seed. Dataset<T> sample(boolean withReplacement, double fraction, long seed) Returns a new Dataset by sampling a fraction of rows. StructType schema() Returns the schema of this Dataset. Dataset<Row> select(Column... cols) Selects a set of column based expressions. Dataset<Row> select(scala.collection.Seq<Column> cols) Selects a set of column based expressions. Dataset<Row> select(String col, scala.collection.Seq<String> cols) Selects a set of columns. Dataset<Row> select(String col, String... cols) Selects a set of columns. <U1> Dataset<U1> select(TypedColumn<T,U1> c1, Encoder<U1> evidence$3) :: Experimental :: Returns a new Dataset by computing the given Column expression for each element. <U1,U2> Dataset<scala.Tuple2<U1,U2>> select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2) :: Experimental :: Returns a new Dataset by computing the given Column expressions for each element. <U1,U2,U3> Dataset<scala.Tuple3<U1,U2,U3>> select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2, TypedColumn<T,U3> c3) :: Experimental :: Returns a new Dataset by computing the given Column expressions for each element. <U1,U2,U3,U4> Dataset<scala.Tuple4<U1,U2,U3,U4>> select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2, TypedColumn<T,U3> c3, TypedColumn<T,U4> c4) :: Experimental :: Returns a new Dataset by computing the given Column expressions for each element. <U1,U2,U3,U4,U5> Dataset<scala.Tuple5<U1,U2,U3,U4,U5>> select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2, TypedColumn<T,U3> c3, TypedColumn<T,U4> c4, TypedColumn<T,U5> c5) :: Experimental :: Returns a new Dataset by computing the given Column expressions for each element. Dataset<Row> selectExpr(scala.collection.Seq<String> exprs) Selects a set of SQL expressions. Dataset<Row> selectExpr(String... exprs) Selects a set of SQL expressions. void show() Displays the top 20 rows of Dataset in a tabular form. void show(boolean truncate) Displays the top 20 rows of Dataset in a tabular form. void show(int numRows) Displays the Dataset in a tabular form. void show(int numRows, boolean truncate) Displays the Dataset in a tabular form. Dataset<T> sort(Column... sortExprs) Returns a new Dataset sorted by the given expressions. Dataset<T> sort(scala.collection.Seq<Column> sortExprs) Returns a new Dataset sorted by the given expressions. Dataset<T> sort(String sortCol, scala.collection.Seq<String> sortCols) Returns a new Dataset sorted by the specified column, all in ascending order. Dataset<T> sort(String sortCol, String... sortCols) Returns a new Dataset sorted by the specified column, all in ascending order. Dataset<T> sortWithinPartitions(Column... sortExprs) Returns a new Dataset with each partition sorted by the given expressions. Dataset<T> sortWithinPartitions(scala.collection.Seq<Column> sortExprs) Returns a new Dataset with each partition sorted by the given expressions. Dataset<T> sortWithinPartitions(String sortCol, scala.collection.Seq<String> sortCols) Returns a new Dataset with each partition sorted by the given expressions. Dataset<T> sortWithinPartitions(String sortCol, String... sortCols) Returns a new Dataset with each partition sorted by the given expressions. SparkSession sparkSession()  SQLContext sqlContext()  DataFrameStatFunctions stat() Returns a DataFrameStatFunctions for working statistic functions support. Object take(int n) Returns the first n rows in the Dataset. java.util.List<T> takeAsList(int n) Returns the first n rows in the Dataset as a list. Dataset<Row> toDF() Converts this strongly typed collection of data to generic Dataframe. Dataset<Row> toDF(scala.collection.Seq<String> colNames) Converts this strongly typed collection of data to generic DataFrame with columns renamed. Dataset<Row> toDF(String... colNames) Converts this strongly typed collection of data to generic DataFrame with columns renamed. JavaRDD<T> toJavaRDD() Returns the content of the Dataset as a JavaRDD of Ts. Dataset<String> toJSON() Returns the content of the Dataset as a Dataset of JSON strings. java.util.Iterator<T> toLocalIterator() Return an iterator that contains all of Rows in this Dataset. String toString()  <U> Dataset<U> transform(scala.Function1<Dataset<T>,Dataset<U>> t) Concise syntax for chaining custom transformations. Dataset<T> union(Dataset<T> other) Returns a new Dataset containing union of rows in this Dataset and another Dataset. Dataset<T> unionAll(Dataset<T> other) Deprecated.  use union(). Since 2.0.0. Dataset<T> unpersist() Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. Dataset<T> unpersist(boolean blocking) Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. Dataset<T> where(Column condition) Filters rows using the given condition. Dataset<T> where(String conditionExpr) Filters rows using the given SQL expression. Dataset<Row> withColumn(String colName, Column col) Returns a new Dataset by adding a column or replacing the existing column that has the same name. Dataset<Row> withColumnRenamed(String existingName, String newName) Returns a new Dataset with a column renamed. DataFrameWriter<T> write() :: Experimental :: Interface for saving the content of the non-streaming Dataset out into external storage. DataStreamWriter<T> writeStream() :: Experimental :: Interface for saving the content of the streaming Dataset out into external storage. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail Dataset public Dataset(SparkSession sparkSession, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan logicalPlan, Encoder<T> encoder) Dataset public Dataset(SQLContext sqlContext, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan logicalPlan, Encoder<T> encoder) Method Detail ofRows public static Dataset<Row> ofRows(SparkSession sparkSession, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan logicalPlan) toDF public Dataset<Row> toDF(String... colNames) Converts this strongly typed collection of data to generic DataFrame with columns renamed. This can be quite convenient in conversion from a RDD of tuples into a DataFrame with meaningful names. For example: val rdd: RDD[(Int, String)] = ... rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` rdd.toDF("id", "name") // this creates a DataFrame with column name "id" and "name" Parameters:colNames - (undocumented) Returns:(undocumented)Since: 2.0.0 sortWithinPartitions public Dataset<T> sortWithinPartitions(String sortCol, String... sortCols) Returns a new Dataset with each partition sorted by the given expressions. This is the same operation as "SORT BY" in SQL (Hive QL). Parameters:sortCol - (undocumented)sortCols - (undocumented) Returns:(undocumented)Since: 2.0.0 sortWithinPartitions public Dataset<T> sortWithinPartitions(Column... sortExprs) Returns a new Dataset with each partition sorted by the given expressions. This is the same operation as "SORT BY" in SQL (Hive QL). Parameters:sortExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 sort public Dataset<T> sort(String sortCol, String... sortCols) Returns a new Dataset sorted by the specified column, all in ascending order. // The following 3 are equivalent ds.sort("sortcol") ds.sort($"sortcol") ds.sort($"sortcol".asc) Parameters:sortCol - (undocumented)sortCols - (undocumented) Returns:(undocumented)Since: 2.0.0 sort public Dataset<T> sort(Column... sortExprs) Returns a new Dataset sorted by the given expressions. For example: ds.sort($"col1", $"col2".desc) Parameters:sortExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 orderBy public Dataset<T> orderBy(String sortCol, String... sortCols) Returns a new Dataset sorted by the given expressions. This is an alias of the sort function. Parameters:sortCol - (undocumented)sortCols - (undocumented) Returns:(undocumented)Since: 2.0.0 orderBy public Dataset<T> orderBy(Column... sortExprs) Returns a new Dataset sorted by the given expressions. This is an alias of the sort function. Parameters:sortExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 select public Dataset<Row> select(Column... cols) Selects a set of column based expressions. ds.select($"colA", $"colB" + 1) Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0.0 select public Dataset<Row> select(String col, String... cols) Selects a set of columns. This is a variant of select that can only select existing columns using column names (i.e. cannot construct expressions). // The following two are equivalent: ds.select("colA", "colB") ds.select($"colA", $"colB") Parameters:col - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 selectExpr public Dataset<Row> selectExpr(String... exprs) Selects a set of SQL expressions. This is a variant of select that accepts SQL expressions. // The following are equivalent: ds.selectExpr("colA", "colB as newName", "abs(colC)") ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)")) Parameters:exprs - (undocumented) Returns:(undocumented)Since: 2.0.0 groupBy public RelationalGroupedDataset groupBy(Column... cols) Groups the Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. // Compute the average for all numeric columns grouped by department. ds.groupBy($"department").avg() // Compute the max age and average salary, grouped by department and gender. ds.groupBy($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0.0 rollup public RelationalGroupedDataset rollup(Column... cols) Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. // Compute the average for all numeric columns rolluped by department and group. ds.rollup($"department", $"group").avg() // Compute the max age and average salary, rolluped by department and gender. ds.rollup($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0.0 cube public RelationalGroupedDataset cube(Column... cols) Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. // Compute the average for all numeric columns cubed by department and group. ds.cube($"department", $"group").avg() // Compute the max age and average salary, cubed by department and gender. ds.cube($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0.0 groupBy public RelationalGroupedDataset groupBy(String col1, String... cols) Groups the Dataset using the specified columns, so that we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. This is a variant of groupBy that can only group by existing columns using column names (i.e. cannot construct expressions). // Compute the average for all numeric columns grouped by department. ds.groupBy("department").avg() // Compute the max age and average salary, grouped by department and gender. ds.groupBy($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:col1 - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 rollup public RelationalGroupedDataset rollup(String col1, String... cols) Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. This is a variant of rollup that can only group by existing columns using column names (i.e. cannot construct expressions). // Compute the average for all numeric columns rolluped by department and group. ds.rollup("department", "group").avg() // Compute the max age and average salary, rolluped by department and gender. ds.rollup($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:col1 - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 cube public RelationalGroupedDataset cube(String col1, String... cols) Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. This is a variant of cube that can only group by existing columns using column names (i.e. cannot construct expressions). // Compute the average for all numeric columns cubed by department and group. ds.cube("department", "group").avg() // Compute the max age and average salary, cubed by department and gender. ds.cube($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:col1 - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 agg public Dataset<Row> agg(Column expr, Column... exprs) Aggregates on the entire Dataset without groups. // ds.agg(...) is a shorthand for ds.groupBy().agg(...) ds.agg(max($"age"), avg($"salary")) ds.groupBy().agg(max($"age"), avg($"salary")) Parameters:expr - (undocumented)exprs - (undocumented) Returns:(undocumented)Since: 2.0.0 drop public Dataset<Row> drop(String... colNames) Returns a new Dataset with columns dropped. This is a no-op if schema doesn't contain column name(s). This method can only be used to drop top level columns. the colName string is treated literally without further interpretation. Parameters:colNames - (undocumented) Returns:(undocumented)Since: 2.0.0 dropDuplicates public Dataset<T> dropDuplicates(String col1, String... cols) Returns a new Dataset with duplicate rows removed, considering only the subset of columns. Parameters:col1 - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 describe public Dataset<Row> describe(String... cols) Computes statistics for numeric columns, including count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical columns. This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting Dataset. If you want to programmatically compute summary statistics, use the agg function instead. ds.describe("age", "height").show() // output: // summary age height // count 10.0 10.0 // mean 53.3 178.05 // stddev 11.6 15.7 // min 18.0 163.0 // max 92.0 192.0 Parameters:cols - (undocumented) Returns:(undocumented)Since: 1.6.0 repartition public Dataset<T> repartition(int numPartitions, Column... partitionExprs) Returns a new Dataset partitioned by the given partitioning expressions into numPartitions. The resulting Dataset is hash partitioned. This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL). Parameters:numPartitions - (undocumented)partitionExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 repartition public Dataset<T> repartition(Column... partitionExprs) Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions. The resulting Dataset is hash partitioned. This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL). Parameters:partitionExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 sparkSession public SparkSession sparkSession() queryExecution public org.apache.spark.sql.execution.QueryExecution queryExecution() classTag public scala.reflect.ClassTag<T> classTag() sqlContext public SQLContext sqlContext() toString public String toString() Overrides: toString in class Object toDF public Dataset<Row> toDF() Converts this strongly typed collection of data to generic Dataframe. In contrast to the strongly typed objects that Dataset operations work on, a Dataframe returns generic Row objects that allow fields to be accessed by ordinal or name. Returns:(undocumented)Since: 1.6.0 as public <U> Dataset<U> as(Encoder<U> evidence$2) :: Experimental :: Returns a new Dataset where each record has been mapped on to the specified type. The method used to map columns depend on the type of U: - When U is a class, fields for the class will be mapped to columns of the same name (case sensitivity is determined by spark.sql.caseSensitive). - When U is a tuple, the columns will be be mapped by ordinal (i.e. the first column will be assigned to _1). - When U is a primitive type (i.e. String, Int, etc), then the first column of the DataFrame will be used. If the schema of the Dataset does not match the desired U type, you can use select along with alias or as to rearrange or rename as required. Parameters:evidence$2 - (undocumented) Returns:(undocumented)Since: 1.6.0 toDF public Dataset<Row> toDF(scala.collection.Seq<String> colNames) Converts this strongly typed collection of data to generic DataFrame with columns renamed. This can be quite convenient in conversion from a RDD of tuples into a DataFrame with meaningful names. For example: val rdd: RDD[(Int, String)] = ... rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` rdd.toDF("id", "name") // this creates a DataFrame with column name "id" and "name" Parameters:colNames - (undocumented) Returns:(undocumented)Since: 2.0.0 schema public StructType schema() Returns the schema of this Dataset. Returns:(undocumented)Since: 1.6.0 printSchema public void printSchema() Prints the schema to the console in a nice tree format. Since: 1.6.0 explain public void explain(boolean extended) Prints the plans (logical and physical) to the console for debugging purposes. Parameters:extended - (undocumented)Since: 1.6.0 explain public void explain() Prints the physical plan to the console for debugging purposes. Since: 1.6.0 dtypes public scala.Tuple2<String,String>[] dtypes() Returns all column names and their data types as an array. Returns:(undocumented)Since: 1.6.0 columns public String[] columns() Returns all column names as an array. Returns:(undocumented)Since: 1.6.0 isLocal public boolean isLocal() Returns true if the collect and take methods can be run locally (without any Spark executors). Returns:(undocumented)Since: 1.6.0 isStreaming public boolean isStreaming() Returns true if this Dataset contains one or more sources that continuously return data as it arrives. A Dataset that reads data from a streaming source must be executed as a StreamingQuery using the start() method in DataStreamWriter. Methods that return a single answer, e.g. count() or collect(), will throw an AnalysisException when there is a streaming source present. Returns:(undocumented)Since: 2.0.0 show public void show(int numRows) Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, and all cells will be aligned right. For example: year month AVG('Adj Close) MAX('Adj Close) 1980 12 0.503218 0.595103 1981 01 0.523289 0.570307 1982 02 0.436504 0.475256 1983 03 0.410516 0.442194 1984 04 0.450090 0.483521 Parameters:numRows - Number of rows to show Since: 1.6.0 show public void show() Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters will be truncated, and all cells will be aligned right. Since: 1.6.0 show public void show(boolean truncate) Displays the top 20 rows of Dataset in a tabular form. Parameters:truncate - Whether truncate long strings. If true, strings more than 20 characters will be truncated and all cells will be aligned right Since: 1.6.0 show public void show(int numRows, boolean truncate) Displays the Dataset in a tabular form. For example: year month AVG('Adj Close) MAX('Adj Close) 1980 12 0.503218 0.595103 1981 01 0.523289 0.570307 1982 02 0.436504 0.475256 1983 03 0.410516 0.442194 1984 04 0.450090 0.483521 Parameters:numRows - Number of rows to showtruncate - Whether truncate long strings. If true, strings more than 20 characters will be truncated and all cells will be aligned right Since: 1.6.0 na public DataFrameNaFunctions na() Returns a DataFrameNaFunctions for working with missing data. // Dropping rows containing any null values. ds.na.drop() Returns:(undocumented)Since: 1.6.0 stat public DataFrameStatFunctions stat() Returns a DataFrameStatFunctions for working statistic functions support. // Finding frequent items in column with name 'a'. ds.stat.freqItems(Seq("a")) Returns:(undocumented)Since: 1.6.0 join public Dataset<Row> join(Dataset<?> right) Cartesian join with another DataFrame. Note that cartesian joins are very expensive without an extra filter that can be pushed down. Parameters:right - Right side of the join operation. Returns:(undocumented)Since: 2.0.0 join public Dataset<Row> join(Dataset<?> right, String usingColumn) Inner equi-join with another DataFrame using the given column. Different from other join functions, the join column will only appear once in the output, i.e. similar to SQL's JOIN USING syntax. // Joining df1 and df2 using the column "user_id" df1.join(df2, "user_id") Note that if you perform a self-join using this function without aliasing the input DataFrames, you will NOT be able to reference any columns after the join, since there is no way to disambiguate which side of the join you would like to reference. Parameters:right - Right side of the join operation.usingColumn - Name of the column to join on. This column must exist on both sides. Returns:(undocumented)Since: 2.0.0 join public Dataset<Row> join(Dataset<?> right, scala.collection.Seq<String> usingColumns) Inner equi-join with another DataFrame using the given columns. Different from other join functions, the join columns will only appear once in the output, i.e. similar to SQL's JOIN USING syntax. // Joining df1 and df2 using the columns "user_id" and "user_name" df1.join(df2, Seq("user_id", "user_name")) Note that if you perform a self-join using this function without aliasing the input DataFrames, you will NOT be able to reference any columns after the join, since there is no way to disambiguate which side of the join you would like to reference. Parameters:right - Right side of the join operation.usingColumns - Names of the columns to join on. This columns must exist on both sides. Returns:(undocumented)Since: 2.0.0 join public Dataset<Row> join(Dataset<?> right, scala.collection.Seq<String> usingColumns, String joinType) Equi-join with another DataFrame using the given columns. Different from other join functions, the join columns will only appear once in the output, i.e. similar to SQL's JOIN USING syntax. Note that if you perform a self-join using this function without aliasing the input DataFrames, you will NOT be able to reference any columns after the join, since there is no way to disambiguate which side of the join you would like to reference. Parameters:right - Right side of the join operation.usingColumns - Names of the columns to join on. This columns must exist on both sides.joinType - One of: inner, outer, left_outer, right_outer, leftsemi. Returns:(undocumented)Since: 2.0.0 join public Dataset<Row> join(Dataset<?> right, Column joinExprs) Inner join with another DataFrame, using the given join expression. // The following two are equivalent: df1.join(df2, $"df1Key" === $"df2Key") df1.join(df2).where($"df1Key" === $"df2Key") Parameters:right - (undocumented)joinExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 join public Dataset<Row> join(Dataset<?> right, Column joinExprs, String joinType) Join with another DataFrame, using the given join expression. The following performs a full outer join between df1 and df2. // Scala: import org.apache.spark.sql.functions._ df1.join(df2, $"df1Key" === $"df2Key", "outer") // Java: import static org.apache.spark.sql.functions.*; df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer"); Parameters:right - Right side of the join.joinExprs - Join expression.joinType - One of: inner, outer, left_outer, right_outer, leftsemi. Returns:(undocumented)Since: 2.0.0 joinWith public <U> Dataset<scala.Tuple2<T,U>> joinWith(Dataset<U> other, Column condition, String joinType) :: Experimental :: Joins this Dataset returning a Tuple2 for each pair where condition evaluates to true. This is similar to the relation join function with one important difference in the result schema. Since joinWith preserves objects present on either side of the join, the result schema is similarly nested into a tuple under the column names _1 and _2. This type of join can be useful both for preserving type-safety with the original object types as well as working with relational data where either side of the join has column names in common. Parameters:other - Right side of the join.condition - Join expression.joinType - One of: inner, outer, left_outer, right_outer, leftsemi. Returns:(undocumented)Since: 1.6.0 joinWith public <U> Dataset<scala.Tuple2<T,U>> joinWith(Dataset<U> other, Column condition) :: Experimental :: Using inner equi-join to join this Dataset returning a Tuple2 for each pair where condition evaluates to true. Parameters:other - Right side of the join.condition - Join expression. Returns:(undocumented)Since: 1.6.0 sortWithinPartitions public Dataset<T> sortWithinPartitions(String sortCol, scala.collection.Seq<String> sortCols) Returns a new Dataset with each partition sorted by the given expressions. This is the same operation as "SORT BY" in SQL (Hive QL). Parameters:sortCol - (undocumented)sortCols - (undocumented) Returns:(undocumented)Since: 2.0.0 sortWithinPartitions public Dataset<T> sortWithinPartitions(scala.collection.Seq<Column> sortExprs) Returns a new Dataset with each partition sorted by the given expressions. This is the same operation as "SORT BY" in SQL (Hive QL). Parameters:sortExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 sort public Dataset<T> sort(String sortCol, scala.collection.Seq<String> sortCols) Returns a new Dataset sorted by the specified column, all in ascending order. // The following 3 are equivalent ds.sort("sortcol") ds.sort($"sortcol") ds.sort($"sortcol".asc) Parameters:sortCol - (undocumented)sortCols - (undocumented) Returns:(undocumented)Since: 2.0.0 sort public Dataset<T> sort(scala.collection.Seq<Column> sortExprs) Returns a new Dataset sorted by the given expressions. For example: ds.sort($"col1", $"col2".desc) Parameters:sortExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 orderBy public Dataset<T> orderBy(String sortCol, scala.collection.Seq<String> sortCols) Returns a new Dataset sorted by the given expressions. This is an alias of the sort function. Parameters:sortCol - (undocumented)sortCols - (undocumented) Returns:(undocumented)Since: 2.0.0 orderBy public Dataset<T> orderBy(scala.collection.Seq<Column> sortExprs) Returns a new Dataset sorted by the given expressions. This is an alias of the sort function. Parameters:sortExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 apply public Column apply(String colName) Selects column based on the column name and return it as a Column. Note that the column name can also reference to a nested column like a.b. Parameters:colName - (undocumented) Returns:(undocumented)Since: 2.0.0 col public Column col(String colName) Selects column based on the column name and return it as a Column. Note that the column name can also reference to a nested column like a.b. Parameters:colName - (undocumented) Returns:(undocumented)Since: 2.0.0 as public Dataset<T> as(String alias) Returns a new Dataset with an alias set. Parameters:alias - (undocumented) Returns:(undocumented)Since: 1.6.0 as public Dataset<T> as(scala.Symbol alias) (Scala-specific) Returns a new Dataset with an alias set. Parameters:alias - (undocumented) Returns:(undocumented)Since: 2.0.0 alias public Dataset<T> alias(String alias) Returns a new Dataset with an alias set. Same as as. Parameters:alias - (undocumented) Returns:(undocumented)Since: 2.0.0 alias public Dataset<T> alias(scala.Symbol alias) (Scala-specific) Returns a new Dataset with an alias set. Same as as. Parameters:alias - (undocumented) Returns:(undocumented)Since: 2.0.0 select public Dataset<Row> select(scala.collection.Seq<Column> cols) Selects a set of column based expressions. ds.select($"colA", $"colB" + 1) Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0.0 select public Dataset<Row> select(String col, scala.collection.Seq<String> cols) Selects a set of columns. This is a variant of select that can only select existing columns using column names (i.e. cannot construct expressions). // The following two are equivalent: ds.select("colA", "colB") ds.select($"colA", $"colB") Parameters:col - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 selectExpr public Dataset<Row> selectExpr(scala.collection.Seq<String> exprs) Selects a set of SQL expressions. This is a variant of select that accepts SQL expressions. // The following are equivalent: ds.selectExpr("colA", "colB as newName", "abs(colC)") ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)")) Parameters:exprs - (undocumented) Returns:(undocumented)Since: 2.0.0 select public <U1> Dataset<U1> select(TypedColumn<T,U1> c1, Encoder<U1> evidence$3) :: Experimental :: Returns a new Dataset by computing the given Column expression for each element. val ds = Seq(1, 2, 3).toDS() val newDS = ds.select(expr("value + 1").as[Int]) Parameters:c1 - (undocumented)evidence$3 - (undocumented) Returns:(undocumented)Since: 1.6.0 select public <U1,U2> Dataset<scala.Tuple2<U1,U2>> select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2) :: Experimental :: Returns a new Dataset by computing the given Column expressions for each element. Parameters:c1 - (undocumented)c2 - (undocumented) Returns:(undocumented)Since: 1.6.0 select public <U1,U2,U3> Dataset<scala.Tuple3<U1,U2,U3>> select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2, TypedColumn<T,U3> c3) :: Experimental :: Returns a new Dataset by computing the given Column expressions for each element. Parameters:c1 - (undocumented)c2 - (undocumented)c3 - (undocumented) Returns:(undocumented)Since: 1.6.0 select public <U1,U2,U3,U4> Dataset<scala.Tuple4<U1,U2,U3,U4>> select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2, TypedColumn<T,U3> c3, TypedColumn<T,U4> c4) :: Experimental :: Returns a new Dataset by computing the given Column expressions for each element. Parameters:c1 - (undocumented)c2 - (undocumented)c3 - (undocumented)c4 - (undocumented) Returns:(undocumented)Since: 1.6.0 select public <U1,U2,U3,U4,U5> Dataset<scala.Tuple5<U1,U2,U3,U4,U5>> select(TypedColumn<T,U1> c1, TypedColumn<T,U2> c2, TypedColumn<T,U3> c3, TypedColumn<T,U4> c4, TypedColumn<T,U5> c5) :: Experimental :: Returns a new Dataset by computing the given Column expressions for each element. Parameters:c1 - (undocumented)c2 - (undocumented)c3 - (undocumented)c4 - (undocumented)c5 - (undocumented) Returns:(undocumented)Since: 1.6.0 filter public Dataset<T> filter(Column condition) Filters rows using the given condition. // The following are equivalent: peopleDs.filter($"age" > 15) peopleDs.where($"age" > 15) Parameters:condition - (undocumented) Returns:(undocumented)Since: 1.6.0 filter public Dataset<T> filter(String conditionExpr) Filters rows using the given SQL expression. peopleDs.filter("age > 15") Parameters:conditionExpr - (undocumented) Returns:(undocumented)Since: 1.6.0 where public Dataset<T> where(Column condition) Filters rows using the given condition. This is an alias for filter. // The following are equivalent: peopleDs.filter($"age" > 15) peopleDs.where($"age" > 15) Parameters:condition - (undocumented) Returns:(undocumented)Since: 1.6.0 where public Dataset<T> where(String conditionExpr) Filters rows using the given SQL expression. peopleDs.where("age > 15") Parameters:conditionExpr - (undocumented) Returns:(undocumented)Since: 1.6.0 groupBy public RelationalGroupedDataset groupBy(scala.collection.Seq<Column> cols) Groups the Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. // Compute the average for all numeric columns grouped by department. ds.groupBy($"department").avg() // Compute the max age and average salary, grouped by department and gender. ds.groupBy($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0.0 rollup public RelationalGroupedDataset rollup(scala.collection.Seq<Column> cols) Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. // Compute the average for all numeric columns rolluped by department and group. ds.rollup($"department", $"group").avg() // Compute the max age and average salary, rolluped by department and gender. ds.rollup($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0.0 cube public RelationalGroupedDataset cube(scala.collection.Seq<Column> cols) Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. // Compute the average for all numeric columns cubed by department and group. ds.cube($"department", $"group").avg() // Compute the max age and average salary, cubed by department and gender. ds.cube($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0.0 groupBy public RelationalGroupedDataset groupBy(String col1, scala.collection.Seq<String> cols) Groups the Dataset using the specified columns, so that we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. This is a variant of groupBy that can only group by existing columns using column names (i.e. cannot construct expressions). // Compute the average for all numeric columns grouped by department. ds.groupBy("department").avg() // Compute the max age and average salary, grouped by department and gender. ds.groupBy($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:col1 - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 reduce public T reduce(scala.Function2<T,T,T> func) :: Experimental :: (Scala-specific) Reduces the elements of this Dataset using the specified binary function. The given func must be commutative and associative or the result may be non-deterministic. Parameters:func - (undocumented) Returns:(undocumented)Since: 1.6.0 reduce public T reduce(ReduceFunction<T> func) :: Experimental :: (Java-specific) Reduces the elements of this Dataset using the specified binary function. The given func must be commutative and associative or the result may be non-deterministic. Parameters:func - (undocumented) Returns:(undocumented)Since: 1.6.0 groupByKey public <K> KeyValueGroupedDataset<K,T> groupByKey(scala.Function1<T,K> func, Encoder<K> evidence$4) :: Experimental :: (Scala-specific) Returns a KeyValueGroupedDataset where the data is grouped by the given key func. Parameters:func - (undocumented)evidence$4 - (undocumented) Returns:(undocumented)Since: 2.0.0 groupByKey public <K> KeyValueGroupedDataset<K,T> groupByKey(MapFunction<T,K> func, Encoder<K> encoder) :: Experimental :: (Java-specific) Returns a KeyValueGroupedDataset where the data is grouped by the given key func. Parameters:func - (undocumented)encoder - (undocumented) Returns:(undocumented)Since: 2.0.0 rollup public RelationalGroupedDataset rollup(String col1, scala.collection.Seq<String> cols) Create a multi-dimensional rollup for the current Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. This is a variant of rollup that can only group by existing columns using column names (i.e. cannot construct expressions). // Compute the average for all numeric columns rolluped by department and group. ds.rollup("department", "group").avg() // Compute the max age and average salary, rolluped by department and gender. ds.rollup($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:col1 - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 cube public RelationalGroupedDataset cube(String col1, scala.collection.Seq<String> cols) Create a multi-dimensional cube for the current Dataset using the specified columns, so we can run aggregation on them. See RelationalGroupedDataset for all the available aggregate functions. This is a variant of cube that can only group by existing columns using column names (i.e. cannot construct expressions). // Compute the average for all numeric columns cubed by department and group. ds.cube("department", "group").avg() // Compute the max age and average salary, cubed by department and gender. ds.cube($"department", $"gender").agg(Map( "salary" -> "avg", "age" -> "max" )) Parameters:col1 - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 agg public Dataset<Row> agg(scala.Tuple2<String,String> aggExpr, scala.collection.Seq<scala.Tuple2<String,String>> aggExprs) (Scala-specific) Aggregates on the entire Dataset without groups. // ds.agg(...) is a shorthand for ds.groupBy().agg(...) ds.agg("age" -> "max", "salary" -> "avg") ds.groupBy().agg("age" -> "max", "salary" -> "avg") Parameters:aggExpr - (undocumented)aggExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 agg public Dataset<Row> agg(scala.collection.immutable.Map<String,String> exprs) (Scala-specific) Aggregates on the entire Dataset without groups. // ds.agg(...) is a shorthand for ds.groupBy().agg(...) ds.agg(Map("age" -> "max", "salary" -> "avg")) ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg")) Parameters:exprs - (undocumented) Returns:(undocumented)Since: 2.0.0 agg public Dataset<Row> agg(java.util.Map<String,String> exprs) (Java-specific) Aggregates on the entire Dataset without groups. // ds.agg(...) is a shorthand for ds.groupBy().agg(...) ds.agg(Map("age" -> "max", "salary" -> "avg")) ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg")) Parameters:exprs - (undocumented) Returns:(undocumented)Since: 2.0.0 agg public Dataset<Row> agg(Column expr, scala.collection.Seq<Column> exprs) Aggregates on the entire Dataset without groups. // ds.agg(...) is a shorthand for ds.groupBy().agg(...) ds.agg(max($"age"), avg($"salary")) ds.groupBy().agg(max($"age"), avg($"salary")) Parameters:expr - (undocumented)exprs - (undocumented) Returns:(undocumented)Since: 2.0.0 limit public Dataset<T> limit(int n) Returns a new Dataset by taking the first n rows. The difference between this function and head is that head is an action and returns an array (by triggering query execution) while limit returns a new Dataset. Parameters:n - (undocumented) Returns:(undocumented)Since: 2.0.0 unionAll public Dataset<T> unionAll(Dataset<T> other) Deprecated. use union(). Since 2.0.0. Returns a new Dataset containing union of rows in this Dataset and another Dataset. This is equivalent to UNION ALL in SQL. To do a SQL-style set union (that does deduplication of elements), use this function followed by a distinct. Parameters:other - (undocumented) Returns:(undocumented)Since: 2.0.0 union public Dataset<T> union(Dataset<T> other) Returns a new Dataset containing union of rows in this Dataset and another Dataset. This is equivalent to UNION ALL in SQL. To do a SQL-style set union (that does deduplication of elements), use this function followed by a distinct. Parameters:other - (undocumented) Returns:(undocumented)Since: 2.0.0 intersect public Dataset<T> intersect(Dataset<T> other) Returns a new Dataset containing rows only in both this Dataset and another Dataset. This is equivalent to INTERSECT in SQL. Note that, equality checking is performed directly on the encoded representation of the data and thus is not affected by a custom equals function defined on T. Parameters:other - (undocumented) Returns:(undocumented)Since: 1.6.0 except public Dataset<T> except(Dataset<T> other) Returns a new Dataset containing rows in this Dataset but not in another Dataset. This is equivalent to EXCEPT in SQL. Note that, equality checking is performed directly on the encoded representation of the data and thus is not affected by a custom equals function defined on T. Parameters:other - (undocumented) Returns:(undocumented)Since: 2.0.0 sample public Dataset<T> sample(boolean withReplacement, double fraction, long seed) Returns a new Dataset by sampling a fraction of rows. Parameters:withReplacement - Sample with replacement or not.fraction - Fraction of rows to generate.seed - Seed for sampling. Returns:(undocumented)Since: 1.6.0 sample public Dataset<T> sample(boolean withReplacement, double fraction) Returns a new Dataset by sampling a fraction of rows, using a random seed. Parameters:withReplacement - Sample with replacement or not.fraction - Fraction of rows to generate. Returns:(undocumented)Since: 1.6.0 randomSplit public Dataset<T>[] randomSplit(double[] weights, long seed) Randomly splits this Dataset with the provided weights. Parameters:weights - weights for splits, will be normalized if they don't sum to 1.seed - Seed for sampling. For Java API, use randomSplitAsList. Returns:(undocumented)Since: 2.0.0 randomSplitAsList public java.util.List<Dataset<T>> randomSplitAsList(double[] weights, long seed) Returns a Java list that contains randomly split Dataset with the provided weights. Parameters:weights - weights for splits, will be normalized if they don't sum to 1.seed - Seed for sampling. Returns:(undocumented)Since: 2.0.0 randomSplit public Dataset<T>[] randomSplit(double[] weights) Randomly splits this Dataset with the provided weights. Parameters:weights - weights for splits, will be normalized if they don't sum to 1. Returns:(undocumented)Since: 2.0.0 explode public <A extends scala.Product> Dataset<Row> explode(scala.collection.Seq<Column> input, scala.Function1<Row,scala.collection.TraversableOnce<A>> f, scala.reflect.api.TypeTags.TypeTag<A> evidence$5) Deprecated. use flatMap() or select() with functions.explode() instead. Since 2.0.0. (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more rows by the provided function. This is similar to a LATERAL VIEW in HiveQL. The columns of the input row are implicitly joined with each row that is output by the function. Given that this is deprecated, as an alternative, you can explode columns either using functions.explode() or flatMap(). The following example uses these alternatives to count the number of books that contain a given word: case class Book(title: String, words: String) val ds: Dataset[Book] val allWords = ds.select('title, explode(split('words, )).as("word")) val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title")) Using flatMap() this can similarly be exploded as: ds.flatMap(_.words.split( )) Parameters:input - (undocumented)f - (undocumented)evidence$5 - (undocumented) Returns:(undocumented)Since: 2.0.0 explode public <A,B> Dataset<Row> explode(String inputColumn, String outputColumn, scala.Function1<A,scala.collection.TraversableOnce<B>> f, scala.reflect.api.TypeTags.TypeTag<B> evidence$6) Deprecated. use flatMap() or select() with functions.explode() instead. Since 2.0.0. (Scala-specific) Returns a new Dataset where a single column has been expanded to zero or more rows by the provided function. This is similar to a LATERAL VIEW in HiveQL. All columns of the input row are implicitly joined with each value that is output by the function. Given that this is deprecated, as an alternative, you can explode columns either using functions.explode(): ds.select(explode(split('words, )).as("word")) or flatMap(): ds.flatMap(_.words.split( )) Parameters:inputColumn - (undocumented)outputColumn - (undocumented)f - (undocumented)evidence$6 - (undocumented) Returns:(undocumented)Since: 2.0.0 withColumn public Dataset<Row> withColumn(String colName, Column col) Returns a new Dataset by adding a column or replacing the existing column that has the same name. Parameters:colName - (undocumented)col - (undocumented) Returns:(undocumented)Since: 2.0.0 withColumnRenamed public Dataset<Row> withColumnRenamed(String existingName, String newName) Returns a new Dataset with a column renamed. This is a no-op if schema doesn't contain existingName. Parameters:existingName - (undocumented)newName - (undocumented) Returns:(undocumented)Since: 2.0.0 drop public Dataset<Row> drop(String colName) Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain column name. This method can only be used to drop top level columns. the colName string is treated literally without further interpretation. Parameters:colName - (undocumented) Returns:(undocumented)Since: 2.0.0 drop public Dataset<Row> drop(scala.collection.Seq<String> colNames) Returns a new Dataset with columns dropped. This is a no-op if schema doesn't contain column name(s). This method can only be used to drop top level columns. the colName string is treated literally without further interpretation. Parameters:colNames - (undocumented) Returns:(undocumented)Since: 2.0.0 drop public Dataset<Row> drop(Column col) Returns a new Dataset with a column dropped. This version of drop accepts a Column rather than a name. This is a no-op if the Dataset doesn't have a column with an equivalent expression. Parameters:col - (undocumented) Returns:(undocumented)Since: 2.0.0 dropDuplicates public Dataset<T> dropDuplicates() Returns a new Dataset that contains only the unique rows from this Dataset. This is an alias for distinct. Returns:(undocumented)Since: 2.0.0 dropDuplicates public Dataset<T> dropDuplicates(scala.collection.Seq<String> colNames) (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only the subset of columns. Parameters:colNames - (undocumented) Returns:(undocumented)Since: 2.0.0 dropDuplicates public Dataset<T> dropDuplicates(String[] colNames) Returns a new Dataset with duplicate rows removed, considering only the subset of columns. Parameters:colNames - (undocumented) Returns:(undocumented)Since: 2.0.0 dropDuplicates public Dataset<T> dropDuplicates(String col1, scala.collection.Seq<String> cols) Returns a new Dataset with duplicate rows removed, considering only the subset of columns. Parameters:col1 - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 2.0.0 describe public Dataset<Row> describe(scala.collection.Seq<String> cols) Computes statistics for numeric columns, including count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical columns. This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting Dataset. If you want to programmatically compute summary statistics, use the agg function instead. ds.describe("age", "height").show() // output: // summary age height // count 10.0 10.0 // mean 53.3 178.05 // stddev 11.6 15.7 // min 18.0 163.0 // max 92.0 192.0 Parameters:cols - (undocumented) Returns:(undocumented)Since: 1.6.0 head public Object head(int n) Returns the first n rows. Parameters:n - (undocumented) Returns:(undocumented)Since: 1.6.0 head public T head() Returns the first row. Returns:(undocumented)Since: 1.6.0 first public T first() Returns the first row. Alias for head(). Returns:(undocumented)Since: 1.6.0 transform public <U> Dataset<U> transform(scala.Function1<Dataset<T>,Dataset<U>> t) Concise syntax for chaining custom transformations. def featurize(ds: Dataset[T]): Dataset[U] = ... ds .transform(featurize) .transform(...) Parameters:t - (undocumented) Returns:(undocumented)Since: 1.6.0 filter public Dataset<T> filter(scala.Function1<T,Object> func) :: Experimental :: (Scala-specific) Returns a new Dataset that only contains elements where func returns true. Parameters:func - (undocumented) Returns:(undocumented)Since: 1.6.0 filter public Dataset<T> filter(FilterFunction<T> func) :: Experimental :: (Java-specific) Returns a new Dataset that only contains elements where func returns true. Parameters:func - (undocumented) Returns:(undocumented)Since: 1.6.0 map public <U> Dataset<U> map(scala.Function1<T,U> func, Encoder<U> evidence$7) :: Experimental :: (Scala-specific) Returns a new Dataset that contains the result of applying func to each element. Parameters:func - (undocumented)evidence$7 - (undocumented) Returns:(undocumented)Since: 1.6.0 map public <U> Dataset<U> map(MapFunction<T,U> func, Encoder<U> encoder) :: Experimental :: (Java-specific) Returns a new Dataset that contains the result of applying func to each element. Parameters:func - (undocumented)encoder - (undocumented) Returns:(undocumented)Since: 1.6.0 mapPartitions public <U> Dataset<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> func, Encoder<U> evidence$8) :: Experimental :: (Scala-specific) Returns a new Dataset that contains the result of applying func to each partition. Parameters:func - (undocumented)evidence$8 - (undocumented) Returns:(undocumented)Since: 1.6.0 mapPartitions public <U> Dataset<U> mapPartitions(MapPartitionsFunction<T,U> f, Encoder<U> encoder) :: Experimental :: (Java-specific) Returns a new Dataset that contains the result of applying f to each partition. Parameters:f - (undocumented)encoder - (undocumented) Returns:(undocumented)Since: 1.6.0 flatMap public <U> Dataset<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> func, Encoder<U> evidence$9) :: Experimental :: (Scala-specific) Returns a new Dataset by first applying a function to all elements of this Dataset, and then flattening the results. Parameters:func - (undocumented)evidence$9 - (undocumented) Returns:(undocumented)Since: 1.6.0 flatMap public <U> Dataset<U> flatMap(FlatMapFunction<T,U> f, Encoder<U> encoder) :: Experimental :: (Java-specific) Returns a new Dataset by first applying a function to all elements of this Dataset, and then flattening the results. Parameters:f - (undocumented)encoder - (undocumented) Returns:(undocumented)Since: 1.6.0 foreach public void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f) Applies a function f to all rows. Parameters:f - (undocumented)Since: 1.6.0 foreach public void foreach(ForeachFunction<T> func) (Java-specific) Runs func on each element of this Dataset. Parameters:func - (undocumented)Since: 1.6.0 foreachPartition public void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) Applies a function f to each partition of this Dataset. Parameters:f - (undocumented)Since: 1.6.0 foreachPartition public void foreachPartition(ForeachPartitionFunction<T> func) (Java-specific) Runs func on each partition of this Dataset. Parameters:func - (undocumented)Since: 1.6.0 take public Object take(int n) Returns the first n rows in the Dataset. Running take requires moving data into the application's driver process, and doing so with a very large n can crash the driver process with OutOfMemoryError. Parameters:n - (undocumented) Returns:(undocumented)Since: 1.6.0 takeAsList public java.util.List<T> takeAsList(int n) Returns the first n rows in the Dataset as a list. Running take requires moving data into the application's driver process, and doing so with a very large n can crash the driver process with OutOfMemoryError. Parameters:n - (undocumented) Returns:(undocumented)Since: 1.6.0 collect public Object collect() Returns an array that contains all of Rows in this Dataset. Running collect requires moving all the data into the application's driver process, and doing so on a very large dataset can crash the driver process with OutOfMemoryError. For Java API, use collectAsList. Returns:(undocumented)Since: 1.6.0 collectAsList public java.util.List<T> collectAsList() Returns a Java list that contains all of Rows in this Dataset. Running collect requires moving all the data into the application's driver process, and doing so on a very large dataset can crash the driver process with OutOfMemoryError. Returns:(undocumented)Since: 1.6.0 toLocalIterator public java.util.Iterator<T> toLocalIterator() Return an iterator that contains all of Rows in this Dataset. The iterator will consume as much memory as the largest partition in this Dataset. Note: this results in multiple Spark jobs, and if the input Dataset is the result of a wide transformation (e.g. join with different partitioners), to avoid recomputing the input Dataset should be cached first. Returns:(undocumented)Since: 2.0.0 count public long count() Returns the number of rows in the Dataset. Returns:(undocumented)Since: 1.6.0 repartition public Dataset<T> repartition(int numPartitions) Returns a new Dataset that has exactly numPartitions partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented)Since: 1.6.0 repartition public Dataset<T> repartition(int numPartitions, scala.collection.Seq<Column> partitionExprs) Returns a new Dataset partitioned by the given partitioning expressions into numPartitions. The resulting Dataset is hash partitioned. This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL). Parameters:numPartitions - (undocumented)partitionExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 repartition public Dataset<T> repartition(scala.collection.Seq<Column> partitionExprs) Returns a new Dataset partitioned by the given partitioning expressions, using spark.sql.shuffle.partitions as number of partitions. The resulting Dataset is hash partitioned. This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL). Parameters:partitionExprs - (undocumented) Returns:(undocumented)Since: 2.0.0 coalesce public Dataset<T> coalesce(int numPartitions) Returns a new Dataset that has exactly numPartitions partitions. Similar to coalesce defined on an RDD, this operation results in a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented)Since: 1.6.0 distinct public Dataset<T> distinct() Returns a new Dataset that contains only the unique rows from this Dataset. This is an alias for dropDuplicates. Note that, equality checking is performed directly on the encoded representation of the data and thus is not affected by a custom equals function defined on T. Returns:(undocumented)Since: 2.0.0 persist public Dataset<T> persist() Persist this Dataset with the default storage level (MEMORY_AND_DISK). Returns:(undocumented)Since: 1.6.0 cache public Dataset<T> cache() Persist this Dataset with the default storage level (MEMORY_AND_DISK). Returns:(undocumented)Since: 1.6.0 persist public Dataset<T> persist(StorageLevel newLevel) Persist this Dataset with the given storage level. Parameters:newLevel - One of: MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. Returns:(undocumented)Since: 1.6.0 unpersist public Dataset<T> unpersist(boolean blocking) Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. Parameters:blocking - Whether to block until all blocks are deleted. Returns:(undocumented)Since: 1.6.0 unpersist public Dataset<T> unpersist() Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. Returns:(undocumented)Since: 1.6.0 rdd public RDD<T> rdd() Represents the content of the Dataset as an RDD of T. Returns:(undocumented)Since: 1.6.0 toJavaRDD public JavaRDD<T> toJavaRDD() Returns the content of the Dataset as a JavaRDD of Ts. Returns:(undocumented)Since: 1.6.0 javaRDD public JavaRDD<T> javaRDD() Returns the content of the Dataset as a JavaRDD of Ts. Returns:(undocumented)Since: 1.6.0 registerTempTable public void registerTempTable(String tableName) Deprecated. Use createOrReplaceTempView(viewName) instead. Since 2.0.0. Registers this Dataset as a temporary table using the given name. The lifetime of this temporary table is tied to the SparkSession that was used to create this Dataset. Parameters:tableName - (undocumented)Since: 1.6.0 createTempView public void createTempView(String viewName) throws AnalysisException Creates a temporary view using the given name. The lifetime of this temporary view is tied to the SparkSession that was used to create this Dataset. Parameters:viewName - (undocumented) Throws: AnalysisException - if the view name already exists Since: 2.0.0 createOrReplaceTempView public void createOrReplaceTempView(String viewName) Creates a temporary view using the given name. The lifetime of this temporary view is tied to the SparkSession that was used to create this Dataset. Parameters:viewName - (undocumented)Since: 2.0.0 write public DataFrameWriter<T> write() :: Experimental :: Interface for saving the content of the non-streaming Dataset out into external storage. Returns:(undocumented)Since: 1.6.0 writeStream public DataStreamWriter<T> writeStream() :: Experimental :: Interface for saving the content of the streaming Dataset out into external storage. Returns:(undocumented)Since: 2.0.0 toJSON public Dataset<String> toJSON() Returns the content of the Dataset as a Dataset of JSON strings. Returns:(undocumented)Since: 2.0.0 inputFiles public String[] inputFiles() Returns a best-effort snapshot of the files that compose this Dataset. This method simply asks each constituent BaseRelation for its respective files and takes the union of all results. Depending on the source relations, this may not find all input files. Duplicates are removed. Returns:(undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DatasetHolder (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DatasetHolder (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class DatasetHolder<T> Object org.apache.spark.sql.DatasetHolder<T> All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class DatasetHolder<T> extends Object implements scala.Product, scala.Serializable A container for a Dataset, used for implicit conversions in Scala. To use this, import implicit conversions in SQL: import sqlContext.implicits._ Since: 1.6.0 See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Dataset<Row> toDF()  Dataset<Row> toDF(scala.collection.Seq<String> colNames)  Dataset<T> toDS()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() toDS public Dataset<T> toDS() toDF public Dataset<Row> toDF() toDF public Dataset<Row> toDF(scala.collection.Seq<String> colNames) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DateType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DateType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class DateType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.DateType public class DateType extends DataType :: DeveloperApi :: A date type, supporting "0001-01-01" through "9999-12-31". Please use the singleton DataTypes.DateType. Internally, this is represented as the number of days from 1970-01-01. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of the DateType is 4 bytes. static String json()  static String prettyJson()  static String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, simpleString, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() simpleString public static String simpleString() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() The default size of a value of the DateType is 4 bytes. Specified by: defaultSize in class DataType Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Decimal.DecimalAsIfIntegral$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Decimal.DecimalAsIfIntegral$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class Decimal.DecimalAsIfIntegral$ Object org.apache.spark.sql.types.Decimal.DecimalAsIfIntegral$ All Implemented Interfaces: java.io.Serializable, java.util.Comparator<Decimal>, scala.math.Equiv<Decimal>, scala.math.Integral<Decimal>, scala.math.Numeric<Decimal>, scala.math.Ordering<Decimal>, scala.math.PartialOrdering<Decimal> Enclosing class: Decimal public static class Decimal.DecimalAsIfIntegral$ extends Object implements scala.math.Integral<Decimal> A Integral evidence parameter for Decimals. See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface scala.math.Integral scala.math.Integral.ExtraImplicits, scala.math.Integral.Implicits$, scala.math.Integral.IntegralOps Nested classes/interfaces inherited from interface scala.math.Numeric scala.math.Numeric.BigDecimalAsIfIntegral, scala.math.Numeric.BigDecimalAsIfIntegral$, scala.math.Numeric.BigDecimalIsConflicted, scala.math.Numeric.BigDecimalIsFractional, scala.math.Numeric.BigDecimalIsFractional$, scala.math.Numeric.BigIntIsIntegral, scala.math.Numeric.BigIntIsIntegral$, scala.math.Numeric.ByteIsIntegral, scala.math.Numeric.ByteIsIntegral$, scala.math.Numeric.CharIsIntegral, scala.math.Numeric.CharIsIntegral$, scala.math.Numeric.DoubleAsIfIntegral, scala.math.Numeric.DoubleAsIfIntegral$, scala.math.Numeric.DoubleIsConflicted, scala.math.Numeric.DoubleIsFractional, scala.math.Numeric.DoubleIsFractional$, scala.math.Numeric.FloatAsIfIntegral, scala.math.Numeric.FloatAsIfIntegral$, scala.math.Numeric.FloatIsConflicted, scala.math.Numeric.FloatIsFractional, scala.math.Numeric.FloatIsFractional$, scala.math.Numeric.IntIsIntegral, scala.math.Numeric.IntIsIntegral$, scala.math.Numeric.LongIsIntegral, scala.math.Numeric.LongIsIntegral$, scala.math.Numeric.Ops, scala.math.Numeric.ShortIsIntegral, scala.math.Numeric.ShortIsIntegral$ Nested classes/interfaces inherited from interface scala.math.Ordering scala.math.Ordering.BigDecimal$, scala.math.Ordering.BigDecimalOrdering, scala.math.Ordering.BigInt$, scala.math.Ordering.BigIntOrdering, scala.math.Ordering.Boolean$, scala.math.Ordering.BooleanOrdering, scala.math.Ordering.Byte$, scala.math.Ordering.ByteOrdering, scala.math.Ordering.Char$, scala.math.Ordering.CharOrdering, scala.math.Ordering.Double$, scala.math.Ordering.DoubleOrdering, scala.math.Ordering.Float$, scala.math.Ordering.FloatOrdering, scala.math.Ordering.Int$, scala.math.Ordering.IntOrdering, scala.math.Ordering.Long$, scala.math.Ordering.LongOrdering, scala.math.Ordering.OptionOrdering<T>, scala.math.Ordering.Short$, scala.math.Ordering.ShortOrdering, scala.math.Ordering.String$, scala.math.Ordering.StringOrdering, scala.math.Ordering.Unit$, scala.math.Ordering.UnitOrdering Field Summary Fields  Modifier and Type Field and Description static Decimal.DecimalAsIfIntegral$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description Decimal.DecimalAsIfIntegral$()  Method Summary Methods  Modifier and Type Method and Description int compare(Decimal x, Decimal y)  Decimal fromInt(int x)  Decimal minus(Decimal x, Decimal y)  Decimal negate(Decimal x)  Decimal plus(Decimal x, Decimal y)  Decimal quot(Decimal x, Decimal y)  Decimal rem(Decimal x, Decimal y)  Decimal times(Decimal x, Decimal y)  double toDouble(Decimal x)  float toFloat(Decimal x)  int toInt(Decimal x)  long toLong(Decimal x)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.math.Integral mkNumericOps Methods inherited from interface scala.math.Numeric abs, one, signum, zero Methods inherited from interface scala.math.Ordering equiv, gt, gteq, lt, lteq, max, min, mkOrderingOps, on, reverse, tryCompare Methods inherited from interface java.util.Comparator equals Field Detail MODULE$ public static final Decimal.DecimalAsIfIntegral$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail Decimal.DecimalAsIfIntegral$ public Decimal.DecimalAsIfIntegral$() Method Detail quot public Decimal quot(Decimal x, Decimal y) Specified by: quot in interface scala.math.Integral<Decimal> rem public Decimal rem(Decimal x, Decimal y) Specified by: rem in interface scala.math.Integral<Decimal> plus public Decimal plus(Decimal x, Decimal y) Specified by: plus in interface scala.math.Numeric<Decimal> times public Decimal times(Decimal x, Decimal y) Specified by: times in interface scala.math.Numeric<Decimal> minus public Decimal minus(Decimal x, Decimal y) Specified by: minus in interface scala.math.Numeric<Decimal> negate public Decimal negate(Decimal x) Specified by: negate in interface scala.math.Numeric<Decimal> toDouble public double toDouble(Decimal x) Specified by: toDouble in interface scala.math.Numeric<Decimal> toFloat public float toFloat(Decimal x) Specified by: toFloat in interface scala.math.Numeric<Decimal> toInt public int toInt(Decimal x) Specified by: toInt in interface scala.math.Numeric<Decimal> toLong public long toLong(Decimal x) Specified by: toLong in interface scala.math.Numeric<Decimal> fromInt public Decimal fromInt(int x) Specified by: fromInt in interface scala.math.Numeric<Decimal> compare public int compare(Decimal x, Decimal y) Specified by: compare in interface java.util.Comparator<Decimal> Specified by: compare in interface scala.math.Ordering<Decimal> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Decimal.DecimalIsFractional$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Decimal.DecimalIsFractional$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class Decimal.DecimalIsFractional$ Object org.apache.spark.sql.types.Decimal.DecimalIsFractional$ All Implemented Interfaces: java.io.Serializable, java.util.Comparator<Decimal>, scala.math.Equiv<Decimal>, scala.math.Fractional<Decimal>, scala.math.Numeric<Decimal>, scala.math.Ordering<Decimal>, scala.math.PartialOrdering<Decimal> Enclosing class: Decimal public static class Decimal.DecimalIsFractional$ extends Object implements scala.math.Fractional<Decimal> A Fractional evidence parameter for Decimals. See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface scala.math.Fractional scala.math.Fractional.ExtraImplicits, scala.math.Fractional.FractionalOps, scala.math.Fractional.Implicits$ Nested classes/interfaces inherited from interface scala.math.Numeric scala.math.Numeric.BigDecimalAsIfIntegral, scala.math.Numeric.BigDecimalAsIfIntegral$, scala.math.Numeric.BigDecimalIsConflicted, scala.math.Numeric.BigDecimalIsFractional, scala.math.Numeric.BigDecimalIsFractional$, scala.math.Numeric.BigIntIsIntegral, scala.math.Numeric.BigIntIsIntegral$, scala.math.Numeric.ByteIsIntegral, scala.math.Numeric.ByteIsIntegral$, scala.math.Numeric.CharIsIntegral, scala.math.Numeric.CharIsIntegral$, scala.math.Numeric.DoubleAsIfIntegral, scala.math.Numeric.DoubleAsIfIntegral$, scala.math.Numeric.DoubleIsConflicted, scala.math.Numeric.DoubleIsFractional, scala.math.Numeric.DoubleIsFractional$, scala.math.Numeric.FloatAsIfIntegral, scala.math.Numeric.FloatAsIfIntegral$, scala.math.Numeric.FloatIsConflicted, scala.math.Numeric.FloatIsFractional, scala.math.Numeric.FloatIsFractional$, scala.math.Numeric.IntIsIntegral, scala.math.Numeric.IntIsIntegral$, scala.math.Numeric.LongIsIntegral, scala.math.Numeric.LongIsIntegral$, scala.math.Numeric.Ops, scala.math.Numeric.ShortIsIntegral, scala.math.Numeric.ShortIsIntegral$ Nested classes/interfaces inherited from interface scala.math.Ordering scala.math.Ordering.BigDecimal$, scala.math.Ordering.BigDecimalOrdering, scala.math.Ordering.BigInt$, scala.math.Ordering.BigIntOrdering, scala.math.Ordering.Boolean$, scala.math.Ordering.BooleanOrdering, scala.math.Ordering.Byte$, scala.math.Ordering.ByteOrdering, scala.math.Ordering.Char$, scala.math.Ordering.CharOrdering, scala.math.Ordering.Double$, scala.math.Ordering.DoubleOrdering, scala.math.Ordering.Float$, scala.math.Ordering.FloatOrdering, scala.math.Ordering.Int$, scala.math.Ordering.IntOrdering, scala.math.Ordering.Long$, scala.math.Ordering.LongOrdering, scala.math.Ordering.OptionOrdering<T>, scala.math.Ordering.Short$, scala.math.Ordering.ShortOrdering, scala.math.Ordering.String$, scala.math.Ordering.StringOrdering, scala.math.Ordering.Unit$, scala.math.Ordering.UnitOrdering Field Summary Fields  Modifier and Type Field and Description static Decimal.DecimalIsFractional$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description Decimal.DecimalIsFractional$()  Method Summary Methods  Modifier and Type Method and Description int compare(Decimal x, Decimal y)  Decimal div(Decimal x, Decimal y)  Decimal fromInt(int x)  Decimal minus(Decimal x, Decimal y)  Decimal negate(Decimal x)  Decimal plus(Decimal x, Decimal y)  Decimal times(Decimal x, Decimal y)  double toDouble(Decimal x)  float toFloat(Decimal x)  int toInt(Decimal x)  long toLong(Decimal x)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.math.Fractional mkNumericOps Methods inherited from interface scala.math.Numeric abs, one, signum, zero Methods inherited from interface scala.math.Ordering equiv, gt, gteq, lt, lteq, max, min, mkOrderingOps, on, reverse, tryCompare Methods inherited from interface java.util.Comparator equals Field Detail MODULE$ public static final Decimal.DecimalIsFractional$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail Decimal.DecimalIsFractional$ public Decimal.DecimalIsFractional$() Method Detail div public Decimal div(Decimal x, Decimal y) Specified by: div in interface scala.math.Fractional<Decimal> plus public Decimal plus(Decimal x, Decimal y) Specified by: plus in interface scala.math.Numeric<Decimal> times public Decimal times(Decimal x, Decimal y) Specified by: times in interface scala.math.Numeric<Decimal> minus public Decimal minus(Decimal x, Decimal y) Specified by: minus in interface scala.math.Numeric<Decimal> negate public Decimal negate(Decimal x) Specified by: negate in interface scala.math.Numeric<Decimal> toDouble public double toDouble(Decimal x) Specified by: toDouble in interface scala.math.Numeric<Decimal> toFloat public float toFloat(Decimal x) Specified by: toFloat in interface scala.math.Numeric<Decimal> toInt public int toInt(Decimal x) Specified by: toInt in interface scala.math.Numeric<Decimal> toLong public long toLong(Decimal x) Specified by: toLong in interface scala.math.Numeric<Decimal> fromInt public Decimal fromInt(int x) Specified by: fromInt in interface scala.math.Numeric<Decimal> compare public int compare(Decimal x, Decimal y) Specified by: compare in interface java.util.Comparator<Decimal> Specified by: compare in interface scala.math.Ordering<Decimal> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Decimal (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Decimal (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class Decimal Object org.apache.spark.sql.types.Decimal All Implemented Interfaces: java.io.Serializable, Comparable<Decimal>, scala.math.Ordered<Decimal> public final class Decimal extends Object implements scala.math.Ordered<Decimal>, scala.Serializable A mutable implementation of BigDecimal that can hold a Long if values are small enough. The semantics of the fields are as follows: - _precision and _scale represent the SQL precision and scale we are looking for - If decimalVal is set, it represents the whole decimal value - Otherwise, the decimal value is longVal / (10 ** _scale) See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  Decimal.DecimalAsIfIntegral$ A Integral evidence parameter for Decimals. static class  Decimal.DecimalIsFractional$ A Fractional evidence parameter for Decimals. Constructor Summary Constructors  Constructor and Description Decimal()  Method Summary Methods  Modifier and Type Method and Description static boolean $greater(A that)  static boolean $greater$eq(A that)  static boolean $less(A that)  static boolean $less$eq(A that)  Decimal abs()  static Decimal apply(scala.math.BigDecimal value)  static Decimal apply(java.math.BigDecimal value)  static Decimal apply(scala.math.BigDecimal value, int precision, int scale)  static Decimal apply(java.math.BigDecimal value, int precision, int scale)  static Decimal apply(scala.math.BigInt value)  static Decimal apply(java.math.BigInteger value)  static Decimal apply(double value)  static Decimal apply(int value)  static Decimal apply(long value)  static Decimal apply(long unscaled, int precision, int scale)  static Decimal apply(String value)  Decimal ceil()  boolean changePrecision(int precision, int scale) Update precision and scale while keeping our value the same, and return true if successful. boolean changePrecision(int precision, int scale, int mode)  Decimal clone()  int compare(Decimal other)  static int compareTo(A that)  static Decimal createUnsafe(long unscaled, int precision, int scale) Creates a decimal from unscaled, precision and scale without checking the bounds. boolean equals(Object other)  Decimal floor()  static Decimal fromDecimal(Object value)  int hashCode()  boolean isZero()  static int MAX_INT_DIGITS() Maximum number of decimal digits an Int can represent static int MAX_LONG_DIGITS() Maximum number of decimal digits a Long can represent int precision()  Decimal remainder(Decimal that)  static scala.Enumeration.Value ROUND_CEILING()  static scala.Enumeration.Value ROUND_FLOOR()  static scala.Enumeration.Value ROUND_HALF_EVEN()  static scala.Enumeration.Value ROUND_HALF_UP()  int scale()  Decimal set(scala.math.BigDecimal decimal) Set this Decimal to the given BigDecimal value, inheriting its precision and scale. Decimal set(scala.math.BigDecimal decimal, int precision, int scale) Set this Decimal to the given BigDecimal value, with a given precision and scale. Decimal set(java.math.BigInteger bigintval) Set this Decimal to the given BigInteger value. Decimal set(Decimal decimal) Set this Decimal to the given Decimal value. Decimal set(int intVal) Set this Decimal to the given Int. Decimal set(long longVal) Set this Decimal to the given Long. Decimal set(long unscaled, int precision, int scale) Set this Decimal to the given unscaled Long, with a given precision and scale. Decimal setOrNull(long unscaled, int precision, int scale) Set this Decimal to the given unscaled Long, with a given precision and scale, and return it, or return null if it cannot be set due to overflow. scala.math.BigDecimal toBigDecimal()  byte toByte()  String toDebugString()  double toDouble()  float toFloat()  int toInt()  java.math.BigDecimal toJavaBigDecimal()  java.math.BigInteger toJavaBigInteger()  long toLong()  scala.math.BigInt toScalaBigInt()  short toShort()  String toString()  long toUnscaledLong()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.math.Ordered $greater, $greater$eq, $less, $less$eq, compareTo Constructor Detail Decimal public Decimal() Method Detail ROUND_HALF_UP public static scala.Enumeration.Value ROUND_HALF_UP() ROUND_HALF_EVEN public static scala.Enumeration.Value ROUND_HALF_EVEN() ROUND_CEILING public static scala.Enumeration.Value ROUND_CEILING() ROUND_FLOOR public static scala.Enumeration.Value ROUND_FLOOR() MAX_INT_DIGITS public static int MAX_INT_DIGITS() Maximum number of decimal digits an Int can represent MAX_LONG_DIGITS public static int MAX_LONG_DIGITS() Maximum number of decimal digits a Long can represent apply public static Decimal apply(double value) apply public static Decimal apply(long value) apply public static Decimal apply(int value) apply public static Decimal apply(scala.math.BigDecimal value) apply public static Decimal apply(java.math.BigDecimal value) apply public static Decimal apply(java.math.BigInteger value) apply public static Decimal apply(scala.math.BigInt value) apply public static Decimal apply(scala.math.BigDecimal value, int precision, int scale) apply public static Decimal apply(java.math.BigDecimal value, int precision, int scale) apply public static Decimal apply(long unscaled, int precision, int scale) apply public static Decimal apply(String value) fromDecimal public static Decimal fromDecimal(Object value) createUnsafe public static Decimal createUnsafe(long unscaled, int precision, int scale) Creates a decimal from unscaled, precision and scale without checking the bounds. Parameters:unscaled - (undocumented)precision - (undocumented)scale - (undocumented) Returns:(undocumented) $less public static boolean $less(A that) $greater public static boolean $greater(A that) $less$eq public static boolean $less$eq(A that) $greater$eq public static boolean $greater$eq(A that) compareTo public static int compareTo(A that) precision public int precision() scale public int scale() set public Decimal set(long longVal) Set this Decimal to the given Long. Will have precision 20 and scale 0. Parameters:longVal - (undocumented) Returns:(undocumented) set public Decimal set(int intVal) Set this Decimal to the given Int. Will have precision 10 and scale 0. Parameters:intVal - (undocumented) Returns:(undocumented) set public Decimal set(long unscaled, int precision, int scale) Set this Decimal to the given unscaled Long, with a given precision and scale. Parameters:unscaled - (undocumented)precision - (undocumented)scale - (undocumented) Returns:(undocumented) setOrNull public Decimal setOrNull(long unscaled, int precision, int scale) Set this Decimal to the given unscaled Long, with a given precision and scale, and return it, or return null if it cannot be set due to overflow. Parameters:unscaled - (undocumented)precision - (undocumented)scale - (undocumented) Returns:(undocumented) set public Decimal set(scala.math.BigDecimal decimal, int precision, int scale) Set this Decimal to the given BigDecimal value, with a given precision and scale. Parameters:decimal - (undocumented)precision - (undocumented)scale - (undocumented) Returns:(undocumented) set public Decimal set(scala.math.BigDecimal decimal) Set this Decimal to the given BigDecimal value, inheriting its precision and scale. Parameters:decimal - (undocumented) Returns:(undocumented) set public Decimal set(java.math.BigInteger bigintval) Set this Decimal to the given BigInteger value. Will have precision 38 and scale 0. Parameters:bigintval - (undocumented) Returns:(undocumented) set public Decimal set(Decimal decimal) Set this Decimal to the given Decimal value. Parameters:decimal - (undocumented) Returns:(undocumented) toBigDecimal public scala.math.BigDecimal toBigDecimal() toJavaBigDecimal public java.math.BigDecimal toJavaBigDecimal() toScalaBigInt public scala.math.BigInt toScalaBigInt() toJavaBigInteger public java.math.BigInteger toJavaBigInteger() toUnscaledLong public long toUnscaledLong() toString public String toString() Overrides: toString in class Object toDebugString public String toDebugString() toDouble public double toDouble() toFloat public float toFloat() toLong public long toLong() toInt public int toInt() toShort public short toShort() toByte public byte toByte() changePrecision public boolean changePrecision(int precision, int scale) Update precision and scale while keeping our value the same, and return true if successful. Parameters:precision - (undocumented)scale - (undocumented) Returns:true if successful, false if overflow would occur changePrecision public boolean changePrecision(int precision, int scale, int mode) clone public Decimal clone() Overrides: clone in class Object compare public int compare(Decimal other) Specified by: compare in interface scala.math.Ordered<Decimal> equals public boolean equals(Object other) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object isZero public boolean isZero() remainder public Decimal remainder(Decimal that) abs public Decimal abs() floor public Decimal floor() ceil public Decimal ceil() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecimalType.Expression$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecimalType.Expression$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class DecimalType.Expression$ Object org.apache.spark.sql.types.DecimalType.Expression$ Enclosing class: DecimalType public static class DecimalType.Expression$ extends Object Field Summary Fields  Modifier and Type Field and Description static DecimalType.Expression$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description DecimalType.Expression$()  Method Summary Methods  Modifier and Type Method and Description scala.Option<scala.Tuple2<Object,Object>> unapply(org.apache.spark.sql.catalyst.expressions.Expression e)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final DecimalType.Expression$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail DecimalType.Expression$ public DecimalType.Expression$() Method Detail unapply public scala.Option<scala.Tuple2<Object,Object>> unapply(org.apache.spark.sql.catalyst.expressions.Expression e) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecimalType.Fixed$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecimalType.Fixed$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class DecimalType.Fixed$ Object org.apache.spark.sql.types.DecimalType.Fixed$ Enclosing class: DecimalType public static class DecimalType.Fixed$ extends Object Field Summary Fields  Modifier and Type Field and Description static DecimalType.Fixed$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description DecimalType.Fixed$()  Method Summary Methods  Modifier and Type Method and Description scala.Option<scala.Tuple2<Object,Object>> unapply(DecimalType t)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final DecimalType.Fixed$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail DecimalType.Fixed$ public DecimalType.Fixed$() Method Detail unapply public scala.Option<scala.Tuple2<Object,Object>> unapply(DecimalType t) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecimalType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecimalType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class DecimalType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.NumericType org.apache.spark.sql.types.DecimalType All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class DecimalType extends NumericType implements scala.Product, scala.Serializable :: DeveloperApi :: The data type representing java.math.BigDecimal values. A Decimal that must have fixed precision (the maximum number of digits) and scale (the number of digits on right side of dot). The precision can be up to 38, scale can also be up to 38 (less or equal to precision). The default precision and scale is (10, 0). Please use DataTypes.createDecimalType() to create a specific instance. See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  DecimalType.Expression$  static class  DecimalType.Fixed$  Constructor Summary Constructors  Constructor and Description DecimalType()  DecimalType(int precision)  DecimalType(int precision, int scale)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  static String catalogString()  int defaultSize() The default size of a value of the DecimalType is 8 bytes (precision <= 18) or 16 bytes. abstract static boolean equals(Object that)  static boolean is32BitDecimalType(DataType dt) Returns if dt is a DecimalType that fits inside an int static boolean is64BitDecimalType(DataType dt) Returns if dt is a DecimalType that fits inside a long static boolean isByteArrayDecimalType(DataType dt) Returns if dt is a DecimalType that doesn't fit inside a long static String json()  static int MAX_PRECISION()  static int MAX_SCALE()  int precision()  static String prettyJson()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  int scale()  String simpleString()  String sql()  static DecimalType SYSTEM_DEFAULT()  String toString()  String typeName()  static boolean unapply(DataType t)  static boolean unapply(org.apache.spark.sql.catalyst.expressions.Expression e)  static DecimalType USER_DEFAULT()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail DecimalType public DecimalType(int precision, int scale) DecimalType public DecimalType(int precision) DecimalType public DecimalType() Method Detail MAX_PRECISION public static int MAX_PRECISION() MAX_SCALE public static int MAX_SCALE() SYSTEM_DEFAULT public static DecimalType SYSTEM_DEFAULT() USER_DEFAULT public static DecimalType USER_DEFAULT() is32BitDecimalType public static boolean is32BitDecimalType(DataType dt) Returns if dt is a DecimalType that fits inside an int Parameters:dt - (undocumented) Returns:(undocumented) is64BitDecimalType public static boolean is64BitDecimalType(DataType dt) Returns if dt is a DecimalType that fits inside a long Parameters:dt - (undocumented) Returns:(undocumented) isByteArrayDecimalType public static boolean isByteArrayDecimalType(DataType dt) Returns if dt is a DecimalType that doesn't fit inside a long Parameters:dt - (undocumented) Returns:(undocumented) unapply public static boolean unapply(DataType t) unapply public static boolean unapply(org.apache.spark.sql.catalyst.expressions.Expression e) json public static String json() prettyJson public static String prettyJson() catalogString public static String catalogString() canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() precision public int precision() scale public int scale() typeName public String typeName() toString public String toString() Overrides: toString in class Object sql public String sql() defaultSize public int defaultSize() The default size of a value of the DecimalType is 8 bytes (precision <= 18) or 16 bytes. Returns:(undocumented) simpleString public String simpleString() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTree (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTree (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree Class DecisionTree Object org.apache.spark.mllib.tree.DecisionTree All Implemented Interfaces: java.io.Serializable public class DecisionTree extends Object implements scala.Serializable A class which implements a decision tree learning algorithm for classification and regression. It supports both continuous and categorical features. param: strategy The configuration parameters for the tree algorithm which specify the type of decision tree (classification or regression), feature type (continuous, categorical), depth of the tree, quantile calculation strategy, etc. param: seed Random seed. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DecisionTree(Strategy strategy)  Method Summary Methods  Modifier and Type Method and Description DecisionTreeModel run(RDD<LabeledPoint> input) Method to train a decision tree model over an RDD static DecisionTreeModel train(RDD<LabeledPoint> input, scala.Enumeration.Value algo, Impurity impurity, int maxDepth) Method to train a decision tree model. static DecisionTreeModel train(RDD<LabeledPoint> input, scala.Enumeration.Value algo, Impurity impurity, int maxDepth, int numClasses) Method to train a decision tree model. static DecisionTreeModel train(RDD<LabeledPoint> input, scala.Enumeration.Value algo, Impurity impurity, int maxDepth, int numClasses, int maxBins, scala.Enumeration.Value quantileCalculationStrategy, scala.collection.immutable.Map<Object,Object> categoricalFeaturesInfo) Method to train a decision tree model. static DecisionTreeModel train(RDD<LabeledPoint> input, Strategy strategy) Method to train a decision tree model. static DecisionTreeModel trainClassifier(JavaRDD<LabeledPoint> input, int numClasses, java.util.Map<Integer,Integer> categoricalFeaturesInfo, String impurity, int maxDepth, int maxBins) Java-friendly API for DecisionTree$.trainClassifier(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, int, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) static DecisionTreeModel trainClassifier(RDD<LabeledPoint> input, int numClasses, scala.collection.immutable.Map<Object,Object> categoricalFeaturesInfo, String impurity, int maxDepth, int maxBins) Method to train a decision tree model for binary or multiclass classification. static DecisionTreeModel trainRegressor(JavaRDD<LabeledPoint> input, java.util.Map<Integer,Integer> categoricalFeaturesInfo, String impurity, int maxDepth, int maxBins) Java-friendly API for DecisionTree$.trainRegressor(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) static DecisionTreeModel trainRegressor(RDD<LabeledPoint> input, scala.collection.immutable.Map<Object,Object> categoricalFeaturesInfo, String impurity, int maxDepth, int maxBins) Method to train a decision tree model for regression. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DecisionTree public DecisionTree(Strategy strategy) Parameters:strategy - The configuration parameters for the tree algorithm which specify the type of decision tree (classification or regression), feature type (continuous, categorical), depth of the tree, quantile calculation strategy, etc. Method Detail train public static DecisionTreeModel train(RDD<LabeledPoint> input, Strategy strategy) Method to train a decision tree model. The method supports binary and multiclass classification and regression. Note: Using DecisionTree$.trainClassifier(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, int, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) and DecisionTree$.trainRegressor(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) is recommended to clearly separate classification and regression. Parameters:input - Training dataset: RDD of LabeledPoint. For classification, labels should take values {0, 1, ..., numClasses-1}. For regression, labels are real numbers.strategy - The configuration parameters for the tree algorithm which specify the type of decision tree (classification or regression), feature type (continuous, categorical), depth of the tree, quantile calculation strategy, etc. Returns:DecisionTreeModel that can be used for prediction. train public static DecisionTreeModel train(RDD<LabeledPoint> input, scala.Enumeration.Value algo, Impurity impurity, int maxDepth) Method to train a decision tree model. The method supports binary and multiclass classification and regression. Note: Using DecisionTree$.trainClassifier(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, int, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) and DecisionTree$.trainRegressor(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) is recommended to clearly separate classification and regression. Parameters:input - Training dataset: RDD of LabeledPoint. For classification, labels should take values {0, 1, ..., numClasses-1}. For regression, labels are real numbers.algo - Type of decision tree, either classification or regression.impurity - Criterion used for information gain calculation.maxDepth - Maximum depth of the tree (e.g. depth 0 means 1 leaf node, depth 1 means 1 internal node + 2 leaf nodes). Returns:DecisionTreeModel that can be used for prediction. train public static DecisionTreeModel train(RDD<LabeledPoint> input, scala.Enumeration.Value algo, Impurity impurity, int maxDepth, int numClasses) Method to train a decision tree model. The method supports binary and multiclass classification and regression. Note: Using DecisionTree$.trainClassifier(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, int, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) and DecisionTree$.trainRegressor(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) is recommended to clearly separate classification and regression. Parameters:input - Training dataset: RDD of LabeledPoint. For classification, labels should take values {0, 1, ..., numClasses-1}. For regression, labels are real numbers.algo - Type of decision tree, either classification or regression.impurity - Criterion used for information gain calculation.maxDepth - Maximum depth of the tree (e.g. depth 0 means 1 leaf node, depth 1 means 1 internal node + 2 leaf nodes).numClasses - Number of classes for classification. Default value of 2. Returns:DecisionTreeModel that can be used for prediction. train public static DecisionTreeModel train(RDD<LabeledPoint> input, scala.Enumeration.Value algo, Impurity impurity, int maxDepth, int numClasses, int maxBins, scala.Enumeration.Value quantileCalculationStrategy, scala.collection.immutable.Map<Object,Object> categoricalFeaturesInfo) Method to train a decision tree model. The method supports binary and multiclass classification and regression. Note: Using DecisionTree$.trainClassifier(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, int, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) and DecisionTree$.trainRegressor(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) is recommended to clearly separate classification and regression. Parameters:input - Training dataset: RDD of LabeledPoint. For classification, labels should take values {0, 1, ..., numClasses-1}. For regression, labels are real numbers.algo - Type of decision tree, either classification or regression.impurity - Criterion used for information gain calculation.maxDepth - Maximum depth of the tree (e.g. depth 0 means 1 leaf node, depth 1 means 1 internal node + 2 leaf nodes).numClasses - Number of classes for classification. Default value of 2.maxBins - Maximum number of bins used for splitting features.quantileCalculationStrategy - Algorithm for calculating quantiles.categoricalFeaturesInfo - Map storing arity of categorical features. An entry (n -> k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, ..., k-1}. Returns:DecisionTreeModel that can be used for prediction. trainClassifier public static DecisionTreeModel trainClassifier(RDD<LabeledPoint> input, int numClasses, scala.collection.immutable.Map<Object,Object> categoricalFeaturesInfo, String impurity, int maxDepth, int maxBins) Method to train a decision tree model for binary or multiclass classification. Parameters:input - Training dataset: RDD of LabeledPoint. Labels should take values {0, 1, ..., numClasses-1}.numClasses - Number of classes for classification.categoricalFeaturesInfo - Map storing arity of categorical features. An entry (n -> k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.impurity - Criterion used for information gain calculation. Supported values: "gini" (recommended) or "entropy".maxDepth - Maximum depth of the tree (e.g. depth 0 means 1 leaf node, depth 1 means 1 internal node + 2 leaf nodes). (suggested value: 5)maxBins - Maximum number of bins used for splitting features. (suggested value: 32) Returns:DecisionTreeModel that can be used for prediction. trainClassifier public static DecisionTreeModel trainClassifier(JavaRDD<LabeledPoint> input, int numClasses, java.util.Map<Integer,Integer> categoricalFeaturesInfo, String impurity, int maxDepth, int maxBins) Java-friendly API for DecisionTree$.trainClassifier(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, int, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) Parameters:input - (undocumented)numClasses - (undocumented)categoricalFeaturesInfo - (undocumented)impurity - (undocumented)maxDepth - (undocumented)maxBins - (undocumented) Returns:(undocumented) trainRegressor public static DecisionTreeModel trainRegressor(RDD<LabeledPoint> input, scala.collection.immutable.Map<Object,Object> categoricalFeaturesInfo, String impurity, int maxDepth, int maxBins) Method to train a decision tree model for regression. Parameters:input - Training dataset: RDD of LabeledPoint. Labels are real numbers.categoricalFeaturesInfo - Map storing arity of categorical features. An entry (n -> k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.impurity - Criterion used for information gain calculation. The only supported value for regression is "variance".maxDepth - Maximum depth of the tree (e.g. depth 0 means 1 leaf node, depth 1 means 1 internal node + 2 leaf nodes). (suggested value: 5)maxBins - Maximum number of bins used for splitting features. (suggested value: 32) Returns:DecisionTreeModel that can be used for prediction. trainRegressor public static DecisionTreeModel trainRegressor(JavaRDD<LabeledPoint> input, java.util.Map<Integer,Integer> categoricalFeaturesInfo, String impurity, int maxDepth, int maxBins) Java-friendly API for DecisionTree$.trainRegressor(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, scala.collection.immutable.Map<java.lang.Object, java.lang.Object>, java.lang.String, int, int) Parameters:input - (undocumented)categoricalFeaturesInfo - (undocumented)impurity - (undocumented)maxDepth - (undocumented)maxBins - (undocumented) Returns:(undocumented) run public DecisionTreeModel run(RDD<LabeledPoint> input) Method to train a decision tree model over an RDD Parameters:input - Training data: RDD of LabeledPoint. Returns:DecisionTreeModel that can be used for prediction. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeClassificationModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeClassificationModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class DecisionTreeClassificationModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<M> org.apache.spark.ml.PredictionModel<FeaturesType,M> org.apache.spark.ml.classification.ClassificationModel<FeaturesType,M> org.apache.spark.ml.classification.ProbabilisticClassificationModel<Vector,DecisionTreeClassificationModel> org.apache.spark.ml.classification.DecisionTreeClassificationModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class DecisionTreeClassificationModel extends ProbabilisticClassificationModel<Vector,DecisionTreeClassificationModel> implements MLWritable, scala.Serializable Decision tree model (http://en.wikipedia.org/wiki/Decision_tree_learning) for classification. It supports both binary and multiclass labels, as well as both continuous and categorical features. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static BooleanParam cacheNodeIds()  static IntParam checkpointInterval()  static Params clear(Param<?> param)  DecisionTreeClassificationModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static int depth()  static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  Vector featureImportances() Estimate of the importance of each feature. static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static <T> scala.Option<T> get(Param<T> param)  static boolean getCacheNodeIds()  static int getCheckpointInterval()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getImpurity()  static String getLabelCol()  String getLabelCol()  static int getMaxBins()  static int getMaxDepth()  static int getMaxMemoryInMB()  static double getMinInfoGain()  static int getMinInstancesPerNode()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static String getProbabilityCol()  static String getRawPredictionCol()  String getRawPredictionCol()  static long getSeed()  static double[] getThresholds()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static Param<String> impurity()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static DecisionTreeClassificationModel load(String path)  static IntParam maxBins()  static IntParam maxDepth()  static IntParam maxMemoryInMB()  static DoubleParam minInfoGain()  static IntParam minInstancesPerNode()  int numClasses()  int numFeatures()  static int numNodes()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static Param<String> probabilityCol()  static Param<String> rawPredictionCol()  Param<String> rawPredictionCol() Param for raw prediction (a.k.a. static MLReader<DecisionTreeClassificationModel> read()  Node rootNode()  static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  static org.apache.spark.ml.tree.DecisionTreeParams setCacheNodeIds(boolean value)  static org.apache.spark.ml.tree.DecisionTreeParams setCheckpointInterval(int value)  static M setFeaturesCol(String value)  static org.apache.spark.ml.tree.TreeClassifierParams setImpurity(String value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxBins(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxDepth(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxMemoryInMB(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMinInfoGain(double value)  static org.apache.spark.ml.tree.DecisionTreeParams setMinInstancesPerNode(int value)  static M setParent(Estimator<M> parent)  static M setPredictionCol(String value)  static M setProbabilityCol(String value)  static M setRawPredictionCol(String value)  static org.apache.spark.ml.tree.DecisionTreeParams setSeed(long value)  static M setThresholds(double[] value)  static DoubleArrayParam thresholds()  static String toDebugString()  String toString()  static Dataset<Row> transform(Dataset<?> dataset)  static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.classification.ProbabilisticClassificationModel normalizeToProbabilitiesInPlace, setProbabilityCol, setThresholds, transform Methods inherited from class org.apache.spark.ml.classification.ClassificationModel setRawPredictionCol Methods inherited from class org.apache.spark.ml.PredictionModel setFeaturesCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Method Detail read public static MLReader<DecisionTreeClassificationModel> read() load public static DecisionTreeClassificationModel load(String path) params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setFeaturesCol public static M setFeaturesCol(String value) setPredictionCol public static M setPredictionCol(String value) transformSchema public static StructType transformSchema(StructType schema) rawPredictionCol public static final Param<String> rawPredictionCol() getRawPredictionCol public static final String getRawPredictionCol() setRawPredictionCol public static M setRawPredictionCol(String value) probabilityCol public static final Param<String> probabilityCol() getProbabilityCol public static final String getProbabilityCol() thresholds public static final DoubleArrayParam thresholds() getThresholds public static double[] getThresholds() setProbabilityCol public static M setProbabilityCol(String value) setThresholds public static M setThresholds(double[] value) transform public static Dataset<Row> transform(Dataset<?> dataset) numNodes public static int numNodes() depth public static int depth() toDebugString public static String toDebugString() checkpointInterval public static final IntParam checkpointInterval() getCheckpointInterval public static final int getCheckpointInterval() seed public static final LongParam seed() getSeed public static final long getSeed() maxDepth public static final IntParam maxDepth() maxBins public static final IntParam maxBins() minInstancesPerNode public static final IntParam minInstancesPerNode() minInfoGain public static final DoubleParam minInfoGain() maxMemoryInMB public static final IntParam maxMemoryInMB() cacheNodeIds public static final BooleanParam cacheNodeIds() setMaxDepth public static org.apache.spark.ml.tree.DecisionTreeParams setMaxDepth(int value) getMaxDepth public static final int getMaxDepth() setMaxBins public static org.apache.spark.ml.tree.DecisionTreeParams setMaxBins(int value) getMaxBins public static final int getMaxBins() setMinInstancesPerNode public static org.apache.spark.ml.tree.DecisionTreeParams setMinInstancesPerNode(int value) getMinInstancesPerNode public static final int getMinInstancesPerNode() setMinInfoGain public static org.apache.spark.ml.tree.DecisionTreeParams setMinInfoGain(double value) getMinInfoGain public static final double getMinInfoGain() setSeed public static org.apache.spark.ml.tree.DecisionTreeParams setSeed(long value) setMaxMemoryInMB public static org.apache.spark.ml.tree.DecisionTreeParams setMaxMemoryInMB(int value) getMaxMemoryInMB public static final int getMaxMemoryInMB() setCacheNodeIds public static org.apache.spark.ml.tree.DecisionTreeParams setCacheNodeIds(boolean value) getCacheNodeIds public static final boolean getCacheNodeIds() setCheckpointInterval public static org.apache.spark.ml.tree.DecisionTreeParams setCheckpointInterval(int value) impurity public static final Param<String> impurity() setImpurity public static org.apache.spark.ml.tree.TreeClassifierParams setImpurity(String value) getImpurity public static final String getImpurity() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Specified by: uid in class ProbabilisticClassificationModel<Vector,DecisionTreeClassificationModel> Returns:(undocumented) rootNode public Node rootNode() numFeatures public int numFeatures() Overrides: numFeatures in class ProbabilisticClassificationModel<Vector,DecisionTreeClassificationModel> numClasses public int numClasses() Specified by: numClasses in class ProbabilisticClassificationModel<Vector,DecisionTreeClassificationModel> copy public DecisionTreeClassificationModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class ProbabilisticClassificationModel<Vector,DecisionTreeClassificationModel> Parameters:extra - (undocumented) Returns:(undocumented) toString public String toString() Specified by: toString in interface Identifiable Overrides: toString in class ProbabilisticClassificationModel<Vector,DecisionTreeClassificationModel> featureImportances public Vector featureImportances() Estimate of the importance of each feature. This generalizes the idea of "Gini" importance to other losses, following the explanation of Gini importance from "Random Forests" documentation by Leo Breiman and Adele Cutler, and following the implementation from scikit-learn. This feature importance is calculated as follows: - importance(feature j) = sum (over nodes which split on feature j) of the gain, where gain is scaled by the number of instances passing through node - Normalize importances for tree to sum to 1. Note: Feature importance for single decision trees can have high variance due to correlated predictor variables. Consider using a RandomForestClassifier to determine feature importance instead. Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) rawPredictionCol public Param<String> rawPredictionCol() Param for raw prediction (a.k.a. confidence) column name. Returns:(undocumented) getRawPredictionCol public String getRawPredictionCol() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeClassifier (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeClassifier (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class DecisionTreeClassifier Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<FeaturesType,E,M> org.apache.spark.ml.classification.Classifier<FeaturesType,E,M> org.apache.spark.ml.classification.ProbabilisticClassifier<Vector,DecisionTreeClassifier,DecisionTreeClassificationModel> org.apache.spark.ml.classification.DecisionTreeClassifier All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class DecisionTreeClassifier extends ProbabilisticClassifier<Vector,DecisionTreeClassifier,DecisionTreeClassificationModel> implements DefaultParamsWritable Decision tree learning algorithm (http://en.wikipedia.org/wiki/Decision_tree_learning) for classification. It supports both binary and multiclass labels, as well as both continuous and categorical features. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DecisionTreeClassifier()  DecisionTreeClassifier(String uid)  Method Summary Methods  Modifier and Type Method and Description static BooleanParam cacheNodeIds()  static IntParam checkpointInterval()  static Params clear(Param<?> param)  DecisionTreeClassifier copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static M fit(Dataset<?> dataset)  static M fit(Dataset<?> dataset, ParamMap paramMap)  static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static <T> scala.Option<T> get(Param<T> param)  static boolean getCacheNodeIds()  static int getCheckpointInterval()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getImpurity()  static String getLabelCol()  String getLabelCol()  static int getMaxBins()  static int getMaxDepth()  static int getMaxMemoryInMB()  static double getMinInfoGain()  static int getMinInstancesPerNode()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static String getProbabilityCol()  static String getRawPredictionCol()  String getRawPredictionCol()  static long getSeed()  static double[] getThresholds()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> impurity()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static DecisionTreeClassifier load(String path)  static IntParam maxBins()  static IntParam maxDepth()  static IntParam maxMemoryInMB()  static DoubleParam minInfoGain()  static IntParam minInstancesPerNode()  static Param<?>[] params()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static Param<String> probabilityCol()  static Param<String> rawPredictionCol()  Param<String> rawPredictionCol() Param for raw prediction (a.k.a. static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  DecisionTreeClassifier setCacheNodeIds(boolean value)  DecisionTreeClassifier setCheckpointInterval(int value)  static Learner setFeaturesCol(String value)  DecisionTreeClassifier setImpurity(String value)  static Learner setLabelCol(String value)  DecisionTreeClassifier setMaxBins(int value)  DecisionTreeClassifier setMaxDepth(int value)  DecisionTreeClassifier setMaxMemoryInMB(int value)  DecisionTreeClassifier setMinInfoGain(double value)  DecisionTreeClassifier setMinInstancesPerNode(int value)  static Learner setPredictionCol(String value)  static E setProbabilityCol(String value)  static E setRawPredictionCol(String value)  DecisionTreeClassifier setSeed(long value)  static E setThresholds(double[] value)  static String[] supportedImpurities() Accessor for supported impurities: entropy, gini static DoubleArrayParam thresholds()  static String toString()  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.classification.ProbabilisticClassifier setProbabilityCol, setThresholds Methods inherited from class org.apache.spark.ml.classification.Classifier setRawPredictionCol Methods inherited from class org.apache.spark.ml.Predictor fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail DecisionTreeClassifier public DecisionTreeClassifier(String uid) DecisionTreeClassifier public DecisionTreeClassifier() Method Detail supportedImpurities public static final String[] supportedImpurities() Accessor for supported impurities: entropy, gini load public static DecisionTreeClassifier load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) fit public static M fit(Dataset<?> dataset, ParamMap paramMap) fit public static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setLabelCol public static Learner setLabelCol(String value) setFeaturesCol public static Learner setFeaturesCol(String value) setPredictionCol public static Learner setPredictionCol(String value) fit public static M fit(Dataset<?> dataset) transformSchema public static StructType transformSchema(StructType schema) rawPredictionCol public static final Param<String> rawPredictionCol() getRawPredictionCol public static final String getRawPredictionCol() setRawPredictionCol public static E setRawPredictionCol(String value) probabilityCol public static final Param<String> probabilityCol() getProbabilityCol public static final String getProbabilityCol() thresholds public static final DoubleArrayParam thresholds() getThresholds public static double[] getThresholds() setProbabilityCol public static E setProbabilityCol(String value) setThresholds public static E setThresholds(double[] value) checkpointInterval public static final IntParam checkpointInterval() getCheckpointInterval public static final int getCheckpointInterval() seed public static final LongParam seed() getSeed public static final long getSeed() maxDepth public static final IntParam maxDepth() maxBins public static final IntParam maxBins() minInstancesPerNode public static final IntParam minInstancesPerNode() minInfoGain public static final DoubleParam minInfoGain() maxMemoryInMB public static final IntParam maxMemoryInMB() cacheNodeIds public static final BooleanParam cacheNodeIds() getMaxDepth public static final int getMaxDepth() getMaxBins public static final int getMaxBins() getMinInstancesPerNode public static final int getMinInstancesPerNode() getMinInfoGain public static final double getMinInfoGain() getMaxMemoryInMB public static final int getMaxMemoryInMB() getCacheNodeIds public static final boolean getCacheNodeIds() impurity public static final Param<String> impurity() getImpurity public static final String getImpurity() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setMaxDepth public DecisionTreeClassifier setMaxDepth(int value) setMaxBins public DecisionTreeClassifier setMaxBins(int value) setMinInstancesPerNode public DecisionTreeClassifier setMinInstancesPerNode(int value) setMinInfoGain public DecisionTreeClassifier setMinInfoGain(double value) setMaxMemoryInMB public DecisionTreeClassifier setMaxMemoryInMB(int value) setCacheNodeIds public DecisionTreeClassifier setCacheNodeIds(boolean value) setCheckpointInterval public DecisionTreeClassifier setCheckpointInterval(int value) setImpurity public DecisionTreeClassifier setImpurity(String value) setSeed public DecisionTreeClassifier setSeed(long value) copy public DecisionTreeClassifier copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Predictor<Vector,DecisionTreeClassifier,DecisionTreeClassificationModel> Parameters:extra - (undocumented) Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) rawPredictionCol public Param<String> rawPredictionCol() Param for raw prediction (a.k.a. confidence) column name. Returns:(undocumented) getRawPredictionCol public String getRawPredictionCol() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.model Class DecisionTreeModel.SaveLoadV1_0$ Object org.apache.spark.mllib.tree.model.DecisionTreeModel.SaveLoadV1_0$ Enclosing class: DecisionTreeModel public static class DecisionTreeModel.SaveLoadV1_0$ extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description class  DecisionTreeModel.SaveLoadV1_0$.NodeData Model data for model import/export class  DecisionTreeModel.SaveLoadV1_0$.PredictData  class  DecisionTreeModel.SaveLoadV1_0$.SplitData  Field Summary Fields  Modifier and Type Field and Description static DecisionTreeModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description DecisionTreeModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description Node constructTree(org.apache.spark.mllib.tree.model.DecisionTreeModel.SaveLoadV1_0.NodeData[] data) Given a list of nodes from a tree, construct the tree. Node[] constructTrees(RDD<org.apache.spark.mllib.tree.model.DecisionTreeModel.SaveLoadV1_0.NodeData> nodes)  DecisionTreeModel load(SparkContext sc, String path, String algo, int numNodes)  void save(SparkContext sc, String path, DecisionTreeModel model)  String thisClassName()  String thisFormatVersion()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final DecisionTreeModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail DecisionTreeModel.SaveLoadV1_0$ public DecisionTreeModel.SaveLoadV1_0$() Method Detail thisFormatVersion public String thisFormatVersion() thisClassName public String thisClassName() save public void save(SparkContext sc, String path, DecisionTreeModel model) load public DecisionTreeModel load(SparkContext sc, String path, String algo, int numNodes) constructTrees public Node[] constructTrees(RDD<org.apache.spark.mllib.tree.model.DecisionTreeModel.SaveLoadV1_0.NodeData> nodes) constructTree public Node constructTree(org.apache.spark.mllib.tree.model.DecisionTreeModel.SaveLoadV1_0.NodeData[] data) Given a list of nodes from a tree, construct the tree. Parameters:data - array of all node data in a tree. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.model Class DecisionTreeModel Object org.apache.spark.mllib.tree.model.DecisionTreeModel All Implemented Interfaces: java.io.Serializable, Saveable public class DecisionTreeModel extends Object implements scala.Serializable, Saveable Decision tree model for classification or regression. This model stores the decision tree structure and parameters. param: topNode root node param: algo algorithm type -- classification or regression See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  DecisionTreeModel.SaveLoadV1_0$  Constructor Summary Constructors  Constructor and Description DecisionTreeModel(Node topNode, scala.Enumeration.Value algo)  Method Summary Methods  Modifier and Type Method and Description scala.Enumeration.Value algo()  int depth() Get depth of tree. static DecisionTreeModel load(SparkContext sc, String path)  int numNodes() Get number of nodes in tree, including leaf nodes. JavaRDD<Double> predict(JavaRDD<Vector> features) Predict values for the given data set using the model trained. RDD<Object> predict(RDD<Vector> features) Predict values for the given data set using the model trained. double predict(Vector features) Predict values for a single data point using the model trained. void save(SparkContext sc, String path) Save this model to the given path. String toDebugString() Print the full model to a string. Node topNode()  String toString() Print a summary of the model. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail DecisionTreeModel public DecisionTreeModel(Node topNode, scala.Enumeration.Value algo) Method Detail load public static DecisionTreeModel load(SparkContext sc, String path) Parameters:sc - Spark context used for loading model files.path - Path specifying the directory to which the model was saved. Returns:Model instance topNode public Node topNode() algo public scala.Enumeration.Value algo() predict public double predict(Vector features) Predict values for a single data point using the model trained. Parameters:features - array representing a single data point Returns:Double prediction from the trained model predict public RDD<Object> predict(RDD<Vector> features) Predict values for the given data set using the model trained. Parameters:features - RDD representing data points to be predicted Returns:RDD of predictions for each of the given data points predict public JavaRDD<Double> predict(JavaRDD<Vector> features) Predict values for the given data set using the model trained. Parameters:features - JavaRDD representing data points to be predicted Returns:JavaRDD of predictions for each of the given data points numNodes public int numNodes() Get number of nodes in tree, including leaf nodes. Returns:(undocumented) depth public int depth() Get depth of tree. E.g.: Depth 0 means 1 leaf node. Depth 1 means 1 internal node and 2 leaf nodes. Returns:(undocumented) toString public String toString() Print a summary of the model. Overrides: toString in class Object Returns:(undocumented) toDebugString public String toDebugString() Print the full model to a string. Returns:(undocumented) save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeModelReadWrite.NodeData$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeModelReadWrite.NodeData$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class DecisionTreeModelReadWrite.NodeData$ Object org.apache.spark.ml.tree.DecisionTreeModelReadWrite.NodeData$ All Implemented Interfaces: java.io.Serializable Enclosing class: DecisionTreeModelReadWrite public static class DecisionTreeModelReadWrite.NodeData$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static DecisionTreeModelReadWrite.NodeData$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description DecisionTreeModelReadWrite.NodeData$()  Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<scala.collection.Seq<DecisionTreeModelReadWrite.NodeData>,Object> build(Node node, int id) Create DecisionTreeModelReadWrite.NodeData instances for this node and all children. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final DecisionTreeModelReadWrite.NodeData$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail DecisionTreeModelReadWrite.NodeData$ public DecisionTreeModelReadWrite.NodeData$() Method Detail build public scala.Tuple2<scala.collection.Seq<DecisionTreeModelReadWrite.NodeData>,Object> build(Node node, int id) Create DecisionTreeModelReadWrite.NodeData instances for this node and all children. Parameters:id - Current ID. IDs are assigned via a pre-order traversal.node - (undocumented) Returns:(sequence of nodes in pre-order traversal order, largest ID in subtree) The nodes are returned in pre-order traversal (root first) so that it is easy to get the ID of the subtree's root node. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeModelReadWrite.NodeData (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeModelReadWrite.NodeData (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class DecisionTreeModelReadWrite.NodeData Object org.apache.spark.ml.tree.DecisionTreeModelReadWrite.NodeData All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: DecisionTreeModelReadWrite public static class DecisionTreeModelReadWrite.NodeData extends Object implements scala.Product, scala.Serializable Info for a Node param: id Index used for tree reconstruction. Indices follow a pre-order traversal. param: impurityStats Stats array. Impurity type is stored in metadata. param: gain Gain, or arbitrary value if leaf node. param: leftChild Left child index, or arbitrary value if leaf node. param: rightChild Right child index, or arbitrary value if leaf node. param: split Split info, or arbitrary value if leaf node. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DecisionTreeModelReadWrite.NodeData(int id, double prediction, double impurity, double[] impurityStats, double gain, int leftChild, int rightChild, DecisionTreeModelReadWrite.SplitData split)  Method Summary Methods  Modifier and Type Method and Description double gain()  int id()  double impurity()  double[] impurityStats()  int leftChild()  double prediction()  int rightChild()  DecisionTreeModelReadWrite.SplitData split()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail DecisionTreeModelReadWrite.NodeData public DecisionTreeModelReadWrite.NodeData(int id, double prediction, double impurity, double[] impurityStats, double gain, int leftChild, int rightChild, DecisionTreeModelReadWrite.SplitData split) Method Detail id public int id() prediction public double prediction() impurity public double impurity() impurityStats public double[] impurityStats() gain public double gain() leftChild public int leftChild() rightChild public int rightChild() split public DecisionTreeModelReadWrite.SplitData split() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeModelReadWrite.SplitData$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeModelReadWrite.SplitData$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class DecisionTreeModelReadWrite.SplitData$ Object org.apache.spark.ml.tree.DecisionTreeModelReadWrite.SplitData$ All Implemented Interfaces: java.io.Serializable Enclosing class: DecisionTreeModelReadWrite public static class DecisionTreeModelReadWrite.SplitData$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static DecisionTreeModelReadWrite.SplitData$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description DecisionTreeModelReadWrite.SplitData$()  Method Summary Methods  Modifier and Type Method and Description DecisionTreeModelReadWrite.SplitData apply(Split split)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final DecisionTreeModelReadWrite.SplitData$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail DecisionTreeModelReadWrite.SplitData$ public DecisionTreeModelReadWrite.SplitData$() Method Detail apply public DecisionTreeModelReadWrite.SplitData apply(Split split) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeModelReadWrite.SplitData (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeModelReadWrite.SplitData (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class DecisionTreeModelReadWrite.SplitData Object org.apache.spark.ml.tree.DecisionTreeModelReadWrite.SplitData All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: DecisionTreeModelReadWrite public static class DecisionTreeModelReadWrite.SplitData extends Object implements scala.Product, scala.Serializable Info for a Split param: featureIndex Index of feature split on param: leftCategoriesOrThreshold For categorical feature, set of leftCategories. For continuous feature, threshold. param: numCategories For categorical feature, number of categories. For continuous feature, -1. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DecisionTreeModelReadWrite.SplitData(int featureIndex, double[] leftCategoriesOrThreshold, int numCategories)  Method Summary Methods  Modifier and Type Method and Description int featureIndex()  Split getSplit()  double[] leftCategoriesOrThreshold()  int numCategories()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail DecisionTreeModelReadWrite.SplitData public DecisionTreeModelReadWrite.SplitData(int featureIndex, double[] leftCategoriesOrThreshold, int numCategories) Method Detail featureIndex public int featureIndex() leftCategoriesOrThreshold public double[] leftCategoriesOrThreshold() numCategories public int numCategories() getSplit public Split getSplit() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeModelReadWrite (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeModelReadWrite (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class DecisionTreeModelReadWrite Object org.apache.spark.ml.tree.DecisionTreeModelReadWrite public class DecisionTreeModelReadWrite extends Object Helper classes for tree model persistence Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  DecisionTreeModelReadWrite.NodeData Info for a Node static class  DecisionTreeModelReadWrite.NodeData$  static class  DecisionTreeModelReadWrite.SplitData Info for a Split static class  DecisionTreeModelReadWrite.SplitData$  Constructor Summary Constructors  Constructor and Description DecisionTreeModelReadWrite()  Method Summary Methods  Modifier and Type Method and Description static Node buildTreeFromNodes(DecisionTreeModelReadWrite.NodeData[] data, String impurityType) Given all data for all nodes in a tree, rebuild the tree. static Node loadTreeNodes(String path, org.apache.spark.ml.util.DefaultParamsReader.Metadata metadata, SparkSession sparkSession) Load a decision tree from a file. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DecisionTreeModelReadWrite public DecisionTreeModelReadWrite() Method Detail loadTreeNodes public static Node loadTreeNodes(String path, org.apache.spark.ml.util.DefaultParamsReader.Metadata metadata, SparkSession sparkSession) Load a decision tree from a file. Parameters:path - (undocumented)metadata - (undocumented)sparkSession - (undocumented) Returns:Root node of reconstructed tree buildTreeFromNodes public static Node buildTreeFromNodes(DecisionTreeModelReadWrite.NodeData[] data, String impurityType) Given all data for all nodes in a tree, rebuild the tree. Parameters:data - Unsorted node dataimpurityType - Impurity type for this tree Returns:Root node of reconstructed tree Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeRegressionModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeRegressionModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class DecisionTreeRegressionModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<M> org.apache.spark.ml.PredictionModel<Vector,DecisionTreeRegressionModel> org.apache.spark.ml.regression.DecisionTreeRegressionModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class DecisionTreeRegressionModel extends PredictionModel<Vector,DecisionTreeRegressionModel> implements MLWritable, scala.Serializable Decision tree model for regression. It supports both continuous and categorical features. param: rootNode Root of the decision tree See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static BooleanParam cacheNodeIds()  static IntParam checkpointInterval()  static Params clear(Param<?> param)  DecisionTreeRegressionModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static int depth()  static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  Vector featureImportances() Estimate of the importance of each feature. static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static <T> scala.Option<T> get(Param<T> param)  static boolean getCacheNodeIds()  static int getCheckpointInterval()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getImpurity()  static String getLabelCol()  String getLabelCol()  static int getMaxBins()  static int getMaxDepth()  static int getMaxMemoryInMB()  static double getMinInfoGain()  static int getMinInstancesPerNode()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static long getSeed()  static String getVarianceCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static Param<String> impurity()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static DecisionTreeRegressionModel load(String path)  static IntParam maxBins()  static IntParam maxDepth()  static IntParam maxMemoryInMB()  static DoubleParam minInfoGain()  static IntParam minInstancesPerNode()  int numFeatures() Returns the number of features the model was trained on. static int numNodes()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static MLReader<DecisionTreeRegressionModel> read()  Node rootNode()  static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  static org.apache.spark.ml.tree.DecisionTreeParams setCacheNodeIds(boolean value)  static org.apache.spark.ml.tree.DecisionTreeParams setCheckpointInterval(int value)  static M setFeaturesCol(String value)  static org.apache.spark.ml.tree.TreeRegressorParams setImpurity(String value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxBins(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxDepth(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxMemoryInMB(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMinInfoGain(double value)  static org.apache.spark.ml.tree.DecisionTreeParams setMinInstancesPerNode(int value)  static M setParent(Estimator<M> parent)  static M setPredictionCol(String value)  static org.apache.spark.ml.tree.DecisionTreeParams setSeed(long value)  DecisionTreeRegressionModel setVarianceCol(String value)  static String toDebugString()  String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms dataset by reading from featuresCol, calling predict, and storing the predictions as a new column predictionCol. static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  static Param<String> varianceCol()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.PredictionModel setFeaturesCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Method Detail read public static MLReader<DecisionTreeRegressionModel> read() load public static DecisionTreeRegressionModel load(String path) params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setFeaturesCol public static M setFeaturesCol(String value) setPredictionCol public static M setPredictionCol(String value) transformSchema public static StructType transformSchema(StructType schema) numNodes public static int numNodes() depth public static int depth() toDebugString public static String toDebugString() checkpointInterval public static final IntParam checkpointInterval() getCheckpointInterval public static final int getCheckpointInterval() seed public static final LongParam seed() getSeed public static final long getSeed() maxDepth public static final IntParam maxDepth() maxBins public static final IntParam maxBins() minInstancesPerNode public static final IntParam minInstancesPerNode() minInfoGain public static final DoubleParam minInfoGain() maxMemoryInMB public static final IntParam maxMemoryInMB() cacheNodeIds public static final BooleanParam cacheNodeIds() setMaxDepth public static org.apache.spark.ml.tree.DecisionTreeParams setMaxDepth(int value) getMaxDepth public static final int getMaxDepth() setMaxBins public static org.apache.spark.ml.tree.DecisionTreeParams setMaxBins(int value) getMaxBins public static final int getMaxBins() setMinInstancesPerNode public static org.apache.spark.ml.tree.DecisionTreeParams setMinInstancesPerNode(int value) getMinInstancesPerNode public static final int getMinInstancesPerNode() setMinInfoGain public static org.apache.spark.ml.tree.DecisionTreeParams setMinInfoGain(double value) getMinInfoGain public static final double getMinInfoGain() setSeed public static org.apache.spark.ml.tree.DecisionTreeParams setSeed(long value) setMaxMemoryInMB public static org.apache.spark.ml.tree.DecisionTreeParams setMaxMemoryInMB(int value) getMaxMemoryInMB public static final int getMaxMemoryInMB() setCacheNodeIds public static org.apache.spark.ml.tree.DecisionTreeParams setCacheNodeIds(boolean value) getCacheNodeIds public static final boolean getCacheNodeIds() setCheckpointInterval public static org.apache.spark.ml.tree.DecisionTreeParams setCheckpointInterval(int value) impurity public static final Param<String> impurity() setImpurity public static org.apache.spark.ml.tree.TreeRegressorParams setImpurity(String value) getImpurity public static final String getImpurity() varianceCol public static final Param<String> varianceCol() getVarianceCol public static final String getVarianceCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) rootNode public Node rootNode() numFeatures public int numFeatures() Description copied from class: PredictionModel Returns the number of features the model was trained on. If unknown, returns -1 Overrides: numFeatures in class PredictionModel<Vector,DecisionTreeRegressionModel> setVarianceCol public DecisionTreeRegressionModel setVarianceCol(String value) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: PredictionModel Transforms dataset by reading from featuresCol, calling predict, and storing the predictions as a new column predictionCol. Overrides: transform in class PredictionModel<Vector,DecisionTreeRegressionModel> Parameters:dataset - input dataset Returns:transformed dataset with predictionCol of type Double copy public DecisionTreeRegressionModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<DecisionTreeRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) toString public String toString() Specified by: toString in interface Identifiable Overrides: toString in class Object featureImportances public Vector featureImportances() Estimate of the importance of each feature. This generalizes the idea of "Gini" importance to other losses, following the explanation of Gini importance from "Random Forests" documentation by Leo Breiman and Adele Cutler, and following the implementation from scikit-learn. This feature importance is calculated as follows: - importance(feature j) = sum (over nodes which split on feature j) of the gain, where gain is scaled by the number of instances passing through node - Normalize importances for tree to sum to 1. Note: Feature importance for single decision trees can have high variance due to correlated predictor variables. Consider using a RandomForestRegressor to determine feature importance instead. Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DecisionTreeRegressor (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DecisionTreeRegressor (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class DecisionTreeRegressor Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<Vector,DecisionTreeRegressor,DecisionTreeRegressionModel> org.apache.spark.ml.regression.DecisionTreeRegressor All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class DecisionTreeRegressor extends Predictor<Vector,DecisionTreeRegressor,DecisionTreeRegressionModel> implements DefaultParamsWritable Decision tree learning algorithm for regression. It supports both continuous and categorical features. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DecisionTreeRegressor()  DecisionTreeRegressor(String uid)  Method Summary Methods  Modifier and Type Method and Description static BooleanParam cacheNodeIds()  static IntParam checkpointInterval()  static Params clear(Param<?> param)  DecisionTreeRegressor copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static M fit(Dataset<?> dataset)  static M fit(Dataset<?> dataset, ParamMap paramMap)  static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static <T> scala.Option<T> get(Param<T> param)  static boolean getCacheNodeIds()  static int getCheckpointInterval()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getImpurity()  static String getLabelCol()  String getLabelCol()  static int getMaxBins()  static int getMaxDepth()  static int getMaxMemoryInMB()  static double getMinInfoGain()  static int getMinInstancesPerNode()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static long getSeed()  static String getVarianceCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> impurity()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static DecisionTreeRegressor load(String path)  static IntParam maxBins()  static IntParam maxDepth()  static IntParam maxMemoryInMB()  static DoubleParam minInfoGain()  static IntParam minInstancesPerNode()  static Param<?>[] params()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  DecisionTreeRegressor setCacheNodeIds(boolean value)  DecisionTreeRegressor setCheckpointInterval(int value)  static Learner setFeaturesCol(String value)  DecisionTreeRegressor setImpurity(String value)  static Learner setLabelCol(String value)  DecisionTreeRegressor setMaxBins(int value)  DecisionTreeRegressor setMaxDepth(int value)  DecisionTreeRegressor setMaxMemoryInMB(int value)  DecisionTreeRegressor setMinInfoGain(double value)  DecisionTreeRegressor setMinInstancesPerNode(int value)  static Learner setPredictionCol(String value)  DecisionTreeRegressor setSeed(long value)  DecisionTreeRegressor setVarianceCol(String value)  static String[] supportedImpurities() Accessor for supported impurities: variance static String toString()  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  static Param<String> varianceCol()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Predictor fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail DecisionTreeRegressor public DecisionTreeRegressor(String uid) DecisionTreeRegressor public DecisionTreeRegressor() Method Detail supportedImpurities public static final String[] supportedImpurities() Accessor for supported impurities: variance load public static DecisionTreeRegressor load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) fit public static M fit(Dataset<?> dataset, ParamMap paramMap) fit public static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setLabelCol public static Learner setLabelCol(String value) setFeaturesCol public static Learner setFeaturesCol(String value) setPredictionCol public static Learner setPredictionCol(String value) fit public static M fit(Dataset<?> dataset) transformSchema public static StructType transformSchema(StructType schema) checkpointInterval public static final IntParam checkpointInterval() getCheckpointInterval public static final int getCheckpointInterval() seed public static final LongParam seed() getSeed public static final long getSeed() maxDepth public static final IntParam maxDepth() maxBins public static final IntParam maxBins() minInstancesPerNode public static final IntParam minInstancesPerNode() minInfoGain public static final DoubleParam minInfoGain() maxMemoryInMB public static final IntParam maxMemoryInMB() cacheNodeIds public static final BooleanParam cacheNodeIds() getMaxDepth public static final int getMaxDepth() getMaxBins public static final int getMaxBins() getMinInstancesPerNode public static final int getMinInstancesPerNode() getMinInfoGain public static final double getMinInfoGain() getMaxMemoryInMB public static final int getMaxMemoryInMB() getCacheNodeIds public static final boolean getCacheNodeIds() impurity public static final Param<String> impurity() getImpurity public static final String getImpurity() varianceCol public static final Param<String> varianceCol() getVarianceCol public static final String getVarianceCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setMaxDepth public DecisionTreeRegressor setMaxDepth(int value) setMaxBins public DecisionTreeRegressor setMaxBins(int value) setMinInstancesPerNode public DecisionTreeRegressor setMinInstancesPerNode(int value) setMinInfoGain public DecisionTreeRegressor setMinInfoGain(double value) setMaxMemoryInMB public DecisionTreeRegressor setMaxMemoryInMB(int value) setCacheNodeIds public DecisionTreeRegressor setCacheNodeIds(boolean value) setCheckpointInterval public DecisionTreeRegressor setCheckpointInterval(int value) setImpurity public DecisionTreeRegressor setImpurity(String value) setSeed public DecisionTreeRegressor setSeed(long value) setVarianceCol public DecisionTreeRegressor setVarianceCol(String value) copy public DecisionTreeRegressor copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Predictor<Vector,DecisionTreeRegressor,DecisionTreeRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DefaultParamsReadable (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DefaultParamsReadable (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.util Interface DefaultParamsReadable<T> All Superinterfaces: MLReadable<T> public interface DefaultParamsReadable<T> extends MLReadable<T> :: DeveloperApi :: Helper trait for making simple Params types readable. If a Params class stores all data as Param values, then extending this trait will provide a default implementation of reading saved instances of the class. This only handles simple Param types; e.g., it will not handle Dataset. See Also:DefaultParamsWritable}, the counterpart to this trait Method Summary Methods  Modifier and Type Method and Description MLReader<T> read() Returns an MLReader instance for this class. Methods inherited from interface org.apache.spark.ml.util.MLReadable load Method Detail read MLReader<T> read() Description copied from interface: MLReadable Returns an MLReader instance for this class. Specified by: read in interface MLReadable<T> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DefaultParamsWritable (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DefaultParamsWritable (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.util Interface DefaultParamsWritable All Superinterfaces: MLWritable All Known Implementing Classes: AFTSurvivalRegression, ALS, Binarizer, BinaryClassificationEvaluator, BisectingKMeans, Bucketizer, ChiSqSelector, CountVectorizer, DCT, DecisionTreeClassifier, DecisionTreeRegressor, ElementwiseProduct, GaussianMixture, GBTClassifier, GBTRegressor, GeneralizedLinearRegression, HashingTF, IDF, IndexToString, Interaction, IsotonicRegression, KMeans, LDA, LinearRegression, LogisticRegression, MaxAbsScaler, MinMaxScaler, MulticlassClassificationEvaluator, MultilayerPerceptronClassifier, NaiveBayes, NGram, Normalizer, OneHotEncoder, PCA, PolynomialExpansion, QuantileDiscretizer, RandomForestClassifier, RandomForestRegressor, RegexTokenizer, RegressionEvaluator, RFormula, SQLTransformer, StandardScaler, StopWordsRemover, StringIndexer, Tokenizer, VectorAssembler, VectorIndexer, VectorSlicer, Word2Vec public interface DefaultParamsWritable extends MLWritable :: DeveloperApi :: Helper trait for making simple Params types writable. If a Params class stores all data as Param values, then extending this trait will provide a default implementation of writing saved instances of the class. This only handles simple Param types; e.g., it will not handle Dataset. See Also:DefaultParamsReadable}, the counterpart to this trait Method Summary Methods  Modifier and Type Method and Description MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from interface org.apache.spark.ml.util.MLWritable save Method Detail write MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DefaultPartitionCoalescer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DefaultPartitionCoalescer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class DefaultPartitionCoalescer Object org.apache.spark.rdd.DefaultPartitionCoalescer All Implemented Interfaces: PartitionCoalescer public class DefaultPartitionCoalescer extends Object implements PartitionCoalescer Coalesce the partitions of a parent RDD (prev) into fewer partitions, so that each partition of this RDD computes one or more of the parent ones. It will produce exactly maxPartitions if the parent had more than maxPartitions, or fewer if the parent had fewer. This transformation is useful when an RDD with many partitions gets filtered into a smaller one, or to avoid having a large number of small tasks when processing a directory with many files. If there is no locality information (no preferredLocations) in the parent, then the coalescing is very simple: chunk parents that are close in the Array in chunks. If there is locality information, it proceeds to pack them with the following four goals: (1) Balance the groups so they roughly have the same number of parent partitions (2) Achieve locality per partition, i.e. find one machine which most parent partitions prefer (3) Be efficient, i.e. O(n) algorithm for n parent partitions (problem is likely NP-hard) (4) Balance preferred machines, i.e. avoid as much as possible picking the same preferred machine Furthermore, it is assumed that the parent RDD may have many partitions, e.g. 100 000. We assume the final number of desired partitions is small, e.g. less than 1000. The algorithm tries to assign unique preferred machines to each partition. If the number of desired partitions is greater than the number of preferred machines (can happen), it needs to start picking duplicate preferred machines. This is determined using coupon collector estimation (2n log(n)). The load balancing is done using power-of-two randomized bins-balls with one twist: it tries to also achieve locality. This is done by allowing a slack (balanceSlack, where 1.0 is all locality, 0 is all balance) between two bins. If two bins are within the slack in terms of balance, the algorithm will assign partitions according to locality. (contact alig for questions) Nested Class Summary Nested Classes  Modifier and Type Class and Description class  DefaultPartitionCoalescer.PartitionLocations  Constructor Summary Constructors  Constructor and Description DefaultPartitionCoalescer(double balanceSlack)  Method Summary Methods  Modifier and Type Method and Description boolean addPartToPGroup(Partition part, PartitionGroup pgroup)  double balanceSlack()  PartitionGroup[] coalesce(int maxPartitions, RDD<?> prev) Runs the packing algorithm and returns an array of PartitionGroups that if possible are load balanced and grouped by locality boolean compare(scala.Option<PartitionGroup> o1, scala.Option<PartitionGroup> o2)  boolean compare(PartitionGroup o1, PartitionGroup o2)  scala.collection.Seq<String> currPrefLocs(Partition part, RDD<?> prev)  scala.Option<PartitionGroup> getLeastGroupHash(String key) Sorts and gets the least element of the list associated with key in groupHash The returned PartitionGroup is the least loaded of all groups that represent the machine "key" PartitionGroup[] getPartitions()  scala.collection.mutable.ArrayBuffer<PartitionGroup> groupArr()  scala.collection.mutable.Map<String,scala.collection.mutable.ArrayBuffer<PartitionGroup>> groupHash()  scala.collection.mutable.Set<Partition> initialHash()  boolean noLocality()  PartitionGroup pickBin(Partition p, RDD<?> prev, double balanceSlack, DefaultPartitionCoalescer.PartitionLocations partitionLocs) Takes a parent RDD partition and decides which of the partition groups to put it in Takes locality into account, but also uses power of 2 choices to load balance It strikes a balance between the two using the balanceSlack variable scala.util.Random rnd()  void setupGroups(int targetLen, DefaultPartitionCoalescer.PartitionLocations partitionLocs) Initializes targetLen partition groups. void throwBalls(int maxPartitions, RDD<?> prev, double balanceSlack, DefaultPartitionCoalescer.PartitionLocations partitionLocs)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DefaultPartitionCoalescer public DefaultPartitionCoalescer(double balanceSlack) Method Detail balanceSlack public double balanceSlack() compare public boolean compare(PartitionGroup o1, PartitionGroup o2) compare public boolean compare(scala.Option<PartitionGroup> o1, scala.Option<PartitionGroup> o2) rnd public scala.util.Random rnd() groupArr public scala.collection.mutable.ArrayBuffer<PartitionGroup> groupArr() groupHash public scala.collection.mutable.Map<String,scala.collection.mutable.ArrayBuffer<PartitionGroup>> groupHash() initialHash public scala.collection.mutable.Set<Partition> initialHash() noLocality public boolean noLocality() currPrefLocs public scala.collection.Seq<String> currPrefLocs(Partition part, RDD<?> prev) getLeastGroupHash public scala.Option<PartitionGroup> getLeastGroupHash(String key) Sorts and gets the least element of the list associated with key in groupHash The returned PartitionGroup is the least loaded of all groups that represent the machine "key" Parameters:key - string representing a partitioned group on preferred machine key Returns:Option of PartitionGroup that has least elements for key addPartToPGroup public boolean addPartToPGroup(Partition part, PartitionGroup pgroup) setupGroups public void setupGroups(int targetLen, DefaultPartitionCoalescer.PartitionLocations partitionLocs) Initializes targetLen partition groups. If there are preferred locations, each group is assigned a preferredLocation. This uses coupon collector to estimate how many preferredLocations it must rotate through until it has seen most of the preferred locations (2 * n log(n)) Parameters:targetLen - partitionLocs - (undocumented) pickBin public PartitionGroup pickBin(Partition p, RDD<?> prev, double balanceSlack, DefaultPartitionCoalescer.PartitionLocations partitionLocs) Takes a parent RDD partition and decides which of the partition groups to put it in Takes locality into account, but also uses power of 2 choices to load balance It strikes a balance between the two using the balanceSlack variable Parameters:p - partition (ball to be thrown)balanceSlack - determines the trade-off between load-balancing the partitions sizes and their locality. e.g., balanceSlack=0.10 means that it allows up to 10% imbalance in favor of localityprev - (undocumented)partitionLocs - (undocumented) Returns:partition group (bin to be put in) throwBalls public void throwBalls(int maxPartitions, RDD<?> prev, double balanceSlack, DefaultPartitionCoalescer.PartitionLocations partitionLocs) getPartitions public PartitionGroup[] getPartitions() coalesce public PartitionGroup[] coalesce(int maxPartitions, RDD<?> prev) Runs the packing algorithm and returns an array of PartitionGroups that if possible are load balanced and grouped by locality Specified by: coalesce in interface PartitionCoalescer Parameters:maxPartitions - (undocumented)prev - (undocumented) Returns:array of partition groups Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DenseMatrix (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DenseMatrix (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg Class DenseMatrix Object org.apache.spark.mllib.linalg.DenseMatrix All Implemented Interfaces: java.io.Serializable, Matrix public class DenseMatrix extends Object implements Matrix Column-major dense matrix. The entry values are stored in a single array of doubles with columns listed in sequence. For example, the following matrix 1.0 2.0 3.0 4.0 5.0 6.0 is stored as [1.0, 3.0, 5.0, 2.0, 4.0, 6.0]. param: numRows number of rows param: numCols number of columns param: values matrix entries in column major if not transposed or in row major otherwise param: isTransposed whether the matrix is transposed. If true, values stores the matrix in row major. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DenseMatrix(int numRows, int numCols, double[] values) Column-major dense matrix. DenseMatrix(int numRows, int numCols, double[] values, boolean isTransposed)  Method Summary Methods  Modifier and Type Method and Description double apply(int i, int j) Gets the (i, j)-th element. DenseMatrix asML() Convert this matrix to the new mllib-local representation. scala.collection.Iterator<Vector> colIter() Returns an iterator of column vectors. DenseMatrix copy() Get a deep copy of the matrix. static DenseMatrix diag(Vector vector) Generate a diagonal matrix in DenseMatrix format from the supplied values. boolean equals(Object o)  static DenseMatrix eye(int n) Generate an Identity Matrix in DenseMatrix format. static DenseMatrix fromML(DenseMatrix m) Convert new linalg type to spark.mllib type. int hashCode()  boolean isTransposed() Flag that keeps track whether the matrix is transposed or not. static DenseMatrix multiply(DenseMatrix y)  static DenseVector multiply(DenseVector y)  static DenseVector multiply(Vector y)  int numActives() Find the number of values stored explicitly. int numCols() Number of columns. int numNonzeros() Find the number of non-zero active values. int numRows() Number of rows. static DenseMatrix ones(int numRows, int numCols) Generate a DenseMatrix consisting of ones. static DenseMatrix rand(int numRows, int numCols, java.util.Random rng) Generate a DenseMatrix consisting of i.i.d. uniform random numbers. static DenseMatrix randn(int numRows, int numCols, java.util.Random rng) Generate a DenseMatrix consisting of i.i.d. gaussian random numbers. static scala.collection.Iterator<Vector> rowIter()  static double[] toArray()  SparseMatrix toSparse() Generate a SparseMatrix from the given DenseMatrix. static String toString()  static String toString(int maxLines, int maxLineWidth)  DenseMatrix transpose() Transpose the Matrix. double[] values()  static DenseMatrix zeros(int numRows, int numCols) Generate a DenseMatrix consisting of zeros. Methods inherited from class Object getClass, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.linalg.Matrix multiply, multiply, multiply, rowIter, toArray, toString, toString Constructor Detail DenseMatrix public DenseMatrix(int numRows, int numCols, double[] values, boolean isTransposed) DenseMatrix public DenseMatrix(int numRows, int numCols, double[] values) Column-major dense matrix. The entry values are stored in a single array of doubles with columns listed in sequence. For example, the following matrix 1.0 2.0 3.0 4.0 5.0 6.0 is stored as [1.0, 3.0, 5.0, 2.0, 4.0, 6.0]. Parameters:numRows - number of rowsnumCols - number of columnsvalues - matrix entries in column major Method Detail zeros public static DenseMatrix zeros(int numRows, int numCols) Generate a DenseMatrix consisting of zeros. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrix Returns:DenseMatrix with size numRows x numCols and values of zeros ones public static DenseMatrix ones(int numRows, int numCols) Generate a DenseMatrix consisting of ones. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrix Returns:DenseMatrix with size numRows x numCols and values of ones eye public static DenseMatrix eye(int n) Generate an Identity Matrix in DenseMatrix format. Parameters:n - number of rows and columns of the matrix Returns:DenseMatrix with size n x n and values of ones on the diagonal rand public static DenseMatrix rand(int numRows, int numCols, java.util.Random rng) Generate a DenseMatrix consisting of i.i.d. uniform random numbers. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrixrng - a random number generator Returns:DenseMatrix with size numRows x numCols and values in U(0, 1) randn public static DenseMatrix randn(int numRows, int numCols, java.util.Random rng) Generate a DenseMatrix consisting of i.i.d. gaussian random numbers. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrixrng - a random number generator Returns:DenseMatrix with size numRows x numCols and values in N(0, 1) diag public static DenseMatrix diag(Vector vector) Generate a diagonal matrix in DenseMatrix format from the supplied values. Parameters:vector - a Vector that will form the values on the diagonal of the matrix Returns:Square DenseMatrix with size values.length x values.length and values on the diagonal fromML public static DenseMatrix fromML(DenseMatrix m) Convert new linalg type to spark.mllib type. Light copy; only copies references Parameters:m - (undocumented) Returns:(undocumented) toArray public static double[] toArray() rowIter public static scala.collection.Iterator<Vector> rowIter() multiply public static DenseMatrix multiply(DenseMatrix y) multiply public static DenseVector multiply(DenseVector y) multiply public static DenseVector multiply(Vector y) toString public static String toString() toString public static String toString(int maxLines, int maxLineWidth) numRows public int numRows() Description copied from interface: Matrix Number of rows. Specified by: numRows in interface Matrix numCols public int numCols() Description copied from interface: Matrix Number of columns. Specified by: numCols in interface Matrix values public double[] values() isTransposed public boolean isTransposed() Description copied from interface: Matrix Flag that keeps track whether the matrix is transposed or not. False by default. Specified by: isTransposed in interface Matrix equals public boolean equals(Object o) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object apply public double apply(int i, int j) Description copied from interface: Matrix Gets the (i, j)-th element. Specified by: apply in interface Matrix copy public DenseMatrix copy() Description copied from interface: Matrix Get a deep copy of the matrix. Specified by: copy in interface Matrix transpose public DenseMatrix transpose() Description copied from interface: Matrix Transpose the Matrix. Returns a new `Matrix` instance sharing the same underlying data. Specified by: transpose in interface Matrix numNonzeros public int numNonzeros() Description copied from interface: Matrix Find the number of non-zero active values. Specified by: numNonzeros in interface Matrix Returns:(undocumented) numActives public int numActives() Description copied from interface: Matrix Find the number of values stored explicitly. These values can be zero as well. Specified by: numActives in interface Matrix Returns:(undocumented) toSparse public SparseMatrix toSparse() Generate a SparseMatrix from the given DenseMatrix. The new matrix will have isTransposed set to false. Returns:(undocumented) colIter public scala.collection.Iterator<Vector> colIter() Description copied from interface: Matrix Returns an iterator of column vectors. This operation could be expensive, depending on the underlying storage. Specified by: colIter in interface Matrix Returns:(undocumented) asML public DenseMatrix asML() Description copied from interface: Matrix Convert this matrix to the new mllib-local representation. This does NOT copy the data; it copies references. Specified by: asML in interface Matrix Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DenseVector (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DenseVector (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg Class DenseVector Object org.apache.spark.mllib.linalg.DenseVector All Implemented Interfaces: java.io.Serializable, Vector public class DenseVector extends Object implements Vector A dense vector represented by a value array. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DenseVector(double[] values)  Method Summary Methods  Modifier and Type Method and Description double apply(int i) Gets the value of the ith element. int argmax() Find the index of a maximal element. DenseVector asML() Convert this vector to the new mllib-local representation. static Vector compressed()  DenseVector copy() Makes a deep copy of this vector. boolean equals(Object other)  void foreachActive(scala.Function2<Object,Object,scala.runtime.BoxedUnit> f) Applies a function f to all the active elements of dense and sparse vector. static DenseVector fromML(DenseVector v) Convert new linalg type to spark.mllib type. int hashCode() Returns a hash code value for the vector. int numActives() Number of active entries. int numNonzeros() Number of nonzero elements. int size() Size of the vector. double[] toArray() Converts the instance to a double array. static DenseVector toDense()  String toJson() Converts the vector to a JSON string. SparseVector toSparse() Converts this vector to a sparse vector with all explicit zeros removed. String toString()  static scala.Option<double[]> unapply(DenseVector dv) Extracts the value array from a dense vector. double[] values()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.linalg.Vector compressed, toDense Constructor Detail DenseVector public DenseVector(double[] values) Method Detail unapply public static scala.Option<double[]> unapply(DenseVector dv) Extracts the value array from a dense vector. fromML public static DenseVector fromML(DenseVector v) Convert new linalg type to spark.mllib type. Light copy; only copies references Parameters:v - (undocumented) Returns:(undocumented) toDense public static DenseVector toDense() compressed public static Vector compressed() values public double[] values() size public int size() Description copied from interface: Vector Size of the vector. Specified by: size in interface Vector Returns:(undocumented) toString public String toString() Overrides: toString in class Object toArray public double[] toArray() Description copied from interface: Vector Converts the instance to a double array. Specified by: toArray in interface Vector Returns:(undocumented) apply public double apply(int i) Description copied from interface: Vector Gets the value of the ith element. Specified by: apply in interface Vector Parameters:i - index Returns:(undocumented) copy public DenseVector copy() Description copied from interface: Vector Makes a deep copy of this vector. Specified by: copy in interface Vector Returns:(undocumented) foreachActive public void foreachActive(scala.Function2<Object,Object,scala.runtime.BoxedUnit> f) Description copied from interface: Vector Applies a function f to all the active elements of dense and sparse vector. Specified by: foreachActive in interface Vector Parameters:f - the function takes two parameters where the first parameter is the index of the vector with type Int, and the second parameter is the corresponding value with type Double. equals public boolean equals(Object other) Specified by: equals in interface Vector Overrides: equals in class Object hashCode public int hashCode() Description copied from interface: Vector Returns a hash code value for the vector. The hash code is based on its size and its first 128 nonzero entries, using a hash algorithm similar to java.util.Arrays.hashCode. Specified by: hashCode in interface Vector Overrides: hashCode in class Object Returns:(undocumented) numActives public int numActives() Description copied from interface: Vector Number of active entries. An "active entry" is an element which is explicitly stored, regardless of its value. Note that inactive entries have value 0. Specified by: numActives in interface Vector Returns:(undocumented) numNonzeros public int numNonzeros() Description copied from interface: Vector Number of nonzero elements. This scans all active values and count nonzeros. Specified by: numNonzeros in interface Vector Returns:(undocumented) toSparse public SparseVector toSparse() Description copied from interface: Vector Converts this vector to a sparse vector with all explicit zeros removed. Specified by: toSparse in interface Vector Returns:(undocumented) argmax public int argmax() Description copied from interface: Vector Find the index of a maximal element. Returns the first maximal element in case of a tie. Returns -1 if vector has length 0. Specified by: argmax in interface Vector Returns:(undocumented) toJson public String toJson() Description copied from interface: Vector Converts the vector to a JSON string. Specified by: toJson in interface Vector Returns:(undocumented) asML public DenseVector asML() Description copied from interface: Vector Convert this vector to the new mllib-local representation. This does NOT copy the data; it copies references. Specified by: asML in interface Vector Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Dependency (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Dependency (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class Dependency<T> Object org.apache.spark.Dependency<T> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: NarrowDependency, ShuffleDependency public abstract class Dependency<T> extends Object implements scala.Serializable :: DeveloperApi :: Base class for dependencies. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Dependency()  Method Summary Methods  Modifier and Type Method and Description abstract RDD<T> rdd()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Dependency public Dependency() Method Detail rdd public abstract RDD<T> rdd() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DerbyDialect (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DerbyDialect (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class DerbyDialect Object org.apache.spark.sql.jdbc.DerbyDialect public class DerbyDialect extends Object Constructor Summary Constructors  Constructor and Description DerbyDialect()  Method Summary Methods  Modifier and Type Method and Description static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties)  static boolean canHandle(String url)  static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md)  static scala.Option<JdbcType> getJDBCType(DataType dt)  static String getTableExistsQuery(String table)  static String quoteIdentifier(String colName)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DerbyDialect public DerbyDialect() Method Detail canHandle public static boolean canHandle(String url) getCatalystType public static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) getJDBCType public static scala.Option<JdbcType> getJDBCType(DataType dt) quoteIdentifier public static String quoteIdentifier(String colName) getTableExistsQuery public static String getTableExistsQuery(String table) beforeFetch public static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DeserializationStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DeserializationStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.serializer Class DeserializationStream Object org.apache.spark.serializer.DeserializationStream public abstract class DeserializationStream extends Object :: DeveloperApi :: A stream for reading serialized objects. Constructor Summary Constructors  Constructor and Description DeserializationStream()  Method Summary Methods  Modifier and Type Method and Description scala.collection.Iterator<Object> asIterator() Read the elements of this stream through an iterator. scala.collection.Iterator<scala.Tuple2<Object,Object>> asKeyValueIterator() Read the elements of this stream through an iterator over key-value pairs. abstract void close()  <T> T readKey(scala.reflect.ClassTag<T> evidence$9) Reads the object representing the key of a key-value pair. abstract <T> T readObject(scala.reflect.ClassTag<T> evidence$8) The most general-purpose method to read an object. <T> T readValue(scala.reflect.ClassTag<T> evidence$10) Reads the object representing the value of a key-value pair. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DeserializationStream public DeserializationStream() Method Detail readObject public abstract <T> T readObject(scala.reflect.ClassTag<T> evidence$8) The most general-purpose method to read an object. readKey public <T> T readKey(scala.reflect.ClassTag<T> evidence$9) Reads the object representing the key of a key-value pair. readValue public <T> T readValue(scala.reflect.ClassTag<T> evidence$10) Reads the object representing the value of a key-value pair. close public abstract void close() asIterator public scala.collection.Iterator<Object> asIterator() Read the elements of this stream through an iterator. This can only be called once, as reading each element will consume data from the input source. Returns:(undocumented) asKeyValueIterator public scala.collection.Iterator<scala.Tuple2<Object,Object>> asKeyValueIterator() Read the elements of this stream through an iterator over key-value pairs. This can only be called once, as reading each element will consume data from the input source. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DeserializedMemoryEntry (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DeserializedMemoryEntry (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage.memory Class DeserializedMemoryEntry<T> Object org.apache.spark.storage.memory.DeserializedMemoryEntry<T> All Implemented Interfaces: java.io.Serializable, MemoryEntry<T>, scala.Equals, scala.Product public class DeserializedMemoryEntry<T> extends Object implements MemoryEntry<T>, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DeserializedMemoryEntry(Object value, long size, scala.reflect.ClassTag<T> classTag)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  scala.reflect.ClassTag<T> classTag()  abstract static boolean equals(Object that)  org.apache.spark.memory.MemoryMode memoryMode()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  long size()  Object value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail DeserializedMemoryEntry public DeserializedMemoryEntry(Object value, long size, scala.reflect.ClassTag<T> classTag) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() value public Object value() size public long size() Specified by: size in interface MemoryEntry<T> classTag public scala.reflect.ClassTag<T> classTag() Specified by: classTag in interface MemoryEntry<T> memoryMode public org.apache.spark.memory.MemoryMode memoryMode() Specified by: memoryMode in interface MemoryEntry<T> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DistributedLDAModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DistributedLDAModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class DistributedLDAModel Object org.apache.spark.mllib.clustering.LDAModel org.apache.spark.mllib.clustering.DistributedLDAModel All Implemented Interfaces: Saveable public class DistributedLDAModel extends LDAModel Distributed LDA model. This model stores the inferred topics, the full training dataset, and the topic distributions. Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<int[],double[]>[] describeTopics(int maxTermsPerTopic) Return the topics described by weighted terms. Vector docConcentration() Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). JavaRDD<scala.Tuple3<Long,int[],int[]>> javaTopicAssignments() Java-friendly version of topicAssignments JavaPairRDD<Long,Vector> javaTopicDistributions() Java-friendly version of topicDistributions JavaRDD<scala.Tuple3<Long,int[],double[]>> javaTopTopicsPerDocument(int k) Java-friendly version of topTopicsPerDocument int k() Number of topics static DistributedLDAModel load(SparkContext sc, String path)  double logLikelihood() Log likelihood of the observed tokens in the training set, given the current parameter estimates: log P(docs | topics, topic distributions for docs, alpha, eta) double logPrior() Log probability of the current parameter estimate: log P(topics, topic distributions for docs | alpha, eta) void save(SparkContext sc, String path) Save this model to the given path. LocalLDAModel toLocal() Convert model to a local model. scala.Tuple2<long[],double[]>[] topDocumentsPerTopic(int maxDocumentsPerTopic) Return the top documents for each topic RDD<scala.Tuple3<Object,int[],int[]>> topicAssignments() Return the top topic for each (doc, term) pair. double topicConcentration() Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. RDD<scala.Tuple2<Object,Vector>> topicDistributions() For each document in the training set, return the distribution over topics for that document ("theta_doc"). Matrix topicsMatrix() Inferred topics, where each topic is represented by a distribution over terms. RDD<scala.Tuple3<Object,int[],double[]>> topTopicsPerDocument(int k) For each document, return the top k weighted topics for that document and their weights. int vocabSize() Vocabulary size (number of terms or terms in the vocabulary) Methods inherited from class org.apache.spark.mllib.clustering.LDAModel describeTopics Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail load public static DistributedLDAModel load(SparkContext sc, String path) k public int k() Description copied from class: LDAModel Number of topics Specified by: k in class LDAModel vocabSize public int vocabSize() Description copied from class: LDAModel Vocabulary size (number of terms or terms in the vocabulary) Specified by: vocabSize in class LDAModel docConcentration public Vector docConcentration() Description copied from class: LDAModel Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). This is the parameter to a Dirichlet distribution. Specified by: docConcentration in class LDAModel Returns:(undocumented) topicConcentration public double topicConcentration() Description copied from class: LDAModel Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. This is the parameter to a symmetric Dirichlet distribution. Note: The topics' distributions over terms are called "beta" in the original LDA paper by Blei et al., but are called "phi" in many later papers such as Asuncion et al., 2009. Specified by: topicConcentration in class LDAModel Returns:(undocumented) toLocal public LocalLDAModel toLocal() Convert model to a local model. The local model stores the inferred topics but not the topic distributions for training documents. Returns:(undocumented) topicsMatrix public Matrix topicsMatrix() Inferred topics, where each topic is represented by a distribution over terms. This is a matrix of size vocabSize x k, where each column is a topic. No guarantees are given about the ordering of the topics. WARNING: This matrix is collected from an RDD. Beware memory usage when vocabSize, k are large. Specified by: topicsMatrix in class LDAModel Returns:(undocumented) describeTopics public scala.Tuple2<int[],double[]>[] describeTopics(int maxTermsPerTopic) Description copied from class: LDAModel Return the topics described by weighted terms. Specified by: describeTopics in class LDAModel Parameters:maxTermsPerTopic - Maximum number of terms to collect for each topic. Returns:Array over topics. Each topic is represented as a pair of matching arrays: (term indices, term weights in topic). Each topic's terms are sorted in order of decreasing weight. topDocumentsPerTopic public scala.Tuple2<long[],double[]>[] topDocumentsPerTopic(int maxDocumentsPerTopic) Return the top documents for each topic Parameters:maxDocumentsPerTopic - Maximum number of documents to collect for each topic. Returns:Array over topics. Each element represent as a pair of matching arrays: (IDs for the documents, weights of the topic in these documents). For each topic, documents are sorted in order of decreasing topic weights. topicAssignments public RDD<scala.Tuple3<Object,int[],int[]>> topicAssignments() Return the top topic for each (doc, term) pair. I.e., for each document, what is the most likely topic generating each term? Returns:RDD of (doc ID, assignment of top topic index for each term), where the assignment is specified via a pair of zippable arrays (term indices, topic indices). Note that terms will be omitted if not present in the document. javaTopicAssignments public JavaRDD<scala.Tuple3<Long,int[],int[]>> javaTopicAssignments() Java-friendly version of topicAssignments logLikelihood public double logLikelihood() Log likelihood of the observed tokens in the training set, given the current parameter estimates: log P(docs | topics, topic distributions for docs, alpha, eta) Note: - This excludes the prior; for that, use logPrior. - Even with logPrior, this is NOT the same as the data log likelihood given the hyperparameters. Returns:(undocumented) logPrior public double logPrior() Log probability of the current parameter estimate: log P(topics, topic distributions for docs | alpha, eta) Returns:(undocumented) topicDistributions public RDD<scala.Tuple2<Object,Vector>> topicDistributions() For each document in the training set, return the distribution over topics for that document ("theta_doc"). Returns:RDD of (document ID, topic distribution) pairs javaTopicDistributions public JavaPairRDD<Long,Vector> javaTopicDistributions() Java-friendly version of topicDistributions Returns:(undocumented) topTopicsPerDocument public RDD<scala.Tuple3<Object,int[],double[]>> topTopicsPerDocument(int k) For each document, return the top k weighted topics for that document and their weights. Parameters:k - (undocumented) Returns:RDD of (doc ID, topic indices, topic weights) javaTopTopicsPerDocument public JavaRDD<scala.Tuple3<Long,int[],double[]>> javaTopTopicsPerDocument(int k) Java-friendly version of topTopicsPerDocument Parameters:k - (undocumented) Returns:(undocumented) save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DistributedMatrix (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DistributedMatrix (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg.distributed Interface DistributedMatrix All Superinterfaces: java.io.Serializable All Known Implementing Classes: BlockMatrix, CoordinateMatrix, IndexedRowMatrix, RowMatrix public interface DistributedMatrix extends scala.Serializable Represents a distributively stored matrix backed by one or more RDDs. Method Summary Methods  Modifier and Type Method and Description long numCols() Gets or computes the number of columns. long numRows() Gets or computes the number of rows. breeze.linalg.DenseMatrix<Object> toBreeze() Collects data and assembles a local dense breeze matrix (for test only). Method Detail numRows long numRows() Gets or computes the number of rows. numCols long numCols() Gets or computes the number of columns. toBreeze breeze.linalg.DenseMatrix<Object> toBreeze() Collects data and assembles a local dense breeze matrix (for test only). Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Dot (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Dot (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class Dot Object org.apache.spark.ml.feature.Dot public class Dot extends Object Constructor Summary Constructors  Constructor and Description Dot()  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Dot public Dot() Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DoubleAccumulator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DoubleAccumulator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class DoubleAccumulator Object org.apache.spark.util.AccumulatorV2<Double,Double> org.apache.spark.util.DoubleAccumulator All Implemented Interfaces: java.io.Serializable public class DoubleAccumulator extends AccumulatorV2<Double,Double> An accumulator for computing sum, count, and averages for double precision floating numbers. Since: 2.0.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DoubleAccumulator()  Method Summary Methods  Modifier and Type Method and Description void add(double v) Adds v to the accumulator, i.e. void add(Double v) Adds v to the accumulator, i.e. double avg() Returns the average of elements added to the accumulator. DoubleAccumulator copy() Creates a new copy of this accumulator. long count() Returns the number of elements added to the accumulator. boolean isZero() Returns if this accumulator is zero value or not. void merge(AccumulatorV2<Double,Double> other) Merges another same-type accumulator into this one and update its state, i.e. void reset() Resets this accumulator, which is zero value. double sum() Returns the sum of elements added to the accumulator. Double value() Defines the current value of this accumulator Methods inherited from class org.apache.spark.util.AccumulatorV2 copyAndReset, id, isRegistered, name, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail DoubleAccumulator public DoubleAccumulator() Method Detail isZero public boolean isZero() Description copied from class: AccumulatorV2 Returns if this accumulator is zero value or not. e.g. for a counter accumulator, 0 is zero value; for a list accumulator, Nil is zero value. Specified by: isZero in class AccumulatorV2<Double,Double> Returns:(undocumented) copy public DoubleAccumulator copy() Description copied from class: AccumulatorV2 Creates a new copy of this accumulator. Specified by: copy in class AccumulatorV2<Double,Double> Returns:(undocumented) reset public void reset() Description copied from class: AccumulatorV2 Resets this accumulator, which is zero value. i.e. call isZero must return true. Specified by: reset in class AccumulatorV2<Double,Double> add public void add(Double v) Adds v to the accumulator, i.e. increment sum by v and count by 1. Specified by: add in class AccumulatorV2<Double,Double> Parameters:v - (undocumented)Since: 2.0.0 add public void add(double v) Adds v to the accumulator, i.e. increment sum by v and count by 1. Parameters:v - (undocumented)Since: 2.0.0 count public long count() Returns the number of elements added to the accumulator. Returns:(undocumented)Since: 2.0.0 sum public double sum() Returns the sum of elements added to the accumulator. Returns:(undocumented)Since: 2.0.0 avg public double avg() Returns the average of elements added to the accumulator. Returns:(undocumented)Since: 2.0.0 merge public void merge(AccumulatorV2<Double,Double> other) Description copied from class: AccumulatorV2 Merges another same-type accumulator into this one and update its state, i.e. this should be merge-in-place. Specified by: merge in class AccumulatorV2<Double,Double> Parameters:other - (undocumented) value public Double value() Description copied from class: AccumulatorV2 Defines the current value of this accumulator Specified by: value in class AccumulatorV2<Double,Double> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DoubleArrayParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DoubleArrayParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class DoubleArrayParam Object org.apache.spark.ml.param.Param<double[]> org.apache.spark.ml.param.DoubleArrayParam All Implemented Interfaces: java.io.Serializable public class DoubleArrayParam extends Param<double[]> :: DeveloperApi :: Specialized version of Param[Array[Double} for Java. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DoubleArrayParam(Params parent, String name, String doc)  DoubleArrayParam(Params parent, String name, String doc, scala.Function1<double[],Object> isValid)  Method Summary Methods  Modifier and Type Method and Description double[] jsonDecode(String json) Decodes a param value from JSON. String jsonEncode(double[] value) Encodes a param value into JSON, which can be decoded by jsonDecode(). ParamPair<double[]> w(java.util.List<Double> value) Creates a param pair with a List of values (for Java and Python). Methods inherited from class org.apache.spark.ml.param.Param doc, equals, hashCode, isValid, name, parent, toString, w Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail DoubleArrayParam public DoubleArrayParam(Params parent, String name, String doc, scala.Function1<double[],Object> isValid) DoubleArrayParam public DoubleArrayParam(Params parent, String name, String doc) Method Detail w public ParamPair<double[]> w(java.util.List<Double> value) Creates a param pair with a List of values (for Java and Python). jsonEncode public String jsonEncode(double[] value) Description copied from class: Param Encodes a param value into JSON, which can be decoded by jsonDecode(). Overrides: jsonEncode in class Param<double[]> jsonDecode public double[] jsonDecode(String json) Description copied from class: Param Decodes a param value from JSON. Overrides: jsonDecode in class Param<double[]> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DoubleFlatMapFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DoubleFlatMapFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface DoubleFlatMapFunction<T> All Superinterfaces: java.io.Serializable public interface DoubleFlatMapFunction<T> extends java.io.Serializable A function that returns zero or more records of type Double from each input record. Method Summary Methods  Modifier and Type Method and Description java.util.Iterator<Double> call(T t)  Method Detail call java.util.Iterator<Double> call(T t) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DoubleFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DoubleFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface DoubleFunction<T> All Superinterfaces: java.io.Serializable public interface DoubleFunction<T> extends java.io.Serializable A function that returns Doubles, and can be used to construct DoubleRDDs. Method Summary Methods  Modifier and Type Method and Description double call(T t)  Method Detail call double call(T t) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DoubleParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DoubleParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class DoubleParam Object org.apache.spark.ml.param.Param<Object> org.apache.spark.ml.param.DoubleParam All Implemented Interfaces: java.io.Serializable public class DoubleParam extends Param<Object> :: DeveloperApi :: Specialized version of Param[Double] for Java. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DoubleParam(Identifiable parent, String name, String doc)  DoubleParam(Identifiable parent, String name, String doc, scala.Function1<Object,Object> isValid)  DoubleParam(String parent, String name, String doc)  DoubleParam(String parent, String name, String doc, scala.Function1<Object,Object> isValid)  Method Summary Methods  Modifier and Type Method and Description static ParamPair<T> $minus$greater(T value)  static String doc()  static boolean equals(Object obj)  static int hashCode()  static scala.Function1<T,Object> isValid()  double jsonDecode(String json)  String jsonEncode(double value)  static double jValueDecode(org.json4s.JsonAST.JValue jValue) Decodes a param value from JValue. static org.json4s.JsonAST.JValue jValueEncode(double value) Encodes a param value into JValue. static String name()  static String parent()  static String toString()  ParamPair<Object> w(double value) Creates a param pair with the given value (for Java). Methods inherited from class org.apache.spark.ml.param.Param doc, equals, hashCode, isValid, jsonEncode, name, parent, toString, w Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail DoubleParam public DoubleParam(String parent, String name, String doc, scala.Function1<Object,Object> isValid) DoubleParam public DoubleParam(String parent, String name, String doc) DoubleParam public DoubleParam(Identifiable parent, String name, String doc, scala.Function1<Object,Object> isValid) DoubleParam public DoubleParam(Identifiable parent, String name, String doc) Method Detail jValueEncode public static org.json4s.JsonAST.JValue jValueEncode(double value) Encodes a param value into JValue. jValueDecode public static double jValueDecode(org.json4s.JsonAST.JValue jValue) Decodes a param value from JValue. parent public static String parent() name public static String name() doc public static String doc() isValid public static scala.Function1<T,Object> isValid() $minus$greater public static ParamPair<T> $minus$greater(T value) toString public static final String toString() hashCode public static final int hashCode() equals public static final boolean equals(Object obj) w public ParamPair<Object> w(double value) Creates a param pair with the given value (for Java). jsonEncode public String jsonEncode(double value) jsonDecode public double jsonDecode(String json) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DoubleRDDFunctions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DoubleRDDFunctions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class DoubleRDDFunctions Object org.apache.spark.rdd.DoubleRDDFunctions All Implemented Interfaces: java.io.Serializable public class DoubleRDDFunctions extends Object implements scala.Serializable Extra functions available on RDDs of Doubles through an implicit conversion. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description DoubleRDDFunctions(RDD<Object> self)  Method Summary Methods  Modifier and Type Method and Description long[] histogram(double[] buckets, boolean evenBuckets) Compute a histogram using the provided buckets. scala.Tuple2<double[],long[]> histogram(int bucketCount) Compute a histogram of the data using bucketCount number of buckets evenly spaced between the minimum and maximum of the RDD. double mean() Compute the mean of this RDD's elements. PartialResult<BoundedDouble> meanApprox(long timeout, double confidence) Approximate operation to return the mean within a timeout. double sampleStdev() Compute the sample standard deviation of this RDD's elements (which corrects for bias in estimating the standard deviation by dividing by N-1 instead of N). double sampleVariance() Compute the sample variance of this RDD's elements (which corrects for bias in estimating the variance by dividing by N-1 instead of N). StatCounter stats() Return a StatCounter object that captures the mean, variance and count of the RDD's elements in one operation. double stdev() Compute the standard deviation of this RDD's elements. double sum() Add up the elements in this RDD. PartialResult<BoundedDouble> sumApprox(long timeout, double confidence) Approximate operation to return the sum within a timeout. double variance() Compute the variance of this RDD's elements. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail DoubleRDDFunctions public DoubleRDDFunctions(RDD<Object> self) Method Detail sum public double sum() Add up the elements in this RDD. stats public StatCounter stats() Return a StatCounter object that captures the mean, variance and count of the RDD's elements in one operation. Returns:(undocumented) mean public double mean() Compute the mean of this RDD's elements. variance public double variance() Compute the variance of this RDD's elements. stdev public double stdev() Compute the standard deviation of this RDD's elements. sampleStdev public double sampleStdev() Compute the sample standard deviation of this RDD's elements (which corrects for bias in estimating the standard deviation by dividing by N-1 instead of N). Returns:(undocumented) sampleVariance public double sampleVariance() Compute the sample variance of this RDD's elements (which corrects for bias in estimating the variance by dividing by N-1 instead of N). Returns:(undocumented) meanApprox public PartialResult<BoundedDouble> meanApprox(long timeout, double confidence) Approximate operation to return the mean within a timeout. Parameters:timeout - (undocumented)confidence - (undocumented) Returns:(undocumented) sumApprox public PartialResult<BoundedDouble> sumApprox(long timeout, double confidence) Approximate operation to return the sum within a timeout. Parameters:timeout - (undocumented)confidence - (undocumented) Returns:(undocumented) histogram public scala.Tuple2<double[],long[]> histogram(int bucketCount) Compute a histogram of the data using bucketCount number of buckets evenly spaced between the minimum and maximum of the RDD. For example if the min value is 0 and the max is 100 and there are two buckets the resulting buckets will be [0, 50) [50, 100]. bucketCount must be at least 1 If the RDD contains infinity, NaN throws an exception If the elements in RDD do not vary (max == min) always returns a single bucket. Parameters:bucketCount - (undocumented) Returns:(undocumented) histogram public long[] histogram(double[] buckets, boolean evenBuckets) Compute a histogram using the provided buckets. The buckets are all open to the right except for the last which is closed e.g. for the array [1, 10, 20, 50] the buckets are [1, 10) [10, 20) [20, 50] e.g 1<=x<10 , 10<=x<20, 20<=x<=50 And on the input of 1 and 50 we would have a histogram of 1, 0, 1 Note: if your histogram is evenly spaced (e.g. [0, 10, 20, 30]) this can be switched from an O(log n) insertion to O(1) per element. (where n = # buckets) if you set evenBuckets to true. buckets must be sorted and not contain any duplicates. buckets array must be at least two elements All NaN entries are treated the same. If you have a NaN bucket it must be the maximum value of the last position and all NaN entries will be counted in that bucket. Parameters:buckets - (undocumented)evenBuckets - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DoubleType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DoubleType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class DoubleType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.NumericType org.apache.spark.sql.types.DoubleType public class DoubleType extends NumericType :: DeveloperApi :: The data type representing Double values. Please use the singleton DataTypes.DoubleType. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of the DoubleType is 8 bytes. static String json()  static String prettyJson()  static String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, simpleString, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() simpleString public static String simpleString() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() The default size of a value of the DoubleType is 8 bytes. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method DummySerializerInstance (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="DummySerializerInstance (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.serializer Class DummySerializerInstance Object org.apache.spark.serializer.SerializerInstance org.apache.spark.serializer.DummySerializerInstance public final class DummySerializerInstance extends SerializerInstance Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter. Our shuffle write path doesn't actually use this serializer (since we end up calling the `write() OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work around this, we pass a dummy no-op serializer. Field Summary Fields  Modifier and Type Field and Description static DummySerializerInstance INSTANCE  Method Summary Methods  Modifier and Type Method and Description <T> T deserialize(java.nio.ByteBuffer bytes, ClassLoader loader, scala.reflect.ClassTag<T> ev1)  <T> T deserialize(java.nio.ByteBuffer bytes, scala.reflect.ClassTag<T> ev1)  DeserializationStream deserializeStream(java.io.InputStream s)  <T> java.nio.ByteBuffer serialize(T t, scala.reflect.ClassTag<T> ev1)  SerializationStream serializeStream(java.io.OutputStream s)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail INSTANCE public static final DummySerializerInstance INSTANCE Method Detail serializeStream public SerializationStream serializeStream(java.io.OutputStream s) Specified by: serializeStream in class SerializerInstance serialize public <T> java.nio.ByteBuffer serialize(T t, scala.reflect.ClassTag<T> ev1) Specified by: serialize in class SerializerInstance deserializeStream public DeserializationStream deserializeStream(java.io.InputStream s) Specified by: deserializeStream in class SerializerInstance deserialize public <T> T deserialize(java.nio.ByteBuffer bytes, ClassLoader loader, scala.reflect.ClassTag<T> ev1) Specified by: deserialize in class SerializerInstance deserialize public <T> T deserialize(java.nio.ByteBuffer bytes, scala.reflect.ClassTag<T> ev1) Specified by: deserialize in class SerializerInstance Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Duration (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Duration (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming Class Duration Object org.apache.spark.streaming.Duration All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class Duration extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Duration(long millis)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  double div(Duration that)  abstract static boolean equals(Object that)  boolean greater(Duration that)  boolean greaterEq(Duration that)  boolean isMultipleOf(Duration that)  boolean isZero()  boolean less(Duration that)  boolean lessEq(Duration that)  Duration max(Duration that)  long milliseconds()  Duration min(Duration that)  Duration minus(Duration that)  Duration plus(Duration that)  String prettyPrint()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Duration times(int times)  String toFormattedString()  String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail Duration public Duration(long millis) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() less public boolean less(Duration that) lessEq public boolean lessEq(Duration that) greater public boolean greater(Duration that) greaterEq public boolean greaterEq(Duration that) plus public Duration plus(Duration that) minus public Duration minus(Duration that) times public Duration times(int times) div public double div(Duration that) isMultipleOf public boolean isMultipleOf(Duration that) min public Duration min(Duration that) max public Duration max(Duration that) isZero public boolean isZero() toString public String toString() Overrides: toString in class Object toFormattedString public String toFormattedString() milliseconds public long milliseconds() prettyPrint public String prettyPrint() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Durations (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Durations (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming Class Durations Object org.apache.spark.streaming.Durations public class Durations extends Object Constructor Summary Constructors  Constructor and Description Durations()  Method Summary Methods  Modifier and Type Method and Description static Duration milliseconds(long milliseconds)  static Duration minutes(long minutes)  static Duration seconds(long seconds)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Durations public Durations() Method Detail milliseconds public static Duration milliseconds(long milliseconds) Parameters:milliseconds - (undocumented) Returns:Duration representing given number of milliseconds. seconds public static Duration seconds(long seconds) Parameters:seconds - (undocumented) Returns:Duration representing given number of seconds. minutes public static Duration minutes(long minutes) Parameters:minutes - (undocumented) Returns:Duration representing given number of minutes. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EMLDAOptimizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EMLDAOptimizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class EMLDAOptimizer Object org.apache.spark.mllib.clustering.EMLDAOptimizer All Implemented Interfaces: LDAOptimizer public final class EMLDAOptimizer extends Object implements LDAOptimizer :: DeveloperApi :: Optimizer for EM algorithm which stores data + parameter graph, plus algorithm parameters. Currently, the underlying implementation uses Expectation-Maximization (EM), implemented according to the Asuncion et al. (2009) paper referenced below. References: - Original LDA paper (journal version): Blei, Ng, and Jordan. "Latent Dirichlet Allocation." JMLR, 2003. - This class implements their "smoothed" LDA model. - Paper which clearly explains several algorithms, including EM: Asuncion, Welling, Smyth, and Teh. "On Smoothing and Inference for Topic Models." UAI, 2009. Constructor Summary Constructors  Constructor and Description EMLDAOptimizer()  Method Summary Methods  Modifier and Type Method and Description boolean getKeepLastCheckpoint() If using checkpointing, this indicates whether to keep the last checkpoint (vs clean up). EMLDAOptimizer setKeepLastCheckpoint(boolean keepLastCheckpoint) If using checkpointing, this indicates whether to keep the last checkpoint (vs clean up). Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail EMLDAOptimizer public EMLDAOptimizer() Method Detail getKeepLastCheckpoint public boolean getKeepLastCheckpoint() If using checkpointing, this indicates whether to keep the last checkpoint (vs clean up). Returns:(undocumented) setKeepLastCheckpoint public EMLDAOptimizer setKeepLastCheckpoint(boolean keepLastCheckpoint) If using checkpointing, this indicates whether to keep the last checkpoint (vs clean up). Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. Note that checkpoints will be cleaned up via reference counting, regardless. Default: true Parameters:keepLastCheckpoint - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Edge (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Edge (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class Edge<ED> Object org.apache.spark.graphx.Edge<ED> All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Direct Known Subclasses: EdgeTriplet public class Edge<ED> extends Object implements scala.Serializable, scala.Product A single directed edge consisting of a source id, target id, and the data associated with the edge. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Edge(long srcId, long dstId, ED attr)  Method Summary Methods  Modifier and Type Method and Description ED attr()  abstract static boolean canEqual(Object that)  long dstId()  abstract static boolean equals(Object that)  long otherVertexId(long vid) Given one vertex in the edge return the other vertex. abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  EdgeDirection relativeDirection(long vid) Return the relative direction of the edge to the corresponding vertex. long srcId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail Edge public Edge(long srcId, long dstId, ED attr) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() srcId public long srcId() dstId public long dstId() attr public ED attr() otherVertexId public long otherVertexId(long vid) Given one vertex in the edge return the other vertex. Parameters:vid - the id one of the two vertices on the edge. Returns:the id of the other vertex on the edge. relativeDirection public EdgeDirection relativeDirection(long vid) Return the relative direction of the edge to the corresponding vertex. Parameters:vid - the id of one of the two vertices in the edge. Returns:the relative direction of the edge to the corresponding vertex. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EdgeActiveness (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EdgeActiveness (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method org.apache.spark.graphx.impl Enum EdgeActiveness Object Enum<EdgeActiveness> org.apache.spark.graphx.impl.EdgeActiveness All Implemented Interfaces: java.io.Serializable, Comparable<EdgeActiveness> public enum EdgeActiveness extends Enum<EdgeActiveness> Criteria for filtering edges based on activeness. For internal use only. Enum Constant Summary Enum Constants  Enum Constant and Description Both Both vertices must be active. DstOnly The destination vertex must be active. Either At least one vertex must be active. Neither Neither the source vertex nor the destination vertex need be active. SrcOnly The source vertex must be active. Method Summary Methods  Modifier and Type Method and Description static EdgeActiveness valueOf(String name) Returns the enum constant of this type with the specified name. static EdgeActiveness[] values() Returns an array containing the constants of this enum type, in the order they are declared. Methods inherited from class Enum compareTo, equals, getDeclaringClass, hashCode, name, ordinal, toString, valueOf Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Enum Constant Detail Neither public static final EdgeActiveness Neither Neither the source vertex nor the destination vertex need be active. SrcOnly public static final EdgeActiveness SrcOnly The source vertex must be active. DstOnly public static final EdgeActiveness DstOnly The destination vertex must be active. Both public static final EdgeActiveness Both Both vertices must be active. Either public static final EdgeActiveness Either At least one vertex must be active. Method Detail values public static EdgeActiveness[] values() Returns an array containing the constants of this enum type, in the order they are declared. This method may be used to iterate over the constants as follows: for (EdgeActiveness c : EdgeActiveness.values())   System.out.println(c); Returns:an array containing the constants of this enum type, in the order they are declared valueOf public static EdgeActiveness valueOf(String name) Returns the enum constant of this type with the specified name. The string must match exactly an identifier used to declare an enum constant in this type. (Extraneous whitespace characters are not permitted.) Parameters:name - the name of the enum constant to be returned. Returns:the enum constant with the specified name Throws: IllegalArgumentException - if this enum type has no constant with the specified name NullPointerException - if the argument is null Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method EdgeContext (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EdgeContext (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class EdgeContext<VD,ED,A> Object org.apache.spark.graphx.EdgeContext<VD,ED,A> Direct Known Subclasses: AggregatingEdgeContext public abstract class EdgeContext<VD,ED,A> extends Object Represents an edge along with its neighboring vertices and allows sending messages along the edge. Used in Graph.aggregateMessages(scala.Function1<org.apache.spark.graphx.EdgeContext<VD, ED, A>, scala.runtime.BoxedUnit>, scala.Function2<A, A, A>, org.apache.spark.graphx.TripletFields, scala.reflect.ClassTag<A>). Constructor Summary Constructors  Constructor and Description EdgeContext()  Method Summary Methods  Modifier and Type Method and Description abstract ED attr() The attribute associated with the edge. abstract VD dstAttr() The vertex attribute of the edge's destination vertex. abstract long dstId() The vertex id of the edge's destination vertex. abstract void sendToDst(A msg) Sends a message to the destination vertex. abstract void sendToSrc(A msg) Sends a message to the source vertex. abstract VD srcAttr() The vertex attribute of the edge's source vertex. abstract long srcId() The vertex id of the edge's source vertex. EdgeTriplet<VD,ED> toEdgeTriplet() Converts the edge and vertex properties into an EdgeTriplet for convenience. static <VD,ED,A> scala.Some<scala.Tuple5<Object,Object,VD,VD,ED>> unapply(EdgeContext<VD,ED,A> edge) Extractor mainly used for Graph#aggregateMessages*. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail EdgeContext public EdgeContext() Method Detail unapply public static <VD,ED,A> scala.Some<scala.Tuple5<Object,Object,VD,VD,ED>> unapply(EdgeContext<VD,ED,A> edge) Extractor mainly used for Graph#aggregateMessages*. Example: val messages = graph.aggregateMessages( case ctx @ EdgeContext(_, _, _, _, attr) => ctx.sendToDst(attr) , _ + _) Parameters:edge - (undocumented) Returns:(undocumented) srcId public abstract long srcId() The vertex id of the edge's source vertex. dstId public abstract long dstId() The vertex id of the edge's destination vertex. srcAttr public abstract VD srcAttr() The vertex attribute of the edge's source vertex. dstAttr public abstract VD dstAttr() The vertex attribute of the edge's destination vertex. attr public abstract ED attr() The attribute associated with the edge. sendToSrc public abstract void sendToSrc(A msg) Sends a message to the source vertex. sendToDst public abstract void sendToDst(A msg) Sends a message to the destination vertex. toEdgeTriplet public EdgeTriplet<VD,ED> toEdgeTriplet() Converts the edge and vertex properties into an EdgeTriplet for convenience. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EdgeDirection (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EdgeDirection (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class EdgeDirection Object org.apache.spark.graphx.EdgeDirection All Implemented Interfaces: java.io.Serializable public class EdgeDirection extends Object implements scala.Serializable The direction of a directed edge relative to a vertex. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static EdgeDirection Both() Edges originating from *and* arriving at a vertex of interest. static EdgeDirection Either() Edges originating from *or* arriving at a vertex of interest. boolean equals(Object o)  int hashCode()  static EdgeDirection In() Edges arriving at a vertex. static EdgeDirection Out() Edges originating from a vertex. EdgeDirection reverse() Reverse the direction of an edge. String toString()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Method Detail In public static final EdgeDirection In() Edges arriving at a vertex. Out public static final EdgeDirection Out() Edges originating from a vertex. Either public static final EdgeDirection Either() Edges originating from *or* arriving at a vertex of interest. Both public static final EdgeDirection Both() Edges originating from *and* arriving at a vertex of interest. reverse public EdgeDirection reverse() Reverse the direction of an edge. An in becomes out, out becomes in and both and either remain the same. Returns:(undocumented) toString public String toString() Overrides: toString in class Object equals public boolean equals(Object o) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EdgeRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EdgeRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class EdgeRDD<ED> Object org.apache.spark.rdd.RDD<Edge<ED>> org.apache.spark.graphx.EdgeRDD<ED> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: EdgeRDDImpl public abstract class EdgeRDD<ED> extends RDD<Edge<ED>> EdgeRDD[ED, VD] extends RDD[Edge[ED} by storing the edges in columnar format on each partition for performance. It may additionally store the vertex attributes associated with each edge to provide the triplet view. Shipping of the vertex attributes is managed by impl.ReplicatedVertexView. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description EdgeRDD(SparkContext sc, scala.collection.Seq<Dependency<?>> deps)  Method Summary Methods  Modifier and Type Method and Description static RDD<T> $plus$plus(RDD<T> other)  static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29)  static RDD<T> cache()  static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5)  static void checkpoint()  static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord)  static boolean coalesce$default$2()  static scala.Option<PartitionCoalescer> coalesce$default$3()  static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer)  static Object collect()  static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28)  scala.collection.Iterator<Edge<ED>> compute(Partition part, TaskContext context) :: DeveloperApi :: Implemented by subclasses to compute a given partition. static SparkContext context()  static long count()  static PartialResult<BoundedDouble> countApprox(long timeout, double confidence)  static double countApprox$default$2()  static long countApproxDistinct(double relativeSD)  static long countApproxDistinct(int p, int sp)  static double countApproxDistinct$default$1()  static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord)  static scala.math.Ordering<T> countByValue$default$1()  static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord)  static double countByValueApprox$default$2()  static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence)  static scala.collection.Seq<Dependency<?>> dependencies()  static RDD<T> distinct()  static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> distinct$default$2(int numPartitions)  static RDD<T> filter(scala.Function1<T,Object> f)  static T first()  static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4)  static T fold(T zeroValue, scala.Function2<T,T,T> op)  static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f)  static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f)  static <ED,VD> EdgeRDDImpl<ED,VD> fromEdges(RDD<Edge<ED>> edges, scala.reflect.ClassTag<ED> evidence$4, scala.reflect.ClassTag<VD> evidence$5) Creates an EdgeRDD from a set of edges. static scala.Option<String> getCheckpointFile()  static int getNumPartitions()  static StorageLevel getStorageLevel()  static RDD<Object> glom()  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord)  static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p)  static int id()  abstract <ED2,ED3> EdgeRDD<ED3> innerJoin(EdgeRDD<ED2> other, scala.Function4<Object,Object,ED,ED2,ED3> f, scala.reflect.ClassTag<ED2> evidence$2, scala.reflect.ClassTag<ED3> evidence$3) Inner joins this EdgeRDD with another EdgeRDD, assuming both are partitioned using the same PartitionStrategy. static RDD<T> intersection(RDD<T> other)  static RDD<T> intersection(RDD<T> other, int numPartitions)  static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner)  static boolean isCheckpointed()  static boolean isEmpty()  static scala.collection.Iterator<T> iterator(Partition split, TaskContext context)  static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f)  static RDD<T> localCheckpoint()  static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3)  static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6)  static <U> boolean mapPartitions$default$2()  static <U> boolean mapPartitionsInternal$default$2()  static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8)  static <U> boolean mapPartitionsWithIndex$default$2()  abstract <ED2> EdgeRDD<ED2> mapValues(scala.Function1<Edge<ED>,ED2> f, scala.reflect.ClassTag<ED2> evidence$1) Map the values in an edge partitioning preserving the structure but changing the values. static T max(scala.math.Ordering<T> ord)  static T min(scala.math.Ordering<T> ord)  static void name_$eq(String x$1)  static String name()  static scala.Option<Partitioner> partitioner()  static Partition[] partitions()  static RDD<T> persist()  static RDD<T> persist(StorageLevel newLevel)  static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding)  static RDD<String> pipe(String command)  static RDD<String> pipe(String command, scala.collection.Map<String,String> env)  static scala.collection.Map<String,String> pipe$default$2()  static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3()  static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4()  static boolean pipe$default$5()  static int pipe$default$6()  static String pipe$default$7()  static scala.collection.Seq<String> preferredLocations(Partition split)  static RDD<T>[] randomSplit(double[] weights, long seed)  static long randomSplit$default$2()  static T reduce(scala.Function2<T,T,T> f)  static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> repartition$default$2(int numPartitions)  abstract EdgeRDD<ED> reverse() Reverse all the edges in this RDD. static RDD<T> sample(boolean withReplacement, double fraction, long seed)  static long sample$default$3()  static void saveAsObjectFile(String path)  static void saveAsTextFile(String path)  static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec)  static RDD<T> setName(String _name)  static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag)  static <K> boolean sortBy$default$2()  static <K> int sortBy$default$3()  static SparkContext sparkContext()  static RDD<T> subtract(RDD<T> other)  static RDD<T> subtract(RDD<T> other, int numPartitions)  static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p)  static Object take(int num)  static Object takeOrdered(int num, scala.math.Ordering<T> ord)  static Object takeSample(boolean withReplacement, int num, long seed)  static long takeSample$default$3()  static String toDebugString()  static JavaRDD<T> toJavaRDD()  static scala.collection.Iterator<T> toLocalIterator()  static Object top(int num, scala.math.Ordering<T> ord)  static String toString()  static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30)  static <U> int treeAggregate$default$4(U zeroValue)  static T treeReduce(scala.Function2<T,T,T> f, int depth)  static int treeReduce$default$2()  static RDD<T> union(RDD<T> other)  static RDD<T> unpersist(boolean blocking)  static boolean unpersist$default$1()  static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27)  static RDD<scala.Tuple2<T,Object>> zipWithIndex()  static RDD<scala.Tuple2<T,Object>> zipWithUniqueId()  Methods inherited from class org.apache.spark.rdd.RDD aggregate, cache, cartesian, checkpoint, coalesce, collect, collect, context, count, countApprox, countApproxDistinct, countApproxDistinct, countByValue, countByValueApprox, dependencies, distinct, distinct, doubleRDDToDoubleRDDFunctions, filter, first, flatMap, fold, foreach, foreachPartition, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupBy, id, intersection, intersection, intersection, isCheckpointed, isEmpty, iterator, keyBy, localCheckpoint, map, mapPartitions, mapPartitionsWithIndex, max, min, name, numericRDDToDoubleRDDFunctions, partitioner, partitions, persist, persist, pipe, pipe, pipe, preferredLocations, randomSplit, rddToAsyncRDDActions, rddToOrderedRDDFunctions, rddToPairRDDFunctions, rddToSequenceFileRDDFunctions, reduce, repartition, sample, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortBy, sparkContext, subtract, subtract, subtract, take, takeOrdered, takeSample, toDebugString, toJavaRDD, toLocalIterator, top, toString, treeAggregate, treeReduce, union, unpersist, zip, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail EdgeRDD public EdgeRDD(SparkContext sc, scala.collection.Seq<Dependency<?>> deps) Method Detail fromEdges public static <ED,VD> EdgeRDDImpl<ED,VD> fromEdges(RDD<Edge<ED>> edges, scala.reflect.ClassTag<ED> evidence$4, scala.reflect.ClassTag<VD> evidence$5) Creates an EdgeRDD from a set of edges. Parameters:edges - (undocumented)evidence$4 - (undocumented)evidence$5 - (undocumented) Returns:(undocumented) partitioner public static scala.Option<Partitioner> partitioner() sparkContext public static SparkContext sparkContext() id public static int id() name public static String name() name_$eq public static void name_$eq(String x$1) setName public static RDD<T> setName(String _name) persist public static RDD<T> persist(StorageLevel newLevel) persist public static RDD<T> persist() cache public static RDD<T> cache() unpersist public static RDD<T> unpersist(boolean blocking) getStorageLevel public static StorageLevel getStorageLevel() dependencies public static final scala.collection.Seq<Dependency<?>> dependencies() partitions public static final Partition[] partitions() getNumPartitions public static final int getNumPartitions() preferredLocations public static final scala.collection.Seq<String> preferredLocations(Partition split) iterator public static final scala.collection.Iterator<T> iterator(Partition split, TaskContext context) map public static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3) flatMap public static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4) filter public static RDD<T> filter(scala.Function1<T,Object> f) distinct public static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord) distinct public static RDD<T> distinct() repartition public static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord) coalesce public static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord) sample public static RDD<T> sample(boolean withReplacement, double fraction, long seed) randomSplit public static RDD<T>[] randomSplit(double[] weights, long seed) takeSample public static Object takeSample(boolean withReplacement, int num, long seed) union public static RDD<T> union(RDD<T> other) $plus$plus public static RDD<T> $plus$plus(RDD<T> other) sortBy public static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag) intersection public static RDD<T> intersection(RDD<T> other) intersection public static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord) intersection public static RDD<T> intersection(RDD<T> other, int numPartitions) glom public static RDD<Object> glom() cartesian public static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord) pipe public static RDD<String> pipe(String command) pipe public static RDD<String> pipe(String command, scala.collection.Map<String,String> env) pipe public static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding) mapPartitions public static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6) mapPartitionsWithIndex public static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8) zip public static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27) foreach public static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f) foreachPartition public static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) collect public static Object collect() toLocalIterator public static scala.collection.Iterator<T> toLocalIterator() collect public static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28) subtract public static RDD<T> subtract(RDD<T> other) subtract public static RDD<T> subtract(RDD<T> other, int numPartitions) subtract public static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord) reduce public static T reduce(scala.Function2<T,T,T> f) treeReduce public static T treeReduce(scala.Function2<T,T,T> f, int depth) fold public static T fold(T zeroValue, scala.Function2<T,T,T> op) aggregate public static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29) treeAggregate public static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30) count public static long count() countApprox public static PartialResult<BoundedDouble> countApprox(long timeout, double confidence) countByValue public static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord) countByValueApprox public static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord) countApproxDistinct public static long countApproxDistinct(int p, int sp) countApproxDistinct public static long countApproxDistinct(double relativeSD) zipWithIndex public static RDD<scala.Tuple2<T,Object>> zipWithIndex() zipWithUniqueId public static RDD<scala.Tuple2<T,Object>> zipWithUniqueId() take public static Object take(int num) first public static T first() top public static Object top(int num, scala.math.Ordering<T> ord) takeOrdered public static Object takeOrdered(int num, scala.math.Ordering<T> ord) max public static T max(scala.math.Ordering<T> ord) min public static T min(scala.math.Ordering<T> ord) isEmpty public static boolean isEmpty() saveAsTextFile public static void saveAsTextFile(String path) saveAsTextFile public static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) saveAsObjectFile public static void saveAsObjectFile(String path) keyBy public static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f) checkpoint public static void checkpoint() localCheckpoint public static RDD<T> localCheckpoint() isCheckpointed public static boolean isCheckpointed() getCheckpointFile public static scala.Option<String> getCheckpointFile() context public static SparkContext context() toDebugString public static String toDebugString() toString public static String toString() toJavaRDD public static JavaRDD<T> toJavaRDD() sample$default$3 public static long sample$default$3() mapPartitionsWithIndex$default$2 public static <U> boolean mapPartitionsWithIndex$default$2() unpersist$default$1 public static boolean unpersist$default$1() distinct$default$2 public static scala.math.Ordering<T> distinct$default$2(int numPartitions) coalesce$default$2 public static boolean coalesce$default$2() coalesce$default$3 public static scala.Option<PartitionCoalescer> coalesce$default$3() coalesce$default$4 public static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer) repartition$default$2 public static scala.math.Ordering<T> repartition$default$2(int numPartitions) subtract$default$3 public static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p) intersection$default$3 public static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner) randomSplit$default$2 public static long randomSplit$default$2() sortBy$default$2 public static <K> boolean sortBy$default$2() sortBy$default$3 public static <K> int sortBy$default$3() mapPartitions$default$2 public static <U> boolean mapPartitions$default$2() groupBy$default$4 public static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p) pipe$default$2 public static scala.collection.Map<String,String> pipe$default$2() pipe$default$3 public static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3() pipe$default$4 public static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4() pipe$default$5 public static boolean pipe$default$5() pipe$default$6 public static int pipe$default$6() pipe$default$7 public static String pipe$default$7() treeReduce$default$2 public static int treeReduce$default$2() treeAggregate$default$4 public static <U> int treeAggregate$default$4(U zeroValue) countApprox$default$2 public static double countApprox$default$2() countByValue$default$1 public static scala.math.Ordering<T> countByValue$default$1() countByValueApprox$default$2 public static double countByValueApprox$default$2() countByValueApprox$default$3 public static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence) takeSample$default$3 public static long takeSample$default$3() countApproxDistinct$default$1 public static double countApproxDistinct$default$1() mapPartitionsInternal$default$2 public static <U> boolean mapPartitionsInternal$default$2() compute public scala.collection.Iterator<Edge<ED>> compute(Partition part, TaskContext context) Description copied from class: RDD :: DeveloperApi :: Implemented by subclasses to compute a given partition. Specified by: compute in class RDD<Edge<ED>> Parameters:part - (undocumented)context - (undocumented) Returns:(undocumented) mapValues public abstract <ED2> EdgeRDD<ED2> mapValues(scala.Function1<Edge<ED>,ED2> f, scala.reflect.ClassTag<ED2> evidence$1) Map the values in an edge partitioning preserving the structure but changing the values. Parameters:f - the function from an edge to a new edge valueevidence$1 - (undocumented) Returns:a new EdgeRDD containing the new edge values reverse public abstract EdgeRDD<ED> reverse() Reverse all the edges in this RDD. Returns:a new EdgeRDD containing all the edges reversed innerJoin public abstract <ED2,ED3> EdgeRDD<ED3> innerJoin(EdgeRDD<ED2> other, scala.Function4<Object,Object,ED,ED2,ED3> f, scala.reflect.ClassTag<ED2> evidence$2, scala.reflect.ClassTag<ED3> evidence$3) Inner joins this EdgeRDD with another EdgeRDD, assuming both are partitioned using the same PartitionStrategy. Parameters:other - the EdgeRDD to join withf - the join function applied to corresponding values of this and otherevidence$2 - (undocumented)evidence$3 - (undocumented) Returns:a new EdgeRDD containing only edges that appear in both this and other, with values supplied by f Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EdgeRDDImpl (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EdgeRDDImpl (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx.impl Class EdgeRDDImpl<ED,VD> Object org.apache.spark.rdd.RDD<Edge<ED>> org.apache.spark.graphx.EdgeRDD<ED> org.apache.spark.graphx.impl.EdgeRDDImpl<ED,VD> All Implemented Interfaces: java.io.Serializable public class EdgeRDDImpl<ED,VD> extends EdgeRDD<ED> See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static RDD<T> $plus$plus(RDD<T> other)  static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29)  EdgeRDDImpl<ED,VD> cache() Persists the edge partitions using `targetStorageLevel`, which defaults to MEMORY_ONLY. static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5)  void checkpoint()  static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord)  static boolean coalesce$default$2()  static scala.Option<PartitionCoalescer> coalesce$default$3()  static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer)  Edge<ED>[] collect()  static scala.collection.Iterator<Edge<ED>> compute(Partition part, TaskContext context)  static SparkContext context()  long count() The number of edges in the RDD. static PartialResult<BoundedDouble> countApprox(long timeout, double confidence)  static double countApprox$default$2()  static long countApproxDistinct(double relativeSD)  static long countApproxDistinct(int p, int sp)  static double countApproxDistinct$default$1()  static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord)  static scala.math.Ordering<T> countByValue$default$1()  static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord)  static double countByValueApprox$default$2()  static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence)  static scala.collection.Seq<Dependency<?>> dependencies()  static RDD<T> distinct()  static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> distinct$default$2(int numPartitions)  EdgeRDDImpl<ED,VD> filter(scala.Function1<EdgeTriplet<VD,ED>,Object> epred, scala.Function2<Object,VD,Object> vpred)  static T first()  static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4)  static T fold(T zeroValue, scala.Function2<T,T,T> op)  static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f)  static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f)  scala.Option<String> getCheckpointFile()  static int getNumPartitions()  StorageLevel getStorageLevel()  static RDD<Object> glom()  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord)  static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p)  static int id()  <ED2,ED3> EdgeRDDImpl<ED3,VD> innerJoin(EdgeRDD<ED2> other, scala.Function4<Object,Object,ED,ED2,ED3> f, scala.reflect.ClassTag<ED2> evidence$4, scala.reflect.ClassTag<ED3> evidence$5) Inner joins this EdgeRDD with another EdgeRDD, assuming both are partitioned using the same PartitionStrategy. static RDD<T> intersection(RDD<T> other)  static RDD<T> intersection(RDD<T> other, int numPartitions)  static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner)  boolean isCheckpointed()  static boolean isEmpty()  static scala.collection.Iterator<T> iterator(Partition split, TaskContext context)  static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f)  static RDD<T> localCheckpoint()  static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3)  <ED2,VD2> EdgeRDDImpl<ED2,VD2> mapEdgePartitions(scala.Function2<Object,org.apache.spark.graphx.impl.EdgePartition<ED,VD>,org.apache.spark.graphx.impl.EdgePartition<ED2,VD2>> f, scala.reflect.ClassTag<ED2> evidence$6, scala.reflect.ClassTag<VD2> evidence$7)  static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6)  static <U> boolean mapPartitions$default$2()  static <U> boolean mapPartitionsInternal$default$2()  static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8)  static <U> boolean mapPartitionsWithIndex$default$2()  <ED2> EdgeRDDImpl<ED2,VD> mapValues(scala.Function1<Edge<ED>,ED2> f, scala.reflect.ClassTag<ED2> evidence$3) Map the values in an edge partitioning preserving the structure but changing the values. static T max(scala.math.Ordering<T> ord)  static T min(scala.math.Ordering<T> ord)  static void name_$eq(String x$1)  static String name()  scala.Option<Partitioner> partitioner() If partitionsRDD already has a partitioner, use it. static Partition[] partitions()  RDD<scala.Tuple2<Object,org.apache.spark.graphx.impl.EdgePartition<ED,VD>>> partitionsRDD()  EdgeRDDImpl<ED,VD> persist(StorageLevel newLevel) Persists the edge partitions at the specified storage level, ignoring any existing target storage level. static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding)  static RDD<String> pipe(String command)  static RDD<String> pipe(String command, scala.collection.Map<String,String> env)  static scala.collection.Map<String,String> pipe$default$2()  static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3()  static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4()  static boolean pipe$default$5()  static int pipe$default$6()  static String pipe$default$7()  static scala.collection.Seq<String> preferredLocations(Partition split)  static RDD<T>[] randomSplit(double[] weights, long seed)  static long randomSplit$default$2()  static T reduce(scala.Function2<T,T,T> f)  static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> repartition$default$2(int numPartitions)  EdgeRDDImpl<ED,VD> reverse() Reverse all the edges in this RDD. static RDD<T> sample(boolean withReplacement, double fraction, long seed)  static long sample$default$3()  static void saveAsObjectFile(String path)  static void saveAsTextFile(String path)  static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec)  EdgeRDDImpl<ED,VD> setName(String _name)  static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag)  static <K> boolean sortBy$default$2()  static <K> int sortBy$default$3()  static SparkContext sparkContext()  static RDD<T> subtract(RDD<T> other)  static RDD<T> subtract(RDD<T> other, int numPartitions)  static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p)  static Object take(int num)  static Object takeOrdered(int num, scala.math.Ordering<T> ord)  static Object takeSample(boolean withReplacement, int num, long seed)  static long takeSample$default$3()  StorageLevel targetStorageLevel()  static String toDebugString()  static JavaRDD<T> toJavaRDD()  static scala.collection.Iterator<T> toLocalIterator()  static Object top(int num, scala.math.Ordering<T> ord)  static String toString()  static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30)  static <U> int treeAggregate$default$4(U zeroValue)  static T treeReduce(scala.Function2<T,T,T> f, int depth)  static int treeReduce$default$2()  static RDD<T> union(RDD<T> other)  EdgeRDDImpl<ED,VD> unpersist(boolean blocking)  static boolean unpersist$default$1()  static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27)  static RDD<scala.Tuple2<T,Object>> zipWithIndex()  static RDD<scala.Tuple2<T,Object>> zipWithUniqueId()  Methods inherited from class org.apache.spark.graphx.EdgeRDD collect, compute, filter, fromEdges, persist Methods inherited from class org.apache.spark.rdd.RDD aggregate, cartesian, coalesce, collect, context, countApprox, countApproxDistinct, countApproxDistinct, countByValue, countByValueApprox, dependencies, distinct, distinct, doubleRDDToDoubleRDDFunctions, filter, first, flatMap, fold, foreach, foreachPartition, getNumPartitions, glom, groupBy, groupBy, groupBy, id, intersection, intersection, intersection, isEmpty, iterator, keyBy, localCheckpoint, map, mapPartitions, mapPartitionsWithIndex, max, min, name, numericRDDToDoubleRDDFunctions, partitions, persist, pipe, pipe, pipe, preferredLocations, randomSplit, rddToAsyncRDDActions, rddToOrderedRDDFunctions, rddToPairRDDFunctions, rddToSequenceFileRDDFunctions, reduce, repartition, sample, saveAsObjectFile, saveAsTextFile, saveAsTextFile, sortBy, sparkContext, subtract, subtract, subtract, take, takeOrdered, takeSample, toDebugString, toJavaRDD, toLocalIterator, top, toString, treeAggregate, treeReduce, union, zip, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Method Detail sparkContext public static SparkContext sparkContext() id public static int id() name public static String name() name_$eq public static void name_$eq(String x$1) dependencies public static final scala.collection.Seq<Dependency<?>> dependencies() partitions public static final Partition[] partitions() getNumPartitions public static final int getNumPartitions() preferredLocations public static final scala.collection.Seq<String> preferredLocations(Partition split) iterator public static final scala.collection.Iterator<T> iterator(Partition split, TaskContext context) map public static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3) flatMap public static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4) distinct public static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord) distinct public static RDD<T> distinct() repartition public static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord) coalesce public static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord) sample public static RDD<T> sample(boolean withReplacement, double fraction, long seed) randomSplit public static RDD<T>[] randomSplit(double[] weights, long seed) takeSample public static Object takeSample(boolean withReplacement, int num, long seed) union public static RDD<T> union(RDD<T> other) $plus$plus public static RDD<T> $plus$plus(RDD<T> other) sortBy public static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag) intersection public static RDD<T> intersection(RDD<T> other) intersection public static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord) intersection public static RDD<T> intersection(RDD<T> other, int numPartitions) glom public static RDD<Object> glom() cartesian public static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord) pipe public static RDD<String> pipe(String command) pipe public static RDD<String> pipe(String command, scala.collection.Map<String,String> env) pipe public static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding) mapPartitions public static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6) mapPartitionsWithIndex public static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8) zip public static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27) foreach public static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f) foreachPartition public static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) toLocalIterator public static scala.collection.Iterator<T> toLocalIterator() subtract public static RDD<T> subtract(RDD<T> other) subtract public static RDD<T> subtract(RDD<T> other, int numPartitions) subtract public static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord) reduce public static T reduce(scala.Function2<T,T,T> f) treeReduce public static T treeReduce(scala.Function2<T,T,T> f, int depth) fold public static T fold(T zeroValue, scala.Function2<T,T,T> op) aggregate public static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29) treeAggregate public static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30) countApprox public static PartialResult<BoundedDouble> countApprox(long timeout, double confidence) countByValue public static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord) countByValueApprox public static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord) countApproxDistinct public static long countApproxDistinct(int p, int sp) countApproxDistinct public static long countApproxDistinct(double relativeSD) zipWithIndex public static RDD<scala.Tuple2<T,Object>> zipWithIndex() zipWithUniqueId public static RDD<scala.Tuple2<T,Object>> zipWithUniqueId() take public static Object take(int num) first public static T first() top public static Object top(int num, scala.math.Ordering<T> ord) takeOrdered public static Object takeOrdered(int num, scala.math.Ordering<T> ord) max public static T max(scala.math.Ordering<T> ord) min public static T min(scala.math.Ordering<T> ord) isEmpty public static boolean isEmpty() saveAsTextFile public static void saveAsTextFile(String path) saveAsTextFile public static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) saveAsObjectFile public static void saveAsObjectFile(String path) keyBy public static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f) localCheckpoint public static RDD<T> localCheckpoint() context public static SparkContext context() toDebugString public static String toDebugString() toString public static String toString() toJavaRDD public static JavaRDD<T> toJavaRDD() sample$default$3 public static long sample$default$3() mapPartitionsWithIndex$default$2 public static <U> boolean mapPartitionsWithIndex$default$2() unpersist$default$1 public static boolean unpersist$default$1() distinct$default$2 public static scala.math.Ordering<T> distinct$default$2(int numPartitions) coalesce$default$2 public static boolean coalesce$default$2() coalesce$default$3 public static scala.Option<PartitionCoalescer> coalesce$default$3() coalesce$default$4 public static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer) repartition$default$2 public static scala.math.Ordering<T> repartition$default$2(int numPartitions) subtract$default$3 public static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p) intersection$default$3 public static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner) randomSplit$default$2 public static long randomSplit$default$2() sortBy$default$2 public static <K> boolean sortBy$default$2() sortBy$default$3 public static <K> int sortBy$default$3() mapPartitions$default$2 public static <U> boolean mapPartitions$default$2() groupBy$default$4 public static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p) pipe$default$2 public static scala.collection.Map<String,String> pipe$default$2() pipe$default$3 public static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3() pipe$default$4 public static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4() pipe$default$5 public static boolean pipe$default$5() pipe$default$6 public static int pipe$default$6() pipe$default$7 public static String pipe$default$7() treeReduce$default$2 public static int treeReduce$default$2() treeAggregate$default$4 public static <U> int treeAggregate$default$4(U zeroValue) countApprox$default$2 public static double countApprox$default$2() countByValue$default$1 public static scala.math.Ordering<T> countByValue$default$1() countByValueApprox$default$2 public static double countByValueApprox$default$2() countByValueApprox$default$3 public static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence) takeSample$default$3 public static long takeSample$default$3() countApproxDistinct$default$1 public static double countApproxDistinct$default$1() mapPartitionsInternal$default$2 public static <U> boolean mapPartitionsInternal$default$2() compute public static scala.collection.Iterator<Edge<ED>> compute(Partition part, TaskContext context) partitionsRDD public RDD<scala.Tuple2<Object,org.apache.spark.graphx.impl.EdgePartition<ED,VD>>> partitionsRDD() targetStorageLevel public StorageLevel targetStorageLevel() setName public EdgeRDDImpl<ED,VD> setName(String _name) Overrides: setName in class EdgeRDD<ED> partitioner public scala.Option<Partitioner> partitioner() If partitionsRDD already has a partitioner, use it. Otherwise assume that the PartitionIDs in partitionsRDD correspond to the actual partitions and create a new partitioner that allows co-partitioning with partitionsRDD. Overrides: partitioner in class EdgeRDD<ED> Returns:(undocumented) collect public Edge<ED>[] collect() Overrides: collect in class EdgeRDD<ED> persist public EdgeRDDImpl<ED,VD> persist(StorageLevel newLevel) Persists the edge partitions at the specified storage level, ignoring any existing target storage level. Overrides: persist in class EdgeRDD<ED> Parameters:newLevel - (undocumented) Returns:(undocumented) unpersist public EdgeRDDImpl<ED,VD> unpersist(boolean blocking) Overrides: unpersist in class EdgeRDD<ED> cache public EdgeRDDImpl<ED,VD> cache() Persists the edge partitions using `targetStorageLevel`, which defaults to MEMORY_ONLY. Overrides: cache in class EdgeRDD<ED> getStorageLevel public StorageLevel getStorageLevel() Overrides: getStorageLevel in class EdgeRDD<ED> checkpoint public void checkpoint() Overrides: checkpoint in class EdgeRDD<ED> isCheckpointed public boolean isCheckpointed() Overrides: isCheckpointed in class EdgeRDD<ED> getCheckpointFile public scala.Option<String> getCheckpointFile() Overrides: getCheckpointFile in class EdgeRDD<ED> count public long count() The number of edges in the RDD. Overrides: count in class EdgeRDD<ED> mapValues public <ED2> EdgeRDDImpl<ED2,VD> mapValues(scala.Function1<Edge<ED>,ED2> f, scala.reflect.ClassTag<ED2> evidence$3) Description copied from class: EdgeRDD Map the values in an edge partitioning preserving the structure but changing the values. Specified by: mapValues in class EdgeRDD<ED> Parameters:f - the function from an edge to a new edge valueevidence$3 - (undocumented) Returns:a new EdgeRDD containing the new edge values reverse public EdgeRDDImpl<ED,VD> reverse() Description copied from class: EdgeRDD Reverse all the edges in this RDD. Specified by: reverse in class EdgeRDD<ED> Returns:a new EdgeRDD containing all the edges reversed filter public EdgeRDDImpl<ED,VD> filter(scala.Function1<EdgeTriplet<VD,ED>,Object> epred, scala.Function2<Object,VD,Object> vpred) innerJoin public <ED2,ED3> EdgeRDDImpl<ED3,VD> innerJoin(EdgeRDD<ED2> other, scala.Function4<Object,Object,ED,ED2,ED3> f, scala.reflect.ClassTag<ED2> evidence$4, scala.reflect.ClassTag<ED3> evidence$5) Description copied from class: EdgeRDD Inner joins this EdgeRDD with another EdgeRDD, assuming both are partitioned using the same PartitionStrategy. Specified by: innerJoin in class EdgeRDD<ED> Parameters:other - the EdgeRDD to join withf - the join function applied to corresponding values of this and otherevidence$4 - (undocumented)evidence$5 - (undocumented) Returns:a new EdgeRDD containing only edges that appear in both this and other, with values supplied by f mapEdgePartitions public <ED2,VD2> EdgeRDDImpl<ED2,VD2> mapEdgePartitions(scala.Function2<Object,org.apache.spark.graphx.impl.EdgePartition<ED,VD>,org.apache.spark.graphx.impl.EdgePartition<ED2,VD2>> f, scala.reflect.ClassTag<ED2> evidence$6, scala.reflect.ClassTag<VD2> evidence$7) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EdgeTriplet (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EdgeTriplet (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class EdgeTriplet<VD,ED> Object org.apache.spark.graphx.Edge<ED> org.apache.spark.graphx.EdgeTriplet<VD,ED> All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class EdgeTriplet<VD,ED> extends Edge<ED> An edge triplet represents an edge along with the vertex attributes of its neighboring vertices. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description EdgeTriplet()  Method Summary Methods  Modifier and Type Method and Description VD dstAttr() The destination vertex attribute VD otherVertexAttr(long vid) Given one vertex in the edge return the other vertex. VD srcAttr() The source vertex attribute String toString()  scala.Tuple3<scala.Tuple2<Object,VD>,scala.Tuple2<Object,VD>,ED> toTuple()  VD vertexAttr(long vid) Get the vertex object for the given vertex in the edge. Methods inherited from class org.apache.spark.graphx.Edge attr, canEqual, dstId, equals, otherVertexId, productArity, productElement, productIterator, productPrefix, relativeDirection, srcId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail EdgeTriplet public EdgeTriplet() Method Detail srcAttr public VD srcAttr() The source vertex attribute Returns:(undocumented) dstAttr public VD dstAttr() The destination vertex attribute Returns:(undocumented) otherVertexAttr public VD otherVertexAttr(long vid) Given one vertex in the edge return the other vertex. Parameters:vid - the id one of the two vertices on the edge Returns:the attribute for the other vertex on the edge vertexAttr public VD vertexAttr(long vid) Get the vertex object for the given vertex in the edge. Parameters:vid - the id of one of the two vertices on the edge Returns:the attr for the vertex with that id toString public String toString() Overrides: toString in class Object toTuple public scala.Tuple3<scala.Tuple2<Object,VD>,scala.Tuple2<Object,VD>,ED> toTuple() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EigenValueDecomposition (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EigenValueDecomposition (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg Class EigenValueDecomposition Object org.apache.spark.mllib.linalg.EigenValueDecomposition public class EigenValueDecomposition extends Object Compute eigen-decomposition. Constructor Summary Constructors  Constructor and Description EigenValueDecomposition()  Method Summary Methods  Modifier and Type Method and Description static scala.Tuple2<breeze.linalg.DenseVector<Object>,breeze.linalg.DenseMatrix<Object>> symmetricEigs(scala.Function1<breeze.linalg.DenseVector<Object>,breeze.linalg.DenseVector<Object>> mul, int n, int k, double tol, int maxIterations) Compute the leading k eigenvalues and eigenvectors on a symmetric square matrix using ARPACK. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail EigenValueDecomposition public EigenValueDecomposition() Method Detail symmetricEigs public static scala.Tuple2<breeze.linalg.DenseVector<Object>,breeze.linalg.DenseMatrix<Object>> symmetricEigs(scala.Function1<breeze.linalg.DenseVector<Object>,breeze.linalg.DenseVector<Object>> mul, int n, int k, double tol, int maxIterations) Compute the leading k eigenvalues and eigenvectors on a symmetric square matrix using ARPACK. The caller needs to ensure that the input matrix is real symmetric. This function requires memory for n*(4*k+4) doubles. Parameters:mul - a function that multiplies the symmetric matrix with a DenseVector.n - dimension of the square matrix (maximum Int.MaxValue).k - number of leading eigenvalues required, 0 < k < n.tol - tolerance of the eigs computation.maxIterations - the maximum number of Arnoldi update iterations. Returns:a dense vector of eigenvalues in descending order and a dense matrix of eigenvectors (columns of the matrix). Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ElementwiseProduct (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ElementwiseProduct (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class ElementwiseProduct Object org.apache.spark.mllib.feature.ElementwiseProduct All Implemented Interfaces: java.io.Serializable, VectorTransformer public class ElementwiseProduct extends Object implements VectorTransformer Outputs the Hadamard product (i.e., the element-wise product) of each input vector with a provided "weight" vector. In other words, it scales each column of the dataset by a scalar multiplier. param: scalingVec The values used to scale the reference vector's individual components. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ElementwiseProduct(Vector scalingVec)  Method Summary Methods  Modifier and Type Method and Description Vector scalingVec()  Vector transform(Vector vector) Does the hadamard product transformation. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.feature.VectorTransformer transform, transform Constructor Detail ElementwiseProduct public ElementwiseProduct(Vector scalingVec) Method Detail scalingVec public Vector scalingVec() transform public Vector transform(Vector vector) Does the hadamard product transformation. Specified by: transform in interface VectorTransformer Parameters:vector - vector to be transformed. Returns:transformed vector. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Encoder (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Encoder (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Interface Encoder<T> All Superinterfaces: java.io.Serializable public interface Encoder<T> extends scala.Serializable :: Experimental :: Used to convert a JVM object of type T to and from the internal Spark SQL representation. == Scala == Encoders are generally created automatically through implicits from a SparkSession, or can be explicitly created by calling static methods on Encoders. import spark.implicits._ val ds = Seq(1, 2, 3).toDS() // implicitly provided (spark.implicits.newIntEncoder) == Java == Encoders are specified by calling static methods on Encoders. List<String> data = Arrays.asList("abc", "abc", "xyz"); Dataset<String> ds = context.createDataset(data, Encoders.STRING()); Encoders can be composed into tuples: Encoder<Tuple2<Integer, String>> encoder2 = Encoders.tuple(Encoders.INT(), Encoders.STRING()); List<Tuple2<Integer, String>> data2 = Arrays.asList(new scala.Tuple2(1, "a"); Dataset<Tuple2<Integer, String>> ds2 = context.createDataset(data2, encoder2); Or constructed from Java Beans: Encoders.bean(MyClass.class); == Implementation == - Encoders are not required to be thread-safe and thus they do not need to use locks to guard against concurrent access if they reuse internal buffers to improve performance. Since: 1.6.0 Method Summary Methods  Modifier and Type Method and Description scala.reflect.ClassTag<T> clsTag() A ClassTag that can be used to construct and Array to contain a collection of `T`. StructType schema() Returns the schema of encoding this type of object as a Row. Method Detail schema StructType schema() Returns the schema of encoding this type of object as a Row. clsTag scala.reflect.ClassTag<T> clsTag() A ClassTag that can be used to construct and Array to contain a collection of `T`. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Encoders (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Encoders (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class Encoders Object org.apache.spark.sql.Encoders public class Encoders extends Object :: Experimental :: Methods for creating an Encoder. Since: 1.6.0 Constructor Summary Constructors  Constructor and Description Encoders()  Method Summary Methods  Modifier and Type Method and Description static <T> Encoder<T> bean(Class<T> beanClass) Creates an encoder for Java Bean of type T. static Encoder<byte[]> BINARY() An encoder for arrays of bytes. static Encoder<Boolean> BOOLEAN() An encoder for nullable boolean type. static Encoder<Byte> BYTE() An encoder for nullable byte type. static Encoder<java.sql.Date> DATE() An encoder for nullable date type. static Encoder<java.math.BigDecimal> DECIMAL() An encoder for nullable decimal type. static Encoder<Double> DOUBLE() An encoder for nullable double type. static Encoder<Float> FLOAT() An encoder for nullable float type. static Encoder<Integer> INT() An encoder for nullable int type. static <T> Encoder<T> javaSerialization(Class<T> clazz) Creates an encoder that serializes objects of type T using generic Java serialization. static <T> Encoder<T> javaSerialization(scala.reflect.ClassTag<T> evidence$2) (Scala-specific) Creates an encoder that serializes objects of type T using generic Java serialization. static <T> Encoder<T> kryo(Class<T> clazz) Creates an encoder that serializes objects of type T using Kryo. static <T> Encoder<T> kryo(scala.reflect.ClassTag<T> evidence$1) (Scala-specific) Creates an encoder that serializes objects of type T using Kryo. static Encoder<Long> LONG() An encoder for nullable long type. static <T extends scala.Product> Encoder<T> product(scala.reflect.api.TypeTags.TypeTag<T> evidence$5) An encoder for Scala's product type (tuples, case classes, etc). static Encoder<Object> scalaBoolean() An encoder for Scala's primitive boolean type. static Encoder<Object> scalaByte() An encoder for Scala's primitive byte type. static Encoder<Object> scalaDouble() An encoder for Scala's primitive double type. static Encoder<Object> scalaFloat() An encoder for Scala's primitive float type. static Encoder<Object> scalaInt() An encoder for Scala's primitive int type. static Encoder<Object> scalaLong() An encoder for Scala's primitive long type. static Encoder<Object> scalaShort() An encoder for Scala's primitive short type. static Encoder<Short> SHORT() An encoder for nullable short type. static Encoder<String> STRING() An encoder for nullable string type. static Encoder<java.sql.Timestamp> TIMESTAMP() An encoder for nullable timestamp type. static <T1,T2> Encoder<scala.Tuple2<T1,T2>> tuple(Encoder<T1> e1, Encoder<T2> e2) An encoder for 2-ary tuples. static <T1,T2,T3> Encoder<scala.Tuple3<T1,T2,T3>> tuple(Encoder<T1> e1, Encoder<T2> e2, Encoder<T3> e3) An encoder for 3-ary tuples. static <T1,T2,T3,T4> Encoder<scala.Tuple4<T1,T2,T3,T4>> tuple(Encoder<T1> e1, Encoder<T2> e2, Encoder<T3> e3, Encoder<T4> e4) An encoder for 4-ary tuples. static <T1,T2,T3,T4,T5> Encoder<scala.Tuple5<T1,T2,T3,T4,T5>> tuple(Encoder<T1> e1, Encoder<T2> e2, Encoder<T3> e3, Encoder<T4> e4, Encoder<T5> e5) An encoder for 5-ary tuples. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Encoders public Encoders() Method Detail BOOLEAN public static Encoder<Boolean> BOOLEAN() An encoder for nullable boolean type. The Scala primitive encoder is available as scalaBoolean. Returns:(undocumented)Since: 1.6.0 BYTE public static Encoder<Byte> BYTE() An encoder for nullable byte type. The Scala primitive encoder is available as scalaByte. Returns:(undocumented)Since: 1.6.0 SHORT public static Encoder<Short> SHORT() An encoder for nullable short type. The Scala primitive encoder is available as scalaShort. Returns:(undocumented)Since: 1.6.0 INT public static Encoder<Integer> INT() An encoder for nullable int type. The Scala primitive encoder is available as scalaInt. Returns:(undocumented)Since: 1.6.0 LONG public static Encoder<Long> LONG() An encoder for nullable long type. The Scala primitive encoder is available as scalaLong. Returns:(undocumented)Since: 1.6.0 FLOAT public static Encoder<Float> FLOAT() An encoder for nullable float type. The Scala primitive encoder is available as scalaFloat. Returns:(undocumented)Since: 1.6.0 DOUBLE public static Encoder<Double> DOUBLE() An encoder for nullable double type. The Scala primitive encoder is available as scalaDouble. Returns:(undocumented)Since: 1.6.0 STRING public static Encoder<String> STRING() An encoder for nullable string type. Returns:(undocumented)Since: 1.6.0 DECIMAL public static Encoder<java.math.BigDecimal> DECIMAL() An encoder for nullable decimal type. Returns:(undocumented)Since: 1.6.0 DATE public static Encoder<java.sql.Date> DATE() An encoder for nullable date type. Returns:(undocumented)Since: 1.6.0 TIMESTAMP public static Encoder<java.sql.Timestamp> TIMESTAMP() An encoder for nullable timestamp type. Returns:(undocumented)Since: 1.6.0 BINARY public static Encoder<byte[]> BINARY() An encoder for arrays of bytes. Returns:(undocumented)Since: 1.6.1 bean public static <T> Encoder<T> bean(Class<T> beanClass) Creates an encoder for Java Bean of type T. T must be publicly accessible. supported types for java bean field: - primitive types: boolean, int, double, etc. - boxed types: Boolean, Integer, Double, etc. - String - java.math.BigDecimal - time related: java.sql.Date, java.sql.Timestamp - collection types: only array and java.util.List currently, map support is in progress - nested java bean. Parameters:beanClass - (undocumented) Returns:(undocumented)Since: 1.6.0 kryo public static <T> Encoder<T> kryo(scala.reflect.ClassTag<T> evidence$1) (Scala-specific) Creates an encoder that serializes objects of type T using Kryo. This encoder maps T into a single byte array (binary) field. T must be publicly accessible. Parameters:evidence$1 - (undocumented) Returns:(undocumented)Since: 1.6.0 kryo public static <T> Encoder<T> kryo(Class<T> clazz) Creates an encoder that serializes objects of type T using Kryo. This encoder maps T into a single byte array (binary) field. T must be publicly accessible. Parameters:clazz - (undocumented) Returns:(undocumented)Since: 1.6.0 javaSerialization public static <T> Encoder<T> javaSerialization(scala.reflect.ClassTag<T> evidence$2) (Scala-specific) Creates an encoder that serializes objects of type T using generic Java serialization. This encoder maps T into a single byte array (binary) field. Note that this is extremely inefficient and should only be used as the last resort. T must be publicly accessible. Parameters:evidence$2 - (undocumented) Returns:(undocumented)Since: 1.6.0 javaSerialization public static <T> Encoder<T> javaSerialization(Class<T> clazz) Creates an encoder that serializes objects of type T using generic Java serialization. This encoder maps T into a single byte array (binary) field. Note that this is extremely inefficient and should only be used as the last resort. T must be publicly accessible. Parameters:clazz - (undocumented) Returns:(undocumented)Since: 1.6.0 tuple public static <T1,T2> Encoder<scala.Tuple2<T1,T2>> tuple(Encoder<T1> e1, Encoder<T2> e2) An encoder for 2-ary tuples. Parameters:e1 - (undocumented)e2 - (undocumented) Returns:(undocumented)Since: 1.6.0 tuple public static <T1,T2,T3> Encoder<scala.Tuple3<T1,T2,T3>> tuple(Encoder<T1> e1, Encoder<T2> e2, Encoder<T3> e3) An encoder for 3-ary tuples. Parameters:e1 - (undocumented)e2 - (undocumented)e3 - (undocumented) Returns:(undocumented)Since: 1.6.0 tuple public static <T1,T2,T3,T4> Encoder<scala.Tuple4<T1,T2,T3,T4>> tuple(Encoder<T1> e1, Encoder<T2> e2, Encoder<T3> e3, Encoder<T4> e4) An encoder for 4-ary tuples. Parameters:e1 - (undocumented)e2 - (undocumented)e3 - (undocumented)e4 - (undocumented) Returns:(undocumented)Since: 1.6.0 tuple public static <T1,T2,T3,T4,T5> Encoder<scala.Tuple5<T1,T2,T3,T4,T5>> tuple(Encoder<T1> e1, Encoder<T2> e2, Encoder<T3> e3, Encoder<T4> e4, Encoder<T5> e5) An encoder for 5-ary tuples. Parameters:e1 - (undocumented)e2 - (undocumented)e3 - (undocumented)e4 - (undocumented)e5 - (undocumented) Returns:(undocumented)Since: 1.6.0 product public static <T extends scala.Product> Encoder<T> product(scala.reflect.api.TypeTags.TypeTag<T> evidence$5) An encoder for Scala's product type (tuples, case classes, etc). Parameters:evidence$5 - (undocumented) Returns:(undocumented)Since: 2.0.0 scalaInt public static Encoder<Object> scalaInt() An encoder for Scala's primitive int type. Returns:(undocumented)Since: 2.0.0 scalaLong public static Encoder<Object> scalaLong() An encoder for Scala's primitive long type. Returns:(undocumented)Since: 2.0.0 scalaDouble public static Encoder<Object> scalaDouble() An encoder for Scala's primitive double type. Returns:(undocumented)Since: 2.0.0 scalaFloat public static Encoder<Object> scalaFloat() An encoder for Scala's primitive float type. Returns:(undocumented)Since: 2.0.0 scalaByte public static Encoder<Object> scalaByte() An encoder for Scala's primitive byte type. Returns:(undocumented)Since: 2.0.0 scalaShort public static Encoder<Object> scalaShort() An encoder for Scala's primitive short type. Returns:(undocumented)Since: 2.0.0 scalaBoolean public static Encoder<Object> scalaBoolean() An encoder for Scala's primitive boolean type. Returns:(undocumented)Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EnsembleCombiningStrategy (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EnsembleCombiningStrategy (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.configuration Class EnsembleCombiningStrategy Object org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy public class EnsembleCombiningStrategy extends Object Enum to select ensemble combining strategy for base learners Constructor Summary Constructors  Constructor and Description EnsembleCombiningStrategy()  Method Summary Methods  Modifier and Type Method and Description static scala.Enumeration.Value apply(int x)  static scala.Enumeration.Value Average()  static int maxId()  static scala.Enumeration.Value Sum()  static String toString()  static scala.Enumeration.ValueSet values()  static scala.Enumeration.Value Vote()  static scala.Enumeration.Value withName(String s)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail EnsembleCombiningStrategy public EnsembleCombiningStrategy() Method Detail Average public static scala.Enumeration.Value Average() Sum public static scala.Enumeration.Value Sum() Vote public static scala.Enumeration.Value Vote() toString public static String toString() values public static scala.Enumeration.ValueSet values() maxId public static final int maxId() apply public static final scala.Enumeration.Value apply(int x) withName public static final scala.Enumeration.Value withName(String s) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EnsembleModelReadWrite.EnsembleNodeData$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EnsembleModelReadWrite.EnsembleNodeData$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class EnsembleModelReadWrite.EnsembleNodeData$ Object org.apache.spark.ml.tree.EnsembleModelReadWrite.EnsembleNodeData$ All Implemented Interfaces: java.io.Serializable Enclosing class: EnsembleModelReadWrite public static class EnsembleModelReadWrite.EnsembleNodeData$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static EnsembleModelReadWrite.EnsembleNodeData$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description EnsembleModelReadWrite.EnsembleNodeData$()  Method Summary Methods  Modifier and Type Method and Description scala.collection.Seq<EnsembleModelReadWrite.EnsembleNodeData> build(org.apache.spark.ml.tree.DecisionTreeModel tree, int treeID) Create EnsembleModelReadWrite.EnsembleNodeData instances for the given tree. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final EnsembleModelReadWrite.EnsembleNodeData$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail EnsembleModelReadWrite.EnsembleNodeData$ public EnsembleModelReadWrite.EnsembleNodeData$() Method Detail build public scala.collection.Seq<EnsembleModelReadWrite.EnsembleNodeData> build(org.apache.spark.ml.tree.DecisionTreeModel tree, int treeID) Create EnsembleModelReadWrite.EnsembleNodeData instances for the given tree. Parameters:tree - (undocumented)treeID - (undocumented) Returns:Sequence of nodes for this tree Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EnsembleModelReadWrite.EnsembleNodeData (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EnsembleModelReadWrite.EnsembleNodeData (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class EnsembleModelReadWrite.EnsembleNodeData Object org.apache.spark.ml.tree.EnsembleModelReadWrite.EnsembleNodeData All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: EnsembleModelReadWrite public static class EnsembleModelReadWrite.EnsembleNodeData extends Object implements scala.Product, scala.Serializable Info for one Node in a tree ensemble param: treeID Tree index param: nodeData Data for this node See Also:Serialized Form Constructor Summary Constructors  Constructor and Description EnsembleModelReadWrite.EnsembleNodeData(int treeID, DecisionTreeModelReadWrite.NodeData nodeData)  Method Summary Methods  Modifier and Type Method and Description DecisionTreeModelReadWrite.NodeData nodeData()  int treeID()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail EnsembleModelReadWrite.EnsembleNodeData public EnsembleModelReadWrite.EnsembleNodeData(int treeID, DecisionTreeModelReadWrite.NodeData nodeData) Method Detail treeID public int treeID() nodeData public DecisionTreeModelReadWrite.NodeData nodeData() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EnsembleModelReadWrite (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EnsembleModelReadWrite (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class EnsembleModelReadWrite Object org.apache.spark.ml.tree.EnsembleModelReadWrite public class EnsembleModelReadWrite extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  EnsembleModelReadWrite.EnsembleNodeData Info for one Node in a tree ensemble static class  EnsembleModelReadWrite.EnsembleNodeData$  Constructor Summary Constructors  Constructor and Description EnsembleModelReadWrite()  Method Summary Methods  Modifier and Type Method and Description static scala.Tuple3<org.apache.spark.ml.util.DefaultParamsReader.Metadata,scala.Tuple2<org.apache.spark.ml.util.DefaultParamsReader.Metadata,Node>[],double[]> loadImpl(String path, SparkSession sql, String className, String treeClassName) Helper method for loading a tree ensemble from disk. static <M extends Params> void saveImpl(M instance, String path, SparkSession sql, org.json4s.JsonAST.JObject extraMetadata) Helper method for saving a tree ensemble to disk. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail EnsembleModelReadWrite public EnsembleModelReadWrite() Method Detail saveImpl public static <M extends Params> void saveImpl(M instance, String path, SparkSession sql, org.json4s.JsonAST.JObject extraMetadata) Helper method for saving a tree ensemble to disk. Parameters:instance - Tree ensemble modelpath - Path to which to save the ensemble model.extraMetadata - Metadata such as numFeatures, numClasses, numTrees.sql - (undocumented) loadImpl public static scala.Tuple3<org.apache.spark.ml.util.DefaultParamsReader.Metadata,scala.Tuple2<org.apache.spark.ml.util.DefaultParamsReader.Metadata,Node>[],double[]> loadImpl(String path, SparkSession sql, String className, String treeClassName) Helper method for loading a tree ensemble from disk. This reconstructs all trees, returning the root nodes. Parameters:path - Path given to saveImplclassName - Class name for ensemble model typetreeClassName - Class name for tree model type in the ensemblesql - (undocumented) Returns:(ensemble metadata, array over trees of (tree metadata, root node)), where the root node is linked with all descendentsSee Also:saveImpl for how the model was saved Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Entropy (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Entropy (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.impurity Class Entropy Object org.apache.spark.mllib.tree.impurity.Entropy public class Entropy extends Object Class for calculating entropy during multiclass classification. Constructor Summary Constructors  Constructor and Description Entropy()  Method Summary Methods  Modifier and Type Method and Description static double calculate(double[] counts, double totalCount) :: DeveloperApi :: information calculation for multiclass classification static double calculate(double count, double sum, double sumSquares) :: DeveloperApi :: variance calculation static org.apache.spark.mllib.tree.impurity.Entropy$ instance() Get this impurity instance. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Entropy public Entropy() Method Detail calculate public static double calculate(double[] counts, double totalCount) :: DeveloperApi :: information calculation for multiclass classification Parameters:counts - Array[Double] with counts for each labeltotalCount - sum of counts for all labels Returns:information value, or 0 if totalCount = 0 calculate public static double calculate(double count, double sum, double sumSquares) :: DeveloperApi :: variance calculation Parameters:count - number of instancessum - sum of labelssumSquares - summation of squares of the labels Returns:information value, or 0 if count = 0 instance public static org.apache.spark.mllib.tree.impurity.Entropy$ instance() Get this impurity instance. This is useful for passing impurity parameters to a Strategy in Java. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EnumUtil (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EnumUtil (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class EnumUtil Object org.apache.spark.util.EnumUtil public class EnumUtil extends Object Constructor Summary Constructors  Constructor and Description EnumUtil()  Method Summary Methods  Modifier and Type Method and Description static <E extends Enum<E>> E parseIgnoreCase(Class<E> clz, String str)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail EnumUtil public EnumUtil() Method Detail parseIgnoreCase public static <E extends Enum<E>> E parseIgnoreCase(Class<E> clz, String str) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EnvironmentListener (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EnvironmentListener (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ui.env Class EnvironmentListener Object org.apache.spark.scheduler.SparkListener org.apache.spark.ui.env.EnvironmentListener public class EnvironmentListener extends SparkListener :: DeveloperApi :: A SparkListener that prepares information to be displayed on the EnvironmentTab Constructor Summary Constructors  Constructor and Description EnvironmentListener()  Method Summary Methods  Modifier and Type Method and Description scala.collection.Seq<scala.Tuple2<String,String>> classpathEntries()  scala.collection.Seq<scala.Tuple2<String,String>> jvmInformation()  void onEnvironmentUpdate(SparkListenerEnvironmentUpdate environmentUpdate) Called when environment properties have been updated scala.collection.Seq<scala.Tuple2<String,String>> sparkProperties()  scala.collection.Seq<scala.Tuple2<String,String>> systemProperties()  Methods inherited from class org.apache.spark.scheduler.SparkListener onApplicationEnd, onApplicationStart, onBlockManagerAdded, onBlockManagerRemoved, onBlockUpdated, onExecutorAdded, onExecutorMetricsUpdate, onExecutorRemoved, onJobEnd, onJobStart, onOtherEvent, onStageCompleted, onStageSubmitted, onTaskEnd, onTaskGettingResult, onTaskStart, onUnpersistRDD Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail EnvironmentListener public EnvironmentListener() Method Detail jvmInformation public scala.collection.Seq<scala.Tuple2<String,String>> jvmInformation() sparkProperties public scala.collection.Seq<scala.Tuple2<String,String>> sparkProperties() systemProperties public scala.collection.Seq<scala.Tuple2<String,String>> systemProperties() classpathEntries public scala.collection.Seq<scala.Tuple2<String,String>> classpathEntries() onEnvironmentUpdate public void onEnvironmentUpdate(SparkListenerEnvironmentUpdate environmentUpdate) Called when environment properties have been updated Overrides: onEnvironmentUpdate in class SparkListener Parameters:environmentUpdate - (undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EqualNullSafe (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EqualNullSafe (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class EqualNullSafe Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.EqualNullSafe All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class EqualNullSafe extends Filter implements scala.Product, scala.Serializable Performs equality comparison, similar to EqualTo. However, this differs from EqualTo in that it returns true (rather than NULL) if both inputs are NULL, and false (rather than NULL) if one of the input is NULL and the other is not NULL. Since: 1.5.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description EqualNullSafe(String attribute, Object value)  Method Summary Methods  Modifier and Type Method and Description String attribute()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Object value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail EqualNullSafe public EqualNullSafe(String attribute, Object value) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() attribute public String attribute() value public Object value() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EqualTo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EqualTo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class EqualTo Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.EqualTo All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class EqualTo extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff the attribute evaluates to a value equal to value. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description EqualTo(String attribute, Object value)  Method Summary Methods  Modifier and Type Method and Description String attribute()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Object value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail EqualTo public EqualTo(String attribute, Object value) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() attribute public String attribute() value public Object value() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Estimator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Estimator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml Class Estimator<M extends Model<M>> Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> All Implemented Interfaces: java.io.Serializable, Params, Identifiable Direct Known Subclasses: AFTSurvivalRegression, ALS, BisectingKMeans, ChiSqSelector, CountVectorizer, CrossValidator, GaussianMixture, IDF, IsotonicRegression, KMeans, LDA, MaxAbsScaler, MinMaxScaler, OneVsRest, PCA, Pipeline, Predictor, QuantileDiscretizer, RFormula, StandardScaler, StringIndexer, TrainValidationSplit, VectorIndexer, Word2Vec public abstract class Estimator<M extends Model<M>> extends PipelineStage :: DeveloperApi :: Abstract class for estimators that fit models to data. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Estimator()  Method Summary Methods  Modifier and Type Method and Description abstract Estimator<M> copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. abstract M fit(Dataset<?> dataset) Fits a model to the input data. M fit(Dataset<?> dataset, ParamMap paramMap) Fits a single model to the input data with provided parameter map. scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) Fits multiple models to the input data with multiple sets of parameters. M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) Fits a single model to the input data with optional parameters. M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) Fits a single model to the input data with optional parameters. Methods inherited from class org.apache.spark.ml.PipelineStage transformSchema Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail Estimator public Estimator() Method Detail fit public M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) Fits a single model to the input data with optional parameters. Parameters:dataset - input datasetfirstParamPair - the first param pair, overrides embedded paramsotherParamPairs - other param pairs. These values override any specified in this Estimator's embedded ParamMap. Returns:fitted model fit public M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) Fits a single model to the input data with optional parameters. Parameters:dataset - input datasetfirstParamPair - the first param pair, overrides embedded paramsotherParamPairs - other param pairs. These values override any specified in this Estimator's embedded ParamMap. Returns:fitted model fit public M fit(Dataset<?> dataset, ParamMap paramMap) Fits a single model to the input data with provided parameter map. Parameters:dataset - input datasetparamMap - Parameter map. These values override any specified in this Estimator's embedded ParamMap. Returns:fitted model fit public abstract M fit(Dataset<?> dataset) Fits a model to the input data. Parameters:dataset - (undocumented) Returns:(undocumented) fit public scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) Fits multiple models to the input data with multiple sets of parameters. The default implementation uses a for loop on each parameter map. Subclasses could override this to optimize multi-model training. Parameters:dataset - input datasetparamMaps - An array of parameter maps. These values override any specified in this Estimator's embedded ParamMap. Returns:fitted models, matching the input parameter maps copy public abstract Estimator<M> copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class PipelineStage Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Evaluator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Evaluator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.evaluation Class Evaluator Object org.apache.spark.ml.evaluation.Evaluator All Implemented Interfaces: java.io.Serializable, Params, Identifiable Direct Known Subclasses: BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator public abstract class Evaluator extends Object implements Params :: DeveloperApi :: Abstract class for evaluators that compute metrics from predictions. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Evaluator()  Method Summary Methods  Modifier and Type Method and Description abstract Evaluator copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. abstract double evaluate(Dataset<?> dataset) Evaluates model output and returns a scalar metric. double evaluate(Dataset<?> dataset, ParamMap paramMap) Evaluates model output and returns a scalar metric. boolean isLargerBetter() Indicates whether the metric returned by evaluate should be maximized (true, default) or minimized (false). Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail Evaluator public Evaluator() Method Detail evaluate public double evaluate(Dataset<?> dataset, ParamMap paramMap) Evaluates model output and returns a scalar metric. The value of isLargerBetter specifies whether larger values are better. Parameters:dataset - a dataset that contains labels/observations and predictions.paramMap - parameter map that specifies the input columns and output metrics Returns:metric evaluate public abstract double evaluate(Dataset<?> dataset) Evaluates model output and returns a scalar metric. The value of isLargerBetter specifies whether larger values are better. Parameters:dataset - a dataset that contains labels/observations and predictions. Returns:metric isLargerBetter public boolean isLargerBetter() Indicates whether the metric returned by evaluate should be maximized (true, default) or minimized (false). A given evaluator may support multiple metrics which may be maximized or minimized. Returns:(undocumented) copy public abstract Evaluator copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method EventTransformer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="EventTransformer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.flume Class EventTransformer Object org.apache.spark.streaming.flume.EventTransformer public class EventTransformer extends Object A simple object that provides the implementation of readExternal and writeExternal for both the wrapper classes for Flume-style Events. Constructor Summary Constructors  Constructor and Description EventTransformer()  Method Summary Methods  Modifier and Type Method and Description static scala.Tuple2<java.util.HashMap<CharSequence,CharSequence>,byte[]> readExternal(java.io.ObjectInput in)  static void writeExternal(java.io.ObjectOutput out, java.util.Map<CharSequence,CharSequence> headers, byte[] body)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail EventTransformer public EventTransformer() Method Detail readExternal public static scala.Tuple2<java.util.HashMap<CharSequence,CharSequence>,byte[]> readExternal(java.io.ObjectInput in) writeExternal public static void writeExternal(java.io.ObjectOutput out, java.util.Map<CharSequence,CharSequence> headers, byte[] body) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExceptionFailure (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExceptionFailure (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class ExceptionFailure Object org.apache.spark.ExceptionFailure All Implemented Interfaces: java.io.Serializable, TaskEndReason, TaskFailedReason, scala.Equals, scala.Product public class ExceptionFailure extends Object implements TaskFailedReason, scala.Product, scala.Serializable :: DeveloperApi :: Task failed due to a runtime exception. This is the most common failure case and also captures user program exceptions. stackTrace contains the stack trace of the exception itself. It still exists for backward compatibility. It's better to use this(e: Throwable, metrics: Option[TaskMetrics]) to create ExceptionFailure as it will handle the backward compatibility properly. fullStackTrace is a better representation of the stack trace because it contains the whole stack trace including the exception and its causes exception is the actual exception that caused the task to fail. It may be None in the case that the exception is not in fact serializable. If a task fails more than once (due to retries), exception is that one that caused the last failure. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ExceptionFailure(String className, String description, StackTraceElement[] stackTrace, String fullStackTrace, scala.Option<org.apache.spark.ThrowableSerializationWrapper> exceptionWrapper, scala.collection.Seq<AccumulableInfo> accumUpdates, scala.collection.Seq<AccumulatorV2<?,?>> accums)  Method Summary Methods  Modifier and Type Method and Description scala.collection.Seq<AccumulableInfo> accumUpdates()  abstract static boolean canEqual(Object that)  String className()  static boolean countTowardsTaskFailures()  String description()  abstract static boolean equals(Object that)  scala.Option<Throwable> exception()  String fullStackTrace()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  StackTraceElement[] stackTrace()  String toErrorString() Error message displayed in the web UI. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.TaskFailedReason countTowardsTaskFailures Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail ExceptionFailure public ExceptionFailure(String className, String description, StackTraceElement[] stackTrace, String fullStackTrace, scala.Option<org.apache.spark.ThrowableSerializationWrapper> exceptionWrapper, scala.collection.Seq<AccumulableInfo> accumUpdates, scala.collection.Seq<AccumulatorV2<?,?>> accums) Method Detail countTowardsTaskFailures public static boolean countTowardsTaskFailures() canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() className public String className() description public String description() stackTrace public StackTraceElement[] stackTrace() fullStackTrace public String fullStackTrace() accumUpdates public scala.collection.Seq<AccumulableInfo> accumUpdates() exception public scala.Option<Throwable> exception() toErrorString public String toErrorString() Description copied from interface: TaskFailedReason Error message displayed in the web UI. Specified by: toErrorString in interface TaskFailedReason Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExecutionListenerManager (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExecutionListenerManager (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.util Class ExecutionListenerManager Object org.apache.spark.sql.util.ExecutionListenerManager public class ExecutionListenerManager extends Object :: Experimental :: Manager for QueryExecutionListener. See org.apache.spark.sql.SQLContext.listenerManager. Method Summary Methods  Modifier and Type Method and Description void clear() Removes all the registered QueryExecutionListener. void register(QueryExecutionListener listener) Registers the specified QueryExecutionListener. void unregister(QueryExecutionListener listener) Unregisters the specified QueryExecutionListener. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail register public void register(QueryExecutionListener listener) Registers the specified QueryExecutionListener. Parameters:listener - (undocumented) unregister public void unregister(QueryExecutionListener listener) Unregisters the specified QueryExecutionListener. Parameters:listener - (undocumented) clear public void clear() Removes all the registered QueryExecutionListener. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExecutorInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExecutorInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster Class ExecutorInfo Object org.apache.spark.scheduler.cluster.ExecutorInfo public class ExecutorInfo extends Object :: DeveloperApi :: Stores information about an executor to pass from the scheduler to SparkListeners. Constructor Summary Constructors  Constructor and Description ExecutorInfo(String executorHost, int totalCores, scala.collection.immutable.Map<String,String> logUrlMap)  Method Summary Methods  Modifier and Type Method and Description boolean canEqual(Object other)  boolean equals(Object other)  String executorHost()  int hashCode()  scala.collection.immutable.Map<String,String> logUrlMap()  int totalCores()  Methods inherited from class Object getClass, notify, notifyAll, toString, wait, wait, wait Constructor Detail ExecutorInfo public ExecutorInfo(String executorHost, int totalCores, scala.collection.immutable.Map<String,String> logUrlMap) Method Detail executorHost public String executorHost() totalCores public int totalCores() logUrlMap public scala.collection.immutable.Map<String,String> logUrlMap() canEqual public boolean canEqual(Object other) equals public boolean equals(Object other) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExecutorKilled (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExecutorKilled (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler Class ExecutorKilled Object org.apache.spark.scheduler.ExecutorKilled public class ExecutorKilled extends Object Constructor Summary Constructors  Constructor and Description ExecutorKilled()  Method Summary Methods  Modifier and Type Method and Description static String message()  static String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ExecutorKilled public ExecutorKilled() Method Detail message public static String message() toString public static String toString() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExecutorLostFailure (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExecutorLostFailure (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class ExecutorLostFailure Object org.apache.spark.ExecutorLostFailure All Implemented Interfaces: java.io.Serializable, TaskEndReason, TaskFailedReason, scala.Equals, scala.Product public class ExecutorLostFailure extends Object implements TaskFailedReason, scala.Product, scala.Serializable :: DeveloperApi :: The task failed because the executor that it was running on was lost. This may happen because the task crashed the JVM. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ExecutorLostFailure(String execId, boolean exitCausedByApp, scala.Option<String> reason)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  boolean countTowardsTaskFailures() Whether this task failure should be counted towards the maximum number of times the task is allowed to fail before the stage is aborted. abstract static boolean equals(Object that)  String execId()  boolean exitCausedByApp()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  scala.Option<String> reason()  String toErrorString() Error message displayed in the web UI. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail ExecutorLostFailure public ExecutorLostFailure(String execId, boolean exitCausedByApp, scala.Option<String> reason) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() execId public String execId() exitCausedByApp public boolean exitCausedByApp() reason public scala.Option<String> reason() toErrorString public String toErrorString() Description copied from interface: TaskFailedReason Error message displayed in the web UI. Specified by: toErrorString in interface TaskFailedReason countTowardsTaskFailures public boolean countTowardsTaskFailures() Description copied from interface: TaskFailedReason Whether this task failure should be counted towards the maximum number of times the task is allowed to fail before the stage is aborted. Set to false in cases where the task's failure was unrelated to the task; for example, if the task failed because the executor it was running on was killed. Specified by: countTowardsTaskFailures in interface TaskFailedReason Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExecutorRegistered (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExecutorRegistered (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class ExecutorRegistered Object org.apache.spark.ExecutorRegistered All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class ExecutorRegistered extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ExecutorRegistered(String executorId)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  String executorId()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail ExecutorRegistered public ExecutorRegistered(String executorId) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() executorId public String executorId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExecutorRemoved (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExecutorRemoved (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class ExecutorRemoved Object org.apache.spark.ExecutorRemoved All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class ExecutorRemoved extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ExecutorRemoved(String executorId)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  String executorId()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail ExecutorRemoved public ExecutorRemoved(String executorId) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() executorId public String executorId() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExecutorStageSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExecutorStageSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class ExecutorStageSummary Object org.apache.spark.status.api.v1.ExecutorStageSummary public class ExecutorStageSummary extends Object Method Summary Methods  Modifier and Type Method and Description long diskBytesSpilled()  int failedTasks()  long inputBytes()  long memoryBytesSpilled()  long outputBytes()  long shuffleRead()  long shuffleWrite()  int succeededTasks()  long taskTime()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail taskTime public long taskTime() failedTasks public int failedTasks() succeededTasks public int succeededTasks() inputBytes public long inputBytes() outputBytes public long outputBytes() shuffleRead public long shuffleRead() shuffleWrite public long shuffleWrite() memoryBytesSpilled public long memoryBytesSpilled() diskBytesSpilled public long diskBytesSpilled() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExecutorSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExecutorSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class ExecutorSummary Object org.apache.spark.status.api.v1.ExecutorSummary public class ExecutorSummary extends Object Method Summary Methods  Modifier and Type Method and Description int activeTasks()  int completedTasks()  long diskUsed()  scala.collection.Map<String,String> executorLogs()  int failedTasks()  String hostPort()  String id()  boolean isActive()  long maxMemory()  int maxTasks()  long memoryUsed()  int rddBlocks()  int totalCores()  long totalDuration()  long totalGCTime()  long totalInputBytes()  long totalShuffleRead()  long totalShuffleWrite()  int totalTasks()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail id public String id() hostPort public String hostPort() isActive public boolean isActive() rddBlocks public int rddBlocks() memoryUsed public long memoryUsed() diskUsed public long diskUsed() totalCores public int totalCores() maxTasks public int maxTasks() activeTasks public int activeTasks() failedTasks public int failedTasks() completedTasks public int completedTasks() totalTasks public int totalTasks() totalDuration public long totalDuration() totalGCTime public long totalGCTime() totalInputBytes public long totalInputBytes() totalShuffleRead public long totalShuffleRead() totalShuffleWrite public long totalShuffleWrite() maxMemory public long maxMemory() executorLogs public scala.collection.Map<String,String> executorLogs() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExecutorsListener (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExecutorsListener (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ui.exec Class ExecutorsListener Object org.apache.spark.scheduler.SparkListener org.apache.spark.ui.exec.ExecutorsListener public class ExecutorsListener extends SparkListener :: DeveloperApi :: A SparkListener that prepares information to be displayed on the ExecutorsTab Constructor Summary Constructors  Constructor and Description ExecutorsListener(StorageStatusListener storageStatusListener, SparkConf conf)  Method Summary Methods  Modifier and Type Method and Description scala.collection.Seq<StorageStatus> activeStorageStatusList()  scala.collection.Seq<StorageStatus> deadStorageStatusList()  scala.collection.mutable.HashMap<String,UIData.ExecutorUIData> executorIdToData()  scala.collection.mutable.HashMap<String,Object> executorToDuration()  scala.collection.mutable.HashMap<String,Object> executorToInputBytes()  scala.collection.mutable.HashMap<String,Object> executorToInputRecords()  scala.collection.mutable.HashMap<String,Object> executorToJvmGCTime()  scala.collection.mutable.HashMap<String,scala.collection.immutable.Map<String,String>> executorToLogUrls()  scala.collection.mutable.HashMap<String,Object> executorToOutputBytes()  scala.collection.mutable.HashMap<String,Object> executorToOutputRecords()  scala.collection.mutable.HashMap<String,Object> executorToShuffleRead()  scala.collection.mutable.HashMap<String,Object> executorToShuffleWrite()  scala.collection.mutable.HashMap<String,Object> executorToTasksActive()  scala.collection.mutable.HashMap<String,Object> executorToTasksComplete()  scala.collection.mutable.HashMap<String,Object> executorToTasksFailed()  scala.collection.mutable.HashMap<String,Object> executorToTasksMax()  scala.collection.mutable.HashMap<String,Object> executorToTotalCores()  void onApplicationStart(SparkListenerApplicationStart applicationStart) Called when the application starts void onExecutorAdded(SparkListenerExecutorAdded executorAdded) Called when the driver registers a new executor. void onExecutorRemoved(SparkListenerExecutorRemoved executorRemoved) Called when the driver removes an executor. void onTaskEnd(SparkListenerTaskEnd taskEnd) Called when a task ends void onTaskStart(SparkListenerTaskStart taskStart) Called when a task starts Methods inherited from class org.apache.spark.scheduler.SparkListener onApplicationEnd, onBlockManagerAdded, onBlockManagerRemoved, onBlockUpdated, onEnvironmentUpdate, onExecutorMetricsUpdate, onJobEnd, onJobStart, onOtherEvent, onStageCompleted, onStageSubmitted, onTaskGettingResult, onUnpersistRDD Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ExecutorsListener public ExecutorsListener(StorageStatusListener storageStatusListener, SparkConf conf) Method Detail executorToTotalCores public scala.collection.mutable.HashMap<String,Object> executorToTotalCores() executorToTasksMax public scala.collection.mutable.HashMap<String,Object> executorToTasksMax() executorToTasksActive public scala.collection.mutable.HashMap<String,Object> executorToTasksActive() executorToTasksComplete public scala.collection.mutable.HashMap<String,Object> executorToTasksComplete() executorToTasksFailed public scala.collection.mutable.HashMap<String,Object> executorToTasksFailed() executorToDuration public scala.collection.mutable.HashMap<String,Object> executorToDuration() executorToJvmGCTime public scala.collection.mutable.HashMap<String,Object> executorToJvmGCTime() executorToInputBytes public scala.collection.mutable.HashMap<String,Object> executorToInputBytes() executorToInputRecords public scala.collection.mutable.HashMap<String,Object> executorToInputRecords() executorToOutputBytes public scala.collection.mutable.HashMap<String,Object> executorToOutputBytes() executorToOutputRecords public scala.collection.mutable.HashMap<String,Object> executorToOutputRecords() executorToShuffleRead public scala.collection.mutable.HashMap<String,Object> executorToShuffleRead() executorToShuffleWrite public scala.collection.mutable.HashMap<String,Object> executorToShuffleWrite() executorToLogUrls public scala.collection.mutable.HashMap<String,scala.collection.immutable.Map<String,String>> executorToLogUrls() executorIdToData public scala.collection.mutable.HashMap<String,UIData.ExecutorUIData> executorIdToData() activeStorageStatusList public scala.collection.Seq<StorageStatus> activeStorageStatusList() deadStorageStatusList public scala.collection.Seq<StorageStatus> deadStorageStatusList() onExecutorAdded public void onExecutorAdded(SparkListenerExecutorAdded executorAdded) Called when the driver registers a new executor. Overrides: onExecutorAdded in class SparkListener Parameters:executorAdded - (undocumented) onExecutorRemoved public void onExecutorRemoved(SparkListenerExecutorRemoved executorRemoved) Called when the driver removes an executor. Overrides: onExecutorRemoved in class SparkListener Parameters:executorRemoved - (undocumented) onApplicationStart public void onApplicationStart(SparkListenerApplicationStart applicationStart) Called when the application starts Overrides: onApplicationStart in class SparkListener Parameters:applicationStart - (undocumented) onTaskStart public void onTaskStart(SparkListenerTaskStart taskStart) Called when a task starts Overrides: onTaskStart in class SparkListener Parameters:taskStart - (undocumented) onTaskEnd public void onTaskEnd(SparkListenerTaskEnd taskEnd) Called when a task ends Overrides: onTaskEnd in class SparkListener Parameters:taskEnd - (undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExpectationSum (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExpectationSum (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class ExpectationSum Object org.apache.spark.mllib.clustering.ExpectationSum All Implemented Interfaces: java.io.Serializable public class ExpectationSum extends Object implements scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ExpectationSum(double logLikelihood, double[] weights, breeze.linalg.DenseVector<Object>[] means, breeze.linalg.DenseMatrix<Object>[] sigmas)  Method Summary Methods  Modifier and Type Method and Description static ExpectationSum add(double[] weights, MultivariateGaussian[] dists, ExpectationSum sums, breeze.linalg.Vector<Object> x)  int k()  double logLikelihood()  breeze.linalg.DenseVector<Object>[] means()  breeze.linalg.DenseMatrix<Object>[] sigmas()  double[] weights()  static ExpectationSum zero(int k, int d)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ExpectationSum public ExpectationSum(double logLikelihood, double[] weights, breeze.linalg.DenseVector<Object>[] means, breeze.linalg.DenseMatrix<Object>[] sigmas) Method Detail zero public static ExpectationSum zero(int k, int d) add public static ExpectationSum add(double[] weights, MultivariateGaussian[] dists, ExpectationSum sums, breeze.linalg.Vector<Object> x) logLikelihood public double logLikelihood() weights public double[] weights() means public breeze.linalg.DenseVector<Object>[] means() sigmas public breeze.linalg.DenseMatrix<Object>[] sigmas() k public int k() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExperimentalMethods (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExperimentalMethods (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class ExperimentalMethods Object org.apache.spark.sql.ExperimentalMethods public class ExperimentalMethods extends Object :: Experimental :: Holder for experimental methods for the bravest. We make NO guarantee about the stability regarding binary compatibility and source compatibility of methods here. spark.experimental.extraStrategies += ... Since: 1.3.0 Method Summary Methods  Modifier and Type Method and Description scala.collection.Seq<org.apache.spark.sql.catalyst.rules.Rule<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan>> extraOptimizations()  scala.collection.Seq<org.apache.spark.sql.execution.SparkStrategy> extraStrategies() Allows extra strategies to be injected into the query planner at runtime. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail extraStrategies public scala.collection.Seq<org.apache.spark.sql.execution.SparkStrategy> extraStrategies() Allows extra strategies to be injected into the query planner at runtime. Note this API should be considered experimental and is not intended to be stable across releases. Returns:(undocumented)Since: 1.3.0 extraOptimizations public scala.collection.Seq<org.apache.spark.sql.catalyst.rules.Rule<org.apache.spark.sql.catalyst.plans.logical.LogicalPlan>> extraOptimizations() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExpireDeadHosts (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExpireDeadHosts (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class ExpireDeadHosts Object org.apache.spark.ExpireDeadHosts public class ExpireDeadHosts extends Object Constructor Summary Constructors  Constructor and Description ExpireDeadHosts()  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ExpireDeadHosts public ExpireDeadHosts() Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ExponentialGenerator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ExponentialGenerator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.random Class ExponentialGenerator Object org.apache.spark.mllib.random.ExponentialGenerator All Implemented Interfaces: java.io.Serializable, RandomDataGenerator<Object>, Pseudorandom public class ExponentialGenerator extends Object implements RandomDataGenerator<Object> :: DeveloperApi :: Generates i.i.d. samples from the exponential distribution with the given mean. param: mean mean for the exponential distribution. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ExponentialGenerator(double mean)  Method Summary Methods  Modifier and Type Method and Description ExponentialGenerator copy() Returns a copy of the RandomDataGenerator with a new instance of the rng object used in the class when applicable for non-locking concurrent usage. double mean()  double nextValue() Returns an i.i.d. void setSeed(long seed) Set random seed. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ExponentialGenerator public ExponentialGenerator(double mean) Method Detail mean public double mean() nextValue public double nextValue() Description copied from interface: RandomDataGenerator Returns an i.i.d. sample as a generic type from an underlying distribution. Specified by: nextValue in interface RandomDataGenerator<Object> Returns:(undocumented) setSeed public void setSeed(long seed) Description copied from interface: Pseudorandom Set random seed. Specified by: setSeed in interface Pseudorandom copy public ExponentialGenerator copy() Description copied from interface: RandomDataGenerator Returns a copy of the RandomDataGenerator with a new instance of the rng object used in the class when applicable for non-locking concurrent usage. Specified by: copy in interface RandomDataGenerator<Object> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FPGrowth.FreqItemset (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FPGrowth.FreqItemset (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class FPGrowth.FreqItemset<Item> Object org.apache.spark.mllib.fpm.FPGrowth.FreqItemset<Item> All Implemented Interfaces: java.io.Serializable Enclosing class: FPGrowth public static class FPGrowth.FreqItemset<Item> extends Object implements scala.Serializable Frequent itemset. param: items items in this itemset. Java users should call javaItems() instead. param: freq frequency See Also:Serialized Form Constructor Summary Constructors  Constructor and Description FPGrowth.FreqItemset(Object items, long freq)  Method Summary Methods  Modifier and Type Method and Description long freq()  Object items()  java.util.List<Item> javaItems() Returns items in a Java List. String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail FPGrowth.FreqItemset public FPGrowth.FreqItemset(Object items, long freq) Method Detail items public Object items() freq public long freq() javaItems public java.util.List<Item> javaItems() Returns items in a Java List. Returns:(undocumented) toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FPGrowth (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FPGrowth (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class FPGrowth Object org.apache.spark.mllib.fpm.FPGrowth All Implemented Interfaces: java.io.Serializable public class FPGrowth extends Object implements scala.Serializable A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in Li et al., PFP: Parallel FP-Growth for Query Recommendation. PFP distributes computation in such a way that each worker executes an independent group of mining tasks. The FP-Growth algorithm is described in Han et al., Mining frequent patterns without candidate generation. param: minSupport the minimal support level of the frequent pattern, any pattern that appears more than (minSupport * size-of-the-dataset) times will be output param: numPartitions number of partitions used by parallel FP-growth See Also:http://en.wikipedia.org/wiki/Association_rule_learning Association rule learning (Wikipedia)} , Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  FPGrowth.FreqItemset<Item> Frequent itemset. Constructor Summary Constructors  Constructor and Description FPGrowth() Constructs a default instance with default parameters {minSupport: 0.3, numPartitions: same as the input data}. Method Summary Methods  Modifier and Type Method and Description <Item,Basket extends Iterable<Item>> FPGrowthModel<Item> run(JavaRDD<Basket> data) Java-friendly version of run. <Item> FPGrowthModel<Item> run(RDD<Object> data, scala.reflect.ClassTag<Item> evidence$3) Computes an FP-Growth model that contains frequent itemsets. FPGrowth setMinSupport(double minSupport) Sets the minimal support level (default: 0.3). FPGrowth setNumPartitions(int numPartitions) Sets the number of partitions used by parallel FP-growth (default: same as input data). Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail FPGrowth public FPGrowth() Constructs a default instance with default parameters {minSupport: 0.3, numPartitions: same as the input data}. Method Detail setMinSupport public FPGrowth setMinSupport(double minSupport) Sets the minimal support level (default: 0.3). Parameters:minSupport - (undocumented) Returns:(undocumented) setNumPartitions public FPGrowth setNumPartitions(int numPartitions) Sets the number of partitions used by parallel FP-growth (default: same as input data). Parameters:numPartitions - (undocumented) Returns:(undocumented) run public <Item> FPGrowthModel<Item> run(RDD<Object> data, scala.reflect.ClassTag<Item> evidence$3) Computes an FP-Growth model that contains frequent itemsets. Parameters:data - input data set, each element contains a transactionevidence$3 - (undocumented) Returns:an FPGrowthModel run public <Item,Basket extends Iterable<Item>> FPGrowthModel<Item> run(JavaRDD<Basket> data) Java-friendly version of run. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FPGrowthModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FPGrowthModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class FPGrowthModel.SaveLoadV1_0$ Object org.apache.spark.mllib.fpm.FPGrowthModel.SaveLoadV1_0$ Enclosing class: FPGrowthModel<Item> public static class FPGrowthModel.SaveLoadV1_0$ extends Object Field Summary Fields  Modifier and Type Field and Description static FPGrowthModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description FPGrowthModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description FPGrowthModel<?> load(SparkContext sc, String path)  <Item> FPGrowthModel<Item> loadImpl(Dataset<Row> freqItemsets, Item sample, scala.reflect.ClassTag<Item> evidence$2)  void save(FPGrowthModel<?> model, String path)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final FPGrowthModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail FPGrowthModel.SaveLoadV1_0$ public FPGrowthModel.SaveLoadV1_0$() Method Detail save public void save(FPGrowthModel<?> model, String path) load public FPGrowthModel<?> load(SparkContext sc, String path) loadImpl public <Item> FPGrowthModel<Item> loadImpl(Dataset<Row> freqItemsets, Item sample, scala.reflect.ClassTag<Item> evidence$2) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FPGrowthModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FPGrowthModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class FPGrowthModel<Item> Object org.apache.spark.mllib.fpm.FPGrowthModel<Item> All Implemented Interfaces: java.io.Serializable, Saveable public class FPGrowthModel<Item> extends Object implements Saveable, scala.Serializable Model trained by FPGrowth, which holds frequent itemsets. param: freqItemsets frequent itemset, which is an RDD of FreqItemset See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  FPGrowthModel.SaveLoadV1_0$  Constructor Summary Constructors  Constructor and Description FPGrowthModel(RDD<FPGrowth.FreqItemset<Item>> freqItemsets, scala.reflect.ClassTag<Item> evidence$1)  Method Summary Methods  Modifier and Type Method and Description RDD<FPGrowth.FreqItemset<Item>> freqItemsets()  RDD<AssociationRules.Rule<Item>> generateAssociationRules(double confidence) Generates association rules for the Items in freqItemsets. static FPGrowthModel<?> load(SparkContext sc, String path)  void save(SparkContext sc, String path) Save this model to the given path. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail FPGrowthModel public FPGrowthModel(RDD<FPGrowth.FreqItemset<Item>> freqItemsets, scala.reflect.ClassTag<Item> evidence$1) Method Detail load public static FPGrowthModel<?> load(SparkContext sc, String path) freqItemsets public RDD<FPGrowth.FreqItemset<Item>> freqItemsets() generateAssociationRules public RDD<AssociationRules.Rule<Item>> generateAssociationRules(double confidence) Generates association rules for the Items in freqItemsets. Parameters:confidence - minimal confidence of the rules produced Returns:(undocumented) save public void save(SparkContext sc, String path) Save this model to the given path. It only works for Item datatypes supported by DataFrames. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using FPGrowthModel.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FallbackConfigEntry (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FallbackConfigEntry (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.internal.config Class FallbackConfigEntry<T> Object org.apache.spark.internal.config.FallbackConfigEntry<T> public class FallbackConfigEntry<T> extends Object A config entry whose default value is defined by another config entry. Constructor Summary Constructors  Constructor and Description FallbackConfigEntry(String key, String doc, boolean isPublic, org.apache.spark.internal.config.ConfigEntry<T> fallback)  Method Summary Methods  Modifier and Type Method and Description scala.Option<T> defaultValue()  String defaultValueString()  String doc()  boolean isPublic()  String key()  T readFrom(SparkConf conf)  scala.Function1<T,String> stringConverter()  String toString()  scala.Function1<String,T> valueConverter()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail FallbackConfigEntry public FallbackConfigEntry(String key, String doc, boolean isPublic, org.apache.spark.internal.config.ConfigEntry<T> fallback) Method Detail defaultValueString public String defaultValueString() readFrom public T readFrom(SparkConf conf) key public String key() valueConverter public scala.Function1<String,T> valueConverter() stringConverter public scala.Function1<T,String> stringConverter() doc public String doc() isPublic public boolean isPublic() defaultValue public scala.Option<T> defaultValue() toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FalsePositiveRate (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FalsePositiveRate (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.evaluation.binary Class FalsePositiveRate Object org.apache.spark.mllib.evaluation.binary.FalsePositiveRate public class FalsePositiveRate extends Object False positive rate. Defined as 0.0 when there are no negative examples. Constructor Summary Constructors  Constructor and Description FalsePositiveRate()  Method Summary Methods  Modifier and Type Method and Description static double apply(org.apache.spark.mllib.evaluation.binary.BinaryConfusionMatrix c)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail FalsePositiveRate public FalsePositiveRate() Method Detail apply public static double apply(org.apache.spark.mllib.evaluation.binary.BinaryConfusionMatrix c) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FeatureType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FeatureType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.configuration Class FeatureType Object org.apache.spark.mllib.tree.configuration.FeatureType public class FeatureType extends Object Enum to describe whether a feature is "continuous" or "categorical" Constructor Summary Constructors  Constructor and Description FeatureType()  Method Summary Methods  Modifier and Type Method and Description static scala.Enumeration.Value apply(int x)  static scala.Enumeration.Value Categorical()  static scala.Enumeration.Value Continuous()  static int maxId()  static String toString()  static scala.Enumeration.ValueSet values()  static scala.Enumeration.Value withName(String s)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail FeatureType public FeatureType() Method Detail Continuous public static scala.Enumeration.Value Continuous() Categorical public static scala.Enumeration.Value Categorical() toString public static String toString() values public static scala.Enumeration.ValueSet values() maxId public static final int maxId() apply public static final scala.Enumeration.Value apply(int x) withName public static final scala.Enumeration.Value withName(String s) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FetchFailed (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FetchFailed (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class FetchFailed Object org.apache.spark.FetchFailed All Implemented Interfaces: java.io.Serializable, TaskEndReason, TaskFailedReason, scala.Equals, scala.Product public class FetchFailed extends Object implements TaskFailedReason, scala.Product, scala.Serializable :: DeveloperApi :: Task failed to fetch shuffle data from a remote node. Probably means we have lost the remote executors the task is trying to fetch from, and thus need to rerun the previous stage. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description FetchFailed(BlockManagerId bmAddress, int shuffleId, int mapId, int reduceId, String message)  Method Summary Methods  Modifier and Type Method and Description BlockManagerId bmAddress()  abstract static boolean canEqual(Object that)  static boolean countTowardsTaskFailures()  abstract static boolean equals(Object that)  int mapId()  String message()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  int reduceId()  int shuffleId()  String toErrorString() Error message displayed in the web UI. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.TaskFailedReason countTowardsTaskFailures Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail FetchFailed public FetchFailed(BlockManagerId bmAddress, int shuffleId, int mapId, int reduceId, String message) Method Detail countTowardsTaskFailures public static boolean countTowardsTaskFailures() canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() bmAddress public BlockManagerId bmAddress() shuffleId public int shuffleId() mapId public int mapId() reduceId public int reduceId() message public String message() toErrorString public String toErrorString() Description copied from interface: TaskFailedReason Error message displayed in the web UI. Specified by: toErrorString in interface TaskFailedReason Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Filter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Filter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class Filter Object org.apache.spark.sql.sources.Filter Direct Known Subclasses: And, EqualNullSafe, EqualTo, GreaterThan, GreaterThanOrEqual, In, IsNotNull, IsNull, LessThan, LessThanOrEqual, Not, Or, StringContains, StringEndsWith, StringStartsWith public abstract class Filter extends Object A filter predicate for data sources. Since: 1.3.0 Constructor Summary Constructors  Constructor and Description Filter()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Filter public Filter() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FilterFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FilterFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface FilterFunction<T> All Superinterfaces: java.io.Serializable public interface FilterFunction<T> extends java.io.Serializable Base interface for a function used in Dataset's filter function. If the function returns true, the element is included in the returned Dataset. Method Summary Methods  Modifier and Type Method and Description boolean call(T value)  Method Detail call boolean call(T value) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FlatMapFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FlatMapFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface FlatMapFunction<T,R> All Superinterfaces: java.io.Serializable public interface FlatMapFunction<T,R> extends java.io.Serializable A function that returns zero or more output records from each input record. Method Summary Methods  Modifier and Type Method and Description java.util.Iterator<R> call(T t)  Method Detail call java.util.Iterator<R> call(T t) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FlatMapFunction2 (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FlatMapFunction2 (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface FlatMapFunction2<T1,T2,R> All Superinterfaces: java.io.Serializable public interface FlatMapFunction2<T1,T2,R> extends java.io.Serializable A function that takes two inputs and returns zero or more output records. Method Summary Methods  Modifier and Type Method and Description java.util.Iterator<R> call(T1 t1, T2 t2)  Method Detail call java.util.Iterator<R> call(T1 t1, T2 t2) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FlatMapGroupsFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FlatMapGroupsFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface FlatMapGroupsFunction<K,V,R> All Superinterfaces: java.io.Serializable public interface FlatMapGroupsFunction<K,V,R> extends java.io.Serializable A function that returns zero or more output records from each grouping key and its values. Method Summary Methods  Modifier and Type Method and Description java.util.Iterator<R> call(K key, java.util.Iterator<V> values)  Method Detail call java.util.Iterator<R> call(K key, java.util.Iterator<V> values) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FloatParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FloatParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class FloatParam Object org.apache.spark.ml.param.Param<Object> org.apache.spark.ml.param.FloatParam All Implemented Interfaces: java.io.Serializable public class FloatParam extends Param<Object> :: DeveloperApi :: Specialized version of Param[Float] for Java. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description FloatParam(Identifiable parent, String name, String doc)  FloatParam(Identifiable parent, String name, String doc, scala.Function1<Object,Object> isValid)  FloatParam(String parent, String name, String doc)  FloatParam(String parent, String name, String doc, scala.Function1<Object,Object> isValid)  Method Summary Methods  Modifier and Type Method and Description static ParamPair<T> $minus$greater(T value)  static String doc()  static boolean equals(Object obj)  static int hashCode()  static scala.Function1<T,Object> isValid()  float jsonDecode(String json)  String jsonEncode(float value)  static float jValueDecode(org.json4s.JsonAST.JValue jValue) Decodes a param value from JValue. static org.json4s.JsonAST.JValue jValueEncode(float value) Encodes a param value into JValue. static String name()  static String parent()  static String toString()  ParamPair<Object> w(float value) Creates a param pair with the given value (for Java). Methods inherited from class org.apache.spark.ml.param.Param doc, equals, hashCode, isValid, jsonEncode, name, parent, toString, w Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail FloatParam public FloatParam(String parent, String name, String doc, scala.Function1<Object,Object> isValid) FloatParam public FloatParam(String parent, String name, String doc) FloatParam public FloatParam(Identifiable parent, String name, String doc, scala.Function1<Object,Object> isValid) FloatParam public FloatParam(Identifiable parent, String name, String doc) Method Detail jValueEncode public static org.json4s.JsonAST.JValue jValueEncode(float value) Encodes a param value into JValue. jValueDecode public static float jValueDecode(org.json4s.JsonAST.JValue jValue) Decodes a param value from JValue. parent public static String parent() name public static String name() doc public static String doc() isValid public static scala.Function1<T,Object> isValid() $minus$greater public static ParamPair<T> $minus$greater(T value) toString public static final String toString() hashCode public static final int hashCode() equals public static final boolean equals(Object obj) w public ParamPair<Object> w(float value) Creates a param pair with the given value (for Java). jsonEncode public String jsonEncode(float value) jsonDecode public float jsonDecode(String json) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FloatType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FloatType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class FloatType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.NumericType org.apache.spark.sql.types.FloatType public class FloatType extends NumericType :: DeveloperApi :: The data type representing Float values. Please use the singleton DataTypes.FloatType. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of the FloatType is 4 bytes. static String json()  static String prettyJson()  static String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, simpleString, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() simpleString public static String simpleString() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() The default size of a value of the FloatType is 4 bytes. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FlumeUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FlumeUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.flume Class FlumeUtils Object org.apache.spark.streaming.flume.FlumeUtils public class FlumeUtils extends Object Constructor Summary Constructors  Constructor and Description FlumeUtils()  Method Summary Methods  Modifier and Type Method and Description static JavaReceiverInputDStream<SparkFlumeEvent> createPollingStream(JavaStreamingContext jssc, java.net.InetSocketAddress[] addresses, StorageLevel storageLevel) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. static JavaReceiverInputDStream<SparkFlumeEvent> createPollingStream(JavaStreamingContext jssc, java.net.InetSocketAddress[] addresses, StorageLevel storageLevel, int maxBatchSize, int parallelism) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. static JavaReceiverInputDStream<SparkFlumeEvent> createPollingStream(JavaStreamingContext jssc, String hostname, int port) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. static JavaReceiverInputDStream<SparkFlumeEvent> createPollingStream(JavaStreamingContext jssc, String hostname, int port, StorageLevel storageLevel) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. static ReceiverInputDStream<SparkFlumeEvent> createPollingStream(StreamingContext ssc, scala.collection.Seq<java.net.InetSocketAddress> addresses, StorageLevel storageLevel) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. static ReceiverInputDStream<SparkFlumeEvent> createPollingStream(StreamingContext ssc, scala.collection.Seq<java.net.InetSocketAddress> addresses, StorageLevel storageLevel, int maxBatchSize, int parallelism) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. static ReceiverInputDStream<SparkFlumeEvent> createPollingStream(StreamingContext ssc, String hostname, int port, StorageLevel storageLevel) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. static JavaReceiverInputDStream<SparkFlumeEvent> createStream(JavaStreamingContext jssc, String hostname, int port) Creates a input stream from a Flume source. static JavaReceiverInputDStream<SparkFlumeEvent> createStream(JavaStreamingContext jssc, String hostname, int port, StorageLevel storageLevel) Creates a input stream from a Flume source. static JavaReceiverInputDStream<SparkFlumeEvent> createStream(JavaStreamingContext jssc, String hostname, int port, StorageLevel storageLevel, boolean enableDecompression) Creates a input stream from a Flume source. static ReceiverInputDStream<SparkFlumeEvent> createStream(StreamingContext ssc, String hostname, int port, StorageLevel storageLevel) Create a input stream from a Flume source. static ReceiverInputDStream<SparkFlumeEvent> createStream(StreamingContext ssc, String hostname, int port, StorageLevel storageLevel, boolean enableDecompression) Create a input stream from a Flume source. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail FlumeUtils public FlumeUtils() Method Detail createStream public static ReceiverInputDStream<SparkFlumeEvent> createStream(StreamingContext ssc, String hostname, int port, StorageLevel storageLevel) Create a input stream from a Flume source. Parameters:ssc - StreamingContext objecthostname - Hostname of the slave machine to which the flume data will be sentport - Port of the slave machine to which the flume data will be sentstorageLevel - Storage level to use for storing the received objects Returns:(undocumented) createStream public static ReceiverInputDStream<SparkFlumeEvent> createStream(StreamingContext ssc, String hostname, int port, StorageLevel storageLevel, boolean enableDecompression) Create a input stream from a Flume source. Parameters:ssc - StreamingContext objecthostname - Hostname of the slave machine to which the flume data will be sentport - Port of the slave machine to which the flume data will be sentstorageLevel - Storage level to use for storing the received objectsenableDecompression - should netty server decompress input stream Returns:(undocumented) createStream public static JavaReceiverInputDStream<SparkFlumeEvent> createStream(JavaStreamingContext jssc, String hostname, int port) Creates a input stream from a Flume source. Storage level of the data will be the default StorageLevel.MEMORY_AND_DISK_SER_2. Parameters:hostname - Hostname of the slave machine to which the flume data will be sentport - Port of the slave machine to which the flume data will be sentjssc - (undocumented) Returns:(undocumented) createStream public static JavaReceiverInputDStream<SparkFlumeEvent> createStream(JavaStreamingContext jssc, String hostname, int port, StorageLevel storageLevel) Creates a input stream from a Flume source. Parameters:hostname - Hostname of the slave machine to which the flume data will be sentport - Port of the slave machine to which the flume data will be sentstorageLevel - Storage level to use for storing the received objectsjssc - (undocumented) Returns:(undocumented) createStream public static JavaReceiverInputDStream<SparkFlumeEvent> createStream(JavaStreamingContext jssc, String hostname, int port, StorageLevel storageLevel, boolean enableDecompression) Creates a input stream from a Flume source. Parameters:hostname - Hostname of the slave machine to which the flume data will be sentport - Port of the slave machine to which the flume data will be sentstorageLevel - Storage level to use for storing the received objectsenableDecompression - should netty server decompress input streamjssc - (undocumented) Returns:(undocumented) createPollingStream public static ReceiverInputDStream<SparkFlumeEvent> createPollingStream(StreamingContext ssc, String hostname, int port, StorageLevel storageLevel) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. This stream will poll the sink for data and will pull events as they are available. This stream will use a batch size of 1000 events and run 5 threads to pull data. Parameters:hostname - Address of the host on which the Spark Sink is runningport - Port of the host at which the Spark Sink is listeningstorageLevel - Storage level to use for storing the received objectsssc - (undocumented) Returns:(undocumented) createPollingStream public static ReceiverInputDStream<SparkFlumeEvent> createPollingStream(StreamingContext ssc, scala.collection.Seq<java.net.InetSocketAddress> addresses, StorageLevel storageLevel) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. This stream will poll the sink for data and will pull events as they are available. This stream will use a batch size of 1000 events and run 5 threads to pull data. Parameters:addresses - List of InetSocketAddresses representing the hosts to connect to.storageLevel - Storage level to use for storing the received objectsssc - (undocumented) Returns:(undocumented) createPollingStream public static ReceiverInputDStream<SparkFlumeEvent> createPollingStream(StreamingContext ssc, scala.collection.Seq<java.net.InetSocketAddress> addresses, StorageLevel storageLevel, int maxBatchSize, int parallelism) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. This stream will poll the sink for data and will pull events as they are available. Parameters:addresses - List of InetSocketAddresses representing the hosts to connect to.maxBatchSize - Maximum number of events to be pulled from the Spark sink in a single RPC callparallelism - Number of concurrent requests this stream should send to the sink. Note that having a higher number of requests concurrently being pulled will result in this stream using more threadsstorageLevel - Storage level to use for storing the received objectsssc - (undocumented) Returns:(undocumented) createPollingStream public static JavaReceiverInputDStream<SparkFlumeEvent> createPollingStream(JavaStreamingContext jssc, String hostname, int port) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. This stream will poll the sink for data and will pull events as they are available. This stream will use a batch size of 1000 events and run 5 threads to pull data. Parameters:hostname - Hostname of the host on which the Spark Sink is runningport - Port of the host at which the Spark Sink is listeningjssc - (undocumented) Returns:(undocumented) createPollingStream public static JavaReceiverInputDStream<SparkFlumeEvent> createPollingStream(JavaStreamingContext jssc, String hostname, int port, StorageLevel storageLevel) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. This stream will poll the sink for data and will pull events as they are available. This stream will use a batch size of 1000 events and run 5 threads to pull data. Parameters:hostname - Hostname of the host on which the Spark Sink is runningport - Port of the host at which the Spark Sink is listeningstorageLevel - Storage level to use for storing the received objectsjssc - (undocumented) Returns:(undocumented) createPollingStream public static JavaReceiverInputDStream<SparkFlumeEvent> createPollingStream(JavaStreamingContext jssc, java.net.InetSocketAddress[] addresses, StorageLevel storageLevel) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. This stream will poll the sink for data and will pull events as they are available. This stream will use a batch size of 1000 events and run 5 threads to pull data. Parameters:addresses - List of InetSocketAddresses on which the Spark Sink is running.storageLevel - Storage level to use for storing the received objectsjssc - (undocumented) Returns:(undocumented) createPollingStream public static JavaReceiverInputDStream<SparkFlumeEvent> createPollingStream(JavaStreamingContext jssc, java.net.InetSocketAddress[] addresses, StorageLevel storageLevel, int maxBatchSize, int parallelism) Creates an input stream that is to be used with the Spark Sink deployed on a Flume agent. This stream will poll the sink for data and will pull events as they are available. Parameters:addresses - List of InetSocketAddresses on which the Spark Sink is runningmaxBatchSize - The maximum number of events to be pulled from the Spark sink in a single RPC callparallelism - Number of concurrent requests this stream should send to the sink. Note that having a higher number of requests concurrently being pulled will result in this stream using more threadsstorageLevel - Storage level to use for storing the received objectsjssc - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ForeachFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ForeachFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface ForeachFunction<T> All Superinterfaces: java.io.Serializable public interface ForeachFunction<T> extends java.io.Serializable Base interface for a function used in Dataset's foreach function. Spark will invoke the call function on each element in the input Dataset. Method Summary Methods  Modifier and Type Method and Description void call(T t)  Method Detail call void call(T t) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ForeachPartitionFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ForeachPartitionFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface ForeachPartitionFunction<T> All Superinterfaces: java.io.Serializable public interface ForeachPartitionFunction<T> extends java.io.Serializable Base interface for a function used in Dataset's foreachPartition function. Method Summary Methods  Modifier and Type Method and Description void call(java.util.Iterator<T> t)  Method Detail call void call(java.util.Iterator<T> t) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ForeachWriter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ForeachWriter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class ForeachWriter<T> Object org.apache.spark.sql.ForeachWriter<T> All Implemented Interfaces: java.io.Serializable public abstract class ForeachWriter<T> extends Object implements scala.Serializable :: Experimental :: A class to consume data generated by a StreamingQuery. Typically this is used to send the generated data to external systems. Each partition will use a new deserialized instance, so you usually should do all the initialization (e.g. opening a connection or initiating a transaction) in the open method. Scala example: datasetOfString.write.foreach(new ForeachWriter[String] { def open(partitionId: Long, version: Long): Boolean = { // open connection } def process(record: String) = { // write string to connection } def close(errorOrNull: Throwable): Unit = { // close the connection } }) Java example: datasetOfString.write().foreach(new ForeachWriter<String>() { @Override public boolean open(long partitionId, long version) { // open connection } @Override public void process(String value) { // write string to connection } @Override public void close(Throwable errorOrNull) { // close the connection } }); Since: 2.0.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ForeachWriter()  Method Summary Methods  Modifier and Type Method and Description abstract void close(Throwable errorOrNull) Called when stopping to process one partition of new data in the executor side. abstract boolean open(long partitionId, long version) Called when starting to process one partition of new data in the executor. abstract void process(T value) Called to process the data in the executor side. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ForeachWriter public ForeachWriter() Method Detail open public abstract boolean open(long partitionId, long version) Called when starting to process one partition of new data in the executor. The version is for data deduplication when there are failures. When recovering from a failure, some data may be generated multiple times but they will always have the same version. If this method finds using the partitionId and version that this partition has already been processed, it can return false to skip the further data processing. However, close still will be called for cleaning up resources. Parameters:partitionId - the partition id.version - a unique id for data deduplication. Returns:true if the corresponding partition and version id should be processed. false indicates the partition should be skipped. process public abstract void process(T value) Called to process the data in the executor side. This method will be called only when open returns true. Parameters:value - (undocumented) close public abstract void close(Throwable errorOrNull) Called when stopping to process one partition of new data in the executor side. This is guaranteed to be called either open returns true or false. However, close won't be called in the following cases: - JVM crashes without throwing a Throwable - open throws a Throwable. Parameters:errorOrNull - the error thrown during processing data or null if there was no error. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Function (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Function (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.catalog Class Function Object org.apache.spark.sql.catalog.Function All Implemented Interfaces: org.apache.spark.sql.catalyst.DefinedByConstructorParams public class Function extends Object implements org.apache.spark.sql.catalyst.DefinedByConstructorParams A user-defined function in Spark, as returned by listFunctions method in Catalog. param: name name of the function. param: database name of the database the function belongs to. param: description description of the function; description can be null. param: className the fully qualified class name of the function. param: isTemporary whether the function is a temporary function or not. Since: 2.0.0 Constructor Summary Constructors  Constructor and Description Function(String name, String database, String description, String className, boolean isTemporary)  Method Summary Methods  Modifier and Type Method and Description String className()  String database()  String description()  boolean isTemporary()  String name()  String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail Function public Function(String name, String database, String description, String className, boolean isTemporary) Method Detail name public String name() database public String database() description public String description() className public String className() isTemporary public boolean isTemporary() toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Function0 (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Function0 (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface Function0<R> All Superinterfaces: java.io.Serializable public interface Function0<R> extends java.io.Serializable A zero-argument function that returns an R. Method Summary Methods  Modifier and Type Method and Description R call()  Method Detail call R call() throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Function2 (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Function2 (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface Function2<T1,T2,R> All Superinterfaces: java.io.Serializable public interface Function2<T1,T2,R> extends java.io.Serializable A two-argument function that takes arguments of type T1 and T2 and returns an R. Method Summary Methods  Modifier and Type Method and Description R call(T1 v1, T2 v2)  Method Detail call R call(T1 v1, T2 v2) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Function3 (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Function3 (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface Function3<T1,T2,T3,R> All Superinterfaces: java.io.Serializable public interface Function3<T1,T2,T3,R> extends java.io.Serializable A three-argument function that takes arguments of type T1, T2 and T3 and returns an R. Method Summary Methods  Modifier and Type Method and Description R call(T1 v1, T2 v2, T3 v3)  Method Detail call R call(T1 v1, T2 v2, T3 v3) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Function4 (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Function4 (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface Function4<T1,T2,T3,T4,R> All Superinterfaces: java.io.Serializable public interface Function4<T1,T2,T3,T4,R> extends java.io.Serializable A four-argument function that takes arguments of type T1, T2, T3 and T4 and returns an R. Method Summary Methods  Modifier and Type Method and Description R call(T1 v1, T2 v2, T3 v3, T4 v4)  Method Detail call R call(T1 v1, T2 v2, T3 v3, T4 v4) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method FutureAction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="FutureAction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Interface FutureAction<T> All Superinterfaces: scala.concurrent.Awaitable<T>, scala.concurrent.Future<T> All Known Implementing Classes: ComplexFutureAction, SimpleFutureAction public interface FutureAction<T> extends scala.concurrent.Future<T> A future for the result of an action to support cancellation. This is an extension of the Scala Future interface to support cancellation. Nested Class Summary Nested classes/interfaces inherited from interface scala.concurrent.Future scala.concurrent.Future.InternalCallbackExecutor$ Method Summary Methods  Modifier and Type Method and Description void cancel() Cancels the execution of this action. T get() Blocks and returns the result of this job. boolean isCancelled() Returns whether the action has been cancelled. boolean isCompleted() Returns whether the action has already been completed with a value or an exception. scala.collection.Seq<Object> jobIds() Returns the job IDs run by the underlying async operation. <U> void onComplete(scala.Function1<scala.util.Try<T>,U> func, scala.concurrent.ExecutionContext executor) When this action is completed, either through an exception, or a value, applies the provided function. FutureAction<T> ready(scala.concurrent.duration.Duration atMost, scala.concurrent.CanAwait permit) Blocks until this action completes. T result(scala.concurrent.duration.Duration atMost, scala.concurrent.CanAwait permit) Awaits and returns the result (of type T) of this action. scala.Option<scala.util.Try<T>> value() The value of this Future. Methods inherited from interface scala.concurrent.Future andThen, collect, failed, fallbackTo, filter, flatMap, foreach, map, mapTo, onFailure, onSuccess, recover, recoverWith, transform, withFilter, zip Method Detail cancel void cancel() Cancels the execution of this action. ready FutureAction<T> ready(scala.concurrent.duration.Duration atMost, scala.concurrent.CanAwait permit) Blocks until this action completes. Specified by: ready in interface scala.concurrent.Awaitable<T> Parameters:atMost - maximum wait time, which may be negative (no waiting is done), Duration.Inf for unbounded waiting, or a finite positive durationpermit - (undocumented) Returns:this FutureAction result T result(scala.concurrent.duration.Duration atMost, scala.concurrent.CanAwait permit) throws Exception Awaits and returns the result (of type T) of this action. Specified by: result in interface scala.concurrent.Awaitable<T> Parameters:atMost - maximum wait time, which may be negative (no waiting is done), Duration.Inf for unbounded waiting, or a finite positive durationpermit - (undocumented) Returns:the result value if the action is completed within the specific maximum wait time Throws: Exception - exception during action execution onComplete <U> void onComplete(scala.Function1<scala.util.Try<T>,U> func, scala.concurrent.ExecutionContext executor) When this action is completed, either through an exception, or a value, applies the provided function. Specified by: onComplete in interface scala.concurrent.Future<T> Parameters:func - (undocumented)executor - (undocumented) isCompleted boolean isCompleted() Returns whether the action has already been completed with a value or an exception. Specified by: isCompleted in interface scala.concurrent.Future<T> Returns:(undocumented) isCancelled boolean isCancelled() Returns whether the action has been cancelled. Returns:(undocumented) value scala.Option<scala.util.Try<T>> value() The value of this Future. If the future is not completed the returned value will be None. If the future is completed the value will be Some(Success(t)) if it contains a valid result, or Some(Failure(error)) if it contains an exception. Specified by: value in interface scala.concurrent.Future<T> Returns:(undocumented) get T get() throws SparkException Blocks and returns the result of this job. Returns:(undocumented) Throws: SparkException jobIds scala.collection.Seq<Object> jobIds() Returns the job IDs run by the underlying async operation. This returns the current snapshot of the job list. Certain operations may run multiple jobs, so multiple calls to this method may return different lists. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GBTClassificationModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GBTClassificationModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class GBTClassificationModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<M> org.apache.spark.ml.PredictionModel<Vector,GBTClassificationModel> org.apache.spark.ml.classification.GBTClassificationModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class GBTClassificationModel extends PredictionModel<Vector,GBTClassificationModel> implements MLWritable, scala.Serializable Gradient-Boosted Trees (GBTs) (http://en.wikipedia.org/wiki/Gradient_boosting) model for classification. It supports binary labels, as well as both continuous and categorical features. Note: Multiclass labels are not currently supported. param: _trees Decision trees in the ensemble. param: _treeWeights Weights for the decision trees in the ensemble. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GBTClassificationModel(String uid, DecisionTreeRegressionModel[] _trees, double[] _treeWeights) Construct a GBTClassificationModel Method Summary Methods  Modifier and Type Method and Description static BooleanParam cacheNodeIds()  static IntParam checkpointInterval()  static Params clear(Param<?> param)  GBTClassificationModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  Vector featureImportances() Estimate of the importance of each feature. static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static <T> scala.Option<T> get(Param<T> param)  static boolean getCacheNodeIds()  static int getCheckpointInterval()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getImpurity()  static String getLabelCol()  String getLabelCol()  static String getLossType()  static int getMaxBins()  static int getMaxDepth()  static int getMaxIter()  static int getMaxMemoryInMB()  static double getMinInfoGain()  static int getMinInstancesPerNode()  static int getNumTrees()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static long getSeed()  static double getStepSize()  static double getSubsamplingRate()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static Param<String> impurity()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static GBTClassificationModel load(String path)  static Param<String> lossType()  static IntParam maxBins()  static IntParam maxDepth()  static IntParam maxIter()  static IntParam maxMemoryInMB()  static DoubleParam minInfoGain()  static IntParam minInstancesPerNode()  int numFeatures() Returns the number of features the model was trained on. int numTrees() Number of trees in ensemble static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static MLReader<GBTClassificationModel> read()  static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  static org.apache.spark.ml.tree.DecisionTreeParams setCacheNodeIds(boolean value)  static org.apache.spark.ml.tree.DecisionTreeParams setCheckpointInterval(int value)  static M setFeaturesCol(String value)  static org.apache.spark.ml.tree.TreeClassifierParams setImpurity(String value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxBins(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxDepth(int value)  static org.apache.spark.ml.tree.GBTParams setMaxIter(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxMemoryInMB(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMinInfoGain(double value)  static org.apache.spark.ml.tree.DecisionTreeParams setMinInstancesPerNode(int value)  static M setParent(Estimator<M> parent)  static M setPredictionCol(String value)  static org.apache.spark.ml.tree.DecisionTreeParams setSeed(long value)  static org.apache.spark.ml.tree.GBTParams setStepSize(double value)  static org.apache.spark.ml.tree.TreeEnsembleParams setSubsamplingRate(double value)  static DoubleParam stepSize()  static DoubleParam subsamplingRate()  static String toDebugString()  String toString()  static int totalNumNodes()  static Dataset<Row> transform(Dataset<?> dataset)  static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static StructType transformSchema(StructType schema)  DecisionTreeRegressionModel[] trees()  double[] treeWeights()  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.PredictionModel setFeaturesCol, setPredictionCol, transform, transformSchema Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Constructor Detail GBTClassificationModel public GBTClassificationModel(String uid, DecisionTreeRegressionModel[] _trees, double[] _treeWeights) Construct a GBTClassificationModel Parameters:_trees - Decision trees in the ensemble._treeWeights - Weights for the decision trees in the ensemble.uid - (undocumented) Method Detail read public static MLReader<GBTClassificationModel> read() load public static GBTClassificationModel load(String path) params public static Param<?>[] params() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setFeaturesCol public static M setFeaturesCol(String value) setPredictionCol public static M setPredictionCol(String value) transformSchema public static StructType transformSchema(StructType schema) transform public static Dataset<Row> transform(Dataset<?> dataset) checkpointInterval public static final IntParam checkpointInterval() getCheckpointInterval public static final int getCheckpointInterval() seed public static final LongParam seed() getSeed public static final long getSeed() maxDepth public static final IntParam maxDepth() maxBins public static final IntParam maxBins() minInstancesPerNode public static final IntParam minInstancesPerNode() minInfoGain public static final DoubleParam minInfoGain() maxMemoryInMB public static final IntParam maxMemoryInMB() cacheNodeIds public static final BooleanParam cacheNodeIds() setMaxDepth public static org.apache.spark.ml.tree.DecisionTreeParams setMaxDepth(int value) getMaxDepth public static final int getMaxDepth() setMaxBins public static org.apache.spark.ml.tree.DecisionTreeParams setMaxBins(int value) getMaxBins public static final int getMaxBins() setMinInstancesPerNode public static org.apache.spark.ml.tree.DecisionTreeParams setMinInstancesPerNode(int value) getMinInstancesPerNode public static final int getMinInstancesPerNode() setMinInfoGain public static org.apache.spark.ml.tree.DecisionTreeParams setMinInfoGain(double value) getMinInfoGain public static final double getMinInfoGain() setSeed public static org.apache.spark.ml.tree.DecisionTreeParams setSeed(long value) setMaxMemoryInMB public static org.apache.spark.ml.tree.DecisionTreeParams setMaxMemoryInMB(int value) getMaxMemoryInMB public static final int getMaxMemoryInMB() setCacheNodeIds public static org.apache.spark.ml.tree.DecisionTreeParams setCacheNodeIds(boolean value) getCacheNodeIds public static final boolean getCacheNodeIds() setCheckpointInterval public static org.apache.spark.ml.tree.DecisionTreeParams setCheckpointInterval(int value) subsamplingRate public static final DoubleParam subsamplingRate() setSubsamplingRate public static org.apache.spark.ml.tree.TreeEnsembleParams setSubsamplingRate(double value) getSubsamplingRate public static final double getSubsamplingRate() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() stepSize public static final DoubleParam stepSize() getStepSize public static final double getStepSize() setMaxIter public static org.apache.spark.ml.tree.GBTParams setMaxIter(int value) setStepSize public static org.apache.spark.ml.tree.GBTParams setStepSize(double value) validateParams public static void validateParams() impurity public static final Param<String> impurity() setImpurity public static org.apache.spark.ml.tree.TreeClassifierParams setImpurity(String value) getImpurity public static final String getImpurity() lossType public static Param<String> lossType() getLossType public static String getLossType() getNumTrees public static int getNumTrees() toDebugString public static String toDebugString() totalNumNodes public static int totalNumNodes() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) numFeatures public int numFeatures() Description copied from class: PredictionModel Returns the number of features the model was trained on. If unknown, returns -1 Overrides: numFeatures in class PredictionModel<Vector,GBTClassificationModel> trees public DecisionTreeRegressionModel[] trees() treeWeights public double[] treeWeights() numTrees public int numTrees() Number of trees in ensemble copy public GBTClassificationModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<GBTClassificationModel> Parameters:extra - (undocumented) Returns:(undocumented) toString public String toString() Specified by: toString in interface Identifiable Overrides: toString in class Object featureImportances public Vector featureImportances() Estimate of the importance of each feature. Each feature's importance is the average of its importance across all trees in the ensemble The importance vector is normalized to sum to 1. This method is suggested by Hastie et al. (Hastie, Tibshirani, Friedman. "The Elements of Statistical Learning, 2nd Edition." 2001.) and follows the implementation from scikit-learn. See DecisionTreeClassificationModel.featureImportances Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GBTClassifier (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GBTClassifier (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class GBTClassifier Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<Vector,GBTClassifier,GBTClassificationModel> org.apache.spark.ml.classification.GBTClassifier All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class GBTClassifier extends Predictor<Vector,GBTClassifier,GBTClassificationModel> implements DefaultParamsWritable Gradient-Boosted Trees (GBTs) (http://en.wikipedia.org/wiki/Gradient_boosting) learning algorithm for classification. It supports binary labels, as well as both continuous and categorical features. Note: Multiclass labels are not currently supported. The implementation is based upon: J.H. Friedman. "Stochastic Gradient Boosting." 1999. Notes on Gradient Boosting vs. TreeBoost: - This implementation is for Stochastic Gradient Boosting, not for TreeBoost. - Both algorithms learn tree ensembles by minimizing loss functions. - TreeBoost (Friedman, 1999) additionally modifies the outputs at tree leaf nodes based on the loss function, whereas the original gradient boosting method does not. - We expect to implement TreeBoost in the future: [https://issues.apache.org/jira/browse/SPARK-4240] See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GBTClassifier()  GBTClassifier(String uid)  Method Summary Methods  Modifier and Type Method and Description static BooleanParam cacheNodeIds()  static IntParam checkpointInterval()  static Params clear(Param<?> param)  GBTClassifier copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static M fit(Dataset<?> dataset)  static M fit(Dataset<?> dataset, ParamMap paramMap)  static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static <T> scala.Option<T> get(Param<T> param)  static boolean getCacheNodeIds()  static int getCheckpointInterval()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getImpurity()  static String getLabelCol()  String getLabelCol()  static String getLossType()  static int getMaxBins()  static int getMaxDepth()  static int getMaxIter()  static int getMaxMemoryInMB()  static double getMinInfoGain()  static int getMinInstancesPerNode()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static long getSeed()  static double getStepSize()  static double getSubsamplingRate()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> impurity()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static GBTClassifier load(String path)  static Param<String> lossType()  static IntParam maxBins()  static IntParam maxDepth()  static IntParam maxIter()  static IntParam maxMemoryInMB()  static DoubleParam minInfoGain()  static IntParam minInstancesPerNode()  static Param<?>[] params()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  GBTClassifier setCacheNodeIds(boolean value)  GBTClassifier setCheckpointInterval(int value)  static Learner setFeaturesCol(String value)  GBTClassifier setImpurity(String value) The impurity setting is ignored for GBT models. static Learner setLabelCol(String value)  GBTClassifier setLossType(String value)  GBTClassifier setMaxBins(int value)  GBTClassifier setMaxDepth(int value)  GBTClassifier setMaxIter(int value)  GBTClassifier setMaxMemoryInMB(int value)  GBTClassifier setMinInfoGain(double value)  GBTClassifier setMinInstancesPerNode(int value)  static Learner setPredictionCol(String value)  GBTClassifier setSeed(long value)  GBTClassifier setStepSize(double value)  GBTClassifier setSubsamplingRate(double value)  static DoubleParam stepSize()  static DoubleParam subsamplingRate()  static String[] supportedLossTypes() Accessor for supported loss settings: logistic static String toString()  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Predictor fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail GBTClassifier public GBTClassifier(String uid) GBTClassifier public GBTClassifier() Method Detail supportedLossTypes public static final String[] supportedLossTypes() Accessor for supported loss settings: logistic load public static GBTClassifier load(String path) toString public static String toString() params public static Param<?>[] params() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) fit public static M fit(Dataset<?> dataset, ParamMap paramMap) fit public static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setLabelCol public static Learner setLabelCol(String value) setFeaturesCol public static Learner setFeaturesCol(String value) setPredictionCol public static Learner setPredictionCol(String value) fit public static M fit(Dataset<?> dataset) transformSchema public static StructType transformSchema(StructType schema) checkpointInterval public static final IntParam checkpointInterval() getCheckpointInterval public static final int getCheckpointInterval() seed public static final LongParam seed() getSeed public static final long getSeed() maxDepth public static final IntParam maxDepth() maxBins public static final IntParam maxBins() minInstancesPerNode public static final IntParam minInstancesPerNode() minInfoGain public static final DoubleParam minInfoGain() maxMemoryInMB public static final IntParam maxMemoryInMB() cacheNodeIds public static final BooleanParam cacheNodeIds() getMaxDepth public static final int getMaxDepth() getMaxBins public static final int getMaxBins() getMinInstancesPerNode public static final int getMinInstancesPerNode() getMinInfoGain public static final double getMinInfoGain() getMaxMemoryInMB public static final int getMaxMemoryInMB() getCacheNodeIds public static final boolean getCacheNodeIds() subsamplingRate public static final DoubleParam subsamplingRate() getSubsamplingRate public static final double getSubsamplingRate() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() stepSize public static final DoubleParam stepSize() getStepSize public static final double getStepSize() validateParams public static void validateParams() impurity public static final Param<String> impurity() getImpurity public static final String getImpurity() lossType public static Param<String> lossType() getLossType public static String getLossType() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setMaxDepth public GBTClassifier setMaxDepth(int value) setMaxBins public GBTClassifier setMaxBins(int value) setMinInstancesPerNode public GBTClassifier setMinInstancesPerNode(int value) setMinInfoGain public GBTClassifier setMinInfoGain(double value) setMaxMemoryInMB public GBTClassifier setMaxMemoryInMB(int value) setCacheNodeIds public GBTClassifier setCacheNodeIds(boolean value) setCheckpointInterval public GBTClassifier setCheckpointInterval(int value) setImpurity public GBTClassifier setImpurity(String value) The impurity setting is ignored for GBT models. Individual trees are built using impurity "Variance." Parameters:value - (undocumented) Returns:(undocumented) setSubsamplingRate public GBTClassifier setSubsamplingRate(double value) setSeed public GBTClassifier setSeed(long value) setMaxIter public GBTClassifier setMaxIter(int value) setStepSize public GBTClassifier setStepSize(double value) setLossType public GBTClassifier setLossType(String value) copy public GBTClassifier copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Predictor<Vector,GBTClassifier,GBTClassificationModel> Parameters:extra - (undocumented) Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GBTRegressionModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GBTRegressionModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GBTRegressionModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<M> org.apache.spark.ml.PredictionModel<Vector,GBTRegressionModel> org.apache.spark.ml.regression.GBTRegressionModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class GBTRegressionModel extends PredictionModel<Vector,GBTRegressionModel> implements MLWritable, scala.Serializable Gradient-Boosted Trees (GBTs) model for regression. It supports both continuous and categorical features. param: _trees Decision trees in the ensemble. param: _treeWeights Weights for the decision trees in the ensemble. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GBTRegressionModel(String uid, DecisionTreeRegressionModel[] _trees, double[] _treeWeights) Construct a GBTRegressionModel Method Summary Methods  Modifier and Type Method and Description static BooleanParam cacheNodeIds()  static IntParam checkpointInterval()  static Params clear(Param<?> param)  GBTRegressionModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  Vector featureImportances() Estimate of the importance of each feature. static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static <T> scala.Option<T> get(Param<T> param)  static boolean getCacheNodeIds()  static int getCheckpointInterval()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getImpurity()  static String getLabelCol()  String getLabelCol()  static String getLossType()  static int getMaxBins()  static int getMaxDepth()  static int getMaxIter()  static int getMaxMemoryInMB()  static double getMinInfoGain()  static int getMinInstancesPerNode()  static int getNumTrees()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static long getSeed()  static double getStepSize()  static double getSubsamplingRate()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static Param<String> impurity()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static GBTRegressionModel load(String path)  static Param<String> lossType()  static IntParam maxBins()  static IntParam maxDepth()  static IntParam maxIter()  static IntParam maxMemoryInMB()  static DoubleParam minInfoGain()  static IntParam minInstancesPerNode()  int numFeatures() Returns the number of features the model was trained on. int numTrees() Number of trees in ensemble static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static MLReader<GBTRegressionModel> read()  static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  static org.apache.spark.ml.tree.DecisionTreeParams setCacheNodeIds(boolean value)  static org.apache.spark.ml.tree.DecisionTreeParams setCheckpointInterval(int value)  static M setFeaturesCol(String value)  static org.apache.spark.ml.tree.TreeRegressorParams setImpurity(String value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxBins(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxDepth(int value)  static org.apache.spark.ml.tree.GBTParams setMaxIter(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMaxMemoryInMB(int value)  static org.apache.spark.ml.tree.DecisionTreeParams setMinInfoGain(double value)  static org.apache.spark.ml.tree.DecisionTreeParams setMinInstancesPerNode(int value)  static M setParent(Estimator<M> parent)  static M setPredictionCol(String value)  static org.apache.spark.ml.tree.DecisionTreeParams setSeed(long value)  static org.apache.spark.ml.tree.GBTParams setStepSize(double value)  static org.apache.spark.ml.tree.TreeEnsembleParams setSubsamplingRate(double value)  static DoubleParam stepSize()  static DoubleParam subsamplingRate()  static String toDebugString()  String toString()  static int totalNumNodes()  static Dataset<Row> transform(Dataset<?> dataset)  static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static StructType transformSchema(StructType schema)  DecisionTreeRegressionModel[] trees()  double[] treeWeights()  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.PredictionModel setFeaturesCol, setPredictionCol, transform, transformSchema Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Constructor Detail GBTRegressionModel public GBTRegressionModel(String uid, DecisionTreeRegressionModel[] _trees, double[] _treeWeights) Construct a GBTRegressionModel Parameters:_trees - Decision trees in the ensemble._treeWeights - Weights for the decision trees in the ensemble.uid - (undocumented) Method Detail read public static MLReader<GBTRegressionModel> read() load public static GBTRegressionModel load(String path) params public static Param<?>[] params() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setFeaturesCol public static M setFeaturesCol(String value) setPredictionCol public static M setPredictionCol(String value) transformSchema public static StructType transformSchema(StructType schema) transform public static Dataset<Row> transform(Dataset<?> dataset) checkpointInterval public static final IntParam checkpointInterval() getCheckpointInterval public static final int getCheckpointInterval() seed public static final LongParam seed() getSeed public static final long getSeed() maxDepth public static final IntParam maxDepth() maxBins public static final IntParam maxBins() minInstancesPerNode public static final IntParam minInstancesPerNode() minInfoGain public static final DoubleParam minInfoGain() maxMemoryInMB public static final IntParam maxMemoryInMB() cacheNodeIds public static final BooleanParam cacheNodeIds() setMaxDepth public static org.apache.spark.ml.tree.DecisionTreeParams setMaxDepth(int value) getMaxDepth public static final int getMaxDepth() setMaxBins public static org.apache.spark.ml.tree.DecisionTreeParams setMaxBins(int value) getMaxBins public static final int getMaxBins() setMinInstancesPerNode public static org.apache.spark.ml.tree.DecisionTreeParams setMinInstancesPerNode(int value) getMinInstancesPerNode public static final int getMinInstancesPerNode() setMinInfoGain public static org.apache.spark.ml.tree.DecisionTreeParams setMinInfoGain(double value) getMinInfoGain public static final double getMinInfoGain() setSeed public static org.apache.spark.ml.tree.DecisionTreeParams setSeed(long value) setMaxMemoryInMB public static org.apache.spark.ml.tree.DecisionTreeParams setMaxMemoryInMB(int value) getMaxMemoryInMB public static final int getMaxMemoryInMB() setCacheNodeIds public static org.apache.spark.ml.tree.DecisionTreeParams setCacheNodeIds(boolean value) getCacheNodeIds public static final boolean getCacheNodeIds() setCheckpointInterval public static org.apache.spark.ml.tree.DecisionTreeParams setCheckpointInterval(int value) subsamplingRate public static final DoubleParam subsamplingRate() setSubsamplingRate public static org.apache.spark.ml.tree.TreeEnsembleParams setSubsamplingRate(double value) getSubsamplingRate public static final double getSubsamplingRate() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() stepSize public static final DoubleParam stepSize() getStepSize public static final double getStepSize() setMaxIter public static org.apache.spark.ml.tree.GBTParams setMaxIter(int value) setStepSize public static org.apache.spark.ml.tree.GBTParams setStepSize(double value) validateParams public static void validateParams() impurity public static final Param<String> impurity() setImpurity public static org.apache.spark.ml.tree.TreeRegressorParams setImpurity(String value) getImpurity public static final String getImpurity() lossType public static Param<String> lossType() getLossType public static String getLossType() getNumTrees public static int getNumTrees() toDebugString public static String toDebugString() totalNumNodes public static int totalNumNodes() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) numFeatures public int numFeatures() Description copied from class: PredictionModel Returns the number of features the model was trained on. If unknown, returns -1 Overrides: numFeatures in class PredictionModel<Vector,GBTRegressionModel> trees public DecisionTreeRegressionModel[] trees() treeWeights public double[] treeWeights() numTrees public int numTrees() Number of trees in ensemble copy public GBTRegressionModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<GBTRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) toString public String toString() Specified by: toString in interface Identifiable Overrides: toString in class Object featureImportances public Vector featureImportances() Estimate of the importance of each feature. Each feature's importance is the average of its importance across all trees in the ensemble The importance vector is normalized to sum to 1. This method is suggested by Hastie et al. (Hastie, Tibshirani, Friedman. "The Elements of Statistical Learning, 2nd Edition." 2001.) and follows the implementation from scikit-learn. Returns:(undocumented)See Also:DecisionTreeRegressionModel.featureImportances write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GBTRegressor (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GBTRegressor (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GBTRegressor Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<Vector,GBTRegressor,GBTRegressionModel> org.apache.spark.ml.regression.GBTRegressor All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class GBTRegressor extends Predictor<Vector,GBTRegressor,GBTRegressionModel> implements DefaultParamsWritable Gradient-Boosted Trees (GBTs) learning algorithm for regression. It supports both continuous and categorical features. The implementation is based upon: J.H. Friedman. "Stochastic Gradient Boosting." 1999. Notes on Gradient Boosting vs. TreeBoost: - This implementation is for Stochastic Gradient Boosting, not for TreeBoost. - Both algorithms learn tree ensembles by minimizing loss functions. - TreeBoost (Friedman, 1999) additionally modifies the outputs at tree leaf nodes based on the loss function, whereas the original gradient boosting method does not. - When the loss is SquaredError, these methods give the same result, but they could differ for other loss functions. - We expect to implement TreeBoost in the future: [https://issues.apache.org/jira/browse/SPARK-4240] See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GBTRegressor()  GBTRegressor(String uid)  Method Summary Methods  Modifier and Type Method and Description static BooleanParam cacheNodeIds()  static IntParam checkpointInterval()  static Params clear(Param<?> param)  GBTRegressor copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static M fit(Dataset<?> dataset)  static M fit(Dataset<?> dataset, ParamMap paramMap)  static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static <T> scala.Option<T> get(Param<T> param)  static boolean getCacheNodeIds()  static int getCheckpointInterval()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getImpurity()  static String getLabelCol()  String getLabelCol()  static String getLossType()  static int getMaxBins()  static int getMaxDepth()  static int getMaxIter()  static int getMaxMemoryInMB()  static double getMinInfoGain()  static int getMinInstancesPerNode()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static long getSeed()  static double getStepSize()  static double getSubsamplingRate()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> impurity()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static GBTRegressor load(String path)  static Param<String> lossType()  static IntParam maxBins()  static IntParam maxDepth()  static IntParam maxIter()  static IntParam maxMemoryInMB()  static DoubleParam minInfoGain()  static IntParam minInstancesPerNode()  static Param<?>[] params()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  GBTRegressor setCacheNodeIds(boolean value)  GBTRegressor setCheckpointInterval(int value)  static Learner setFeaturesCol(String value)  GBTRegressor setImpurity(String value) The impurity setting is ignored for GBT models. static Learner setLabelCol(String value)  GBTRegressor setLossType(String value)  GBTRegressor setMaxBins(int value)  GBTRegressor setMaxDepth(int value)  GBTRegressor setMaxIter(int value)  GBTRegressor setMaxMemoryInMB(int value)  GBTRegressor setMinInfoGain(double value)  GBTRegressor setMinInstancesPerNode(int value)  static Learner setPredictionCol(String value)  GBTRegressor setSeed(long value)  GBTRegressor setStepSize(double value)  GBTRegressor setSubsamplingRate(double value)  static DoubleParam stepSize()  static DoubleParam subsamplingRate()  static String[] supportedLossTypes() Accessor for supported loss settings: squared (L2), absolute (L1) static String toString()  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Predictor fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail GBTRegressor public GBTRegressor(String uid) GBTRegressor public GBTRegressor() Method Detail supportedLossTypes public static final String[] supportedLossTypes() Accessor for supported loss settings: squared (L2), absolute (L1) load public static GBTRegressor load(String path) toString public static String toString() params public static Param<?>[] params() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) fit public static M fit(Dataset<?> dataset, ParamMap paramMap) fit public static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setLabelCol public static Learner setLabelCol(String value) setFeaturesCol public static Learner setFeaturesCol(String value) setPredictionCol public static Learner setPredictionCol(String value) fit public static M fit(Dataset<?> dataset) transformSchema public static StructType transformSchema(StructType schema) checkpointInterval public static final IntParam checkpointInterval() getCheckpointInterval public static final int getCheckpointInterval() seed public static final LongParam seed() getSeed public static final long getSeed() maxDepth public static final IntParam maxDepth() maxBins public static final IntParam maxBins() minInstancesPerNode public static final IntParam minInstancesPerNode() minInfoGain public static final DoubleParam minInfoGain() maxMemoryInMB public static final IntParam maxMemoryInMB() cacheNodeIds public static final BooleanParam cacheNodeIds() getMaxDepth public static final int getMaxDepth() getMaxBins public static final int getMaxBins() getMinInstancesPerNode public static final int getMinInstancesPerNode() getMinInfoGain public static final double getMinInfoGain() getMaxMemoryInMB public static final int getMaxMemoryInMB() getCacheNodeIds public static final boolean getCacheNodeIds() subsamplingRate public static final DoubleParam subsamplingRate() getSubsamplingRate public static final double getSubsamplingRate() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() stepSize public static final DoubleParam stepSize() getStepSize public static final double getStepSize() validateParams public static void validateParams() impurity public static final Param<String> impurity() getImpurity public static final String getImpurity() lossType public static Param<String> lossType() getLossType public static String getLossType() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setMaxDepth public GBTRegressor setMaxDepth(int value) setMaxBins public GBTRegressor setMaxBins(int value) setMinInstancesPerNode public GBTRegressor setMinInstancesPerNode(int value) setMinInfoGain public GBTRegressor setMinInfoGain(double value) setMaxMemoryInMB public GBTRegressor setMaxMemoryInMB(int value) setCacheNodeIds public GBTRegressor setCacheNodeIds(boolean value) setCheckpointInterval public GBTRegressor setCheckpointInterval(int value) setImpurity public GBTRegressor setImpurity(String value) The impurity setting is ignored for GBT models. Individual trees are built using impurity "Variance." Parameters:value - (undocumented) Returns:(undocumented) setSubsamplingRate public GBTRegressor setSubsamplingRate(double value) setSeed public GBTRegressor setSeed(long value) setMaxIter public GBTRegressor setMaxIter(int value) setStepSize public GBTRegressor setStepSize(double value) setLossType public GBTRegressor setLossType(String value) copy public GBTRegressor copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Predictor<Vector,GBTRegressor,GBTRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GLMClassificationModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GLMClassificationModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification.impl Class GLMClassificationModel.SaveLoadV1_0$ Object org.apache.spark.mllib.classification.impl.GLMClassificationModel.SaveLoadV1_0$ Enclosing class: GLMClassificationModel public static class GLMClassificationModel.SaveLoadV1_0$ extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description class  GLMClassificationModel.SaveLoadV1_0$.Data Model data for import/export Field Summary Fields  Modifier and Type Field and Description static GLMClassificationModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GLMClassificationModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.mllib.classification.impl.GLMClassificationModel.SaveLoadV1_0.Data loadData(SparkContext sc, String path, String modelClass) Helper method for loading GLM classification model data. void save(SparkContext sc, String path, String modelClass, int numFeatures, int numClasses, Vector weights, double intercept, scala.Option<Object> threshold) Helper method for saving GLM classification model metadata and data. String thisFormatVersion()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GLMClassificationModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GLMClassificationModel.SaveLoadV1_0$ public GLMClassificationModel.SaveLoadV1_0$() Method Detail thisFormatVersion public String thisFormatVersion() save public void save(SparkContext sc, String path, String modelClass, int numFeatures, int numClasses, Vector weights, double intercept, scala.Option<Object> threshold) Helper method for saving GLM classification model metadata and data. Parameters:modelClass - String name for model class, to be saved with metadatanumClasses - Number of classes label can take, to be saved with metadatasc - (undocumented)path - (undocumented)numFeatures - (undocumented)weights - (undocumented)intercept - (undocumented)threshold - (undocumented) loadData public org.apache.spark.mllib.classification.impl.GLMClassificationModel.SaveLoadV1_0.Data loadData(SparkContext sc, String path, String modelClass) Helper method for loading GLM classification model data. NOTE: Callers of this method should check numClasses, numFeatures on their own. Parameters:modelClass - String name for model class (used for error messages)sc - (undocumented)path - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GLMClassificationModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GLMClassificationModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification.impl Class GLMClassificationModel Object org.apache.spark.mllib.classification.impl.GLMClassificationModel public class GLMClassificationModel extends Object Helper class for import/export of GLM classification models. Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  GLMClassificationModel.SaveLoadV1_0$  Constructor Summary Constructors  Constructor and Description GLMClassificationModel()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GLMClassificationModel public GLMClassificationModel() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GLMRegressionModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GLMRegressionModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression.impl Class GLMRegressionModel.SaveLoadV1_0$ Object org.apache.spark.mllib.regression.impl.GLMRegressionModel.SaveLoadV1_0$ Enclosing class: GLMRegressionModel public static class GLMRegressionModel.SaveLoadV1_0$ extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description class  GLMRegressionModel.SaveLoadV1_0$.Data Model data for model import/export Field Summary Fields  Modifier and Type Field and Description static GLMRegressionModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GLMRegressionModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.mllib.regression.impl.GLMRegressionModel.SaveLoadV1_0.Data loadData(SparkContext sc, String path, String modelClass, int numFeatures) Helper method for loading GLM regression model data. void save(SparkContext sc, String path, String modelClass, Vector weights, double intercept) Helper method for saving GLM regression model metadata and data. String thisFormatVersion()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GLMRegressionModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GLMRegressionModel.SaveLoadV1_0$ public GLMRegressionModel.SaveLoadV1_0$() Method Detail thisFormatVersion public String thisFormatVersion() save public void save(SparkContext sc, String path, String modelClass, Vector weights, double intercept) Helper method for saving GLM regression model metadata and data. Parameters:modelClass - String name for model class, to be saved with metadatasc - (undocumented)path - (undocumented)weights - (undocumented)intercept - (undocumented) loadData public org.apache.spark.mllib.regression.impl.GLMRegressionModel.SaveLoadV1_0.Data loadData(SparkContext sc, String path, String modelClass, int numFeatures) Helper method for loading GLM regression model data. Parameters:modelClass - String name for model class (used for error messages)numFeatures - Number of features, to be checked against loaded data. The length of the weights vector should equal numFeatures.sc - (undocumented)path - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GLMRegressionModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GLMRegressionModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression.impl Class GLMRegressionModel Object org.apache.spark.mllib.regression.impl.GLMRegressionModel public class GLMRegressionModel extends Object Helper methods for import/export of GLM regression models. Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  GLMRegressionModel.SaveLoadV1_0$  Constructor Summary Constructors  Constructor and Description GLMRegressionModel()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GLMRegressionModel public GLMRegressionModel() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GammaGenerator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GammaGenerator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.random Class GammaGenerator Object org.apache.spark.mllib.random.GammaGenerator All Implemented Interfaces: java.io.Serializable, RandomDataGenerator<Object>, Pseudorandom public class GammaGenerator extends Object implements RandomDataGenerator<Object> :: DeveloperApi :: Generates i.i.d. samples from the gamma distribution with the given shape and scale. param: shape shape for the gamma distribution. param: scale scale for the gamma distribution See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GammaGenerator(double shape, double scale)  Method Summary Methods  Modifier and Type Method and Description GammaGenerator copy() Returns a copy of the RandomDataGenerator with a new instance of the rng object used in the class when applicable for non-locking concurrent usage. double nextValue() Returns an i.i.d. double scale()  void setSeed(long seed) Set random seed. double shape()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GammaGenerator public GammaGenerator(double shape, double scale) Method Detail shape public double shape() scale public double scale() nextValue public double nextValue() Description copied from interface: RandomDataGenerator Returns an i.i.d. sample as a generic type from an underlying distribution. Specified by: nextValue in interface RandomDataGenerator<Object> Returns:(undocumented) setSeed public void setSeed(long seed) Description copied from interface: Pseudorandom Set random seed. Specified by: setSeed in interface Pseudorandom copy public GammaGenerator copy() Description copied from interface: RandomDataGenerator Returns a copy of the RandomDataGenerator with a new instance of the rng object used in the class when applicable for non-locking concurrent usage. Specified by: copy in interface RandomDataGenerator<Object> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GaussianMixture (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GaussianMixture (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class GaussianMixture Object org.apache.spark.mllib.clustering.GaussianMixture All Implemented Interfaces: java.io.Serializable public class GaussianMixture extends Object implements scala.Serializable This class performs expectation maximization for multivariate Gaussian Mixture Models (GMMs). A GMM represents a composite distribution of independent Gaussian distributions with associated "mixing" weights specifying each's contribution to the composite. Given a set of sample points, this class will maximize the log-likelihood for a mixture of k Gaussians, iterating until the log-likelihood changes by less than convergenceTol, or until it has reached the max number of iterations. While this process is generally guaranteed to converge, it is not guaranteed to find a global optimum. Note: For high-dimensional data (with many features), this algorithm may perform poorly. This is due to high-dimensional data (a) making it difficult to cluster at all (based on statistical/theoretical arguments) and (b) numerical issues with Gaussian distributions. param: k Number of independent Gaussians in the mixture model. param: convergenceTol Maximum change in log-likelihood at which convergence is considered to have occurred. param: maxIterations Maximum number of iterations allowed. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GaussianMixture() Constructs a default instance. Method Summary Methods  Modifier and Type Method and Description double getConvergenceTol() Return the largest change in log-likelihood at which convergence is considered to have occurred. scala.Option<GaussianMixtureModel> getInitialModel() Return the user supplied initial GMM, if supplied int getK() Return the number of Gaussians in the mixture model int getMaxIterations() Return the maximum number of iterations allowed long getSeed() Return the random seed GaussianMixtureModel run(JavaRDD<Vector> data) Java-friendly version of run() GaussianMixtureModel run(RDD<Vector> data) Perform expectation maximization GaussianMixture setConvergenceTol(double convergenceTol) Set the largest change in log-likelihood at which convergence is considered to have occurred. GaussianMixture setInitialModel(GaussianMixtureModel model) Set the initial GMM starting point, bypassing the random initialization. GaussianMixture setK(int k) Set the number of Gaussians in the mixture model. GaussianMixture setMaxIterations(int maxIterations) Set the maximum number of iterations allowed. GaussianMixture setSeed(long seed) Set the random seed static boolean shouldDistributeGaussians(int k, int d) Heuristic to distribute the computation of the MultivariateGaussians, approximately when d > 25 except for when k is very small. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GaussianMixture public GaussianMixture() Constructs a default instance. The default parameters are {k: 2, convergenceTol: 0.01, maxIterations: 100, seed: random}. Method Detail shouldDistributeGaussians public static boolean shouldDistributeGaussians(int k, int d) Heuristic to distribute the computation of the MultivariateGaussians, approximately when d > 25 except for when k is very small. Parameters:k - Number of topicsd - Number of features Returns:(undocumented) setInitialModel public GaussianMixture setInitialModel(GaussianMixtureModel model) Set the initial GMM starting point, bypassing the random initialization. You must call setK() prior to calling this method, and the condition (model.k == this.k) must be met; failure will result in an IllegalArgumentException Parameters:model - (undocumented) Returns:(undocumented) getInitialModel public scala.Option<GaussianMixtureModel> getInitialModel() Return the user supplied initial GMM, if supplied Returns:(undocumented) setK public GaussianMixture setK(int k) Set the number of Gaussians in the mixture model. Default: 2 Parameters:k - (undocumented) Returns:(undocumented) getK public int getK() Return the number of Gaussians in the mixture model Returns:(undocumented) setMaxIterations public GaussianMixture setMaxIterations(int maxIterations) Set the maximum number of iterations allowed. Default: 100 Parameters:maxIterations - (undocumented) Returns:(undocumented) getMaxIterations public int getMaxIterations() Return the maximum number of iterations allowed Returns:(undocumented) setConvergenceTol public GaussianMixture setConvergenceTol(double convergenceTol) Set the largest change in log-likelihood at which convergence is considered to have occurred. Parameters:convergenceTol - (undocumented) Returns:(undocumented) getConvergenceTol public double getConvergenceTol() Return the largest change in log-likelihood at which convergence is considered to have occurred. Returns:(undocumented) setSeed public GaussianMixture setSeed(long seed) Set the random seed Parameters:seed - (undocumented) Returns:(undocumented) getSeed public long getSeed() Return the random seed Returns:(undocumented) run public GaussianMixtureModel run(RDD<Vector> data) Perform expectation maximization Parameters:data - (undocumented) Returns:(undocumented) run public GaussianMixtureModel run(JavaRDD<Vector> data) Java-friendly version of run() Parameters:data - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GaussianMixtureModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GaussianMixtureModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class GaussianMixtureModel Object org.apache.spark.mllib.clustering.GaussianMixtureModel All Implemented Interfaces: java.io.Serializable, Saveable public class GaussianMixtureModel extends Object implements scala.Serializable, Saveable Multivariate Gaussian Mixture Model (GMM) consisting of k Gaussians, where points are drawn from each Gaussian i=1..k with probability w(i); mu(i) and sigma(i) are the respective mean and covariance for each Gaussian distribution i=1..k. param: weights Weights for each Gaussian distribution in the mixture, where weights(i) is the weight for Gaussian i, and weights.sum == 1 param: gaussians Array of MultivariateGaussian where gaussians(i) represents the Multivariate Gaussian (Normal) Distribution for Gaussian i See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GaussianMixtureModel(double[] weights, MultivariateGaussian[] gaussians)  Method Summary Methods  Modifier and Type Method and Description MultivariateGaussian[] gaussians()  int k() Number of gaussians in mixture static GaussianMixtureModel load(SparkContext sc, String path)  JavaRDD<Integer> predict(JavaRDD<Vector> points) Java-friendly version of predict() RDD<Object> predict(RDD<Vector> points) Maps given points to their cluster indices. int predict(Vector point) Maps given point to its cluster index. RDD<double[]> predictSoft(RDD<Vector> points) Given the input vectors, return the membership value of each vector to all mixture components. double[] predictSoft(Vector point) Given the input vector, return the membership values to all mixture components. void save(SparkContext sc, String path) Save this model to the given path. double[] weights()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GaussianMixtureModel public GaussianMixtureModel(double[] weights, MultivariateGaussian[] gaussians) Method Detail load public static GaussianMixtureModel load(SparkContext sc, String path) weights public double[] weights() gaussians public MultivariateGaussian[] gaussians() save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. k public int k() Number of gaussians in mixture Returns:(undocumented) predict public RDD<Object> predict(RDD<Vector> points) Maps given points to their cluster indices. Parameters:points - (undocumented) Returns:(undocumented) predict public int predict(Vector point) Maps given point to its cluster index. Parameters:point - (undocumented) Returns:(undocumented) predict public JavaRDD<Integer> predict(JavaRDD<Vector> points) Java-friendly version of predict() Parameters:points - (undocumented) Returns:(undocumented) predictSoft public RDD<double[]> predictSoft(RDD<Vector> points) Given the input vectors, return the membership value of each vector to all mixture components. Parameters:points - (undocumented) Returns:(undocumented) predictSoft public double[] predictSoft(Vector point) Given the input vector, return the membership values to all mixture components. Parameters:point - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GaussianMixtureSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GaussianMixtureSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.clustering Class GaussianMixtureSummary Object org.apache.spark.ml.clustering.GaussianMixtureSummary All Implemented Interfaces: java.io.Serializable public class GaussianMixtureSummary extends Object implements scala.Serializable :: Experimental :: Summary of GaussianMixture. param: predictions DataFrame produced by GaussianMixtureModel.transform() param: predictionCol Name for column of predicted clusters in predictions param: probabilityCol Name for column of predicted probability of each cluster in predictions param: featuresCol Name for column of features in predictions param: k Number of clusters See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description Dataset<Row> cluster() Cluster centers of the transformed data. long[] clusterSizes() Size of (number of data points in) each cluster. String featuresCol()  int k()  String predictionCol()  Dataset<Row> predictions()  Dataset<Row> probability() Probability of each cluster. String probabilityCol()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail predictions public Dataset<Row> predictions() predictionCol public String predictionCol() probabilityCol public String probabilityCol() featuresCol public String featuresCol() k public int k() cluster public Dataset<Row> cluster() Cluster centers of the transformed data. Returns:(undocumented) probability public Dataset<Row> probability() Probability of each cluster. Returns:(undocumented) clusterSizes public long[] clusterSizes() Size of (number of data points in) each cluster. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearAlgorithm (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearAlgorithm (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression Class GeneralizedLinearAlgorithm<M extends GeneralizedLinearModel> Object org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm<M> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: LassoWithSGD, LinearRegressionWithSGD, LogisticRegressionWithLBFGS, LogisticRegressionWithSGD, RidgeRegressionWithSGD, SVMWithSGD public abstract class GeneralizedLinearAlgorithm<M extends GeneralizedLinearModel> extends Object implements scala.Serializable :: DeveloperApi :: GeneralizedLinearAlgorithm implements methods to train a Generalized Linear Model (GLM). This class should be extended with an Optimizer to create a new GLM. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GeneralizedLinearAlgorithm()  Method Summary Methods  Modifier and Type Method and Description int getNumFeatures() The dimension of training features. boolean isAddIntercept() Get if the algorithm uses addIntercept abstract Optimizer optimizer() The optimizer to solve the problem. M run(RDD<LabeledPoint> input) Run the algorithm with the configured parameters on an input RDD of LabeledPoint entries. M run(RDD<LabeledPoint> input, Vector initialWeights) Run the algorithm with the configured parameters on an input RDD of LabeledPoint entries starting from the initial weights provided. GeneralizedLinearAlgorithm<M> setIntercept(boolean addIntercept) Set if the algorithm should add an intercept. GeneralizedLinearAlgorithm<M> setValidateData(boolean validateData) Set if the algorithm should validate data before training. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GeneralizedLinearAlgorithm public GeneralizedLinearAlgorithm() Method Detail optimizer public abstract Optimizer optimizer() The optimizer to solve the problem. Returns:(undocumented) getNumFeatures public int getNumFeatures() The dimension of training features. Returns:(undocumented) isAddIntercept public boolean isAddIntercept() Get if the algorithm uses addIntercept Returns:(undocumented) setIntercept public GeneralizedLinearAlgorithm<M> setIntercept(boolean addIntercept) Set if the algorithm should add an intercept. Default false. We set the default to false because adding the intercept will cause memory allocation. Parameters:addIntercept - (undocumented) Returns:(undocumented) setValidateData public GeneralizedLinearAlgorithm<M> setValidateData(boolean validateData) Set if the algorithm should validate data before training. Default true. Parameters:validateData - (undocumented) Returns:(undocumented) run public M run(RDD<LabeledPoint> input) Run the algorithm with the configured parameters on an input RDD of LabeledPoint entries. Parameters:input - (undocumented) Returns:(undocumented) run public M run(RDD<LabeledPoint> input, Vector initialWeights) Run the algorithm with the configured parameters on an input RDD of LabeledPoint entries starting from the initial weights provided. Parameters:input - (undocumented)initialWeights - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression Class GeneralizedLinearModel Object org.apache.spark.mllib.regression.GeneralizedLinearModel All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: LassoModel, LinearRegressionModel, LogisticRegressionModel, RidgeRegressionModel, SVMModel public abstract class GeneralizedLinearModel extends Object implements scala.Serializable :: DeveloperApi :: GeneralizedLinearModel (GLM) represents a model trained using GeneralizedLinearAlgorithm. GLMs consist of a weight vector and an intercept. param: weights Weights computed for every feature. param: intercept Intercept computed for this model. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GeneralizedLinearModel(Vector weights, double intercept)  Method Summary Methods  Modifier and Type Method and Description double intercept()  RDD<Object> predict(RDD<Vector> testData) Predict values for the given data set using the model trained. double predict(Vector testData) Predict values for a single data point using the model trained. String toString() Print a summary of the model. Vector weights()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail GeneralizedLinearModel public GeneralizedLinearModel(Vector weights, double intercept) Method Detail weights public Vector weights() intercept public double intercept() predict public RDD<Object> predict(RDD<Vector> testData) Predict values for the given data set using the model trained. Parameters:testData - RDD representing data points to be predicted Returns:RDD[Double] where each entry contains the corresponding prediction predict public double predict(Vector testData) Predict values for a single data point using the model trained. Parameters:testData - array representing a single data point Returns:Double prediction from the trained model toString public String toString() Print a summary of the model. Overrides: toString in class Object Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Binomial$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Binomial$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Binomial$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Binomial$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Binomial$ extends Object Binomial exponential family distribution. The default link for the Binomial family is the logit link. See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Binomial$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Binomial$()  Method Summary Methods  Modifier and Type Method and Description double aic(RDD<scala.Tuple3<Object,Object,Object>> predictions, double deviance, double numInstances, double weightSum) Akaike Information Criterion (AIC) value of the family for a given dataset. org.apache.spark.ml.regression.GeneralizedLinearRegression.Link defaultLink() The default link instance of this family. double deviance(double y, double mu, double weight) Deviance of (y, mu) pair. double initialize(double y, double weight) Initialize the starting value for mu. String name()  double project(double mu) Trim the fitted value so that it will be in valid range. double variance(double mu) The variance of the endogenous variable's mean, given the value mu. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Binomial$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Binomial$ public GeneralizedLinearRegression.Binomial$() Method Detail defaultLink public org.apache.spark.ml.regression.GeneralizedLinearRegression.Link defaultLink() The default link instance of this family. initialize public double initialize(double y, double weight) Initialize the starting value for mu. variance public double variance(double mu) The variance of the endogenous variable's mean, given the value mu. deviance public double deviance(double y, double mu, double weight) Deviance of (y, mu) pair. aic public double aic(RDD<scala.Tuple3<Object,Object,Object>> predictions, double deviance, double numInstances, double weightSum) Akaike Information Criterion (AIC) value of the family for a given dataset. Parameters:predictions - an RDD of (y, mu, weight) of instances in evaluation datasetdeviance - the deviance for the fitted model in evaluation datasetnumInstances - number of instances in evaluation datasetweightSum - weights sum of instances in evaluation dataset Returns:(undocumented) project public double project(double mu) Trim the fitted value so that it will be in valid range. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.CLogLog$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.CLogLog$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.CLogLog$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.CLogLog$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.CLogLog$ extends Object See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.CLogLog$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.CLogLog$()  Method Summary Methods  Modifier and Type Method and Description double deriv(double mu) Derivative of the link function. double link(double mu) The link function. String name()  double unlink(double eta) The inverse link function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.CLogLog$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.CLogLog$ public GeneralizedLinearRegression.CLogLog$() Method Detail link public double link(double mu) The link function. deriv public double deriv(double mu) Derivative of the link function. unlink public double unlink(double eta) The inverse link function. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Family$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Family$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Family$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Family$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Family$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Family$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Family$()  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.ml.regression.GeneralizedLinearRegression.Family fromName(String name) Gets the Family object from its name. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Family$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Family$ public GeneralizedLinearRegression.Family$() Method Detail fromName public org.apache.spark.ml.regression.GeneralizedLinearRegression.Family fromName(String name) Gets the Family object from its name. Parameters:name - family name: "gaussian", "binomial", "poisson" or "gamma". Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Gamma$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Gamma$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Gamma$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Gamma$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Gamma$ extends Object Gamma exponential family distribution. The default link for the Gamma family is the inverse link. See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Gamma$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Gamma$()  Method Summary Methods  Modifier and Type Method and Description double aic(RDD<scala.Tuple3<Object,Object,Object>> predictions, double deviance, double numInstances, double weightSum) Akaike Information Criterion (AIC) value of the family for a given dataset. org.apache.spark.ml.regression.GeneralizedLinearRegression.Link defaultLink() The default link instance of this family. double deviance(double y, double mu, double weight) Deviance of (y, mu) pair. double initialize(double y, double weight) Initialize the starting value for mu. String name()  double project(double mu) Trim the fitted value so that it will be in valid range. double variance(double mu) The variance of the endogenous variable's mean, given the value mu. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Gamma$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Gamma$ public GeneralizedLinearRegression.Gamma$() Method Detail defaultLink public org.apache.spark.ml.regression.GeneralizedLinearRegression.Link defaultLink() The default link instance of this family. initialize public double initialize(double y, double weight) Initialize the starting value for mu. variance public double variance(double mu) The variance of the endogenous variable's mean, given the value mu. deviance public double deviance(double y, double mu, double weight) Deviance of (y, mu) pair. aic public double aic(RDD<scala.Tuple3<Object,Object,Object>> predictions, double deviance, double numInstances, double weightSum) Akaike Information Criterion (AIC) value of the family for a given dataset. Parameters:predictions - an RDD of (y, mu, weight) of instances in evaluation datasetdeviance - the deviance for the fitted model in evaluation datasetnumInstances - number of instances in evaluation datasetweightSum - weights sum of instances in evaluation dataset Returns:(undocumented) project public double project(double mu) Trim the fitted value so that it will be in valid range. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Gaussian$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Gaussian$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Gaussian$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Gaussian$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Gaussian$ extends Object Gaussian exponential family distribution. The default link for the Gaussian family is the identity link. See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Gaussian$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Gaussian$()  Method Summary Methods  Modifier and Type Method and Description double aic(RDD<scala.Tuple3<Object,Object,Object>> predictions, double deviance, double numInstances, double weightSum) Akaike Information Criterion (AIC) value of the family for a given dataset. org.apache.spark.ml.regression.GeneralizedLinearRegression.Link defaultLink() The default link instance of this family. double deviance(double y, double mu, double weight) Deviance of (y, mu) pair. double initialize(double y, double weight) Initialize the starting value for mu. String name()  double project(double mu) Trim the fitted value so that it will be in valid range. double variance(double mu) The variance of the endogenous variable's mean, given the value mu. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Gaussian$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Gaussian$ public GeneralizedLinearRegression.Gaussian$() Method Detail defaultLink public org.apache.spark.ml.regression.GeneralizedLinearRegression.Link defaultLink() The default link instance of this family. initialize public double initialize(double y, double weight) Initialize the starting value for mu. variance public double variance(double mu) The variance of the endogenous variable's mean, given the value mu. deviance public double deviance(double y, double mu, double weight) Deviance of (y, mu) pair. aic public double aic(RDD<scala.Tuple3<Object,Object,Object>> predictions, double deviance, double numInstances, double weightSum) Akaike Information Criterion (AIC) value of the family for a given dataset. Parameters:predictions - an RDD of (y, mu, weight) of instances in evaluation datasetdeviance - the deviance for the fitted model in evaluation datasetnumInstances - number of instances in evaluation datasetweightSum - weights sum of instances in evaluation dataset Returns:(undocumented) project public double project(double mu) Trim the fitted value so that it will be in valid range. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Identity$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Identity$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Identity$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Identity$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Identity$ extends Object See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Identity$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Identity$()  Method Summary Methods  Modifier and Type Method and Description double deriv(double mu) Derivative of the link function. double link(double mu) The link function. String name()  double unlink(double eta) The inverse link function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Identity$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Identity$ public GeneralizedLinearRegression.Identity$() Method Detail link public double link(double mu) The link function. deriv public double deriv(double mu) Derivative of the link function. unlink public double unlink(double eta) The inverse link function. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Inverse$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Inverse$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Inverse$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Inverse$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Inverse$ extends Object See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Inverse$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Inverse$()  Method Summary Methods  Modifier and Type Method and Description double deriv(double mu) Derivative of the link function. double link(double mu) The link function. String name()  double unlink(double eta) The inverse link function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Inverse$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Inverse$ public GeneralizedLinearRegression.Inverse$() Method Detail link public double link(double mu) The link function. deriv public double deriv(double mu) Derivative of the link function. unlink public double unlink(double eta) The inverse link function. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Link$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Link$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Link$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Link$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Link$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Link$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Link$()  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.ml.regression.GeneralizedLinearRegression.Link fromName(String name) Gets the Link object from its name. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Link$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Link$ public GeneralizedLinearRegression.Link$() Method Detail fromName public org.apache.spark.ml.regression.GeneralizedLinearRegression.Link fromName(String name) Gets the Link object from its name. Parameters:name - link name: "identity", "logit", "log", "inverse", "probit", "cloglog" or "sqrt". Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Log$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Log$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Log$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Log$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Log$ extends Object See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Log$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Log$()  Method Summary Methods  Modifier and Type Method and Description double deriv(double mu) Derivative of the link function. double link(double mu) The link function. String name()  double unlink(double eta) The inverse link function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Log$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Log$ public GeneralizedLinearRegression.Log$() Method Detail link public double link(double mu) The link function. deriv public double deriv(double mu) Derivative of the link function. unlink public double unlink(double eta) The inverse link function. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Logit$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Logit$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Logit$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Logit$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Logit$ extends Object See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Logit$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Logit$()  Method Summary Methods  Modifier and Type Method and Description double deriv(double mu) Derivative of the link function. double link(double mu) The link function. String name()  double unlink(double eta) The inverse link function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Logit$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Logit$ public GeneralizedLinearRegression.Logit$() Method Detail link public double link(double mu) The link function. deriv public double deriv(double mu) Derivative of the link function. unlink public double unlink(double eta) The inverse link function. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Poisson$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Poisson$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Poisson$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Poisson$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Poisson$ extends Object Poisson exponential family distribution. The default link for the Poisson family is the log link. See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Poisson$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Poisson$()  Method Summary Methods  Modifier and Type Method and Description double aic(RDD<scala.Tuple3<Object,Object,Object>> predictions, double deviance, double numInstances, double weightSum) Akaike Information Criterion (AIC) value of the family for a given dataset. org.apache.spark.ml.regression.GeneralizedLinearRegression.Link defaultLink() The default link instance of this family. double deviance(double y, double mu, double weight) Deviance of (y, mu) pair. double initialize(double y, double weight) Initialize the starting value for mu. String name()  double project(double mu) Trim the fitted value so that it will be in valid range. double variance(double mu) The variance of the endogenous variable's mean, given the value mu. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Poisson$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Poisson$ public GeneralizedLinearRegression.Poisson$() Method Detail defaultLink public org.apache.spark.ml.regression.GeneralizedLinearRegression.Link defaultLink() The default link instance of this family. initialize public double initialize(double y, double weight) Initialize the starting value for mu. variance public double variance(double mu) The variance of the endogenous variable's mean, given the value mu. deviance public double deviance(double y, double mu, double weight) Deviance of (y, mu) pair. aic public double aic(RDD<scala.Tuple3<Object,Object,Object>> predictions, double deviance, double numInstances, double weightSum) Akaike Information Criterion (AIC) value of the family for a given dataset. Parameters:predictions - an RDD of (y, mu, weight) of instances in evaluation datasetdeviance - the deviance for the fitted model in evaluation datasetnumInstances - number of instances in evaluation datasetweightSum - weights sum of instances in evaluation dataset Returns:(undocumented) project public double project(double mu) Trim the fitted value so that it will be in valid range. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Probit$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Probit$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Probit$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Probit$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Probit$ extends Object See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Probit$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Probit$()  Method Summary Methods  Modifier and Type Method and Description double deriv(double mu) Derivative of the link function. double link(double mu) The link function. String name()  double unlink(double eta) The inverse link function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Probit$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Probit$ public GeneralizedLinearRegression.Probit$() Method Detail link public double link(double mu) The link function. deriv public double deriv(double mu) Derivative of the link function. unlink public double unlink(double eta) The inverse link function. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression.Sqrt$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression.Sqrt$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression.Sqrt$ Object org.apache.spark.ml.regression.GeneralizedLinearRegression.Sqrt$ All Implemented Interfaces: java.io.Serializable Enclosing class: GeneralizedLinearRegression public static class GeneralizedLinearRegression.Sqrt$ extends Object See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static GeneralizedLinearRegression.Sqrt$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression.Sqrt$()  Method Summary Methods  Modifier and Type Method and Description double deriv(double mu) Derivative of the link function. double link(double mu) The link function. String name()  double unlink(double eta) The inverse link function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final GeneralizedLinearRegression.Sqrt$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail GeneralizedLinearRegression.Sqrt$ public GeneralizedLinearRegression.Sqrt$() Method Detail link public double link(double mu) The link function. deriv public double deriv(double mu) Derivative of the link function. unlink public double unlink(double eta) The inverse link function. name public String name() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegression (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegression (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegression Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<FeaturesType,Learner,M> org.apache.spark.ml.regression.GeneralizedLinearRegression All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class GeneralizedLinearRegression extends Predictor<FeaturesType,Learner,M> implements DefaultParamsWritable :: Experimental :: Fit a Generalized Linear Model (https://en.wikipedia.org/wiki/Generalized_linear_model) specified by giving a symbolic description of the linear predictor (link function) and a description of the error distribution (family). It supports "gaussian", "binomial", "poisson" and "gamma" as family. Valid link functions for each family is listed below. The first link function of each family is the default one. - "gaussian" -> "identity", "log", "inverse" - "binomial" -> "logit", "probit", "cloglog" - "poisson" -> "log", "identity", "sqrt" - "gamma" -> "inverse", "identity", "log" See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  GeneralizedLinearRegression.Binomial$ Binomial exponential family distribution. static class  GeneralizedLinearRegression.CLogLog$  static class  GeneralizedLinearRegression.Family$  static class  GeneralizedLinearRegression.Gamma$ Gamma exponential family distribution. static class  GeneralizedLinearRegression.Gaussian$ Gaussian exponential family distribution. static class  GeneralizedLinearRegression.Identity$  static class  GeneralizedLinearRegression.Inverse$  static class  GeneralizedLinearRegression.Link$  static class  GeneralizedLinearRegression.Log$  static class  GeneralizedLinearRegression.Logit$  static class  GeneralizedLinearRegression.Poisson$ Poisson exponential family distribution. static class  GeneralizedLinearRegression.Probit$  static class  GeneralizedLinearRegression.Sqrt$  Constructor Summary Constructors  Constructor and Description GeneralizedLinearRegression()  GeneralizedLinearRegression(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  GeneralizedLinearRegression copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> family()  Param<String> family() Param for the name of family which is a description of the error distribution to be used in the model. static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static M fit(Dataset<?> dataset)  static M fit(Dataset<?> dataset, ParamMap paramMap)  static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static BooleanParam fitIntercept()  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFamily()  String getFamily()  static String getFeaturesCol()  String getFeaturesCol()  static boolean getFitIntercept()  static String getLabelCol()  String getLabelCol()  static String getLink()  String getLink()  static String getLinkPredictionCol()  String getLinkPredictionCol()  static int getMaxIter()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static double getRegParam()  static String getSolver()  static double getTol()  static String getWeightCol()  static <T> boolean hasDefault(Param<T> param)  boolean hasLinkPredictionCol() Checks whether we should output link prediction. static boolean hasParam(String paramName)  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static Param<String> link()  Param<String> link() Param for the name of link function which provides the relationship between the linear predictor and the mean of the distribution function. static Param<String> linkPredictionCol()  Param<String> linkPredictionCol() Param for link prediction (linear predictor) column name. static GeneralizedLinearRegression load(String path)  static IntParam maxIter()  static Param<?>[] params()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static DoubleParam regParam()  static void save(String path)  static <T> Params set(Param<T> param, T value)  GeneralizedLinearRegression setFamily(String value) Sets the value of param family. static Learner setFeaturesCol(String value)  GeneralizedLinearRegression setFitIntercept(boolean value) Sets if we should fit the intercept. static Learner setLabelCol(String value)  GeneralizedLinearRegression setLink(String value) Sets the value of param link. GeneralizedLinearRegression setLinkPredictionCol(String value) Sets the link prediction (linear predictor) column name. GeneralizedLinearRegression setMaxIter(int value) Sets the maximum number of iterations (applicable for solver "irls"). static Learner setPredictionCol(String value)  GeneralizedLinearRegression setRegParam(double value) Sets the regularization parameter for L2 regularization. GeneralizedLinearRegression setSolver(String value) Sets the solver algorithm used for optimization. GeneralizedLinearRegression setTol(double value) Sets the convergence tolerance of iterations. GeneralizedLinearRegression setWeightCol(String value) Sets the value of param weightCol. static Param<String> solver()  static DoubleParam tol()  static String toString()  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. static StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  static Param<String> weightCol()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Predictor fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail GeneralizedLinearRegression public GeneralizedLinearRegression(String uid) GeneralizedLinearRegression public GeneralizedLinearRegression() Method Detail load public static GeneralizedLinearRegression load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) fit public static M fit(Dataset<?> dataset, ParamMap paramMap) fit public static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setLabelCol public static Learner setLabelCol(String value) setFeaturesCol public static Learner setFeaturesCol(String value) setPredictionCol public static Learner setPredictionCol(String value) fit public static M fit(Dataset<?> dataset) transformSchema public static StructType transformSchema(StructType schema) fitIntercept public static final BooleanParam fitIntercept() getFitIntercept public static final boolean getFitIntercept() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() tol public static final DoubleParam tol() getTol public static final double getTol() regParam public static final DoubleParam regParam() getRegParam public static final double getRegParam() weightCol public static final Param<String> weightCol() getWeightCol public static final String getWeightCol() solver public static final Param<String> solver() getSolver public static final String getSolver() family public static final Param<String> family() getFamily public static String getFamily() link public static final Param<String> link() getLink public static String getLink() linkPredictionCol public static final Param<String> linkPredictionCol() getLinkPredictionCol public static String getLinkPredictionCol() validateAndTransformSchema public static StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setFamily public GeneralizedLinearRegression setFamily(String value) Sets the value of param family. Default is "gaussian". Parameters:value - (undocumented) Returns:(undocumented) setLink public GeneralizedLinearRegression setLink(String value) Sets the value of param link. Parameters:value - (undocumented) Returns:(undocumented) setFitIntercept public GeneralizedLinearRegression setFitIntercept(boolean value) Sets if we should fit the intercept. Default is true. Parameters:value - (undocumented) Returns:(undocumented) setMaxIter public GeneralizedLinearRegression setMaxIter(int value) Sets the maximum number of iterations (applicable for solver "irls"). Default is 25. Parameters:value - (undocumented) Returns:(undocumented) setTol public GeneralizedLinearRegression setTol(double value) Sets the convergence tolerance of iterations. Smaller value will lead to higher accuracy with the cost of more iterations. Default is 1E-6. Parameters:value - (undocumented) Returns:(undocumented) setRegParam public GeneralizedLinearRegression setRegParam(double value) Sets the regularization parameter for L2 regularization. The regularization term is 0.5 * regParam * L2norm(coefficients)^2 Default is 0.0. Parameters:value - (undocumented) Returns:(undocumented) setWeightCol public GeneralizedLinearRegression setWeightCol(String value) Sets the value of param weightCol. If this is not set or empty, we treat all instance weights as 1.0. Default is not set, so all instances have weight one. Parameters:value - (undocumented) Returns:(undocumented) setSolver public GeneralizedLinearRegression setSolver(String value) Sets the solver algorithm used for optimization. Currently only supports "irls" which is also the default solver. Parameters:value - (undocumented) Returns:(undocumented) setLinkPredictionCol public GeneralizedLinearRegression setLinkPredictionCol(String value) Sets the link prediction (linear predictor) column name. Parameters:value - (undocumented) Returns:(undocumented) copy public GeneralizedLinearRegression copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Predictor<Vector,GeneralizedLinearRegression,GeneralizedLinearRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) family public Param<String> family() Param for the name of family which is a description of the error distribution to be used in the model. Supported options: "gaussian", "binomial", "poisson" and "gamma". Default is "gaussian". Returns:(undocumented) getFamily public String getFamily() link public Param<String> link() Param for the name of link function which provides the relationship between the linear predictor and the mean of the distribution function. Supported options: "identity", "log", "inverse", "logit", "probit", "cloglog" and "sqrt". Returns:(undocumented) getLink public String getLink() linkPredictionCol public Param<String> linkPredictionCol() Param for link prediction (linear predictor) column name. Default is not set, which means we do not output link prediction. Returns:(undocumented) getLinkPredictionCol public String getLinkPredictionCol() hasLinkPredictionCol public boolean hasLinkPredictionCol() Checks whether we should output link prediction. validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegressionModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegressionModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegressionModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<M> org.apache.spark.ml.PredictionModel<FeaturesType,M> org.apache.spark.ml.regression.RegressionModel<Vector,GeneralizedLinearRegressionModel> org.apache.spark.ml.regression.GeneralizedLinearRegressionModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class GeneralizedLinearRegressionModel extends RegressionModel<Vector,GeneralizedLinearRegressionModel> implements MLWritable :: Experimental :: Model produced by GeneralizedLinearRegression. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  Vector coefficients()  GeneralizedLinearRegressionModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. GeneralizedLinearRegressionSummary evaluate(Dataset<?> dataset) Evaluate the model on the given dataset, returning a summary of the results. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> family()  Param<String> family() Param for the name of family which is a description of the error distribution to be used in the model. static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static BooleanParam fitIntercept()  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFamily()  String getFamily()  static String getFeaturesCol()  String getFeaturesCol()  static boolean getFitIntercept()  static String getLabelCol()  String getLabelCol()  static String getLink()  String getLink()  static String getLinkPredictionCol()  String getLinkPredictionCol()  static int getMaxIter()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static double getRegParam()  static String getSolver()  static double getTol()  static String getWeightCol()  static <T> boolean hasDefault(Param<T> param)  boolean hasLinkPredictionCol() Checks whether we should output link prediction. static boolean hasParam(String paramName)  static boolean hasParent()  boolean hasSummary() Indicates if summary is available. double intercept()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static Param<String> link()  Param<String> link() Param for the name of link function which provides the relationship between the linear predictor and the mean of the distribution function. static Param<String> linkPredictionCol()  Param<String> linkPredictionCol() Param for link prediction (linear predictor) column name. static GeneralizedLinearRegressionModel load(String path)  static IntParam maxIter()  static int numFeatures()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static MLReader<GeneralizedLinearRegressionModel> read()  static DoubleParam regParam()  static void save(String path)  static <T> Params set(Param<T> param, T value)  static M setFeaturesCol(String value)  GeneralizedLinearRegressionModel setLinkPredictionCol(String value) Sets the link prediction (linear predictor) column name. static M setParent(Estimator<M> parent)  static M setPredictionCol(String value)  static Param<String> solver()  GeneralizedLinearRegressionTrainingSummary summary() Gets R-like summary of model on training set. static DoubleParam tol()  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms dataset by reading from featuresCol, calling predict, and storing the predictions as a new column predictionCol. static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. static StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  static Param<String> weightCol()  MLWriter write() Returns a MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.PredictionModel numFeatures, setFeaturesCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Method Detail read public static MLReader<GeneralizedLinearRegressionModel> read() load public static GeneralizedLinearRegressionModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setFeaturesCol public static M setFeaturesCol(String value) setPredictionCol public static M setPredictionCol(String value) numFeatures public static int numFeatures() transformSchema public static StructType transformSchema(StructType schema) fitIntercept public static final BooleanParam fitIntercept() getFitIntercept public static final boolean getFitIntercept() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() tol public static final DoubleParam tol() getTol public static final double getTol() regParam public static final DoubleParam regParam() getRegParam public static final double getRegParam() weightCol public static final Param<String> weightCol() getWeightCol public static final String getWeightCol() solver public static final Param<String> solver() getSolver public static final String getSolver() family public static final Param<String> family() getFamily public static String getFamily() link public static final Param<String> link() getLink public static String getLink() linkPredictionCol public static final Param<String> linkPredictionCol() getLinkPredictionCol public static String getLinkPredictionCol() validateAndTransformSchema public static StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) coefficients public Vector coefficients() intercept public double intercept() setLinkPredictionCol public GeneralizedLinearRegressionModel setLinkPredictionCol(String value) Sets the link prediction (linear predictor) column name. Parameters:value - (undocumented) Returns:(undocumented) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: PredictionModel Transforms dataset by reading from featuresCol, calling predict, and storing the predictions as a new column predictionCol. Overrides: transform in class PredictionModel<Vector,GeneralizedLinearRegressionModel> Parameters:dataset - input dataset Returns:transformed dataset with predictionCol of type Double summary public GeneralizedLinearRegressionTrainingSummary summary() Gets R-like summary of model on training set. An exception is thrown if there is no summary available. Returns:(undocumented) hasSummary public boolean hasSummary() Indicates if summary is available. Returns:(undocumented) evaluate public GeneralizedLinearRegressionSummary evaluate(Dataset<?> dataset) Evaluate the model on the given dataset, returning a summary of the results. Parameters:dataset - (undocumented) Returns:(undocumented) copy public GeneralizedLinearRegressionModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<GeneralizedLinearRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Returns a MLWriter instance for this ML instance. For GeneralizedLinearRegressionModel, this does NOT currently save the training summary. An option to save summary may be added in the future. Specified by: write in interface MLWritable Returns:(undocumented) family public Param<String> family() Param for the name of family which is a description of the error distribution to be used in the model. Supported options: "gaussian", "binomial", "poisson" and "gamma". Default is "gaussian". Returns:(undocumented) getFamily public String getFamily() link public Param<String> link() Param for the name of link function which provides the relationship between the linear predictor and the mean of the distribution function. Supported options: "identity", "log", "inverse", "logit", "probit", "cloglog" and "sqrt". Returns:(undocumented) getLink public String getLink() linkPredictionCol public Param<String> linkPredictionCol() Param for link prediction (linear predictor) column name. Default is not set, which means we do not output link prediction. Returns:(undocumented) getLinkPredictionCol public String getLinkPredictionCol() hasLinkPredictionCol public boolean hasLinkPredictionCol() Checks whether we should output link prediction. validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegressionSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegressionSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegressionSummary Object org.apache.spark.ml.regression.GeneralizedLinearRegressionSummary All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: GeneralizedLinearRegressionTrainingSummary public class GeneralizedLinearRegressionSummary extends Object implements scala.Serializable :: Experimental :: Summary of GeneralizedLinearRegression model and predictions. param: dataset Dataset to be summarized. param: origModel Model to be summarized. This is copied to create an internal model which cannot be modified from outside. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description double aic() Akaike Information Criterion (AIC) for the fitted model. long degreesOfFreedom() Degrees of freedom. double deviance() The deviance for the fitted model. double dispersion() The dispersion of the fitted model. double nullDeviance() The deviance for the null model. String predictionCol() Field in "predictions" which gives the predicted value of each instance. Dataset<Row> predictions() Predictions output by the model's `transform` method. long rank() The numeric rank of the fitted linear model. long residualDegreeOfFreedom() The residual degrees of freedom. long residualDegreeOfFreedomNull() The residual degrees of freedom for the null model. Dataset<Row> residuals() Get the default residuals (deviance residuals) of the fitted model. Dataset<Row> residuals(String residualsType) Get the residuals of the fitted model by type. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail predictionCol public String predictionCol() Field in "predictions" which gives the predicted value of each instance. This is set to a new column name if the original model's predictionCol is not set. Returns:(undocumented) predictions public Dataset<Row> predictions() Predictions output by the model's `transform` method. rank public long rank() The numeric rank of the fitted linear model. degreesOfFreedom public long degreesOfFreedom() Degrees of freedom. residualDegreeOfFreedom public long residualDegreeOfFreedom() The residual degrees of freedom. residualDegreeOfFreedomNull public long residualDegreeOfFreedomNull() The residual degrees of freedom for the null model. residuals public Dataset<Row> residuals() Get the default residuals (deviance residuals) of the fitted model. Returns:(undocumented) residuals public Dataset<Row> residuals(String residualsType) Get the residuals of the fitted model by type. Parameters:residualsType - The type of residuals which should be returned. Supported options: deviance, pearson, working and response. Returns:(undocumented) nullDeviance public double nullDeviance() The deviance for the null model. Returns:(undocumented) deviance public double deviance() The deviance for the fitted model. Returns:(undocumented) dispersion public double dispersion() The dispersion of the fitted model. It is taken as 1.0 for the "binomial" and "poisson" families, and otherwise estimated by the residual Pearson's Chi-Squared statistic (which is defined as sum of the squares of the Pearson residuals) divided by the residual degrees of freedom. Returns:(undocumented) aic public double aic() Akaike Information Criterion (AIC) for the fitted model. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GeneralizedLinearRegressionTrainingSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GeneralizedLinearRegressionTrainingSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class GeneralizedLinearRegressionTrainingSummary Object org.apache.spark.ml.regression.GeneralizedLinearRegressionSummary org.apache.spark.ml.regression.GeneralizedLinearRegressionTrainingSummary All Implemented Interfaces: java.io.Serializable public class GeneralizedLinearRegressionTrainingSummary extends GeneralizedLinearRegressionSummary implements scala.Serializable :: Experimental :: Summary of GeneralizedLinearRegression fitting and model. param: dataset Dataset to be summarized. param: origModel Model to be summarized. This is copied to create an internal model which cannot be modified from outside. param: diagInvAtWA diagonal of matrix (A^T * W * A)^-1 in the last iteration param: numIterations number of iterations param: solver the solver algorithm used for model training See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description double[] coefficientStandardErrors() Standard error of estimated coefficients and intercept. int numIterations()  double[] pValues() Two-sided p-value of estimated coefficients and intercept. String solver()  double[] tValues() T-statistic of estimated coefficients and intercept. Methods inherited from class org.apache.spark.ml.regression.GeneralizedLinearRegressionSummary aic, degreesOfFreedom, deviance, dispersion, nullDeviance, predictionCol, predictions, rank, residualDegreeOfFreedom, residualDegreeOfFreedomNull, residuals, residuals Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail numIterations public int numIterations() solver public String solver() coefficientStandardErrors public double[] coefficientStandardErrors() Standard error of estimated coefficients and intercept. If GeneralizedLinearRegression.fitIntercept is set to true, then the last element returned corresponds to the intercept. Returns:(undocumented) tValues public double[] tValues() T-statistic of estimated coefficients and intercept. If GeneralizedLinearRegression.fitIntercept is set to true, then the last element returned corresponds to the intercept. Returns:(undocumented) pValues public double[] pValues() Two-sided p-value of estimated coefficients and intercept. If GeneralizedLinearRegression.fitIntercept is set to true, then the last element returned corresponds to the intercept. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GetAllReceiverInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GetAllReceiverInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.scheduler Class GetAllReceiverInfo Object org.apache.spark.streaming.scheduler.GetAllReceiverInfo public class GetAllReceiverInfo extends Object Constructor Summary Constructors  Constructor and Description GetAllReceiverInfo()  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GetAllReceiverInfo public GetAllReceiverInfo() Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Gini (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Gini (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.impurity Class Gini Object org.apache.spark.mllib.tree.impurity.Gini public class Gini extends Object Class for calculating the Gini impurity (http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) during multiclass classification. Constructor Summary Constructors  Constructor and Description Gini()  Method Summary Methods  Modifier and Type Method and Description static double calculate(double[] counts, double totalCount) :: DeveloperApi :: information calculation for multiclass classification static double calculate(double count, double sum, double sumSquares) :: DeveloperApi :: variance calculation static org.apache.spark.mllib.tree.impurity.Gini$ instance() Get this impurity instance. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Gini public Gini() Method Detail calculate public static double calculate(double[] counts, double totalCount) :: DeveloperApi :: information calculation for multiclass classification Parameters:counts - Array[Double] with counts for each labeltotalCount - sum of counts for all labels Returns:information value, or 0 if totalCount = 0 calculate public static double calculate(double count, double sum, double sumSquares) :: DeveloperApi :: variance calculation Parameters:count - number of instancessum - sum of labelssumSquares - summation of squares of the labels Returns:information value, or 0 if count = 0 instance public static org.apache.spark.mllib.tree.impurity.Gini$ instance() Get this impurity instance. This is useful for passing impurity parameters to a Strategy in Java. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Gradient (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Gradient (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Class Gradient Object org.apache.spark.mllib.optimization.Gradient All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: HingeGradient, LeastSquaresGradient, LogisticGradient public abstract class Gradient extends Object implements scala.Serializable :: DeveloperApi :: Class used to compute the gradient for a loss function, given a single data point. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Gradient()  Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<Vector,Object> compute(Vector data, double label, Vector weights) Compute the gradient and loss given the features of a single data point. abstract double compute(Vector data, double label, Vector weights, Vector cumGradient) Compute the gradient and loss given the features of a single data point, add the gradient to a provided vector to avoid creating new objects, and return loss. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Gradient public Gradient() Method Detail compute public scala.Tuple2<Vector,Object> compute(Vector data, double label, Vector weights) Compute the gradient and loss given the features of a single data point. Parameters:data - features for one data pointlabel - label for this data pointweights - weights/coefficients corresponding to features Returns:(gradient: Vector, loss: Double) compute public abstract double compute(Vector data, double label, Vector weights, Vector cumGradient) Compute the gradient and loss given the features of a single data point, add the gradient to a provided vector to avoid creating new objects, and return loss. Parameters:data - features for one data pointlabel - label for this data pointweights - weights/coefficients corresponding to featurescumGradient - the computed gradient will be added to this vector Returns:loss Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GradientBoostedTrees (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GradientBoostedTrees (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree Class GradientBoostedTrees Object org.apache.spark.mllib.tree.GradientBoostedTrees All Implemented Interfaces: java.io.Serializable public class GradientBoostedTrees extends Object implements scala.Serializable A class that implements Stochastic Gradient Boosting for regression and binary classification. The implementation is based upon: J.H. Friedman. "Stochastic Gradient Boosting." 1999. Notes on Gradient Boosting vs. TreeBoost: - This implementation is for Stochastic Gradient Boosting, not for TreeBoost. - Both algorithms learn tree ensembles by minimizing loss functions. - TreeBoost (Friedman, 1999) additionally modifies the outputs at tree leaf nodes based on the loss function, whereas the original gradient boosting method does not. - When the loss is SquaredError, these methods give the same result, but they could differ for other loss functions. param: boostingStrategy Parameters for the gradient boosting algorithm. param: seed Random seed. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GradientBoostedTrees(BoostingStrategy boostingStrategy)  Method Summary Methods  Modifier and Type Method and Description GradientBoostedTreesModel run(JavaRDD<LabeledPoint> input) Java-friendly API for org.apache.spark.mllib.tree.GradientBoostedTrees!#run. GradientBoostedTreesModel run(RDD<LabeledPoint> input) Method to train a gradient boosting model GradientBoostedTreesModel runWithValidation(JavaRDD<LabeledPoint> input, JavaRDD<LabeledPoint> validationInput) Java-friendly API for org.apache.spark.mllib.tree.GradientBoostedTrees!#runWithValidation. GradientBoostedTreesModel runWithValidation(RDD<LabeledPoint> input, RDD<LabeledPoint> validationInput) Method to validate a gradient boosting model static GradientBoostedTreesModel train(JavaRDD<LabeledPoint> input, BoostingStrategy boostingStrategy) Java-friendly API for GradientBoostedTrees$.train(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, org.apache.spark.mllib.tree.configuration.BoostingStrategy) static GradientBoostedTreesModel train(RDD<LabeledPoint> input, BoostingStrategy boostingStrategy) Method to train a gradient boosting model. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GradientBoostedTrees public GradientBoostedTrees(BoostingStrategy boostingStrategy) Parameters:boostingStrategy - Parameters for the gradient boosting algorithm. Method Detail train public static GradientBoostedTreesModel train(RDD<LabeledPoint> input, BoostingStrategy boostingStrategy) Method to train a gradient boosting model. Parameters:input - Training dataset: RDD of LabeledPoint. For classification, labels should take values {0, 1, ..., numClasses-1}. For regression, labels are real numbers.boostingStrategy - Configuration options for the boosting algorithm. Returns:GradientBoostedTreesModel that can be used for prediction. train public static GradientBoostedTreesModel train(JavaRDD<LabeledPoint> input, BoostingStrategy boostingStrategy) Java-friendly API for GradientBoostedTrees$.train(org.apache.spark.rdd.RDD<org.apache.spark.mllib.regression.LabeledPoint>, org.apache.spark.mllib.tree.configuration.BoostingStrategy) Parameters:input - (undocumented)boostingStrategy - (undocumented) Returns:(undocumented) run public GradientBoostedTreesModel run(RDD<LabeledPoint> input) Method to train a gradient boosting model Parameters:input - Training dataset: RDD of LabeledPoint. Returns:GradientBoostedTreesModel that can be used for prediction. run public GradientBoostedTreesModel run(JavaRDD<LabeledPoint> input) Java-friendly API for org.apache.spark.mllib.tree.GradientBoostedTrees!#run. Parameters:input - (undocumented) Returns:(undocumented) runWithValidation public GradientBoostedTreesModel runWithValidation(RDD<LabeledPoint> input, RDD<LabeledPoint> validationInput) Method to validate a gradient boosting model Parameters:input - Training dataset: RDD of LabeledPoint.validationInput - Validation dataset. This dataset should be different from the training dataset, but it should follow the same distribution. E.g., these two datasets could be created from an original dataset by using org.apache.spark.rdd.RDD.randomSplit() Returns:GradientBoostedTreesModel that can be used for prediction. runWithValidation public GradientBoostedTreesModel runWithValidation(JavaRDD<LabeledPoint> input, JavaRDD<LabeledPoint> validationInput) Java-friendly API for org.apache.spark.mllib.tree.GradientBoostedTrees!#runWithValidation. Parameters:input - (undocumented)validationInput - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GradientBoostedTreesModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GradientBoostedTreesModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.model Class GradientBoostedTreesModel Object org.apache.spark.mllib.tree.model.GradientBoostedTreesModel All Implemented Interfaces: java.io.Serializable, Saveable public class GradientBoostedTreesModel extends Object implements Saveable Represents a gradient boosted trees model. param: algo algorithm for the ensemble model, either Classification or Regression param: trees tree ensembles param: treeWeights tree ensemble weights See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GradientBoostedTreesModel(scala.Enumeration.Value algo, DecisionTreeModel[] trees, double[] treeWeights)  Method Summary Methods  Modifier and Type Method and Description scala.Enumeration.Value algo()  static RDD<scala.Tuple2<Object,Object>> computeInitialPredictionAndError(RDD<LabeledPoint> data, double initTreeWeight, DecisionTreeModel initTree, Loss loss) :: DeveloperApi :: Compute the initial predictions and errors for a dataset for the first iteration of gradient boosting. double[] evaluateEachIteration(RDD<LabeledPoint> data, Loss loss) Method to compute error or loss for every iteration of gradient boosting. static GradientBoostedTreesModel load(SparkContext sc, String path)  static int numTrees()  int numTrees() Get number of trees in ensemble. static JavaRDD<Double> predict(JavaRDD<Vector> features)  JavaRDD<Double> predict(JavaRDD<Vector> features) Java-friendly version of TreeEnsembleModel.predict(org.apache.spark.mllib.linalg.Vector). static RDD<Object> predict(RDD<Vector> features)  RDD<Object> predict(RDD<Vector> features) Predict values for the given data set. static double predict(Vector features)  double predict(Vector features) Predict values for a single data point using the model trained. void save(SparkContext sc, String path) Save this model to the given path. static String toDebugString()  String toDebugString() Print the full model to a string. static String toString()  String toString() Print a summary of the model. static int totalNumNodes()  int totalNumNodes() Get total number of nodes, summed over all trees in the ensemble. DecisionTreeModel[] trees()  double[] treeWeights()  static RDD<scala.Tuple2<Object,Object>> updatePredictionError(RDD<LabeledPoint> data, RDD<scala.Tuple2<Object,Object>> predictionAndError, double treeWeight, DecisionTreeModel tree, Loss loss) :: DeveloperApi :: Update a zipped predictionError RDD (as obtained with computeInitialPredictionAndError) Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail GradientBoostedTreesModel public GradientBoostedTreesModel(scala.Enumeration.Value algo, DecisionTreeModel[] trees, double[] treeWeights) Method Detail computeInitialPredictionAndError public static RDD<scala.Tuple2<Object,Object>> computeInitialPredictionAndError(RDD<LabeledPoint> data, double initTreeWeight, DecisionTreeModel initTree, Loss loss) :: DeveloperApi :: Compute the initial predictions and errors for a dataset for the first iteration of gradient boosting. Parameters:data: - training data.initTreeWeight: - learning rate assigned to the first tree.initTree: - first DecisionTreeModel.loss: - evaluation metric. Returns:a RDD with each element being a zip of the prediction and error corresponding to every sample. updatePredictionError public static RDD<scala.Tuple2<Object,Object>> updatePredictionError(RDD<LabeledPoint> data, RDD<scala.Tuple2<Object,Object>> predictionAndError, double treeWeight, DecisionTreeModel tree, Loss loss) :: DeveloperApi :: Update a zipped predictionError RDD (as obtained with computeInitialPredictionAndError) Parameters:data: - training data.predictionAndError: - predictionError RDDtreeWeight: - Learning rate.tree: - Tree using which the prediction and error should be updated.loss: - evaluation metric. Returns:a RDD with each element being a zip of the prediction and error corresponding to each sample. load public static GradientBoostedTreesModel load(SparkContext sc, String path) Parameters:sc - Spark context used for loading model files.path - Path specifying the directory to which the model was saved. Returns:Model instance predict public static double predict(Vector features) predict public static RDD<Object> predict(RDD<Vector> features) predict public static JavaRDD<Double> predict(JavaRDD<Vector> features) toString public static String toString() toDebugString public static String toDebugString() numTrees public static int numTrees() totalNumNodes public static int totalNumNodes() algo public scala.Enumeration.Value algo() trees public DecisionTreeModel[] trees() treeWeights public double[] treeWeights() save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. evaluateEachIteration public double[] evaluateEachIteration(RDD<LabeledPoint> data, Loss loss) Method to compute error or loss for every iteration of gradient boosting. Parameters:data - RDD of LabeledPointloss - evaluation metric. Returns:an array with index i having the losses or errors for the ensemble containing the first i+1 trees predict public double predict(Vector features) Predict values for a single data point using the model trained. Parameters:features - array representing a single data point Returns:predicted category from the trained model predict public RDD<Object> predict(RDD<Vector> features) Predict values for the given data set. Parameters:features - RDD representing data points to be predicted Returns:RDD[Double] where each entry contains the corresponding prediction predict public JavaRDD<Double> predict(JavaRDD<Vector> features) Java-friendly version of TreeEnsembleModel.predict(org.apache.spark.mllib.linalg.Vector). Parameters:features - (undocumented) Returns:(undocumented) toString public String toString() Print a summary of the model. Overrides: toString in class Object Returns:(undocumented) toDebugString public String toDebugString() Print the full model to a string. Returns:(undocumented) numTrees public int numTrees() Get number of trees in ensemble. Returns:(undocumented) totalNumNodes public int totalNumNodes() Get total number of nodes, summed over all trees in the ensemble. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GradientDescent (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GradientDescent (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Class GradientDescent Object org.apache.spark.mllib.optimization.GradientDescent All Implemented Interfaces: java.io.Serializable, Optimizer public class GradientDescent extends Object implements Optimizer Class used to solve an optimization problem using Gradient Descent. param: gradient Gradient function to be used. param: updater Updater to be used to update weights after every iteration. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description Vector optimize(RDD<scala.Tuple2<Object,Vector>> data, Vector initialWeights) :: DeveloperApi :: Runs gradient descent on the given training data. static scala.Tuple2<Vector,double[]> runMiniBatchSGD(RDD<scala.Tuple2<Object,Vector>> data, Gradient gradient, Updater updater, double stepSize, int numIterations, double regParam, double miniBatchFraction, Vector initialWeights) Alias of runMiniBatchSGD with convergenceTol set to default value of 0.001. static scala.Tuple2<Vector,double[]> runMiniBatchSGD(RDD<scala.Tuple2<Object,Vector>> data, Gradient gradient, Updater updater, double stepSize, int numIterations, double regParam, double miniBatchFraction, Vector initialWeights, double convergenceTol) Run stochastic gradient descent (SGD) in parallel using mini batches. GradientDescent setConvergenceTol(double tolerance) Set the convergence tolerance. GradientDescent setGradient(Gradient gradient) Set the gradient function (of the loss function of one single data example) to be used for SGD. GradientDescent setMiniBatchFraction(double fraction) Set fraction of data to be used for each SGD iteration. GradientDescent setNumIterations(int iters) Set the number of iterations for SGD. GradientDescent setRegParam(double regParam) Set the regularization parameter. GradientDescent setStepSize(double step) Set the initial step size of SGD for the first step. GradientDescent setUpdater(Updater updater) Set the updater function to actually perform a gradient step in a given direction. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail runMiniBatchSGD public static scala.Tuple2<Vector,double[]> runMiniBatchSGD(RDD<scala.Tuple2<Object,Vector>> data, Gradient gradient, Updater updater, double stepSize, int numIterations, double regParam, double miniBatchFraction, Vector initialWeights, double convergenceTol) Run stochastic gradient descent (SGD) in parallel using mini batches. In each iteration, we sample a subset (fraction miniBatchFraction) of the total data in order to compute a gradient estimate. Sampling, and averaging the subgradients over this subset is performed using one standard spark map-reduce in each iteration. Parameters:data - Input data for SGD. RDD of the set of data examples, each of the form (label, [feature values]).gradient - Gradient object (used to compute the gradient of the loss function of one single data example)updater - Updater function to actually perform a gradient step in a given direction.stepSize - initial step size for the first stepnumIterations - number of iterations that SGD should be run.regParam - regularization parameterminiBatchFraction - fraction of the input data set that should be used for one iteration of SGD. Default value 1.0.convergenceTol - Minibatch iteration will end before numIterations if the relative difference between the current weight and the previous weight is less than this value. In measuring convergence, L2 norm is calculated. Default value 0.001. Must be between 0.0 and 1.0 inclusively.initialWeights - (undocumented) Returns:A tuple containing two elements. The first element is a column matrix containing weights for every feature, and the second element is an array containing the stochastic loss computed for every iteration. runMiniBatchSGD public static scala.Tuple2<Vector,double[]> runMiniBatchSGD(RDD<scala.Tuple2<Object,Vector>> data, Gradient gradient, Updater updater, double stepSize, int numIterations, double regParam, double miniBatchFraction, Vector initialWeights) Alias of runMiniBatchSGD with convergenceTol set to default value of 0.001. Parameters:data - (undocumented)gradient - (undocumented)updater - (undocumented)stepSize - (undocumented)numIterations - (undocumented)regParam - (undocumented)miniBatchFraction - (undocumented)initialWeights - (undocumented) Returns:(undocumented) setStepSize public GradientDescent setStepSize(double step) Set the initial step size of SGD for the first step. Default 1.0. In subsequent steps, the step size will decrease with stepSize/sqrt(t) Parameters:step - (undocumented) Returns:(undocumented) setMiniBatchFraction public GradientDescent setMiniBatchFraction(double fraction) Set fraction of data to be used for each SGD iteration. Default 1.0 (corresponding to deterministic/classical gradient descent) Parameters:fraction - (undocumented) Returns:(undocumented) setNumIterations public GradientDescent setNumIterations(int iters) Set the number of iterations for SGD. Default 100. Parameters:iters - (undocumented) Returns:(undocumented) setRegParam public GradientDescent setRegParam(double regParam) Set the regularization parameter. Default 0.0. Parameters:regParam - (undocumented) Returns:(undocumented) setConvergenceTol public GradientDescent setConvergenceTol(double tolerance) Set the convergence tolerance. Default 0.001 convergenceTol is a condition which decides iteration termination. The end of iteration is decided based on below logic. - If the norm of the new solution vector is >1, the diff of solution vectors is compared to relative tolerance which means normalizing by the norm of the new solution vector. - If the norm of the new solution vector is <=1, the diff of solution vectors is compared to absolute tolerance which is not normalizing. Must be between 0.0 and 1.0 inclusively. Parameters:tolerance - (undocumented) Returns:(undocumented) setGradient public GradientDescent setGradient(Gradient gradient) Set the gradient function (of the loss function of one single data example) to be used for SGD. Parameters:gradient - (undocumented) Returns:(undocumented) setUpdater public GradientDescent setUpdater(Updater updater) Set the updater function to actually perform a gradient step in a given direction. The updater is responsible to perform the update from the regularization term as well, and therefore determines what kind or regularization is used, if any. Parameters:updater - (undocumented) Returns:(undocumented) optimize public Vector optimize(RDD<scala.Tuple2<Object,Vector>> data, Vector initialWeights) :: DeveloperApi :: Runs gradient descent on the given training data. Specified by: optimize in interface Optimizer Parameters:data - training datainitialWeights - initial weights Returns:solution vector Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Graph (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Graph (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class Graph<VD,ED> Object org.apache.spark.graphx.Graph<VD,ED> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: GraphImpl public abstract class Graph<VD,ED> extends Object implements scala.Serializable The Graph abstractly represents a graph with arbitrary objects associated with vertices and edges. The graph provides basic operations to access and manipulate the data associated with vertices and edges as well as the underlying structure. Like Spark RDDs, the graph is a functional data-structure in which mutating operations return new graphs. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description <A> VertexRDD<A> aggregateMessages(scala.Function1<EdgeContext<VD,ED,A>,scala.runtime.BoxedUnit> sendMsg, scala.Function2<A,A,A> mergeMsg, TripletFields tripletFields, scala.reflect.ClassTag<A> evidence$11) Aggregates values from the neighboring edges and vertices of each vertex. static <VD,ED> Graph<VD,ED> apply(RDD<scala.Tuple2<Object,VD>> vertices, RDD<Edge<ED>> edges, VD defaultVertexAttr, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$18, scala.reflect.ClassTag<ED> evidence$19) Construct a graph from a collection of vertices and edges with attributes. abstract Graph<VD,ED> cache() Caches the vertices and edges associated with this graph at the previously-specified target storage levels, which default to MEMORY_ONLY. abstract void checkpoint() Mark this Graph for checkpointing. abstract EdgeRDD<ED> edges() An RDD containing the edges and their associated attributes. static <VD,ED> Graph<VD,ED> fromEdges(RDD<Edge<ED>> edges, VD defaultValue, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$16, scala.reflect.ClassTag<ED> evidence$17) Construct a graph from a collection of edges. static <VD> Graph<VD,Object> fromEdgeTuples(RDD<scala.Tuple2<Object,Object>> rawEdges, VD defaultValue, scala.Option<PartitionStrategy> uniqueEdges, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$15) Construct a graph from a collection of edges encoded as vertex id pairs. abstract scala.collection.Seq<String> getCheckpointFiles() Gets the name of the files to which this Graph was checkpointed. static <VD,ED> GraphOps<VD,ED> graphToGraphOps(Graph<VD,ED> g, scala.reflect.ClassTag<VD> evidence$20, scala.reflect.ClassTag<ED> evidence$21) Implicitly extracts the GraphOps member from a graph. abstract Graph<VD,ED> groupEdges(scala.Function2<ED,ED,ED> merge) Merges multiple edges between two vertices into a single edge. abstract boolean isCheckpointed() Return whether this Graph has been checkpointed or not. <ED2> Graph<VD,ED2> mapEdges(scala.Function1<Edge<ED>,ED2> map, scala.reflect.ClassTag<ED2> evidence$4) Transforms each edge attribute in the graph using the map function. abstract <ED2> Graph<VD,ED2> mapEdges(scala.Function2<Object,scala.collection.Iterator<Edge<ED>>,scala.collection.Iterator<ED2>> map, scala.reflect.ClassTag<ED2> evidence$5) Transforms each edge attribute using the map function, passing it a whole partition at a time. <ED2> Graph<VD,ED2> mapTriplets(scala.Function1<EdgeTriplet<VD,ED>,ED2> map, scala.reflect.ClassTag<ED2> evidence$6) Transforms each edge attribute using the map function, passing it the adjacent vertex attributes as well. <ED2> Graph<VD,ED2> mapTriplets(scala.Function1<EdgeTriplet<VD,ED>,ED2> map, TripletFields tripletFields, scala.reflect.ClassTag<ED2> evidence$7) Transforms each edge attribute using the map function, passing it the adjacent vertex attributes as well. abstract <ED2> Graph<VD,ED2> mapTriplets(scala.Function2<Object,scala.collection.Iterator<EdgeTriplet<VD,ED>>,scala.collection.Iterator<ED2>> map, TripletFields tripletFields, scala.reflect.ClassTag<ED2> evidence$8) Transforms each edge attribute a partition at a time using the map function, passing it the adjacent vertex attributes as well. abstract <VD2> Graph<VD2,ED> mapVertices(scala.Function2<Object,VD,VD2> map, scala.reflect.ClassTag<VD2> evidence$3, scala.Predef.$eq$colon$eq<VD,VD2> eq) Transforms each vertex attribute in the graph using the map function. abstract <VD2,ED2> Graph<VD,ED> mask(Graph<VD2,ED2> other, scala.reflect.ClassTag<VD2> evidence$9, scala.reflect.ClassTag<ED2> evidence$10) Restricts the graph to only the vertices and edges that are also in other, but keeps the attributes from this graph. GraphOps<VD,ED> ops() The associated GraphOps object. abstract <U,VD2> Graph<VD2,ED> outerJoinVertices(RDD<scala.Tuple2<Object,U>> other, scala.Function3<Object,VD,scala.Option<U>,VD2> mapFunc, scala.reflect.ClassTag<U> evidence$13, scala.reflect.ClassTag<VD2> evidence$14, scala.Predef.$eq$colon$eq<VD,VD2> eq) Joins the vertices with entries in the table RDD and merges the results using mapFunc. abstract Graph<VD,ED> partitionBy(PartitionStrategy partitionStrategy) Repartitions the edges in the graph according to partitionStrategy. abstract Graph<VD,ED> partitionBy(PartitionStrategy partitionStrategy, int numPartitions) Repartitions the edges in the graph according to partitionStrategy. abstract Graph<VD,ED> persist(StorageLevel newLevel) Caches the vertices and edges associated with this graph at the specified storage level, ignoring any target storage levels previously set. abstract Graph<VD,ED> reverse() Reverses all edges in the graph. abstract Graph<VD,ED> subgraph(scala.Function1<EdgeTriplet<VD,ED>,Object> epred, scala.Function2<Object,VD,Object> vpred) Restricts the graph to only the vertices and edges satisfying the predicates. abstract RDD<EdgeTriplet<VD,ED>> triplets() An RDD containing the edge triplets, which are edges along with the vertex data associated with the adjacent vertices. abstract Graph<VD,ED> unpersist(boolean blocking) Uncaches both vertices and edges of this graph. abstract Graph<VD,ED> unpersistVertices(boolean blocking) Uncaches only the vertices of this graph, leaving the edges alone. abstract VertexRDD<VD> vertices() An RDD containing the vertices and their associated attributes. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail fromEdgeTuples public static <VD> Graph<VD,Object> fromEdgeTuples(RDD<scala.Tuple2<Object,Object>> rawEdges, VD defaultValue, scala.Option<PartitionStrategy> uniqueEdges, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$15) Construct a graph from a collection of edges encoded as vertex id pairs. Parameters:rawEdges - a collection of edges in (src, dst) formdefaultValue - the vertex attributes with which to create vertices referenced by the edgesuniqueEdges - if multiple identical edges are found they are combined and the edge attribute is set to the sum. Otherwise duplicate edges are treated as separate. To enable uniqueEdges, a PartitionStrategy must be provided.edgeStorageLevel - the desired storage level at which to cache the edges if necessaryvertexStorageLevel - the desired storage level at which to cache the vertices if necessary evidence$15 - (undocumented) Returns:a graph with edge attributes containing either the count of duplicate edges or 1 (if uniqueEdges is None) and vertex attributes containing the total degree of each vertex. fromEdges public static <VD,ED> Graph<VD,ED> fromEdges(RDD<Edge<ED>> edges, VD defaultValue, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$16, scala.reflect.ClassTag<ED> evidence$17) Construct a graph from a collection of edges. Parameters:edges - the RDD containing the set of edges in the graphdefaultValue - the default vertex attribute to use for each vertexedgeStorageLevel - the desired storage level at which to cache the edges if necessaryvertexStorageLevel - the desired storage level at which to cache the vertices if necessary evidence$16 - (undocumented)evidence$17 - (undocumented) Returns:a graph with edge attributes described by edges and vertices given by all vertices in edges with value defaultValue apply public static <VD,ED> Graph<VD,ED> apply(RDD<scala.Tuple2<Object,VD>> vertices, RDD<Edge<ED>> edges, VD defaultVertexAttr, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$18, scala.reflect.ClassTag<ED> evidence$19) Construct a graph from a collection of vertices and edges with attributes. Duplicate vertices are picked arbitrarily and vertices found in the edge collection but not in the input vertices are assigned the default attribute. Parameters:vertices - the "set" of vertices and their attributesedges - the collection of edges in the graphdefaultVertexAttr - the default vertex attribute to use for vertices that are mentioned in edges but not in verticesedgeStorageLevel - the desired storage level at which to cache the edges if necessaryvertexStorageLevel - the desired storage level at which to cache the vertices if necessaryevidence$18 - (undocumented)evidence$19 - (undocumented) Returns:(undocumented) graphToGraphOps public static <VD,ED> GraphOps<VD,ED> graphToGraphOps(Graph<VD,ED> g, scala.reflect.ClassTag<VD> evidence$20, scala.reflect.ClassTag<ED> evidence$21) Implicitly extracts the GraphOps member from a graph. To improve modularity the Graph type only contains a small set of basic operations. All the convenience operations are defined in the GraphOps class which may be shared across multiple graph implementations. Parameters:g - (undocumented)evidence$20 - (undocumented)evidence$21 - (undocumented) Returns:(undocumented) vertices public abstract VertexRDD<VD> vertices() An RDD containing the vertices and their associated attributes. Returns:an RDD containing the vertices in this graph edges public abstract EdgeRDD<ED> edges() An RDD containing the edges and their associated attributes. The entries in the RDD contain just the source id and target id along with the edge data. Returns:an RDD containing the edges in this graph See Also:Edge} for the edge type., Graph#triplets} to get an RDD which contains all the edges along with their vertex data. triplets public abstract RDD<EdgeTriplet<VD,ED>> triplets() An RDD containing the edge triplets, which are edges along with the vertex data associated with the adjacent vertices. The caller should use edges if the vertex data are not needed, i.e. if only the edge data and adjacent vertex ids are needed. Returns:an RDD containing edge triplets persist public abstract Graph<VD,ED> persist(StorageLevel newLevel) Caches the vertices and edges associated with this graph at the specified storage level, ignoring any target storage levels previously set. Parameters:newLevel - the level at which to cache the graph. Returns:A reference to this graph for convenience. cache public abstract Graph<VD,ED> cache() Caches the vertices and edges associated with this graph at the previously-specified target storage levels, which default to MEMORY_ONLY. This is used to pin a graph in memory enabling multiple queries to reuse the same construction process. Returns:(undocumented) checkpoint public abstract void checkpoint() Mark this Graph for checkpointing. It will be saved to a file inside the checkpoint directory set with SparkContext.setCheckpointDir() and all references to its parent RDDs will be removed. It is strongly recommended that this Graph is persisted in memory, otherwise saving it on a file will require recomputation. isCheckpointed public abstract boolean isCheckpointed() Return whether this Graph has been checkpointed or not. This returns true iff both the vertices RDD and edges RDD have been checkpointed. Returns:(undocumented) getCheckpointFiles public abstract scala.collection.Seq<String> getCheckpointFiles() Gets the name of the files to which this Graph was checkpointed. (The vertices RDD and edges RDD are checkpointed separately.) Returns:(undocumented) unpersist public abstract Graph<VD,ED> unpersist(boolean blocking) Uncaches both vertices and edges of this graph. This is useful in iterative algorithms that build a new graph in each iteration. Parameters:blocking - (undocumented) Returns:(undocumented) unpersistVertices public abstract Graph<VD,ED> unpersistVertices(boolean blocking) Uncaches only the vertices of this graph, leaving the edges alone. This is useful in iterative algorithms that modify the vertex attributes but reuse the edges. This method can be used to uncache the vertex attributes of previous iterations once they are no longer needed, improving GC performance. Parameters:blocking - (undocumented) Returns:(undocumented) partitionBy public abstract Graph<VD,ED> partitionBy(PartitionStrategy partitionStrategy) Repartitions the edges in the graph according to partitionStrategy. Parameters:partitionStrategy - the partitioning strategy to use when partitioning the edges in the graph. Returns:(undocumented) partitionBy public abstract Graph<VD,ED> partitionBy(PartitionStrategy partitionStrategy, int numPartitions) Repartitions the edges in the graph according to partitionStrategy. Parameters:partitionStrategy - the partitioning strategy to use when partitioning the edges in the graph.numPartitions - the number of edge partitions in the new graph. Returns:(undocumented) mapVertices public abstract <VD2> Graph<VD2,ED> mapVertices(scala.Function2<Object,VD,VD2> map, scala.reflect.ClassTag<VD2> evidence$3, scala.Predef.$eq$colon$eq<VD,VD2> eq) Transforms each vertex attribute in the graph using the map function. Parameters:map - the function from a vertex object to a new vertex value evidence$3 - (undocumented)eq - (undocumented) Returns:(undocumented) mapEdges public <ED2> Graph<VD,ED2> mapEdges(scala.Function1<Edge<ED>,ED2> map, scala.reflect.ClassTag<ED2> evidence$4) Transforms each edge attribute in the graph using the map function. The map function is not passed the vertex value for the vertices adjacent to the edge. If vertex values are desired, use mapTriplets. Parameters:map - the function from an edge object to a new edge value. evidence$4 - (undocumented) Returns:(undocumented) mapEdges public abstract <ED2> Graph<VD,ED2> mapEdges(scala.Function2<Object,scala.collection.Iterator<Edge<ED>>,scala.collection.Iterator<ED2>> map, scala.reflect.ClassTag<ED2> evidence$5) Transforms each edge attribute using the map function, passing it a whole partition at a time. The map function is given an iterator over edges within a logical partition as well as the partition's ID, and it should return a new iterator over the new values of each edge. The new iterator's elements must correspond one-to-one with the old iterator's elements. If adjacent vertex values are desired, use mapTriplets. Parameters:map - a function that takes a partition id and an iterator over all the edges in the partition, and must return an iterator over the new values for each edge in the order of the input iterator evidence$5 - (undocumented) Returns:(undocumented) mapTriplets public <ED2> Graph<VD,ED2> mapTriplets(scala.Function1<EdgeTriplet<VD,ED>,ED2> map, scala.reflect.ClassTag<ED2> evidence$6) Transforms each edge attribute using the map function, passing it the adjacent vertex attributes as well. If adjacent vertex values are not required, consider using mapEdges instead. Parameters:map - the function from an edge object to a new edge value. evidence$6 - (undocumented) Returns:(undocumented) mapTriplets public <ED2> Graph<VD,ED2> mapTriplets(scala.Function1<EdgeTriplet<VD,ED>,ED2> map, TripletFields tripletFields, scala.reflect.ClassTag<ED2> evidence$7) Transforms each edge attribute using the map function, passing it the adjacent vertex attributes as well. If adjacent vertex values are not required, consider using mapEdges instead. Parameters:map - the function from an edge object to a new edge value.tripletFields - which fields should be included in the edge triplet passed to the map function. If not all fields are needed, specifying this can improve performance. evidence$7 - (undocumented) Returns:(undocumented) mapTriplets public abstract <ED2> Graph<VD,ED2> mapTriplets(scala.Function2<Object,scala.collection.Iterator<EdgeTriplet<VD,ED>>,scala.collection.Iterator<ED2>> map, TripletFields tripletFields, scala.reflect.ClassTag<ED2> evidence$8) Transforms each edge attribute a partition at a time using the map function, passing it the adjacent vertex attributes as well. The map function is given an iterator over edge triplets within a logical partition and should yield a new iterator over the new values of each edge in the order in which they are provided. If adjacent vertex values are not required, consider using mapEdges instead. Parameters:map - the iterator transformtripletFields - which fields should be included in the edge triplet passed to the map function. If not all fields are needed, specifying this can improve performance. evidence$8 - (undocumented) Returns:(undocumented) reverse public abstract Graph<VD,ED> reverse() Reverses all edges in the graph. If this graph contains an edge from a to b then the returned graph contains an edge from b to a. Returns:(undocumented) subgraph public abstract Graph<VD,ED> subgraph(scala.Function1<EdgeTriplet<VD,ED>,Object> epred, scala.Function2<Object,VD,Object> vpred) Restricts the graph to only the vertices and edges satisfying the predicates. The resulting subgraph satisfies V' = {v : for all v in V where vpred(v)} E' = {(u,v): for all (u,v) in E where epred((u,v)) && vpred(u) && vpred(v)} Parameters:epred - the edge predicate, which takes a triplet and evaluates to true if the edge is to remain in the subgraph. Note that only edges where both vertices satisfy the vertex predicate are considered. vpred - the vertex predicate, which takes a vertex object and evaluates to true if the vertex is to be included in the subgraph Returns:the subgraph containing only the vertices and edges that satisfy the predicates mask public abstract <VD2,ED2> Graph<VD,ED> mask(Graph<VD2,ED2> other, scala.reflect.ClassTag<VD2> evidence$9, scala.reflect.ClassTag<ED2> evidence$10) Restricts the graph to only the vertices and edges that are also in other, but keeps the attributes from this graph. Parameters:other - the graph to project this graph ontoevidence$9 - (undocumented)evidence$10 - (undocumented) Returns:a graph with vertices and edges that exist in both the current graph and other, with vertex and edge data from the current graph groupEdges public abstract Graph<VD,ED> groupEdges(scala.Function2<ED,ED,ED> merge) Merges multiple edges between two vertices into a single edge. For correct results, the graph must have been partitioned using partitionBy. Parameters:merge - the user-supplied commutative associative function to merge edge attributes for duplicate edges. Returns:The resulting graph with a single edge for each (source, dest) vertex pair. aggregateMessages public <A> VertexRDD<A> aggregateMessages(scala.Function1<EdgeContext<VD,ED,A>,scala.runtime.BoxedUnit> sendMsg, scala.Function2<A,A,A> mergeMsg, TripletFields tripletFields, scala.reflect.ClassTag<A> evidence$11) Aggregates values from the neighboring edges and vertices of each vertex. The user-supplied sendMsg function is invoked on each edge of the graph, generating 0 or more messages to be sent to either vertex in the edge. The mergeMsg function is then used to combine all messages destined to the same vertex. Parameters:sendMsg - runs on each edge, sending messages to neighboring vertices using the EdgeContext.mergeMsg - used to combine messages from sendMsg destined to the same vertex. This combiner should be commutative and associative.tripletFields - which fields should be included in the EdgeContext passed to the sendMsg function. If not all fields are needed, specifying this can improve performance. evidence$11 - (undocumented) Returns:(undocumented) outerJoinVertices public abstract <U,VD2> Graph<VD2,ED> outerJoinVertices(RDD<scala.Tuple2<Object,U>> other, scala.Function3<Object,VD,scala.Option<U>,VD2> mapFunc, scala.reflect.ClassTag<U> evidence$13, scala.reflect.ClassTag<VD2> evidence$14, scala.Predef.$eq$colon$eq<VD,VD2> eq) Joins the vertices with entries in the table RDD and merges the results using mapFunc. The input table should contain at most one entry for each vertex. If no entry in other is provided for a particular vertex in the graph, the map function receives None. Parameters:other - the table to join with the vertices in the graph. The table should contain at most one entry for each vertex.mapFunc - the function used to compute the new vertex values. The map function is invoked for all vertices, even those that do not have a corresponding entry in the table. evidence$13 - (undocumented)evidence$14 - (undocumented)eq - (undocumented) Returns:(undocumented) ops public GraphOps<VD,ED> ops() The associated GraphOps object. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GraphGenerators (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GraphGenerators (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx.util Class GraphGenerators Object org.apache.spark.graphx.util.GraphGenerators public class GraphGenerators extends Object A collection of graph generating functions. Constructor Summary Constructors  Constructor and Description GraphGenerators()  Method Summary Methods  Modifier and Type Method and Description static Edge<Object>[] generateRandomEdges(int src, int numEdges, int maxVertexId, long seed)  static Graph<scala.Tuple2<Object,Object>,Object> gridGraph(SparkContext sc, int rows, int cols) Create rows by cols grid graph with each vertex connected to its row+1 and col+1 neighbors. static Graph<Object,Object> logNormalGraph(SparkContext sc, int numVertices, int numEParts, double mu, double sigma, long seed) Generate a graph whose vertex out degree distribution is log normal. static double RMATa()  static double RMATb()  static double RMATc()  static double RMATd()  static Graph<Object,Object> rmatGraph(SparkContext sc, int requestedNumVertices, int numEdges) A random graph generator using the R-MAT model, proposed in "R-MAT: A Recursive Model for Graph Mining" by Chakrabarti et al. static Graph<Object,Object> starGraph(SparkContext sc, int nverts) Create a star graph with vertex 0 being the center. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GraphGenerators public GraphGenerators() Method Detail RMATa public static double RMATa() RMATb public static double RMATb() RMATd public static double RMATd() logNormalGraph public static Graph<Object,Object> logNormalGraph(SparkContext sc, int numVertices, int numEParts, double mu, double sigma, long seed) Generate a graph whose vertex out degree distribution is log normal. The default values for mu and sigma are taken from the Pregel paper: Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. 2010. Pregel: a system for large-scale graph processing. SIGMOD '10. If the seed is -1 (default), a random seed is chosen. Otherwise, use the user-specified seed. Parameters:sc - Spark ContextnumVertices - number of vertices in generated graphnumEParts - (optional) number of partitionsmu - (optional, default: 4.0) mean of out-degree distributionsigma - (optional, default: 1.3) standard deviation of out-degree distributionseed - (optional, default: -1) seed for RNGs, -1 causes a random seed to be chosen Returns:Graph object RMATc public static double RMATc() generateRandomEdges public static Edge<Object>[] generateRandomEdges(int src, int numEdges, int maxVertexId, long seed) rmatGraph public static Graph<Object,Object> rmatGraph(SparkContext sc, int requestedNumVertices, int numEdges) A random graph generator using the R-MAT model, proposed in "R-MAT: A Recursive Model for Graph Mining" by Chakrabarti et al. See http://www.cs.cmu.edu/~christos/PUBLICATIONS/siam04.pdf. Parameters:sc - (undocumented)requestedNumVertices - (undocumented)numEdges - (undocumented) Returns:(undocumented) gridGraph public static Graph<scala.Tuple2<Object,Object>,Object> gridGraph(SparkContext sc, int rows, int cols) Create rows by cols grid graph with each vertex connected to its row+1 and col+1 neighbors. Vertex ids are assigned in row major order. Parameters:sc - the spark context in which to construct the graphrows - the number of rowscols - the number of columns Returns:A graph containing vertices with the row and column ids as their attributes and edge values as 1.0. starGraph public static Graph<Object,Object> starGraph(SparkContext sc, int nverts) Create a star graph with vertex 0 being the center. Parameters:sc - the spark context in which to construct the graphnverts - the number of vertices in the star Returns:A star graph containing nverts vertices with vertex 0 being the center vertex. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GraphImpl (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GraphImpl (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx.impl Class GraphImpl<VD,ED> Object org.apache.spark.graphx.Graph<VD,ED> org.apache.spark.graphx.impl.GraphImpl<VD,ED> All Implemented Interfaces: java.io.Serializable public class GraphImpl<VD,ED> extends Graph<VD,ED> implements scala.Serializable An implementation of Graph to support computation on graphs. Graphs are represented using two RDDs: vertices, which contains vertex attributes and the routing information for shipping vertex attributes to edge partitions, and replicatedVertexView, which contains edges and the vertex attributes mentioned by each edge. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static <A> VertexRDD<A> aggregateMessages(scala.Function1<EdgeContext<VD,ED,A>,scala.runtime.BoxedUnit> sendMsg, scala.Function2<A,A,A> mergeMsg, TripletFields tripletFields, scala.reflect.ClassTag<A> evidence$11)  static <A> TripletFields aggregateMessages$default$3()  <A> VertexRDD<A> aggregateMessagesWithActiveSet(scala.Function1<EdgeContext<VD,ED,A>,scala.runtime.BoxedUnit> sendMsg, scala.Function2<A,A,A> mergeMsg, TripletFields tripletFields, scala.Option<scala.Tuple2<VertexRDD<?>,EdgeDirection>> activeSetOpt, scala.reflect.ClassTag<A> evidence$10)  static <VD,ED> GraphImpl<VD,ED> apply(RDD<Edge<ED>> edges, VD defaultVertexAttr, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$13, scala.reflect.ClassTag<ED> evidence$14) Create a graph from edges, setting referenced vertices to `defaultVertexAttr`. static <VD,ED> GraphImpl<VD,ED> apply(RDD<scala.Tuple2<Object,VD>> vertices, RDD<Edge<ED>> edges, VD defaultVertexAttr, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$17, scala.reflect.ClassTag<ED> evidence$18) Create a graph from vertices and edges, setting missing vertices to `defaultVertexAttr`. static <VD,ED> GraphImpl<VD,ED> apply(VertexRDD<VD> vertices, EdgeRDD<ED> edges, scala.reflect.ClassTag<VD> evidence$19, scala.reflect.ClassTag<ED> evidence$20) Create a graph from a VertexRDD and an EdgeRDD with arbitrary replicated vertices. Graph<VD,ED> cache() Caches the vertices and edges associated with this graph at the previously-specified target storage levels, which default to MEMORY_ONLY. void checkpoint() Mark this Graph for checkpointing. EdgeRDDImpl<ED,VD> edges() An RDD containing the edges and their associated attributes. static <VD,ED> GraphImpl<VD,ED> fromEdgePartitions(RDD<scala.Tuple2<Object,org.apache.spark.graphx.impl.EdgePartition<ED,VD>>> edgePartitions, VD defaultVertexAttr, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$15, scala.reflect.ClassTag<ED> evidence$16) Create a graph from EdgePartitions, setting referenced vertices to `defaultVertexAttr`. static <VD,ED> GraphImpl<VD,ED> fromExistingRDDs(VertexRDD<VD> vertices, EdgeRDD<ED> edges, scala.reflect.ClassTag<VD> evidence$21, scala.reflect.ClassTag<ED> evidence$22) Create a graph from a VertexRDD and an EdgeRDD with the same replicated vertex type as the vertices. scala.collection.Seq<String> getCheckpointFiles() Gets the name of the files to which this Graph was checkpointed. Graph<VD,ED> groupEdges(scala.Function2<ED,ED,ED> merge) Merges multiple edges between two vertices into a single edge. boolean isCheckpointed() Return whether this Graph has been checkpointed or not. <ED2> Graph<VD,ED2> mapEdges(scala.Function2<Object,scala.collection.Iterator<Edge<ED>>,scala.collection.Iterator<ED2>> f, scala.reflect.ClassTag<ED2> evidence$6) Transforms each edge attribute using the map function, passing it a whole partition at a time. <ED2> Graph<VD,ED2> mapTriplets(scala.Function2<Object,scala.collection.Iterator<EdgeTriplet<VD,ED>>,scala.collection.Iterator<ED2>> f, TripletFields tripletFields, scala.reflect.ClassTag<ED2> evidence$7) Transforms each edge attribute a partition at a time using the map function, passing it the adjacent vertex attributes as well. <VD2> Graph<VD2,ED> mapVertices(scala.Function2<Object,VD,VD2> f, scala.reflect.ClassTag<VD2> evidence$5, scala.Predef.$eq$colon$eq<VD,VD2> eq) Transforms each vertex attribute in the graph using the map function. static <VD2> scala.runtime.Null$ mapVertices$default$3(scala.Function2<Object,VD,VD2> map)  <VD2,ED2> Graph<VD,ED> mask(Graph<VD2,ED2> other, scala.reflect.ClassTag<VD2> evidence$8, scala.reflect.ClassTag<ED2> evidence$9) Restricts the graph to only the vertices and edges that are also in other, but keeps the attributes from this graph. static GraphOps<VD,ED> ops()  <U,VD2> Graph<VD2,ED> outerJoinVertices(RDD<scala.Tuple2<Object,U>> other, scala.Function3<Object,VD,scala.Option<U>,VD2> updateF, scala.reflect.ClassTag<U> evidence$11, scala.reflect.ClassTag<VD2> evidence$12, scala.Predef.$eq$colon$eq<VD,VD2> eq) Joins the vertices with entries in the table RDD and merges the results using mapFunc. static <U,VD2> scala.runtime.Null$ outerJoinVertices$default$5(RDD<scala.Tuple2<Object,U>> other, scala.Function3<Object,VD,scala.Option<U>,VD2> mapFunc)  Graph<VD,ED> partitionBy(PartitionStrategy partitionStrategy) Repartitions the edges in the graph according to partitionStrategy. Graph<VD,ED> partitionBy(PartitionStrategy partitionStrategy, int numPartitions) Repartitions the edges in the graph according to partitionStrategy. Graph<VD,ED> persist(StorageLevel newLevel) Caches the vertices and edges associated with this graph at the specified storage level, ignoring any target storage levels previously set. static StorageLevel persist$default$1()  org.apache.spark.graphx.impl.ReplicatedVertexView<VD,ED> replicatedVertexView()  Graph<VD,ED> reverse() Reverses all edges in the graph. Graph<VD,ED> subgraph(scala.Function1<EdgeTriplet<VD,ED>,Object> epred, scala.Function2<Object,VD,Object> vpred) Restricts the graph to only the vertices and edges satisfying the predicates. static scala.Function1<EdgeTriplet<VD,ED>,Object> subgraph$default$1()  static scala.Function2<Object,VD,Object> subgraph$default$2()  RDD<EdgeTriplet<VD,ED>> triplets() Return a RDD that brings edges together with their source and destination vertices. Graph<VD,ED> unpersist(boolean blocking) Uncaches both vertices and edges of this graph. static boolean unpersist$default$1()  Graph<VD,ED> unpersistVertices(boolean blocking) Uncaches only the vertices of this graph, leaving the edges alone. static boolean unpersistVertices$default$1()  VertexRDD<VD> vertices() An RDD containing the vertices and their associated attributes. Methods inherited from class org.apache.spark.graphx.Graph aggregateMessages, fromEdges, fromEdgeTuples, graphToGraphOps, mapEdges, mapTriplets, mapTriplets, ops Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail apply public static <VD,ED> GraphImpl<VD,ED> apply(RDD<Edge<ED>> edges, VD defaultVertexAttr, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$13, scala.reflect.ClassTag<ED> evidence$14) Create a graph from edges, setting referenced vertices to `defaultVertexAttr`. fromEdgePartitions public static <VD,ED> GraphImpl<VD,ED> fromEdgePartitions(RDD<scala.Tuple2<Object,org.apache.spark.graphx.impl.EdgePartition<ED,VD>>> edgePartitions, VD defaultVertexAttr, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$15, scala.reflect.ClassTag<ED> evidence$16) Create a graph from EdgePartitions, setting referenced vertices to `defaultVertexAttr`. apply public static <VD,ED> GraphImpl<VD,ED> apply(RDD<scala.Tuple2<Object,VD>> vertices, RDD<Edge<ED>> edges, VD defaultVertexAttr, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel, scala.reflect.ClassTag<VD> evidence$17, scala.reflect.ClassTag<ED> evidence$18) Create a graph from vertices and edges, setting missing vertices to `defaultVertexAttr`. apply public static <VD,ED> GraphImpl<VD,ED> apply(VertexRDD<VD> vertices, EdgeRDD<ED> edges, scala.reflect.ClassTag<VD> evidence$19, scala.reflect.ClassTag<ED> evidence$20) Create a graph from a VertexRDD and an EdgeRDD with arbitrary replicated vertices. The VertexRDD must already be set up for efficient joins with the EdgeRDD by calling VertexRDD.withEdges or an appropriate VertexRDD constructor. Parameters:vertices - (undocumented)edges - (undocumented)evidence$19 - (undocumented)evidence$20 - (undocumented) Returns:(undocumented) fromExistingRDDs public static <VD,ED> GraphImpl<VD,ED> fromExistingRDDs(VertexRDD<VD> vertices, EdgeRDD<ED> edges, scala.reflect.ClassTag<VD> evidence$21, scala.reflect.ClassTag<ED> evidence$22) Create a graph from a VertexRDD and an EdgeRDD with the same replicated vertex type as the vertices. The VertexRDD must already be set up for efficient joins with the EdgeRDD by calling VertexRDD.withEdges or an appropriate VertexRDD constructor. Parameters:vertices - (undocumented)edges - (undocumented)evidence$21 - (undocumented)evidence$22 - (undocumented) Returns:(undocumented) aggregateMessages public static <A> VertexRDD<A> aggregateMessages(scala.Function1<EdgeContext<VD,ED,A>,scala.runtime.BoxedUnit> sendMsg, scala.Function2<A,A,A> mergeMsg, TripletFields tripletFields, scala.reflect.ClassTag<A> evidence$11) ops public static GraphOps<VD,ED> ops() persist$default$1 public static StorageLevel persist$default$1() unpersist$default$1 public static boolean unpersist$default$1() unpersistVertices$default$1 public static boolean unpersistVertices$default$1() mapVertices$default$3 public static <VD2> scala.runtime.Null$ mapVertices$default$3(scala.Function2<Object,VD,VD2> map) subgraph$default$1 public static scala.Function1<EdgeTriplet<VD,ED>,Object> subgraph$default$1() subgraph$default$2 public static scala.Function2<Object,VD,Object> subgraph$default$2() aggregateMessages$default$3 public static <A> TripletFields aggregateMessages$default$3() outerJoinVertices$default$5 public static <U,VD2> scala.runtime.Null$ outerJoinVertices$default$5(RDD<scala.Tuple2<Object,U>> other, scala.Function3<Object,VD,scala.Option<U>,VD2> mapFunc) vertices public VertexRDD<VD> vertices() Description copied from class: Graph An RDD containing the vertices and their associated attributes. Specified by: vertices in class Graph<VD,ED> Returns:an RDD containing the vertices in this graph replicatedVertexView public org.apache.spark.graphx.impl.ReplicatedVertexView<VD,ED> replicatedVertexView() edges public EdgeRDDImpl<ED,VD> edges() Description copied from class: Graph An RDD containing the edges and their associated attributes. The entries in the RDD contain just the source id and target id along with the edge data. Specified by: edges in class Graph<VD,ED> Returns:an RDD containing the edges in this graph See Also:Edge} for the edge type., Graph#triplets} to get an RDD which contains all the edges along with their vertex data. triplets public RDD<EdgeTriplet<VD,ED>> triplets() Return a RDD that brings edges together with their source and destination vertices. Specified by: triplets in class Graph<VD,ED> Returns:an RDD containing edge triplets persist public Graph<VD,ED> persist(StorageLevel newLevel) Description copied from class: Graph Caches the vertices and edges associated with this graph at the specified storage level, ignoring any target storage levels previously set. Specified by: persist in class Graph<VD,ED> Parameters:newLevel - the level at which to cache the graph. Returns:A reference to this graph for convenience. cache public Graph<VD,ED> cache() Description copied from class: Graph Caches the vertices and edges associated with this graph at the previously-specified target storage levels, which default to MEMORY_ONLY. This is used to pin a graph in memory enabling multiple queries to reuse the same construction process. Specified by: cache in class Graph<VD,ED> Returns:(undocumented) checkpoint public void checkpoint() Description copied from class: Graph Mark this Graph for checkpointing. It will be saved to a file inside the checkpoint directory set with SparkContext.setCheckpointDir() and all references to its parent RDDs will be removed. It is strongly recommended that this Graph is persisted in memory, otherwise saving it on a file will require recomputation. Specified by: checkpoint in class Graph<VD,ED> isCheckpointed public boolean isCheckpointed() Description copied from class: Graph Return whether this Graph has been checkpointed or not. This returns true iff both the vertices RDD and edges RDD have been checkpointed. Specified by: isCheckpointed in class Graph<VD,ED> Returns:(undocumented) getCheckpointFiles public scala.collection.Seq<String> getCheckpointFiles() Description copied from class: Graph Gets the name of the files to which this Graph was checkpointed. (The vertices RDD and edges RDD are checkpointed separately.) Specified by: getCheckpointFiles in class Graph<VD,ED> Returns:(undocumented) unpersist public Graph<VD,ED> unpersist(boolean blocking) Description copied from class: Graph Uncaches both vertices and edges of this graph. This is useful in iterative algorithms that build a new graph in each iteration. Specified by: unpersist in class Graph<VD,ED> Parameters:blocking - (undocumented) Returns:(undocumented) unpersistVertices public Graph<VD,ED> unpersistVertices(boolean blocking) Description copied from class: Graph Uncaches only the vertices of this graph, leaving the edges alone. This is useful in iterative algorithms that modify the vertex attributes but reuse the edges. This method can be used to uncache the vertex attributes of previous iterations once they are no longer needed, improving GC performance. Specified by: unpersistVertices in class Graph<VD,ED> Parameters:blocking - (undocumented) Returns:(undocumented) partitionBy public Graph<VD,ED> partitionBy(PartitionStrategy partitionStrategy) Description copied from class: Graph Repartitions the edges in the graph according to partitionStrategy. Specified by: partitionBy in class Graph<VD,ED> Parameters:partitionStrategy - the partitioning strategy to use when partitioning the edges in the graph. Returns:(undocumented) partitionBy public Graph<VD,ED> partitionBy(PartitionStrategy partitionStrategy, int numPartitions) Description copied from class: Graph Repartitions the edges in the graph according to partitionStrategy. Specified by: partitionBy in class Graph<VD,ED> Parameters:partitionStrategy - the partitioning strategy to use when partitioning the edges in the graph.numPartitions - the number of edge partitions in the new graph. Returns:(undocumented) reverse public Graph<VD,ED> reverse() Description copied from class: Graph Reverses all edges in the graph. If this graph contains an edge from a to b then the returned graph contains an edge from b to a. Specified by: reverse in class Graph<VD,ED> Returns:(undocumented) mapVertices public <VD2> Graph<VD2,ED> mapVertices(scala.Function2<Object,VD,VD2> f, scala.reflect.ClassTag<VD2> evidence$5, scala.Predef.$eq$colon$eq<VD,VD2> eq) Description copied from class: Graph Transforms each vertex attribute in the graph using the map function. Specified by: mapVertices in class Graph<VD,ED> Parameters:f - the function from a vertex object to a new vertex value evidence$5 - (undocumented)eq - (undocumented) Returns:(undocumented) mapEdges public <ED2> Graph<VD,ED2> mapEdges(scala.Function2<Object,scala.collection.Iterator<Edge<ED>>,scala.collection.Iterator<ED2>> f, scala.reflect.ClassTag<ED2> evidence$6) Description copied from class: Graph Transforms each edge attribute using the map function, passing it a whole partition at a time. The map function is given an iterator over edges within a logical partition as well as the partition's ID, and it should return a new iterator over the new values of each edge. The new iterator's elements must correspond one-to-one with the old iterator's elements. If adjacent vertex values are desired, use mapTriplets. Specified by: mapEdges in class Graph<VD,ED> Parameters:f - a function that takes a partition id and an iterator over all the edges in the partition, and must return an iterator over the new values for each edge in the order of the input iterator evidence$6 - (undocumented) Returns:(undocumented) mapTriplets public <ED2> Graph<VD,ED2> mapTriplets(scala.Function2<Object,scala.collection.Iterator<EdgeTriplet<VD,ED>>,scala.collection.Iterator<ED2>> f, TripletFields tripletFields, scala.reflect.ClassTag<ED2> evidence$7) Description copied from class: Graph Transforms each edge attribute a partition at a time using the map function, passing it the adjacent vertex attributes as well. The map function is given an iterator over edge triplets within a logical partition and should yield a new iterator over the new values of each edge in the order in which they are provided. If adjacent vertex values are not required, consider using mapEdges instead. Specified by: mapTriplets in class Graph<VD,ED> Parameters:f - the iterator transformtripletFields - which fields should be included in the edge triplet passed to the map function. If not all fields are needed, specifying this can improve performance. evidence$7 - (undocumented) Returns:(undocumented) subgraph public Graph<VD,ED> subgraph(scala.Function1<EdgeTriplet<VD,ED>,Object> epred, scala.Function2<Object,VD,Object> vpred) Description copied from class: Graph Restricts the graph to only the vertices and edges satisfying the predicates. The resulting subgraph satisfies V' = {v : for all v in V where vpred(v)} E' = {(u,v): for all (u,v) in E where epred((u,v)) && vpred(u) && vpred(v)} Specified by: subgraph in class Graph<VD,ED> Parameters:epred - the edge predicate, which takes a triplet and evaluates to true if the edge is to remain in the subgraph. Note that only edges where both vertices satisfy the vertex predicate are considered. vpred - the vertex predicate, which takes a vertex object and evaluates to true if the vertex is to be included in the subgraph Returns:the subgraph containing only the vertices and edges that satisfy the predicates mask public <VD2,ED2> Graph<VD,ED> mask(Graph<VD2,ED2> other, scala.reflect.ClassTag<VD2> evidence$8, scala.reflect.ClassTag<ED2> evidence$9) Description copied from class: Graph Restricts the graph to only the vertices and edges that are also in other, but keeps the attributes from this graph. Specified by: mask in class Graph<VD,ED> Parameters:other - the graph to project this graph ontoevidence$8 - (undocumented)evidence$9 - (undocumented) Returns:a graph with vertices and edges that exist in both the current graph and other, with vertex and edge data from the current graph groupEdges public Graph<VD,ED> groupEdges(scala.Function2<ED,ED,ED> merge) Description copied from class: Graph Merges multiple edges between two vertices into a single edge. For correct results, the graph must have been partitioned using partitionBy. Specified by: groupEdges in class Graph<VD,ED> Parameters:merge - the user-supplied commutative associative function to merge edge attributes for duplicate edges. Returns:The resulting graph with a single edge for each (source, dest) vertex pair. aggregateMessagesWithActiveSet public <A> VertexRDD<A> aggregateMessagesWithActiveSet(scala.Function1<EdgeContext<VD,ED,A>,scala.runtime.BoxedUnit> sendMsg, scala.Function2<A,A,A> mergeMsg, TripletFields tripletFields, scala.Option<scala.Tuple2<VertexRDD<?>,EdgeDirection>> activeSetOpt, scala.reflect.ClassTag<A> evidence$10) outerJoinVertices public <U,VD2> Graph<VD2,ED> outerJoinVertices(RDD<scala.Tuple2<Object,U>> other, scala.Function3<Object,VD,scala.Option<U>,VD2> updateF, scala.reflect.ClassTag<U> evidence$11, scala.reflect.ClassTag<VD2> evidence$12, scala.Predef.$eq$colon$eq<VD,VD2> eq) Description copied from class: Graph Joins the vertices with entries in the table RDD and merges the results using mapFunc. The input table should contain at most one entry for each vertex. If no entry in other is provided for a particular vertex in the graph, the map function receives None. Specified by: outerJoinVertices in class Graph<VD,ED> Parameters:other - the table to join with the vertices in the graph. The table should contain at most one entry for each vertex.updateF - the function used to compute the new vertex values. The map function is invoked for all vertices, even those that do not have a corresponding entry in the table. evidence$11 - (undocumented)evidence$12 - (undocumented)eq - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GraphLoader (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GraphLoader (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class GraphLoader Object org.apache.spark.graphx.GraphLoader public class GraphLoader extends Object Provides utilities for loading Graphs from files. Constructor Summary Constructors  Constructor and Description GraphLoader()  Method Summary Methods  Modifier and Type Method and Description static Graph<Object,Object> edgeListFile(SparkContext sc, String path, boolean canonicalOrientation, int numEdgePartitions, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel) Loads a graph from an edge list formatted file where each line contains two integers: a source id and a target id. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GraphLoader public GraphLoader() Method Detail edgeListFile public static Graph<Object,Object> edgeListFile(SparkContext sc, String path, boolean canonicalOrientation, int numEdgePartitions, StorageLevel edgeStorageLevel, StorageLevel vertexStorageLevel) Loads a graph from an edge list formatted file where each line contains two integers: a source id and a target id. Skips lines that begin with #. If desired the edges can be automatically oriented in the positive direction (source Id < target Id) by setting canonicalOrientation to true. Parameters:sc - SparkContextpath - the path to the file (e.g., /home/data/file or hdfs://file)canonicalOrientation - whether to orient edges in the positive directionnumEdgePartitions - the number of partitions for the edge RDD Setting this value to -1 will use the default parallelism.edgeStorageLevel - the desired storage level for the edge partitionsvertexStorageLevel - the desired storage level for the vertex partitions Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GraphOps (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GraphOps (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class GraphOps<VD,ED> Object org.apache.spark.graphx.GraphOps<VD,ED> All Implemented Interfaces: java.io.Serializable public class GraphOps<VD,ED> extends Object implements scala.Serializable Contains additional functionality for Graph. All operations are expressed in terms of the efficient GraphX API. This class is implicitly constructed for each Graph object. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GraphOps(Graph<VD,ED> graph, scala.reflect.ClassTag<VD> evidence$1, scala.reflect.ClassTag<ED> evidence$2)  Method Summary Methods  Modifier and Type Method and Description VertexRDD<Edge<ED>[]> collectEdges(EdgeDirection edgeDirection) Returns an RDD that contains for each vertex v its local edges, i.e., the edges that are incident on v, in the user-specified direction. VertexRDD<long[]> collectNeighborIds(EdgeDirection edgeDirection) Collect the neighbor vertex ids for each vertex. VertexRDD<scala.Tuple2<Object,VD>[]> collectNeighbors(EdgeDirection edgeDirection) Collect the neighbor vertex attributes for each vertex. Graph<Object,ED> connectedComponents() Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. Graph<Object,ED> connectedComponents(int maxIterations) Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. Graph<VD,ED> convertToCanonicalEdges(scala.Function2<ED,ED,ED> mergeFunc) Convert bi-directional edges into uni-directional ones. VertexRDD<Object> degrees() The degree of each vertex in the graph. <VD2,ED2> Graph<VD,ED> filter(scala.Function1<Graph<VD,ED>,Graph<VD2,ED2>> preprocess, scala.Function1<EdgeTriplet<VD2,ED2>,Object> epred, scala.Function2<Object,VD2,Object> vpred, scala.reflect.ClassTag<VD2> evidence$4, scala.reflect.ClassTag<ED2> evidence$5) Filter the graph by computing some values to filter on, and applying the predicates. VertexRDD<Object> inDegrees() The in-degree of each vertex in the graph. <U> Graph<VD,ED> joinVertices(RDD<scala.Tuple2<Object,U>> table, scala.Function3<Object,VD,U,VD> mapFunc, scala.reflect.ClassTag<U> evidence$3) Join the vertices with an RDD and then apply a function from the vertex and RDD entry to a new vertex value. long numEdges() The number of edges in the graph. long numVertices() The number of vertices in the graph. VertexRDD<Object> outDegrees() The out-degree of each vertex in the graph. Graph<Object,Object> pageRank(double tol, double resetProb) Run a dynamic version of PageRank returning a graph with vertex attributes containing the PageRank and edge attributes containing the normalized edge weight. Graph<Object,Object> personalizedPageRank(long src, double tol, double resetProb) Run personalized PageRank for a given vertex, such that all random walks are started relative to the source node. long pickRandomVertex() Picks a random vertex from the graph and returns its ID. <A> Graph<VD,ED> pregel(A initialMsg, int maxIterations, EdgeDirection activeDirection, scala.Function3<Object,VD,A,VD> vprog, scala.Function1<EdgeTriplet<VD,ED>,scala.collection.Iterator<scala.Tuple2<Object,A>>> sendMsg, scala.Function2<A,A,A> mergeMsg, scala.reflect.ClassTag<A> evidence$6) Execute a Pregel-like iterative vertex-parallel abstraction. Graph<VD,ED> removeSelfEdges() Remove self edges. Graph<Object,Object> staticPageRank(int numIter, double resetProb) Run PageRank for a fixed number of iterations returning a graph with vertex attributes containing the PageRank and edge attributes the normalized edge weight. Graph<Object,Object> staticPersonalizedPageRank(long src, int numIter, double resetProb) Run Personalized PageRank for a fixed number of iterations with with all iterations originating at the source node returning a graph with vertex attributes containing the PageRank and edge attributes the normalized edge weight. Graph<Object,ED> stronglyConnectedComponents(int numIter) Compute the strongly connected component (SCC) of each vertex and return a graph with the vertex value containing the lowest vertex id in the SCC containing that vertex. Graph<Object,ED> triangleCount() Compute the number of triangles passing through each vertex. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GraphOps public GraphOps(Graph<VD,ED> graph, scala.reflect.ClassTag<VD> evidence$1, scala.reflect.ClassTag<ED> evidence$2) Method Detail numEdges public long numEdges() The number of edges in the graph. numVertices public long numVertices() The number of vertices in the graph. inDegrees public VertexRDD<Object> inDegrees() The in-degree of each vertex in the graph. Returns:(undocumented) outDegrees public VertexRDD<Object> outDegrees() The out-degree of each vertex in the graph. Returns:(undocumented) degrees public VertexRDD<Object> degrees() The degree of each vertex in the graph. Returns:(undocumented) collectNeighborIds public VertexRDD<long[]> collectNeighborIds(EdgeDirection edgeDirection) Collect the neighbor vertex ids for each vertex. Parameters:edgeDirection - the direction along which to collect neighboring vertices Returns:the set of neighboring ids for each vertex collectNeighbors public VertexRDD<scala.Tuple2<Object,VD>[]> collectNeighbors(EdgeDirection edgeDirection) Collect the neighbor vertex attributes for each vertex. Parameters:edgeDirection - the direction along which to collect neighboring vertices Returns:the vertex set of neighboring vertex attributes for each vertex collectEdges public VertexRDD<Edge<ED>[]> collectEdges(EdgeDirection edgeDirection) Returns an RDD that contains for each vertex v its local edges, i.e., the edges that are incident on v, in the user-specified direction. Warning: note that singleton vertices, those with no edges in the given direction will not be part of the return value. Parameters:edgeDirection - the direction along which to collect the local edges of vertices Returns:the local edges for each vertex removeSelfEdges public Graph<VD,ED> removeSelfEdges() Remove self edges. Returns:a graph with all self edges removed joinVertices public <U> Graph<VD,ED> joinVertices(RDD<scala.Tuple2<Object,U>> table, scala.Function3<Object,VD,U,VD> mapFunc, scala.reflect.ClassTag<U> evidence$3) Join the vertices with an RDD and then apply a function from the vertex and RDD entry to a new vertex value. The input table should contain at most one entry for each vertex. If no entry is provided the map function is skipped and the old value is used. Parameters:table - the table to join with the vertices in the graph. The table should contain at most one entry for each vertex.mapFunc - the function used to compute the new vertex values. The map function is invoked only for vertices with a corresponding entry in the table otherwise the old vertex value is used. evidence$3 - (undocumented) Returns:(undocumented) filter public <VD2,ED2> Graph<VD,ED> filter(scala.Function1<Graph<VD,ED>,Graph<VD2,ED2>> preprocess, scala.Function1<EdgeTriplet<VD2,ED2>,Object> epred, scala.Function2<Object,VD2,Object> vpred, scala.reflect.ClassTag<VD2> evidence$4, scala.reflect.ClassTag<ED2> evidence$5) Filter the graph by computing some values to filter on, and applying the predicates. Parameters:preprocess - a function to compute new vertex and edge data before filteringepred - edge pred to filter on after preprocess, see more details under Graph.subgraph(scala.Function1<org.apache.spark.graphx.EdgeTriplet<VD, ED>, java.lang.Object>, scala.Function2<java.lang.Object, VD, java.lang.Object>)vpred - vertex pred to filter on after preprocess, see more details under Graph.subgraph(scala.Function1<org.apache.spark.graphx.EdgeTriplet<VD, ED>, java.lang.Object>, scala.Function2<java.lang.Object, VD, java.lang.Object>)evidence$4 - (undocumented)evidence$5 - (undocumented) Returns:a subgraph of the original graph, with its data unchanged pickRandomVertex public long pickRandomVertex() Picks a random vertex from the graph and returns its ID. Returns:(undocumented) convertToCanonicalEdges public Graph<VD,ED> convertToCanonicalEdges(scala.Function2<ED,ED,ED> mergeFunc) Convert bi-directional edges into uni-directional ones. Some graph algorithms (e.g., TriangleCount) assume that an input graph has its edges in canonical direction. This function rewrites the vertex ids of edges so that srcIds are smaller than dstIds, and merges the duplicated edges. Parameters:mergeFunc - the user defined reduce function which should be commutative and associative and is used to combine the output of the map phase Returns:the resulting graph with canonical edges pregel public <A> Graph<VD,ED> pregel(A initialMsg, int maxIterations, EdgeDirection activeDirection, scala.Function3<Object,VD,A,VD> vprog, scala.Function1<EdgeTriplet<VD,ED>,scala.collection.Iterator<scala.Tuple2<Object,A>>> sendMsg, scala.Function2<A,A,A> mergeMsg, scala.reflect.ClassTag<A> evidence$6) Execute a Pregel-like iterative vertex-parallel abstraction. The user-defined vertex-program vprog is executed in parallel on each vertex receiving any inbound messages and computing a new value for the vertex. The sendMsg function is then invoked on all out-edges and is used to compute an optional message to the destination vertex. The mergeMsg function is a commutative associative function used to combine messages destined to the same vertex. On the first iteration all vertices receive the initialMsg and on subsequent iterations if a vertex does not receive a message then the vertex-program is not invoked. This function iterates until there are no remaining messages, or for maxIterations iterations. Parameters:initialMsg - the message each vertex will receive at the on the first iteration maxIterations - the maximum number of iterations to run for activeDirection - the direction of edges incident to a vertex that received a message in the previous round on which to run sendMsg. For example, if this is EdgeDirection.Out, only out-edges of vertices that received a message in the previous round will run. vprog - the user-defined vertex program which runs on each vertex and receives the inbound message and computes a new vertex value. On the first iteration the vertex program is invoked on all vertices and is passed the default message. On subsequent iterations the vertex program is only invoked on those vertices that receive messages. sendMsg - a user supplied function that is applied to out edges of vertices that received messages in the current iteration mergeMsg - a user supplied function that takes two incoming messages of type A and merges them into a single message of type A. ''This function must be commutative and associative and ideally the size of A should not increase.'' evidence$6 - (undocumented) Returns:the resulting graph at the end of the computation pageRank public Graph<Object,Object> pageRank(double tol, double resetProb) Run a dynamic version of PageRank returning a graph with vertex attributes containing the PageRank and edge attributes containing the normalized edge weight. Parameters:tol - (undocumented)resetProb - (undocumented) Returns:(undocumented)See Also:PageRank$.runUntilConvergence(org.apache.spark.graphx.Graph<VD, ED>, double, double, scala.reflect.ClassTag<VD>, scala.reflect.ClassTag<ED>) personalizedPageRank public Graph<Object,Object> personalizedPageRank(long src, double tol, double resetProb) Run personalized PageRank for a given vertex, such that all random walks are started relative to the source node. Parameters:src - (undocumented)tol - (undocumented)resetProb - (undocumented) Returns:(undocumented)See Also:PageRank$.runUntilConvergenceWithOptions(org.apache.spark.graphx.Graph<VD, ED>, double, double, scala.Option<java.lang.Object>, scala.reflect.ClassTag<VD>, scala.reflect.ClassTag<ED>) staticPersonalizedPageRank public Graph<Object,Object> staticPersonalizedPageRank(long src, int numIter, double resetProb) Run Personalized PageRank for a fixed number of iterations with with all iterations originating at the source node returning a graph with vertex attributes containing the PageRank and edge attributes the normalized edge weight. Parameters:src - (undocumented)numIter - (undocumented)resetProb - (undocumented) Returns:(undocumented)See Also:PageRank$.runWithOptions(org.apache.spark.graphx.Graph<VD, ED>, int, double, scala.Option<java.lang.Object>, scala.reflect.ClassTag<VD>, scala.reflect.ClassTag<ED>) staticPageRank public Graph<Object,Object> staticPageRank(int numIter, double resetProb) Run PageRank for a fixed number of iterations returning a graph with vertex attributes containing the PageRank and edge attributes the normalized edge weight. Parameters:numIter - (undocumented)resetProb - (undocumented) Returns:(undocumented)See Also:PageRank$.run(org.apache.spark.graphx.Graph<VD, ED>, int, double, scala.reflect.ClassTag<VD>, scala.reflect.ClassTag<ED>) connectedComponents public Graph<Object,ED> connectedComponents() Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. Returns:(undocumented)See Also:ConnectedComponents$.run(org.apache.spark.graphx.Graph<VD, ED>, int, scala.reflect.ClassTag<VD>, scala.reflect.ClassTag<ED>) connectedComponents public Graph<Object,ED> connectedComponents(int maxIterations) Compute the connected component membership of each vertex and return a graph with the vertex value containing the lowest vertex id in the connected component containing that vertex. Parameters:maxIterations - (undocumented) Returns:(undocumented)See Also:ConnectedComponents$.run(org.apache.spark.graphx.Graph<VD, ED>, int, scala.reflect.ClassTag<VD>, scala.reflect.ClassTag<ED>) triangleCount public Graph<Object,ED> triangleCount() Compute the number of triangles passing through each vertex. Returns:(undocumented)See Also:TriangleCount$.run(org.apache.spark.graphx.Graph<VD, ED>, scala.reflect.ClassTag<VD>, scala.reflect.ClassTag<ED>) stronglyConnectedComponents public Graph<Object,ED> stronglyConnectedComponents(int numIter) Compute the strongly connected component (SCC) of each vertex and return a graph with the vertex value containing the lowest vertex id in the SCC containing that vertex. Parameters:numIter - (undocumented) Returns:(undocumented)See Also:StronglyConnectedComponents$.run(org.apache.spark.graphx.Graph<VD, ED>, int, scala.reflect.ClassTag<VD>, scala.reflect.ClassTag<ED>) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GraphXUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GraphXUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class GraphXUtils Object org.apache.spark.graphx.GraphXUtils public class GraphXUtils extends Object Constructor Summary Constructors  Constructor and Description GraphXUtils()  Method Summary Methods  Modifier and Type Method and Description static void registerKryoClasses(SparkConf conf) Registers classes that GraphX uses with Kryo. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail GraphXUtils public GraphXUtils() Method Detail registerKryoClasses public static void registerKryoClasses(SparkConf conf) Registers classes that GraphX uses with Kryo. Parameters:conf - (undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GreaterThan (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GreaterThan (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class GreaterThan Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.GreaterThan All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class GreaterThan extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff the attribute evaluates to a value greater than value. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GreaterThan(String attribute, Object value)  Method Summary Methods  Modifier and Type Method and Description String attribute()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Object value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail GreaterThan public GreaterThan(String attribute, Object value) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() attribute public String attribute() value public Object value() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GreaterThanOrEqual (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GreaterThanOrEqual (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class GreaterThanOrEqual Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.GreaterThanOrEqual All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class GreaterThanOrEqual extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff the attribute evaluates to a value greater than or equal to value. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description GreaterThanOrEqual(String attribute, Object value)  Method Summary Methods  Modifier and Type Method and Description String attribute()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Object value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail GreaterThanOrEqual public GreaterThanOrEqual(String attribute, Object value) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() attribute public String attribute() value public Object value() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method GroupMappingServiceProvider (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="GroupMappingServiceProvider (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.security Interface GroupMappingServiceProvider public interface GroupMappingServiceProvider This Spark trait is used for mapping a given userName to a set of groups which it belongs to. This is useful for specifying a common group of admins/developers to provide them admin, modify and/or view access rights. Based on whether access control checks are enabled using spark.acls.enable, every time a user tries to access or modify the application, the SecurityManager gets the corresponding groups a user belongs to from the instance of the groups mapping provider specified by the entry spark.user.groups.mapping. Method Summary Methods  Modifier and Type Method and Description scala.collection.immutable.Set<String> getGroups(String userName) Get the groups the user belongs to. Method Detail getGroups scala.collection.immutable.Set<String> getGroups(String userName) Get the groups the user belongs to. Parameters:userName - User's Name Returns:set of groups that the user belongs to. Empty in case of an invalid user. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HadoopRDD.HadoopMapPartitionsWithSplitRDD$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HadoopRDD.HadoopMapPartitionsWithSplitRDD$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class HadoopRDD.HadoopMapPartitionsWithSplitRDD$ Object org.apache.spark.rdd.HadoopRDD.HadoopMapPartitionsWithSplitRDD$ All Implemented Interfaces: java.io.Serializable Enclosing class: HadoopRDD<K,V> public static class HadoopRDD.HadoopMapPartitionsWithSplitRDD$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static HadoopRDD.HadoopMapPartitionsWithSplitRDD$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description HadoopRDD.HadoopMapPartitionsWithSplitRDD$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final HadoopRDD.HadoopMapPartitionsWithSplitRDD$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail HadoopRDD.HadoopMapPartitionsWithSplitRDD$ public HadoopRDD.HadoopMapPartitionsWithSplitRDD$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HadoopRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HadoopRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class HadoopRDD<K,V> Object org.apache.spark.rdd.RDD<scala.Tuple2<K,V>> org.apache.spark.rdd.HadoopRDD<K,V> All Implemented Interfaces: java.io.Serializable public class HadoopRDD<K,V> extends RDD<scala.Tuple2<K,V>> :: DeveloperApi :: An RDD that provides core functionality for reading data stored in Hadoop (e.g., files in HDFS, sources in HBase, or S3), using the older MapReduce API (org.apache.hadoop.mapred). Note: Instantiating this class directly is not recommended, please use org.apache.spark.SparkContext.hadoopRDD() param: sc The SparkContext to associate the RDD with. param: broadcastedConf A general Hadoop Configuration, or a subclass of it. If the enclosed variable references an instance of JobConf, then that JobConf will be used for the Hadoop job. Otherwise, a new JobConf will be created on each slave using the enclosed Configuration. param: initLocalJobConfFuncOpt Optional closure used to initialize any JobConf that HadoopRDD creates. param: inputFormatClass Storage format of the data to be read. param: keyClass Class of the key associated with the inputFormatClass. param: valueClass Class of the value associated with the inputFormatClass. param: minPartitions Minimum number of HadoopRDD partitions (Hadoop Splits) to generate. See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  HadoopRDD.HadoopMapPartitionsWithSplitRDD$  Constructor Summary Constructors  Constructor and Description HadoopRDD(SparkContext sc, Broadcast<org.apache.spark.util.SerializableConfiguration> broadcastedConf, scala.Option<scala.Function1<org.apache.hadoop.mapred.JobConf,scala.runtime.BoxedUnit>> initLocalJobConfFuncOpt, Class<? extends org.apache.hadoop.mapred.InputFormat<K,V>> inputFormatClass, Class<K> keyClass, Class<V> valueClass, int minPartitions)  HadoopRDD(SparkContext sc, org.apache.hadoop.mapred.JobConf conf, Class<? extends org.apache.hadoop.mapred.InputFormat<K,V>> inputFormatClass, Class<K> keyClass, Class<V> valueClass, int minPartitions)  Method Summary Methods  Modifier and Type Method and Description static RDD<T> $plus$plus(RDD<T> other)  static void addLocalConfiguration(String jobTrackerId, int jobId, int splitId, int attemptId, org.apache.hadoop.mapred.JobConf conf) Add Hadoop configuration specific to a single partition and attempt. static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29)  static RDD<T> cache()  static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5)  void checkpoint() Mark this RDD for checkpointing. static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord)  static boolean coalesce$default$2()  static scala.Option<PartitionCoalescer> coalesce$default$3()  static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer)  static Object collect()  static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28)  InterruptibleIterator<scala.Tuple2<K,V>> compute(Partition theSplit, TaskContext context) :: DeveloperApi :: Implemented by subclasses to compute a given partition. static Object CONFIGURATION_INSTANTIATION_LOCK() Configuration's constructor is not threadsafe (see SPARK-1097 and HADOOP-10456). static boolean containsCachedMetadata(String key)  static SparkContext context()  static long count()  static PartialResult<BoundedDouble> countApprox(long timeout, double confidence)  static double countApprox$default$2()  static long countApproxDistinct(double relativeSD)  static long countApproxDistinct(int p, int sp)  static double countApproxDistinct$default$1()  static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord)  static scala.math.Ordering<T> countByValue$default$1()  static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord)  static double countByValueApprox$default$2()  static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence)  static scala.collection.Seq<Dependency<?>> dependencies()  static RDD<T> distinct()  static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> distinct$default$2(int numPartitions)  static RDD<T> filter(scala.Function1<T,Object> f)  static T first()  static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4)  static T fold(T zeroValue, scala.Function2<T,T,T> op)  static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f)  static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f)  static Object getCachedMetadata(String key) The three methods below are helpers for accessing the local map, a property of the SparkEnv of the local process. static scala.Option<String> getCheckpointFile()  org.apache.hadoop.conf.Configuration getConf()  static int getNumPartitions()  Partition[] getPartitions() Implemented by subclasses to return the set of partitions in this RDD. scala.collection.Seq<String> getPreferredLocations(Partition split) Optionally overridden by subclasses to specify placement preferences. static StorageLevel getStorageLevel()  static RDD<Object> glom()  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord)  static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p)  static int id()  static RDD<T> intersection(RDD<T> other)  static RDD<T> intersection(RDD<T> other, int numPartitions)  static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner)  static boolean isCheckpointed()  static boolean isEmpty()  static scala.collection.Iterator<T> iterator(Partition split, TaskContext context)  static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f)  static RDD<T> localCheckpoint()  static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3)  static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6)  static <U> boolean mapPartitions$default$2()  static <U> boolean mapPartitionsInternal$default$2()  static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8)  static <U> boolean mapPartitionsWithIndex$default$2()  <U> RDD<U> mapPartitionsWithInputSplit(scala.Function2<org.apache.hadoop.mapred.InputSplit,scala.collection.Iterator<scala.Tuple2<K,V>>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$1) Maps over a partition, providing the InputSplit that was used as the base of the partition. static T max(scala.math.Ordering<T> ord)  static T min(scala.math.Ordering<T> ord)  static void name_$eq(String x$1)  static String name()  static scala.Option<Partitioner> partitioner()  static Partition[] partitions()  HadoopRDD<K,V> persist(StorageLevel storageLevel) Set this RDD's storage level to persist its values across operations after the first time it is computed. static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding)  static RDD<String> pipe(String command)  static RDD<String> pipe(String command, scala.collection.Map<String,String> env)  static scala.collection.Map<String,String> pipe$default$2()  static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3()  static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4()  static boolean pipe$default$5()  static int pipe$default$6()  static String pipe$default$7()  static scala.collection.Seq<String> preferredLocations(Partition split)  static RDD<T>[] randomSplit(double[] weights, long seed)  static long randomSplit$default$2()  static int RECORDS_BETWEEN_BYTES_READ_METRIC_UPDATES() Update the input bytes read metric each time this number of records has been read static T reduce(scala.Function2<T,T,T> f)  static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> repartition$default$2(int numPartitions)  static RDD<T> sample(boolean withReplacement, double fraction, long seed)  static long sample$default$3()  static void saveAsObjectFile(String path)  static void saveAsTextFile(String path)  static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec)  static RDD<T> setName(String _name)  static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag)  static <K> boolean sortBy$default$2()  static <K> int sortBy$default$3()  static SparkContext sparkContext()  static RDD<T> subtract(RDD<T> other)  static RDD<T> subtract(RDD<T> other, int numPartitions)  static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p)  static Object take(int num)  static Object takeOrdered(int num, scala.math.Ordering<T> ord)  static Object takeSample(boolean withReplacement, int num, long seed)  static long takeSample$default$3()  static String toDebugString()  static JavaRDD<T> toJavaRDD()  static scala.collection.Iterator<T> toLocalIterator()  static Object top(int num, scala.math.Ordering<T> ord)  static String toString()  static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30)  static <U> int treeAggregate$default$4(U zeroValue)  static T treeReduce(scala.Function2<T,T,T> f, int depth)  static int treeReduce$default$2()  static RDD<T> union(RDD<T> other)  static RDD<T> unpersist(boolean blocking)  static boolean unpersist$default$1()  static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27)  static RDD<scala.Tuple2<T,Object>> zipWithIndex()  static RDD<scala.Tuple2<T,Object>> zipWithUniqueId()  Methods inherited from class org.apache.spark.rdd.RDD aggregate, cache, cartesian, coalesce, collect, collect, context, count, countApprox, countApproxDistinct, countApproxDistinct, countByValue, countByValueApprox, dependencies, distinct, distinct, doubleRDDToDoubleRDDFunctions, filter, first, flatMap, fold, foreach, foreachPartition, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupBy, id, intersection, intersection, intersection, isCheckpointed, isEmpty, iterator, keyBy, localCheckpoint, map, mapPartitions, mapPartitionsWithIndex, max, min, name, numericRDDToDoubleRDDFunctions, partitioner, partitions, persist, pipe, pipe, pipe, preferredLocations, randomSplit, rddToAsyncRDDActions, rddToOrderedRDDFunctions, rddToPairRDDFunctions, rddToSequenceFileRDDFunctions, reduce, repartition, sample, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortBy, sparkContext, subtract, subtract, subtract, take, takeOrdered, takeSample, toDebugString, toJavaRDD, toLocalIterator, top, toString, treeAggregate, treeReduce, union, unpersist, zip, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail HadoopRDD public HadoopRDD(SparkContext sc, Broadcast<org.apache.spark.util.SerializableConfiguration> broadcastedConf, scala.Option<scala.Function1<org.apache.hadoop.mapred.JobConf,scala.runtime.BoxedUnit>> initLocalJobConfFuncOpt, Class<? extends org.apache.hadoop.mapred.InputFormat<K,V>> inputFormatClass, Class<K> keyClass, Class<V> valueClass, int minPartitions) HadoopRDD public HadoopRDD(SparkContext sc, org.apache.hadoop.mapred.JobConf conf, Class<? extends org.apache.hadoop.mapred.InputFormat<K,V>> inputFormatClass, Class<K> keyClass, Class<V> valueClass, int minPartitions) Method Detail CONFIGURATION_INSTANTIATION_LOCK public static Object CONFIGURATION_INSTANTIATION_LOCK() Configuration's constructor is not threadsafe (see SPARK-1097 and HADOOP-10456). Therefore, we synchronize on this lock before calling new JobConf() or new Configuration(). Returns:(undocumented) RECORDS_BETWEEN_BYTES_READ_METRIC_UPDATES public static int RECORDS_BETWEEN_BYTES_READ_METRIC_UPDATES() Update the input bytes read metric each time this number of records has been read getCachedMetadata public static Object getCachedMetadata(String key) The three methods below are helpers for accessing the local map, a property of the SparkEnv of the local process. Parameters:key - (undocumented) Returns:(undocumented) containsCachedMetadata public static boolean containsCachedMetadata(String key) addLocalConfiguration public static void addLocalConfiguration(String jobTrackerId, int jobId, int splitId, int attemptId, org.apache.hadoop.mapred.JobConf conf) Add Hadoop configuration specific to a single partition and attempt. partitioner public static scala.Option<Partitioner> partitioner() sparkContext public static SparkContext sparkContext() id public static int id() name public static String name() name_$eq public static void name_$eq(String x$1) setName public static RDD<T> setName(String _name) cache public static RDD<T> cache() unpersist public static RDD<T> unpersist(boolean blocking) getStorageLevel public static StorageLevel getStorageLevel() dependencies public static final scala.collection.Seq<Dependency<?>> dependencies() partitions public static final Partition[] partitions() getNumPartitions public static final int getNumPartitions() preferredLocations public static final scala.collection.Seq<String> preferredLocations(Partition split) iterator public static final scala.collection.Iterator<T> iterator(Partition split, TaskContext context) map public static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3) flatMap public static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4) filter public static RDD<T> filter(scala.Function1<T,Object> f) distinct public static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord) distinct public static RDD<T> distinct() repartition public static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord) coalesce public static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord) sample public static RDD<T> sample(boolean withReplacement, double fraction, long seed) randomSplit public static RDD<T>[] randomSplit(double[] weights, long seed) takeSample public static Object takeSample(boolean withReplacement, int num, long seed) union public static RDD<T> union(RDD<T> other) $plus$plus public static RDD<T> $plus$plus(RDD<T> other) sortBy public static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag) intersection public static RDD<T> intersection(RDD<T> other) intersection public static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord) intersection public static RDD<T> intersection(RDD<T> other, int numPartitions) glom public static RDD<Object> glom() cartesian public static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord) pipe public static RDD<String> pipe(String command) pipe public static RDD<String> pipe(String command, scala.collection.Map<String,String> env) pipe public static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding) mapPartitions public static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6) mapPartitionsWithIndex public static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8) zip public static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27) foreach public static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f) foreachPartition public static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) collect public static Object collect() toLocalIterator public static scala.collection.Iterator<T> toLocalIterator() collect public static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28) subtract public static RDD<T> subtract(RDD<T> other) subtract public static RDD<T> subtract(RDD<T> other, int numPartitions) subtract public static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord) reduce public static T reduce(scala.Function2<T,T,T> f) treeReduce public static T treeReduce(scala.Function2<T,T,T> f, int depth) fold public static T fold(T zeroValue, scala.Function2<T,T,T> op) aggregate public static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29) treeAggregate public static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30) count public static long count() countApprox public static PartialResult<BoundedDouble> countApprox(long timeout, double confidence) countByValue public static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord) countByValueApprox public static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord) countApproxDistinct public static long countApproxDistinct(int p, int sp) countApproxDistinct public static long countApproxDistinct(double relativeSD) zipWithIndex public static RDD<scala.Tuple2<T,Object>> zipWithIndex() zipWithUniqueId public static RDD<scala.Tuple2<T,Object>> zipWithUniqueId() take public static Object take(int num) first public static T first() top public static Object top(int num, scala.math.Ordering<T> ord) takeOrdered public static Object takeOrdered(int num, scala.math.Ordering<T> ord) max public static T max(scala.math.Ordering<T> ord) min public static T min(scala.math.Ordering<T> ord) isEmpty public static boolean isEmpty() saveAsTextFile public static void saveAsTextFile(String path) saveAsTextFile public static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) saveAsObjectFile public static void saveAsObjectFile(String path) keyBy public static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f) localCheckpoint public static RDD<T> localCheckpoint() isCheckpointed public static boolean isCheckpointed() getCheckpointFile public static scala.Option<String> getCheckpointFile() context public static SparkContext context() toDebugString public static String toDebugString() toString public static String toString() toJavaRDD public static JavaRDD<T> toJavaRDD() sample$default$3 public static long sample$default$3() mapPartitionsWithIndex$default$2 public static <U> boolean mapPartitionsWithIndex$default$2() unpersist$default$1 public static boolean unpersist$default$1() distinct$default$2 public static scala.math.Ordering<T> distinct$default$2(int numPartitions) coalesce$default$2 public static boolean coalesce$default$2() coalesce$default$3 public static scala.Option<PartitionCoalescer> coalesce$default$3() coalesce$default$4 public static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer) repartition$default$2 public static scala.math.Ordering<T> repartition$default$2(int numPartitions) subtract$default$3 public static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p) intersection$default$3 public static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner) randomSplit$default$2 public static long randomSplit$default$2() sortBy$default$2 public static <K> boolean sortBy$default$2() sortBy$default$3 public static <K> int sortBy$default$3() mapPartitions$default$2 public static <U> boolean mapPartitions$default$2() groupBy$default$4 public static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p) pipe$default$2 public static scala.collection.Map<String,String> pipe$default$2() pipe$default$3 public static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3() pipe$default$4 public static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4() pipe$default$5 public static boolean pipe$default$5() pipe$default$6 public static int pipe$default$6() pipe$default$7 public static String pipe$default$7() treeReduce$default$2 public static int treeReduce$default$2() treeAggregate$default$4 public static <U> int treeAggregate$default$4(U zeroValue) countApprox$default$2 public static double countApprox$default$2() countByValue$default$1 public static scala.math.Ordering<T> countByValue$default$1() countByValueApprox$default$2 public static double countByValueApprox$default$2() countByValueApprox$default$3 public static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence) takeSample$default$3 public static long takeSample$default$3() countApproxDistinct$default$1 public static double countApproxDistinct$default$1() mapPartitionsInternal$default$2 public static <U> boolean mapPartitionsInternal$default$2() getPartitions public Partition[] getPartitions() Description copied from class: RDD Implemented by subclasses to return the set of partitions in this RDD. This method will only be called once, so it is safe to implement a time-consuming computation in it. The partitions in this array must satisfy the following property: rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index } Returns:(undocumented) compute public InterruptibleIterator<scala.Tuple2<K,V>> compute(Partition theSplit, TaskContext context) Description copied from class: RDD :: DeveloperApi :: Implemented by subclasses to compute a given partition. Specified by: compute in class RDD<scala.Tuple2<K,V>> Parameters:theSplit - (undocumented)context - (undocumented) Returns:(undocumented) mapPartitionsWithInputSplit public <U> RDD<U> mapPartitionsWithInputSplit(scala.Function2<org.apache.hadoop.mapred.InputSplit,scala.collection.Iterator<scala.Tuple2<K,V>>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$1) Maps over a partition, providing the InputSplit that was used as the base of the partition. getPreferredLocations public scala.collection.Seq<String> getPreferredLocations(Partition split) Description copied from class: RDD Optionally overridden by subclasses to specify placement preferences. Parameters:split - (undocumented) Returns:(undocumented) checkpoint public void checkpoint() Description copied from class: RDD Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint directory set with SparkContext#setCheckpointDir and all references to its parent RDDs will be removed. This function must be called before any job has been executed on this RDD. It is strongly recommended that this RDD is persisted in memory, otherwise saving it on a file will require recomputation. Overrides: checkpoint in class RDD<scala.Tuple2<K,V>> persist public HadoopRDD<K,V> persist(StorageLevel storageLevel) Description copied from class: RDD Set this RDD's storage level to persist its values across operations after the first time it is computed. This can only be used to assign a new storage level if the RDD does not have a storage level set yet. Local checkpointing is an exception. Overrides: persist in class RDD<scala.Tuple2<K,V>> Parameters:storageLevel - (undocumented) Returns:(undocumented) getConf public org.apache.hadoop.conf.Configuration getConf() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HasOffsetRanges (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HasOffsetRanges (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kafka Interface HasOffsetRanges public interface HasOffsetRanges Represents any object that has a collection of OffsetRanges. This can be used to access the offset ranges in RDDs generated by the direct Kafka DStream (see KafkaUtils.createDirectStream()). KafkaUtils.createDirectStream(...).foreachRDD { rdd => val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges ... } Method Summary Methods  Modifier and Type Method and Description OffsetRange[] offsetRanges()  Method Detail offsetRanges OffsetRange[] offsetRanges() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HashPartitioner (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HashPartitioner (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class HashPartitioner Object org.apache.spark.Partitioner org.apache.spark.HashPartitioner All Implemented Interfaces: java.io.Serializable public class HashPartitioner extends Partitioner A Partitioner that implements hash-based partitioning using Java's Object.hashCode. Java arrays have hashCodes that are based on the arrays' identities rather than their contents, so attempting to partition an RDD[Array[_} or RDD[(Array[_], _)] using a HashPartitioner will produce an unexpected or incorrect result. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description HashPartitioner(int partitions)  Method Summary Methods  Modifier and Type Method and Description boolean equals(Object other)  int getPartition(Object key)  int hashCode()  int numPartitions()  Methods inherited from class org.apache.spark.Partitioner defaultPartitioner Methods inherited from class Object getClass, notify, notifyAll, toString, wait, wait, wait Constructor Detail HashPartitioner public HashPartitioner(int partitions) Method Detail numPartitions public int numPartitions() Specified by: numPartitions in class Partitioner getPartition public int getPartition(Object key) Specified by: getPartition in class Partitioner equals public boolean equals(Object other) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HashingTF (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HashingTF (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class HashingTF Object org.apache.spark.mllib.feature.HashingTF All Implemented Interfaces: java.io.Serializable public class HashingTF extends Object implements scala.Serializable Maps a sequence of terms to their term frequencies using the hashing trick. param: numFeatures number of features (default: 2^20^) See Also:Serialized Form Constructor Summary Constructors  Constructor and Description HashingTF()  HashingTF(int numFeatures)  Method Summary Methods  Modifier and Type Method and Description int indexOf(Object term) Returns the index of the input term. int numFeatures()  HashingTF setBinary(boolean value) If true, term frequency vector will be binary such that non-zero term counts will be set to 1 (default: false) HashingTF setHashAlgorithm(String value) Set the hash algorithm used when mapping term to integer. Vector transform(scala.collection.Iterable<?> document) Transforms the input document into a sparse term frequency vector. Vector transform(Iterable<?> document) Transforms the input document into a sparse term frequency vector (Java version). <D extends Iterable<?>> JavaRDD<Vector> transform(JavaRDD<D> dataset) Transforms the input document to term frequency vectors (Java version). <D extends scala.collection.Iterable<?>> RDD<Vector> transform(RDD<D> dataset) Transforms the input document to term frequency vectors. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail HashingTF public HashingTF(int numFeatures) HashingTF public HashingTF() Method Detail numFeatures public int numFeatures() setBinary public HashingTF setBinary(boolean value) If true, term frequency vector will be binary such that non-zero term counts will be set to 1 (default: false) Parameters:value - (undocumented) Returns:(undocumented) setHashAlgorithm public HashingTF setHashAlgorithm(String value) Set the hash algorithm used when mapping term to integer. (default: murmur3) Parameters:value - (undocumented) Returns:(undocumented) indexOf public int indexOf(Object term) Returns the index of the input term. Parameters:term - (undocumented) Returns:(undocumented) transform public Vector transform(scala.collection.Iterable<?> document) Transforms the input document into a sparse term frequency vector. Parameters:document - (undocumented) Returns:(undocumented) transform public Vector transform(Iterable<?> document) Transforms the input document into a sparse term frequency vector (Java version). Parameters:document - (undocumented) Returns:(undocumented) transform public <D extends scala.collection.Iterable<?>> RDD<Vector> transform(RDD<D> dataset) Transforms the input document to term frequency vectors. Parameters:dataset - (undocumented) Returns:(undocumented) transform public <D extends Iterable<?>> JavaRDD<Vector> transform(JavaRDD<D> dataset) Transforms the input document to term frequency vectors (Java version). Parameters:dataset - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HdfsUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HdfsUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.util Class HdfsUtils Object org.apache.spark.streaming.util.HdfsUtils public class HdfsUtils extends Object Constructor Summary Constructors  Constructor and Description HdfsUtils()  Method Summary Methods  Modifier and Type Method and Description static boolean checkFileExists(String path, org.apache.hadoop.conf.Configuration conf) Check if the file exists at the given path. static void checkState(boolean state, scala.Function0<String> errorMsg)  static String[] getFileSegmentLocations(String path, long offset, long length, org.apache.hadoop.conf.Configuration conf) Get the locations of the HDFS blocks containing the given file segment. static org.apache.hadoop.fs.FileSystem getFileSystemForPath(org.apache.hadoop.fs.Path path, org.apache.hadoop.conf.Configuration conf)  static org.apache.hadoop.fs.FSDataInputStream getInputStream(String path, org.apache.hadoop.conf.Configuration conf)  static org.apache.hadoop.fs.FSDataOutputStream getOutputStream(String path, org.apache.hadoop.conf.Configuration conf)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail HdfsUtils public HdfsUtils() Method Detail getOutputStream public static org.apache.hadoop.fs.FSDataOutputStream getOutputStream(String path, org.apache.hadoop.conf.Configuration conf) getInputStream public static org.apache.hadoop.fs.FSDataInputStream getInputStream(String path, org.apache.hadoop.conf.Configuration conf) checkState public static void checkState(boolean state, scala.Function0<String> errorMsg) getFileSegmentLocations public static String[] getFileSegmentLocations(String path, long offset, long length, org.apache.hadoop.conf.Configuration conf) Get the locations of the HDFS blocks containing the given file segment. getFileSystemForPath public static org.apache.hadoop.fs.FileSystem getFileSystemForPath(org.apache.hadoop.fs.Path path, org.apache.hadoop.conf.Configuration conf) checkFileExists public static boolean checkFileExists(String path, org.apache.hadoop.conf.Configuration conf) Check if the file exists at the given path. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HingeGradient (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HingeGradient (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Class HingeGradient Object org.apache.spark.mllib.optimization.Gradient org.apache.spark.mllib.optimization.HingeGradient All Implemented Interfaces: java.io.Serializable public class HingeGradient extends Gradient :: DeveloperApi :: Compute gradient and loss for a Hinge loss function, as used in SVM binary classification. See also the documentation for the precise formulation. NOTE: This assumes that the labels are {0,1} See Also:Serialized Form Constructor Summary Constructors  Constructor and Description HingeGradient()  Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<Vector,Object> compute(Vector data, double label, Vector weights) Compute the gradient and loss given the features of a single data point. double compute(Vector data, double label, Vector weights, Vector cumGradient) Compute the gradient and loss given the features of a single data point, add the gradient to a provided vector to avoid creating new objects, and return loss. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail HingeGradient public HingeGradient() Method Detail compute public scala.Tuple2<Vector,Object> compute(Vector data, double label, Vector weights) Description copied from class: Gradient Compute the gradient and loss given the features of a single data point. Overrides: compute in class Gradient Parameters:data - features for one data pointlabel - label for this data pointweights - weights/coefficients corresponding to features Returns:(gradient: Vector, loss: Double) compute public double compute(Vector data, double label, Vector weights, Vector cumGradient) Description copied from class: Gradient Compute the gradient and loss given the features of a single data point, add the gradient to a provided vector to avoid creating new objects, and return loss. Specified by: compute in class Gradient Parameters:data - features for one data pointlabel - label for this data pointweights - weights/coefficients corresponding to featurescumGradient - the computed gradient will be added to this vector Returns:loss Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HiveContext (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HiveContext (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive Class HiveContext Object org.apache.spark.sql.SQLContext org.apache.spark.sql.hive.HiveContext All Implemented Interfaces: java.io.Serializable Deprecated.  Use SparkSession.builder.enableHiveSupport instead. Since 2.0.0. public class HiveContext extends SQLContext An instance of the Spark SQL execution engine that integrates with data stored in Hive. Configuration for Hive is read from hive-site.xml on the classpath. See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from class org.apache.spark.sql.SQLContext SQLContext.implicits$ Constructor Summary Constructors  Constructor and Description HiveContext(JavaSparkContext sc) Deprecated.    HiveContext(SparkContext sc) Deprecated.    Method Summary Methods  Modifier and Type Method and Description HiveContext newSession() Deprecated.  Returns a new HiveContext as new session, which will have separated SQLConf, UDF/UDAF, temporary tables and SessionState, but sharing the same CacheManager, IsolatedClientLoader and Hive client (both of execution and metadata) with existing HiveContext. void refreshTable(String tableName) Deprecated.  Invalidate and refresh all the cached the metadata of the given table. Methods inherited from class org.apache.spark.sql.SQLContext applySchema, applySchema, applySchema, applySchema, baseRelationToDataFrame, cacheTable, clearActive, clearCache, createDataFrame, createDataFrame, createDataFrame, createDataFrame, createDataFrame, createDataFrame, createDataFrame, createDataFrame, createDataset, createDataset, createDataset, createExternalTable, createExternalTable, createExternalTable, createExternalTable, createExternalTable, createExternalTable, dropTempTable, emptyDataFrame, experimental, getAllConfs, getConf, getConf, getOrCreate, implicits, isCached, jdbc, jdbc, jdbc, jsonFile, jsonFile, jsonFile, jsonRDD, jsonRDD, jsonRDD, jsonRDD, jsonRDD, jsonRDD, listenerManager, load, load, load, load, load, load, parquetFile, parquetFile, range, range, range, range, read, readStream, setActive, setConf, setConf, sparkContext, sparkSession, sql, streams, table, tableNames, tableNames, tables, tables, udf, uncacheTable Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail HiveContext public HiveContext(SparkContext sc) Deprecated.  HiveContext public HiveContext(JavaSparkContext sc) Deprecated.  Method Detail newSession public HiveContext newSession() Deprecated.  Returns a new HiveContext as new session, which will have separated SQLConf, UDF/UDAF, temporary tables and SessionState, but sharing the same CacheManager, IsolatedClientLoader and Hive client (both of execution and metadata) with existing HiveContext. Overrides: newSession in class SQLContext Returns:(undocumented) refreshTable public void refreshTable(String tableName) Deprecated.  Invalidate and refresh all the cached the metadata of the given table. For performance reasons, Spark SQL or the external data source library it uses might cache certain metadata about a table, such as the location of blocks. When those change outside of Spark SQL, users should call this function to invalidate the cache. Parameters:tableName - (undocumented)Since: 1.3.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HiveScriptIOSchema (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HiveScriptIOSchema (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive.execution Class HiveScriptIOSchema Object org.apache.spark.sql.hive.execution.HiveScriptIOSchema All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class HiveScriptIOSchema extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description HiveScriptIOSchema(scala.collection.Seq<scala.Tuple2<String,String>> inputRowFormat, scala.collection.Seq<scala.Tuple2<String,String>> outputRowFormat, scala.Option<String> inputSerdeClass, scala.Option<String> outputSerdeClass, scala.collection.Seq<scala.Tuple2<String,String>> inputSerdeProps, scala.collection.Seq<scala.Tuple2<String,String>> outputSerdeProps, scala.Option<String> recordReaderClass, scala.Option<String> recordWriterClass, boolean schemaLess)  Method Summary Methods  Modifier and Type Method and Description static HiveScriptIOSchema apply(org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema input)  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  scala.Option<scala.Tuple2<org.apache.hadoop.hive.serde2.AbstractSerDe,org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector>> initInputSerDe(scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> input)  scala.Option<scala.Tuple2<org.apache.hadoop.hive.serde2.AbstractSerDe,org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector>> initOutputSerDe(scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> output)  scala.collection.Seq<scala.Tuple2<String,String>> inputRowFormat()  scala.collection.immutable.Map<String,String> inputRowFormatMap()  scala.Option<String> inputSerdeClass()  scala.collection.Seq<scala.Tuple2<String,String>> inputSerdeProps()  static DataType inspectorToDataType(org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector inspector)  static DataType javaClassToDataType(Class<?> clz)  scala.collection.Seq<scala.Tuple2<String,String>> outputRowFormat()  scala.collection.immutable.Map<String,String> outputRowFormatMap()  scala.Option<String> outputSerdeClass()  scala.collection.Seq<scala.Tuple2<String,String>> outputSerdeProps()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  scala.Option<org.apache.hadoop.hive.ql.exec.RecordReader> recordReader(java.io.InputStream inputStream, org.apache.hadoop.conf.Configuration conf)  scala.Option<String> recordReaderClass()  scala.Option<org.apache.hadoop.hive.ql.exec.RecordWriter> recordWriter(java.io.OutputStream outputStream, org.apache.hadoop.conf.Configuration conf)  scala.Option<String> recordWriterClass()  boolean schemaLess()  static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector toInspector(DataType dataType)  static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector toInspector(org.apache.spark.sql.catalyst.expressions.Expression expr)  static org.apache.spark.sql.hive.HiveInspectors.typeInfoConversions typeInfoConversions(DataType dt)  static Object unwrap(Object data, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector oi)  static scala.Function3<Object,org.apache.spark.sql.catalyst.expressions.MutableRow,Object,scala.runtime.BoxedUnit> unwrapperFor(org.apache.hadoop.hive.serde2.objectinspector.StructField field)  static Object[] wrap(org.apache.spark.sql.catalyst.InternalRow row, scala.collection.Seq<org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector> inspectors, Object[] cache, DataType[] dataTypes)  static Object wrap(Object a, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector oi, DataType dataType)  static Object[] wrap(scala.collection.Seq<Object> row, scala.collection.Seq<org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector> inspectors, Object[] cache, DataType[] dataTypes)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail HiveScriptIOSchema public HiveScriptIOSchema(scala.collection.Seq<scala.Tuple2<String,String>> inputRowFormat, scala.collection.Seq<scala.Tuple2<String,String>> outputRowFormat, scala.Option<String> inputSerdeClass, scala.Option<String> outputSerdeClass, scala.collection.Seq<scala.Tuple2<String,String>> inputSerdeProps, scala.collection.Seq<scala.Tuple2<String,String>> outputSerdeProps, scala.Option<String> recordReaderClass, scala.Option<String> recordWriterClass, boolean schemaLess) Method Detail apply public static HiveScriptIOSchema apply(org.apache.spark.sql.catalyst.plans.logical.ScriptInputOutputSchema input) javaClassToDataType public static DataType javaClassToDataType(Class<?> clz) unwrap public static Object unwrap(Object data, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector oi) unwrapperFor public static scala.Function3<Object,org.apache.spark.sql.catalyst.expressions.MutableRow,Object,scala.runtime.BoxedUnit> unwrapperFor(org.apache.hadoop.hive.serde2.objectinspector.StructField field) wrap public static Object wrap(Object a, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector oi, DataType dataType) wrap public static Object[] wrap(org.apache.spark.sql.catalyst.InternalRow row, scala.collection.Seq<org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector> inspectors, Object[] cache, DataType[] dataTypes) wrap public static Object[] wrap(scala.collection.Seq<Object> row, scala.collection.Seq<org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector> inspectors, Object[] cache, DataType[] dataTypes) toInspector public static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector toInspector(DataType dataType) toInspector public static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector toInspector(org.apache.spark.sql.catalyst.expressions.Expression expr) inspectorToDataType public static DataType inspectorToDataType(org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector inspector) typeInfoConversions public static org.apache.spark.sql.hive.HiveInspectors.typeInfoConversions typeInfoConversions(DataType dt) canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() inputRowFormat public scala.collection.Seq<scala.Tuple2<String,String>> inputRowFormat() outputRowFormat public scala.collection.Seq<scala.Tuple2<String,String>> outputRowFormat() inputSerdeClass public scala.Option<String> inputSerdeClass() outputSerdeClass public scala.Option<String> outputSerdeClass() inputSerdeProps public scala.collection.Seq<scala.Tuple2<String,String>> inputSerdeProps() outputSerdeProps public scala.collection.Seq<scala.Tuple2<String,String>> outputSerdeProps() recordReaderClass public scala.Option<String> recordReaderClass() recordWriterClass public scala.Option<String> recordWriterClass() schemaLess public boolean schemaLess() inputRowFormatMap public scala.collection.immutable.Map<String,String> inputRowFormatMap() outputRowFormatMap public scala.collection.immutable.Map<String,String> outputRowFormatMap() initInputSerDe public scala.Option<scala.Tuple2<org.apache.hadoop.hive.serde2.AbstractSerDe,org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector>> initInputSerDe(scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> input) initOutputSerDe public scala.Option<scala.Tuple2<org.apache.hadoop.hive.serde2.AbstractSerDe,org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector>> initOutputSerDe(scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> output) recordReader public scala.Option<org.apache.hadoop.hive.ql.exec.RecordReader> recordReader(java.io.InputStream inputStream, org.apache.hadoop.conf.Configuration conf) recordWriter public scala.Option<org.apache.hadoop.hive.ql.exec.RecordWriter> recordWriter(java.io.OutputStream outputStream, org.apache.hadoop.conf.Configuration conf) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HiveSerDe (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HiveSerDe (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.internal Class HiveSerDe Object org.apache.spark.sql.internal.HiveSerDe All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class HiveSerDe extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description HiveSerDe(scala.Option<String> inputFormat, scala.Option<String> outputFormat, scala.Option<String> serde)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  scala.Option<String> inputFormat()  scala.Option<String> outputFormat()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  scala.Option<String> serde()  static scala.Option<HiveSerDe> sourceToSerDe(String source, org.apache.spark.sql.internal.SQLConf conf) Get the Hive SerDe information from the data source abbreviation string or classname. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail HiveSerDe public HiveSerDe(scala.Option<String> inputFormat, scala.Option<String> outputFormat, scala.Option<String> serde) Method Detail sourceToSerDe public static scala.Option<HiveSerDe> sourceToSerDe(String source, org.apache.spark.sql.internal.SQLConf conf) Get the Hive SerDe information from the data source abbreviation string or classname. Parameters:source - Currently the source abbreviation can be one of the following: SequenceFile, RCFile, ORC, PARQUET, and case insensitive.conf - SQLConf Returns:HiveSerDe associated with the specified source canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() inputFormat public scala.Option<String> inputFormat() outputFormat public scala.Option<String> outputFormat() serde public scala.Option<String> serde() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HiveShim.HiveFunctionWrapper$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HiveShim.HiveFunctionWrapper$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive Class HiveShim.HiveFunctionWrapper$ Object scala.runtime.AbstractFunction2<String,Object,org.apache.spark.sql.hive.HiveShim.HiveFunctionWrapper> org.apache.spark.sql.hive.HiveShim.HiveFunctionWrapper$ All Implemented Interfaces: java.io.Serializable, scala.Function2<String,Object,org.apache.spark.sql.hive.HiveShim.HiveFunctionWrapper> Enclosing class: HiveShim public static class HiveShim.HiveFunctionWrapper$ extends scala.runtime.AbstractFunction2<String,Object,org.apache.spark.sql.hive.HiveShim.HiveFunctionWrapper> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static HiveShim.HiveFunctionWrapper$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description HiveShim.HiveFunctionWrapper$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction2 apply$mcDDD$sp, apply$mcDDI$sp, apply$mcDDJ$sp, apply$mcDID$sp, apply$mcDII$sp, apply$mcDIJ$sp, apply$mcDJD$sp, apply$mcDJI$sp, apply$mcDJJ$sp, apply$mcFDD$sp, apply$mcFDI$sp, apply$mcFDJ$sp, apply$mcFID$sp, apply$mcFII$sp, apply$mcFIJ$sp, apply$mcFJD$sp, apply$mcFJI$sp, apply$mcFJJ$sp, apply$mcIDD$sp, apply$mcIDI$sp, apply$mcIDJ$sp, apply$mcIID$sp, apply$mcIII$sp, apply$mcIIJ$sp, apply$mcIJD$sp, apply$mcIJI$sp, apply$mcIJJ$sp, apply$mcJDD$sp, apply$mcJDI$sp, apply$mcJDJ$sp, apply$mcJID$sp, apply$mcJII$sp, apply$mcJIJ$sp, apply$mcJJD$sp, apply$mcJJI$sp, apply$mcJJJ$sp, apply$mcVDD$sp, apply$mcVDI$sp, apply$mcVDJ$sp, apply$mcVID$sp, apply$mcVII$sp, apply$mcVIJ$sp, apply$mcVJD$sp, apply$mcVJI$sp, apply$mcVJJ$sp, apply$mcZDD$sp, apply$mcZDI$sp, apply$mcZDJ$sp, apply$mcZID$sp, apply$mcZII$sp, apply$mcZIJ$sp, apply$mcZJD$sp, apply$mcZJI$sp, apply$mcZJJ$sp, curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function2 apply Field Detail MODULE$ public static final HiveShim.HiveFunctionWrapper$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail HiveShim.HiveFunctionWrapper$ public HiveShim.HiveFunctionWrapper$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HiveShim (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HiveShim (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive Class HiveShim Object org.apache.spark.sql.hive.HiveShim public class HiveShim extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  HiveShim.HiveFunctionWrapper$  Constructor Summary Constructors  Constructor and Description HiveShim()  Method Summary Methods  Modifier and Type Method and Description static void appendReadColumns(org.apache.hadoop.conf.Configuration conf, scala.collection.Seq<Integer> ids, scala.collection.Seq<String> names)  static String HIVE_GENERIC_UDF_MACRO_CLS()  static org.apache.hadoop.io.Writable prepareWritable(org.apache.hadoop.io.Writable w, scala.collection.Seq<scala.Tuple2<String,String>> serDeProps)  static Decimal toCatalystDecimal(org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector hdoi, Object data)  static int UNLIMITED_DECIMAL_PRECISION()  static int UNLIMITED_DECIMAL_SCALE()  static org.apache.hadoop.hive.ql.plan.FileSinkDesc wrapperToFileSinkDesc(org.apache.spark.sql.hive.HiveShim.ShimFileSinkDesc w)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail HiveShim public HiveShim() Method Detail UNLIMITED_DECIMAL_PRECISION public static int UNLIMITED_DECIMAL_PRECISION() UNLIMITED_DECIMAL_SCALE public static int UNLIMITED_DECIMAL_SCALE() HIVE_GENERIC_UDF_MACRO_CLS public static String HIVE_GENERIC_UDF_MACRO_CLS() appendReadColumns public static void appendReadColumns(org.apache.hadoop.conf.Configuration conf, scala.collection.Seq<Integer> ids, scala.collection.Seq<String> names) prepareWritable public static org.apache.hadoop.io.Writable prepareWritable(org.apache.hadoop.io.Writable w, scala.collection.Seq<scala.Tuple2<String,String>> serDeProps) toCatalystDecimal public static Decimal toCatalystDecimal(org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector hdoi, Object data) wrapperToFileSinkDesc public static org.apache.hadoop.hive.ql.plan.FileSinkDesc wrapperToFileSinkDesc(org.apache.spark.sql.hive.HiveShim.ShimFileSinkDesc w) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HiveTableUtil (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HiveTableUtil (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive Class HiveTableUtil Object org.apache.spark.sql.hive.HiveTableUtil public class HiveTableUtil extends Object Constructor Summary Constructors  Constructor and Description HiveTableUtil()  Method Summary Methods  Modifier and Type Method and Description static void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc tableDesc, org.apache.hadoop.mapred.JobConf jobConf, boolean input)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail HiveTableUtil public HiveTableUtil() Method Detail configureJobPropertiesForStorageHandler public static void configureJobPropertiesForStorageHandler(org.apache.hadoop.hive.ql.plan.TableDesc tableDesc, org.apache.hadoop.mapred.JobConf jobConf, boolean input) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method HiveUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="HiveUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive Class HiveUtils Object org.apache.spark.sql.hive.HiveUtils public class HiveUtils extends Object Constructor Summary Constructors  Constructor and Description HiveUtils()  Method Summary Methods  Modifier and Type Method and Description static CONVERT_METASTORE_ORC()  static CONVERT_METASTORE_PARQUET_WITH_SCHEMA_MERGING()  static CONVERT_METASTORE_PARQUET()  static HIVE_EXECUTION_VERSION()  static HIVE_METASTORE_BARRIER_PREFIXES()  static HIVE_METASTORE_JARS()  static HIVE_METASTORE_SHARED_PREFIXES()  static HIVE_METASTORE_VERSION()  static HIVE_THRIFT_SERVER_ASYNC()  static String hiveExecutionVersion() The version of hive used internally by Spark SQL. static scala.collection.immutable.Map<String,String> newTemporaryConfiguration(boolean useInMemoryDerby) Constructs a configuration for hive, where the metastore is located in a temp directory. static SparkContext withHiveExternalCatalog(SparkContext sc)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail HiveUtils public HiveUtils() Method Detail withHiveExternalCatalog public static SparkContext withHiveExternalCatalog(SparkContext sc) hiveExecutionVersion public static String hiveExecutionVersion() The version of hive used internally by Spark SQL. HIVE_METASTORE_VERSION public static  HIVE_METASTORE_VERSION() HIVE_EXECUTION_VERSION public static  HIVE_EXECUTION_VERSION() HIVE_METASTORE_JARS public static  HIVE_METASTORE_JARS() CONVERT_METASTORE_PARQUET public static  CONVERT_METASTORE_PARQUET() CONVERT_METASTORE_PARQUET_WITH_SCHEMA_MERGING public static  CONVERT_METASTORE_PARQUET_WITH_SCHEMA_MERGING() CONVERT_METASTORE_ORC public static  CONVERT_METASTORE_ORC() HIVE_METASTORE_SHARED_PREFIXES public static  HIVE_METASTORE_SHARED_PREFIXES() HIVE_METASTORE_BARRIER_PREFIXES public static  HIVE_METASTORE_BARRIER_PREFIXES() HIVE_THRIFT_SERVER_ASYNC public static  HIVE_THRIFT_SERVER_ASYNC() newTemporaryConfiguration public static scala.collection.immutable.Map<String,String> newTemporaryConfiguration(boolean useInMemoryDerby) Constructs a configuration for hive, where the metastore is located in a temp directory. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IDF.DocumentFrequencyAggregator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IDF.DocumentFrequencyAggregator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class IDF.DocumentFrequencyAggregator Object org.apache.spark.mllib.feature.IDF.DocumentFrequencyAggregator All Implemented Interfaces: java.io.Serializable Enclosing class: IDF public static class IDF.DocumentFrequencyAggregator extends Object implements scala.Serializable Document frequency aggregator. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description IDF.DocumentFrequencyAggregator()  IDF.DocumentFrequencyAggregator(int minDocFreq)  Method Summary Methods  Modifier and Type Method and Description IDF.DocumentFrequencyAggregator add(Vector doc) Adds a new document. Vector idf() Returns the current IDF vector. IDF.DocumentFrequencyAggregator merge(IDF.DocumentFrequencyAggregator other) Merges another. int minDocFreq()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail IDF.DocumentFrequencyAggregator public IDF.DocumentFrequencyAggregator(int minDocFreq) IDF.DocumentFrequencyAggregator public IDF.DocumentFrequencyAggregator() Method Detail minDocFreq public int minDocFreq() add public IDF.DocumentFrequencyAggregator add(Vector doc) Adds a new document. merge public IDF.DocumentFrequencyAggregator merge(IDF.DocumentFrequencyAggregator other) Merges another. idf public Vector idf() Returns the current IDF vector. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IDF (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IDF (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class IDF Object org.apache.spark.mllib.feature.IDF public class IDF extends Object Inverse document frequency (IDF). The standard formulation is used: idf = log((m + 1) / (d(t) + 1)), where m is the total number of documents and d(t) is the number of documents that contain term t. This implementation supports filtering out terms which do not appear in a minimum number of documents (controlled by the variable minDocFreq). For terms that are not in at least minDocFreq documents, the IDF is found as 0, resulting in TF-IDFs of 0. param: minDocFreq minimum of documents in which a term should appear for filtering Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  IDF.DocumentFrequencyAggregator Document frequency aggregator. Constructor Summary Constructors  Constructor and Description IDF()  IDF(int minDocFreq)  Method Summary Methods  Modifier and Type Method and Description IDFModel fit(JavaRDD<Vector> dataset) Computes the inverse document frequency. IDFModel fit(RDD<Vector> dataset) Computes the inverse document frequency. int minDocFreq()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail IDF public IDF(int minDocFreq) IDF public IDF() Method Detail minDocFreq public int minDocFreq() fit public IDFModel fit(RDD<Vector> dataset) Computes the inverse document frequency. Parameters:dataset - an RDD of term frequency vectors Returns:(undocumented) fit public IDFModel fit(JavaRDD<Vector> dataset) Computes the inverse document frequency. Parameters:dataset - a JavaRDD of term frequency vectors Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IDFModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IDFModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class IDFModel Object org.apache.spark.mllib.feature.IDFModel All Implemented Interfaces: java.io.Serializable public class IDFModel extends Object implements scala.Serializable Represents an IDF model that can transform term frequency vectors. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description Vector idf()  JavaRDD<Vector> transform(JavaRDD<Vector> dataset) Transforms term frequency (TF) vectors to TF-IDF vectors (Java version). RDD<Vector> transform(RDD<Vector> dataset) Transforms term frequency (TF) vectors to TF-IDF vectors. Vector transform(Vector v) Transforms a term frequency (TF) vector to a TF-IDF vector Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail idf public Vector idf() transform public RDD<Vector> transform(RDD<Vector> dataset) Transforms term frequency (TF) vectors to TF-IDF vectors. If minDocFreq was set for the IDF calculation, the terms which occur in fewer than minDocFreq documents will have an entry of 0. Parameters:dataset - an RDD of term frequency vectors Returns:an RDD of TF-IDF vectors transform public Vector transform(Vector v) Transforms a term frequency (TF) vector to a TF-IDF vector Parameters:v - a term frequency vector Returns:a TF-IDF vector transform public JavaRDD<Vector> transform(JavaRDD<Vector> dataset) Transforms term frequency (TF) vectors to TF-IDF vectors (Java version). Parameters:dataset - a JavaRDD of term frequency vectors Returns:a JavaRDD of TF-IDF vectors Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Identifiable (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Identifiable (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.util Interface Identifiable All Known Subinterfaces: Params All Known Implementing Classes: AFTSurvivalRegression, AFTSurvivalRegressionModel, ALS, ALSModel, Binarizer, BinaryClassificationEvaluator, BisectingKMeans, BisectingKMeansModel, Bucketizer, ChiSqSelector, ChiSqSelectorModel, ClassificationModel, Classifier, ColumnPruner, CountVectorizer, CountVectorizerModel, CrossValidator, CrossValidatorModel, DCT, DecisionTreeClassificationModel, DecisionTreeClassifier, DecisionTreeRegressionModel, DecisionTreeRegressor, DistributedLDAModel, ElementwiseProduct, Estimator, Evaluator, GaussianMixture, GaussianMixtureModel, GBTClassificationModel, GBTClassifier, GBTRegressionModel, GBTRegressor, GeneralizedLinearRegression, GeneralizedLinearRegressionModel, HashingTF, IDF, IDFModel, IndexToString, Interaction, IsotonicRegression, IsotonicRegressionModel, JavaParams, KMeans, KMeansModel, LDA, LDAModel, LinearRegression, LinearRegressionModel, LocalLDAModel, LogisticRegression, LogisticRegressionModel, MaxAbsScaler, MaxAbsScalerModel, MinMaxScaler, MinMaxScalerModel, Model, MulticlassClassificationEvaluator, MultilayerPerceptronClassificationModel, MultilayerPerceptronClassifier, NaiveBayes, NaiveBayesModel, NGram, Normalizer, OneHotEncoder, OneVsRest, OneVsRestModel, PCA, PCAModel, Pipeline, PipelineModel, PipelineStage, PolynomialExpansion, PredictionModel, Predictor, ProbabilisticClassificationModel, ProbabilisticClassifier, QuantileDiscretizer, RandomForestClassificationModel, RandomForestClassifier, RandomForestRegressionModel, RandomForestRegressor, RegexTokenizer, RegressionEvaluator, RegressionModel, RFormula, RFormulaModel, SQLTransformer, StandardScaler, StandardScalerModel, StopWordsRemover, StringIndexer, StringIndexerModel, Tokenizer, TrainValidationSplit, TrainValidationSplitModel, Transformer, UnaryTransformer, VectorAssembler, VectorAttributeRewriter, VectorIndexer, VectorIndexerModel, VectorSlicer, Word2Vec, Word2VecModel public interface Identifiable :: DeveloperApi :: Trait for an object with an immutable unique ID that identifies itself and its derivatives. WARNING: There have not yet been final discussions on this API, so it may be broken in future releases. Method Summary Methods  Modifier and Type Method and Description String toString()  String uid() An immutable unique ID for the object and its derivatives. Method Detail uid String uid() An immutable unique ID for the object and its derivatives. Returns:(undocumented) toString String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Impurities (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Impurities (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.impurity Class Impurities Object org.apache.spark.mllib.tree.impurity.Impurities public class Impurities extends Object Factory for Impurity instances. Constructor Summary Constructors  Constructor and Description Impurities()  Method Summary Methods  Modifier and Type Method and Description static Impurity fromString(String name)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Impurities public Impurities() Method Detail fromString public static Impurity fromString(String name) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Impurity (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Impurity (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.impurity Interface Impurity All Superinterfaces: java.io.Serializable public interface Impurity extends scala.Serializable Trait for calculating information gain. This trait is used for (a) setting the impurity parameter in Strategy (b) calculating impurity values from sufficient statistics. Method Summary Methods  Modifier and Type Method and Description double calculate(double[] counts, double totalCount) :: DeveloperApi :: information calculation for multiclass classification double calculate(double count, double sum, double sumSquares) :: DeveloperApi :: information calculation for regression Method Detail calculate double calculate(double[] counts, double totalCount) :: DeveloperApi :: information calculation for multiclass classification Parameters:counts - Array[Double] with counts for each labeltotalCount - sum of counts for all labels Returns:information value, or 0 if totalCount = 0 calculate double calculate(double count, double sum, double sumSquares) :: DeveloperApi :: information calculation for regression Parameters:count - number of instancessum - sum of labelssumSquares - summation of squares of the labels Returns:information value, or 0 if count = 0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method In (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="In (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class In Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.In All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class In extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff the attribute evaluates to one of the values in the array. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description In(String attribute, Object[] values)  Method Summary Methods  Modifier and Type Method and Description String attribute()  abstract static boolean canEqual(Object that)  boolean equals(Object o)  int hashCode()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  String toString()  Object[] values()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual Constructor Detail In public In(String attribute, Object[] values) Method Detail canEqual public abstract static boolean canEqual(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() attribute public String attribute() values public Object[] values() hashCode public int hashCode() Overrides: hashCode in class Object equals public boolean equals(Object o) Specified by: equals in interface scala.Equals Overrides: equals in class Object toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IncompatibleMergeException (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IncompatibleMergeException (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.sketch Class IncompatibleMergeException Object Throwable Exception org.apache.spark.util.sketch.IncompatibleMergeException All Implemented Interfaces: java.io.Serializable public class IncompatibleMergeException extends Exception See Also:Serialized Form Constructor Summary Constructors  Constructor and Description IncompatibleMergeException(String message)  Method Summary Methods inherited from class Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail IncompatibleMergeException public IncompatibleMergeException(String message) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IndexToString (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IndexToString (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class IndexToString Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.feature.IndexToString All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class IndexToString extends Transformer implements DefaultParamsWritable A Transformer that maps a column of indices back to a new column of corresponding string values. The index-string mapping is either from the ML attributes of the input column, or from user-supplied labels (which take precedence over ML attributes). See Also:StringIndexer} for converting strings into indices, Serialized Form Constructor Summary Constructors  Constructor and Description IndexToString()  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  IndexToString copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  String[] getLabels()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  StringArrayParam labels() Optional param for array of labels specifying index-string mapping. static IndexToString load(String path)  static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  IndexToString setInputCol(String value)  IndexToString setLabels(String[] value)  IndexToString setOutputCol(String value)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail IndexToString public IndexToString() Method Detail load public static IndexToString load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setInputCol public IndexToString setInputCol(String value) setOutputCol public IndexToString setOutputCol(String value) setLabels public IndexToString setLabels(String[] value) labels public final StringArrayParam labels() Optional param for array of labels specifying index-string mapping. Default: Not specified, in which case inputCol metadata is used for labels. Returns:(undocumented) getLabels public final String[] getLabels() transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) copy public IndexToString copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Transformer Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IndexedRow (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IndexedRow (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg.distributed Class IndexedRow Object org.apache.spark.mllib.linalg.distributed.IndexedRow All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class IndexedRow extends Object implements scala.Product, scala.Serializable Represents a row of IndexedRowMatrix. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description IndexedRow(long index, Vector vector)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  long index()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Vector vector()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail IndexedRow public IndexedRow(long index, Vector vector) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() index public long index() vector public Vector vector() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IndexedRowMatrix (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IndexedRowMatrix (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg.distributed Class IndexedRowMatrix Object org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix All Implemented Interfaces: java.io.Serializable, DistributedMatrix public class IndexedRowMatrix extends Object implements DistributedMatrix Represents a row-oriented DistributedMatrix with indexed rows. param: rows indexed rows of this matrix param: nRows number of rows. A non-positive value means unknown, and then the number of rows will be determined by the max row index plus one. param: nCols number of columns. A non-positive value means unknown, and then the number of columns will be determined by the size of the first row. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description IndexedRowMatrix(RDD<IndexedRow> rows) Alternative constructor leaving matrix dimensions to be determined automatically. IndexedRowMatrix(RDD<IndexedRow> rows, long nRows, int nCols)  Method Summary Methods  Modifier and Type Method and Description CoordinateMatrix columnSimilarities() Compute all cosine similarities between columns of this matrix using the brute-force approach of computing normalized dot products. Matrix computeGramianMatrix() Computes the Gramian matrix A^T A. SingularValueDecomposition<IndexedRowMatrix,Matrix> computeSVD(int k, boolean computeU, double rCond) Computes the singular value decomposition of this IndexedRowMatrix. IndexedRowMatrix multiply(Matrix B) Multiply this matrix by a local matrix on the right. long numCols() Gets or computes the number of columns. long numRows() Gets or computes the number of rows. RDD<IndexedRow> rows()  BlockMatrix toBlockMatrix() Converts to BlockMatrix. BlockMatrix toBlockMatrix(int rowsPerBlock, int colsPerBlock) Converts to BlockMatrix. CoordinateMatrix toCoordinateMatrix() Converts this matrix to a CoordinateMatrix. RowMatrix toRowMatrix() Drops row indices and converts this matrix to a RowMatrix. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail IndexedRowMatrix public IndexedRowMatrix(RDD<IndexedRow> rows, long nRows, int nCols) IndexedRowMatrix public IndexedRowMatrix(RDD<IndexedRow> rows) Alternative constructor leaving matrix dimensions to be determined automatically. Method Detail rows public RDD<IndexedRow> rows() numCols public long numCols() Description copied from interface: DistributedMatrix Gets or computes the number of columns. Specified by: numCols in interface DistributedMatrix numRows public long numRows() Description copied from interface: DistributedMatrix Gets or computes the number of rows. Specified by: numRows in interface DistributedMatrix columnSimilarities public CoordinateMatrix columnSimilarities() Compute all cosine similarities between columns of this matrix using the brute-force approach of computing normalized dot products. Returns:An n x n sparse upper-triangular matrix of cosine similarities between columns of this matrix. toRowMatrix public RowMatrix toRowMatrix() Drops row indices and converts this matrix to a RowMatrix. Returns:(undocumented) toBlockMatrix public BlockMatrix toBlockMatrix() Converts to BlockMatrix. Creates blocks of SparseMatrix with size 1024 x 1024. toBlockMatrix public BlockMatrix toBlockMatrix(int rowsPerBlock, int colsPerBlock) Converts to BlockMatrix. Creates blocks of SparseMatrix. Parameters:rowsPerBlock - The number of rows of each block. The blocks at the bottom edge may have a smaller value. Must be an integer value greater than 0.colsPerBlock - The number of columns of each block. The blocks at the right edge may have a smaller value. Must be an integer value greater than 0. Returns:a BlockMatrix toCoordinateMatrix public CoordinateMatrix toCoordinateMatrix() Converts this matrix to a CoordinateMatrix. Returns:(undocumented) computeSVD public SingularValueDecomposition<IndexedRowMatrix,Matrix> computeSVD(int k, boolean computeU, double rCond) Computes the singular value decomposition of this IndexedRowMatrix. Denote this matrix by A (m x n), this will compute matrices U, S, V such that A = U * S * V'. The cost and implementation of this method is identical to that in RowMatrix With the addition of indices. At most k largest non-zero singular values and associated vectors are returned. If there are k such values, then the dimensions of the return will be: U is an IndexedRowMatrix of size m x k that satisfies U'U = eye(k), s is a Vector of size k, holding the singular values in descending order, and V is a local Matrix of size n x k that satisfies V'V = eye(k). Parameters:k - number of singular values to keep. We might return less than k if there are numerically zero singular values. See rCond.computeU - whether to compute UrCond - the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value. Returns:SingularValueDecomposition(U, s, V) multiply public IndexedRowMatrix multiply(Matrix B) Multiply this matrix by a local matrix on the right. Parameters:B - a local matrix whose number of rows must match the number of columns of this matrix Returns:an IndexedRowMatrix representing the product, which preserves partitioning computeGramianMatrix public Matrix computeGramianMatrix() Computes the Gramian matrix A^T A. Note that this cannot be computed on matrices with more than 65535 columns. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InformationGainStats (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InformationGainStats (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.model Class InformationGainStats Object org.apache.spark.mllib.tree.model.InformationGainStats All Implemented Interfaces: java.io.Serializable public class InformationGainStats extends Object implements scala.Serializable :: DeveloperApi :: Information gain statistics for each split param: gain information gain value param: impurity current node impurity param: leftImpurity left node impurity param: rightImpurity right node impurity param: leftPredict left node predict param: rightPredict right node predict See Also:Serialized Form Constructor Summary Constructors  Constructor and Description InformationGainStats(double gain, double impurity, double leftImpurity, double rightImpurity, Predict leftPredict, Predict rightPredict)  Method Summary Methods  Modifier and Type Method and Description boolean equals(Object o)  double gain()  int hashCode()  double impurity()  double leftImpurity()  Predict leftPredict()  double rightImpurity()  Predict rightPredict()  String toString()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail InformationGainStats public InformationGainStats(double gain, double impurity, double leftImpurity, double rightImpurity, Predict leftPredict, Predict rightPredict) Method Detail gain public double gain() impurity public double impurity() leftImpurity public double leftImpurity() rightImpurity public double rightImpurity() leftPredict public Predict leftPredict() rightPredict public Predict rightPredict() toString public String toString() Overrides: toString in class Object equals public boolean equals(Object o) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InnerClosureFinder (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InnerClosureFinder (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class InnerClosureFinder Object org.apache.xbean.asm5.ClassVisitor org.apache.spark.util.InnerClosureFinder public class InnerClosureFinder extends org.apache.xbean.asm5.ClassVisitor Constructor Summary Constructors  Constructor and Description InnerClosureFinder(scala.collection.mutable.Set<Class<?>> output)  Method Summary Methods  Modifier and Type Method and Description String myName()  void visit(int version, int access, String name, String sig, String superName, String[] interfaces)  org.apache.xbean.asm5.MethodVisitor visitMethod(int access, String name, String desc, String sig, String[] exceptions)  Methods inherited from class org.apache.xbean.asm5.ClassVisitor visitAnnotation, visitAttribute, visitEnd, visitField, visitInnerClass, visitOuterClass, visitSource, visitTypeAnnotation Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail InnerClosureFinder public InnerClosureFinder(scala.collection.mutable.Set<Class<?>> output) Method Detail myName public String myName() visit public void visit(int version, int access, String name, String sig, String superName, String[] interfaces) Overrides: visit in class org.apache.xbean.asm5.ClassVisitor visitMethod public org.apache.xbean.asm5.MethodVisitor visitMethod(int access, String name, String desc, String sig, String[] exceptions) Overrides: visitMethod in class org.apache.xbean.asm5.ClassVisitor Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InputDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InputDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.dstream Class InputDStream<T> Object org.apache.spark.streaming.dstream.DStream<T> org.apache.spark.streaming.dstream.InputDStream<T> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: ConstantInputDStream, ReceiverInputDStream public abstract class InputDStream<T> extends DStream<T> This is the abstract base class for all input streams. This class provides methods start() and stop() which are called by Spark Streaming system to start and stop receiving data, respectively. Input streams that can generate RDDs from new data by running a service/thread only on the driver node (that is, without running a receiver on worker nodes), can be implemented by directly inheriting this InputDStream. For example, FileInputDStream, a subclass of InputDStream, monitors a HDFS directory from the driver for new files and generates RDDs with the new files. For implementing input streams that requires running a receiver on the worker nodes, use ReceiverInputDStream as the parent class. param: _ssc Streaming context that will execute this input stream See Also:Serialized Form Constructor Summary Constructors  Constructor and Description InputDStream(StreamingContext _ssc, scala.reflect.ClassTag<T> evidence$1)  Method Summary Methods  Modifier and Type Method and Description scala.collection.immutable.List<DStream<?>> dependencies() List of parent DStreams on which this DStream depends on int id() This is an unique identifier for the input stream. Duration slideDuration() Time interval after which the DStream generates a RDD abstract void start() Method called to start receiving data. abstract void stop() Method called to stop receiving data. Methods inherited from class org.apache.spark.streaming.dstream.DStream cache, checkpoint, compute, context, count, countByValue, countByValueAndWindow, countByWindow, filter, flatMap, foreachRDD, foreachRDD, glom, map, mapPartitions, persist, persist, print, print, reduce, reduceByWindow, reduceByWindow, repartition, saveAsObjectFiles, saveAsTextFiles, slice, slice, toPairDStreamFunctions, transform, transform, transformWith, transformWith, union, window, window Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail InputDStream public InputDStream(StreamingContext _ssc, scala.reflect.ClassTag<T> evidence$1) Method Detail id public int id() This is an unique identifier for the input stream. dependencies public scala.collection.immutable.List<DStream<?>> dependencies() Description copied from class: DStream List of parent DStreams on which this DStream depends on Specified by: dependencies in class DStream<T> slideDuration public Duration slideDuration() Description copied from class: DStream Time interval after which the DStream generates a RDD Specified by: slideDuration in class DStream<T> start public abstract void start() Method called to start receiving data. Subclasses must implement this method. stop public abstract void stop() Method called to stop receiving data. Subclasses must implement this method. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InputFileNameHolder (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InputFileNameHolder (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class InputFileNameHolder Object org.apache.spark.rdd.InputFileNameHolder public class InputFileNameHolder extends Object This holds file names of the current Spark task. This is used in HadoopRDD, FileScanRDD, NewHadoopRDD and InputFileName function in Spark SQL. Constructor Summary Constructors  Constructor and Description InputFileNameHolder()  Method Summary Methods  Modifier and Type Method and Description static org.apache.spark.unsafe.types.UTF8String getInputFileName() The thread variable for the name of the current file being read. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail InputFileNameHolder public InputFileNameHolder() Method Detail getInputFileName public static org.apache.spark.unsafe.types.UTF8String getInputFileName() The thread variable for the name of the current file being read. This is used by the InputFileName function in Spark SQL. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InputFormatInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InputFormatInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler Class InputFormatInfo Object org.apache.spark.scheduler.InputFormatInfo public class InputFormatInfo extends Object :: DeveloperApi :: Parses and holds information about inputFormat (and files) specified as a parameter. Constructor Summary Constructors  Constructor and Description InputFormatInfo(org.apache.hadoop.conf.Configuration configuration, Class<?> inputFormatClazz, String path)  Method Summary Methods  Modifier and Type Method and Description static scala.collection.immutable.Map<String,scala.collection.immutable.Set<SplitInfo>> computePreferredLocations(scala.collection.Seq<InputFormatInfo> formats) Computes the preferred locations based on input(s) and returned a location to block map. org.apache.hadoop.conf.Configuration configuration()  boolean equals(Object other)  int hashCode()  Class<?> inputFormatClazz()  boolean mapredInputFormat()  boolean mapreduceInputFormat()  String path()  String toString()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail InputFormatInfo public InputFormatInfo(org.apache.hadoop.conf.Configuration configuration, Class<?> inputFormatClazz, String path) Method Detail computePreferredLocations public static scala.collection.immutable.Map<String,scala.collection.immutable.Set<SplitInfo>> computePreferredLocations(scala.collection.Seq<InputFormatInfo> formats) Computes the preferred locations based on input(s) and returned a location to block map. Typical use of this method for allocation would follow some algo like this: a) For each host, count number of splits hosted on that host. b) Decrement the currently allocated containers on that host. c) Compute rack info for each host and update rack -> count map based on (b). d) Allocate nodes based on (c) e) On the allocation result, ensure that we don't allocate "too many" jobs on a single node (even if data locality on that is very high) : this is to prevent fragility of job if a single (or small set of) hosts go down. go to (a) until required nodes are allocated. If a node 'dies', follow same procedure. PS: I know the wording here is weird, hopefully it makes some sense ! Parameters:formats - (undocumented) Returns:(undocumented) configuration public org.apache.hadoop.conf.Configuration configuration() inputFormatClazz public Class<?> inputFormatClazz() path public String path() mapreduceInputFormat public boolean mapreduceInputFormat() mapredInputFormat public boolean mapredInputFormat() toString public String toString() Overrides: toString in class Object hashCode public int hashCode() Overrides: hashCode in class Object equals public boolean equals(Object other) Overrides: equals in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InputMetricDistributions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InputMetricDistributions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class InputMetricDistributions Object org.apache.spark.status.api.v1.InputMetricDistributions public class InputMetricDistributions extends Object Method Summary Methods  Modifier and Type Method and Description scala.collection.IndexedSeq<Object> bytesRead()  scala.collection.IndexedSeq<Object> recordsRead()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail bytesRead public scala.collection.IndexedSeq<Object> bytesRead() recordsRead public scala.collection.IndexedSeq<Object> recordsRead() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InputMetrics (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InputMetrics (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class InputMetrics Object org.apache.spark.status.api.v1.InputMetrics public class InputMetrics extends Object Method Summary Methods  Modifier and Type Method and Description long bytesRead()  long recordsRead()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail bytesRead public long bytesRead() recordsRead public long recordsRead() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InsertIntoHiveTable (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InsertIntoHiveTable (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive.execution Class InsertIntoHiveTable Object org.apache.spark.sql.catalyst.trees.TreeNode<PlanType> org.apache.spark.sql.catalyst.plans.QueryPlan<org.apache.spark.sql.execution.SparkPlan> org.apache.spark.sql.execution.SparkPlan org.apache.spark.sql.hive.execution.InsertIntoHiveTable All Implemented Interfaces: java.io.Serializable, org.apache.spark.sql.execution.UnaryExecNode, scala.Equals, scala.Product public class InsertIntoHiveTable extends org.apache.spark.sql.execution.SparkPlan implements org.apache.spark.sql.execution.UnaryExecNode, scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description InsertIntoHiveTable(org.apache.spark.sql.hive.MetastoreRelation table, scala.collection.immutable.Map<String,scala.Option<String>> partition, org.apache.spark.sql.execution.SparkPlan child, boolean overwrite, boolean ifNotExists)  Method Summary Methods  Modifier and Type Method and Description static org.apache.spark.sql.catalyst.expressions.AttributeSeq allAttributes()  static BaseType apply(int number)  static String argString()  static String asCode()  abstract static boolean canEqual(Object that)  org.apache.spark.sql.execution.SparkPlan child()  static scala.collection.Seq<org.apache.spark.sql.execution.SparkPlan> children()  static <B> scala.collection.Seq<B> collect(scala.PartialFunction<BaseType,B> pf)  static <B> scala.Option<B> collectFirst(scala.PartialFunction<BaseType,B> pf)  static scala.collection.Seq<BaseType> collectLeaves()  static org.apache.spark.sql.catalyst.expressions.ExpressionSet constraints()  static scala.collection.immutable.Set<org.apache.spark.sql.catalyst.trees.TreeNode<?>> containsChild()  abstract static boolean equals(Object that)  static RDD<org.apache.spark.sql.catalyst.InternalRow> execute()  static <T> Broadcast<T> executeBroadcast()  org.apache.spark.sql.catalyst.InternalRow[] executeCollect()  static Row[] executeCollectPublic()  static org.apache.spark.sql.catalyst.InternalRow[] executeTake(int n)  static scala.collection.Iterator<org.apache.spark.sql.catalyst.InternalRow> executeToIterator()  static scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> expressions()  static boolean fastEquals(org.apache.spark.sql.catalyst.trees.TreeNode<?> other)  static scala.Option<BaseType> find(scala.Function1<BaseType,Object> f)  static <A> scala.collection.Seq<A> flatMap(scala.Function1<BaseType,scala.collection.TraversableOnce<A>> f)  static void foreach(scala.Function1<BaseType,scala.runtime.BoxedUnit> f)  static void foreachUp(scala.Function1<BaseType,scala.runtime.BoxedUnit> f)  static scala.collection.mutable.StringBuilder generateTreeString(int depth, scala.collection.Seq<Object> lastChildren, scala.collection.mutable.StringBuilder builder, boolean verbose, String prefix)  static String generateTreeString$default$5()  org.apache.hadoop.fs.Path getExternalTmpPath(org.apache.hadoop.fs.Path path, org.apache.hadoop.conf.Configuration hadoopConf)  org.apache.hadoop.fs.Path getExtTmpPathRelTo(org.apache.hadoop.fs.Path path, org.apache.hadoop.conf.Configuration hadoopConf)  static int hashCode()  boolean ifNotExists()  void initializeLogging(boolean isInterpreter)  static org.apache.spark.sql.catalyst.expressions.AttributeSet inputSet()  org.slf4j.Logger log_()  static org.apache.spark.sql.execution.metric.SQLMetric longMetric(String name)  static org.apache.spark.sql.execution.SparkPlan makeCopy(Object[] newArgs)  static <A> scala.collection.Seq<A> map(scala.Function1<BaseType,A> f)  static BaseType mapChildren(scala.Function1<BaseType,BaseType> f)  static scala.collection.immutable.Map<String,String> metadata()  static scala.collection.immutable.Map<String,org.apache.spark.sql.execution.metric.SQLMetric> metrics()  static org.apache.spark.sql.catalyst.expressions.AttributeSet missingInput()  static String nodeName()  static String numberedTreeString()  static org.apache.spark.sql.catalyst.trees.Origin origin()  scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> output()  static scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.SortOrder> outputOrdering()  static org.apache.spark.sql.catalyst.plans.physical.Partitioning outputPartitioning()  static org.apache.spark.sql.catalyst.expressions.AttributeSet outputSet()  boolean overwrite()  scala.collection.immutable.Map<String,scala.Option<String>> partition()  static void prepare()  static String prettyJson()  static void printSchema()  static org.apache.spark.sql.catalyst.expressions.AttributeSet producedAttributes()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  static org.apache.spark.sql.catalyst.expressions.AttributeSet references()  static scala.collection.Seq<org.apache.spark.sql.catalyst.plans.physical.Distribution> requiredChildDistribution()  static scala.collection.Seq<scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.SortOrder>> requiredChildOrdering()  static void resetMetrics()  static boolean sameResult(PlanType plan)  static StructType schema()  static String schemaString()  static String simpleString()  static SQLContext sqlContext()  String stagingDir()  static boolean subexpressionEliminationEnabled()  static scala.collection.Seq<PlanType> subqueries()  org.apache.spark.sql.hive.MetastoreRelation table()  static String toJSON()  static String toString()  static BaseType transform(scala.PartialFunction<BaseType,BaseType> rule)  static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformAllExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule)  static BaseType transformDown(scala.PartialFunction<BaseType,BaseType> rule)  static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule)  static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressionsDown(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule)  static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressionsUp(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule)  static BaseType transformUp(scala.PartialFunction<BaseType,BaseType> rule)  static String treeString()  static String treeString(boolean verbose)  static String verboseString()  static BaseType withNewChildren(scala.collection.Seq<BaseType> newChildren)  Methods inherited from class org.apache.spark.sql.execution.SparkPlan doExecuteBroadcast, doPrepare, execute, executeBroadcast, executeCollectPublic, executeQuery, executeTake, executeToIterator, initializeLogIfNecessary, isTraceEnabled, log, logDebug, logDebug, logError, logError, logInfo, logInfo, logName, logTrace, logTrace, logWarning, logWarning, longMetric, makeCopy, metadata, metrics, newMutableProjection, newMutableProjection$default$3, newNaturalAscendingOrdering, newOrdering, newPredicate, org$apache$spark$internal$Logging$$log__$eq, org$apache$spark$internal$Logging$$log_, org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows, org$apache$spark$sql$execution$SparkPlan$$subqueryResults, outputOrdering, outputPartitioning, prepare, prepareSubqueries, requiredChildDistribution, requiredChildOrdering, resetMetrics, sparkContext, sqlContext, subexpressionEliminationEnabled, waitForSubqueries Methods inherited from class org.apache.spark.sql.catalyst.plans.QueryPlan allAttributes, canonicalized, cleanArgs, constraints, expressions, getRelevantConstraints, innerChildren, inputSet, missingInput, org$apache$spark$sql$catalyst$plans$QueryPlan$$aliasMap, org$apache$spark$sql$catalyst$plans$QueryPlan$$cleanArg$1, org$apache$spark$sql$catalyst$plans$QueryPlan$$getConstraintClass, org$apache$spark$sql$catalyst$plans$QueryPlan$$isRecursiveDeduction, org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1, org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2, org$apache$spark$sql$catalyst$plans$QueryPlan$$scanNullIntolerantExpr, org$apache$spark$sql$catalyst$plans$QueryPlan$$seqToExpressions$1, outputSet, printSchema, producedAttributes, references, sameResult, schema, schemaString, simpleString, statePrefix, subqueries, transformAllExpressions, transformExpressions, transformExpressionsDown, transformExpressionsUp, validConstraints, verboseString Methods inherited from class org.apache.spark.sql.catalyst.trees.TreeNode apply, argString, asCode, children, collect, collectFirst, collectLeaves, containsChild, fastEquals, find, flatMap, foreach, foreachUp, fromJSON, generateTreeString, generateTreeString$default$5, getNodeNumbered, hashCode, jsonFields, map, mapChildren, mapProductIterator, nodeName, numberedTreeString, org$apache$spark$sql$catalyst$trees$TreeNode$$allChildren, org$apache$spark$sql$catalyst$trees$TreeNode$$collectJsonValue$1, org$apache$spark$sql$catalyst$trees$TreeNode$$parseToJson, origin, otherCopyArgs, prettyJson, productIterator, productPrefix, stringArgs, toJSON, toString, transform, transformChildren, transformDown, transformUp, treeString, treeString, withNewChildren Methods inherited from class Object equals, getClass, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.sql.execution.UnaryExecNode children, outputPartitioning Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail InsertIntoHiveTable public InsertIntoHiveTable(org.apache.spark.sql.hive.MetastoreRelation table, scala.collection.immutable.Map<String,scala.Option<String>> partition, org.apache.spark.sql.execution.SparkPlan child, boolean overwrite, boolean ifNotExists) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() origin public static org.apache.spark.sql.catalyst.trees.Origin origin() containsChild public static scala.collection.immutable.Set<org.apache.spark.sql.catalyst.trees.TreeNode<?>> containsChild() hashCode public static int hashCode() fastEquals public static boolean fastEquals(org.apache.spark.sql.catalyst.trees.TreeNode<?> other) find public static scala.Option<BaseType> find(scala.Function1<BaseType,Object> f) foreach public static void foreach(scala.Function1<BaseType,scala.runtime.BoxedUnit> f) foreachUp public static void foreachUp(scala.Function1<BaseType,scala.runtime.BoxedUnit> f) map public static <A> scala.collection.Seq<A> map(scala.Function1<BaseType,A> f) flatMap public static <A> scala.collection.Seq<A> flatMap(scala.Function1<BaseType,scala.collection.TraversableOnce<A>> f) collect public static <B> scala.collection.Seq<B> collect(scala.PartialFunction<BaseType,B> pf) collectLeaves public static scala.collection.Seq<BaseType> collectLeaves() collectFirst public static <B> scala.Option<B> collectFirst(scala.PartialFunction<BaseType,B> pf) mapChildren public static BaseType mapChildren(scala.Function1<BaseType,BaseType> f) withNewChildren public static BaseType withNewChildren(scala.collection.Seq<BaseType> newChildren) transform public static BaseType transform(scala.PartialFunction<BaseType,BaseType> rule) transformDown public static BaseType transformDown(scala.PartialFunction<BaseType,BaseType> rule) transformUp public static BaseType transformUp(scala.PartialFunction<BaseType,BaseType> rule) nodeName public static String nodeName() argString public static String argString() toString public static String toString() treeString public static String treeString() treeString public static String treeString(boolean verbose) numberedTreeString public static String numberedTreeString() apply public static BaseType apply(int number) generateTreeString public static scala.collection.mutable.StringBuilder generateTreeString(int depth, scala.collection.Seq<Object> lastChildren, scala.collection.mutable.StringBuilder builder, boolean verbose, String prefix) asCode public static String asCode() toJSON public static String toJSON() prettyJson public static String prettyJson() generateTreeString$default$5 public static String generateTreeString$default$5() constraints public static org.apache.spark.sql.catalyst.expressions.ExpressionSet constraints() outputSet public static org.apache.spark.sql.catalyst.expressions.AttributeSet outputSet() references public static org.apache.spark.sql.catalyst.expressions.AttributeSet references() inputSet public static org.apache.spark.sql.catalyst.expressions.AttributeSet inputSet() producedAttributes public static org.apache.spark.sql.catalyst.expressions.AttributeSet producedAttributes() missingInput public static org.apache.spark.sql.catalyst.expressions.AttributeSet missingInput() transformExpressions public static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule) transformExpressionsDown public static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressionsDown(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule) transformExpressionsUp public static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformExpressionsUp(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule) transformAllExpressions public static org.apache.spark.sql.catalyst.plans.QueryPlan<PlanType> transformAllExpressions(scala.PartialFunction<org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression> rule) expressions public static final scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Expression> expressions() schema public static StructType schema() schemaString public static String schemaString() printSchema public static void printSchema() simpleString public static String simpleString() verboseString public static String verboseString() subqueries public static scala.collection.Seq<PlanType> subqueries() sameResult public static boolean sameResult(PlanType plan) allAttributes public static org.apache.spark.sql.catalyst.expressions.AttributeSeq allAttributes() sqlContext public static final SQLContext sqlContext() subexpressionEliminationEnabled public static boolean subexpressionEliminationEnabled() makeCopy public static org.apache.spark.sql.execution.SparkPlan makeCopy(Object[] newArgs) metadata public static scala.collection.immutable.Map<String,String> metadata() metrics public static scala.collection.immutable.Map<String,org.apache.spark.sql.execution.metric.SQLMetric> metrics() resetMetrics public static void resetMetrics() longMetric public static org.apache.spark.sql.execution.metric.SQLMetric longMetric(String name) requiredChildDistribution public static scala.collection.Seq<org.apache.spark.sql.catalyst.plans.physical.Distribution> requiredChildDistribution() outputOrdering public static scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.SortOrder> outputOrdering() requiredChildOrdering public static scala.collection.Seq<scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.SortOrder>> requiredChildOrdering() execute public static final RDD<org.apache.spark.sql.catalyst.InternalRow> execute() executeBroadcast public static final <T> Broadcast<T> executeBroadcast() prepare public static final void prepare() executeToIterator public static scala.collection.Iterator<org.apache.spark.sql.catalyst.InternalRow> executeToIterator() executeCollectPublic public static Row[] executeCollectPublic() executeTake public static org.apache.spark.sql.catalyst.InternalRow[] executeTake(int n) children public static scala.collection.Seq<org.apache.spark.sql.execution.SparkPlan> children() outputPartitioning public static org.apache.spark.sql.catalyst.plans.physical.Partitioning outputPartitioning() table public org.apache.spark.sql.hive.MetastoreRelation table() partition public scala.collection.immutable.Map<String,scala.Option<String>> partition() child public org.apache.spark.sql.execution.SparkPlan child() Specified by: child in interface org.apache.spark.sql.execution.UnaryExecNode overwrite public boolean overwrite() ifNotExists public boolean ifNotExists() output public scala.collection.Seq<org.apache.spark.sql.catalyst.expressions.Attribute> output() Specified by: output in class org.apache.spark.sql.catalyst.plans.QueryPlan<org.apache.spark.sql.execution.SparkPlan> stagingDir public String stagingDir() getExternalTmpPath public org.apache.hadoop.fs.Path getExternalTmpPath(org.apache.hadoop.fs.Path path, org.apache.hadoop.conf.Configuration hadoopConf) getExtTmpPathRelTo public org.apache.hadoop.fs.Path getExtTmpPathRelTo(org.apache.hadoop.fs.Path path, org.apache.hadoop.conf.Configuration hadoopConf) executeCollect public org.apache.spark.sql.catalyst.InternalRow[] executeCollect() Overrides: executeCollect in class org.apache.spark.sql.execution.SparkPlan log_ public org.slf4j.Logger log_() initializeLogging public void initializeLogging(boolean isInterpreter) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InsertableRelation (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InsertableRelation (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Interface InsertableRelation public interface InsertableRelation ::DeveloperApi:: A BaseRelation that can be used to insert data into it through the insert method. If overwrite in insert method is true, the old data in the relation should be overwritten with the new data. If overwrite in insert method is false, the new data should be appended. InsertableRelation has the following three assumptions. 1. It assumes that the data (Rows in the DataFrame) provided to the insert method exactly matches the ordinal of fields in the schema of the BaseRelation. 2. It assumes that the schema of this relation will not be changed. Even if the insert method updates the schema (e.g. a relation of JSON or Parquet data may have a schema update after an insert operation), the new schema will not be used. 3. It assumes that fields of the data provided in the insert method are nullable. If a data source needs to check the actual nullability of a field, it needs to do it in the insert method. Since: 1.3.0 Method Summary Methods  Modifier and Type Method and Description void insert(Dataset<Row> data, boolean overwrite)  Method Detail insert void insert(Dataset<Row> data, boolean overwrite) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IntArrayParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IntArrayParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class IntArrayParam Object org.apache.spark.ml.param.Param<int[]> org.apache.spark.ml.param.IntArrayParam All Implemented Interfaces: java.io.Serializable public class IntArrayParam extends Param<int[]> :: DeveloperApi :: Specialized version of Param[Array[Int} for Java. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description IntArrayParam(Params parent, String name, String doc)  IntArrayParam(Params parent, String name, String doc, scala.Function1<int[],Object> isValid)  Method Summary Methods  Modifier and Type Method and Description int[] jsonDecode(String json) Decodes a param value from JSON. String jsonEncode(int[] value) Encodes a param value into JSON, which can be decoded by jsonDecode(). ParamPair<int[]> w(java.util.List<Integer> value) Creates a param pair with a List of values (for Java and Python). Methods inherited from class org.apache.spark.ml.param.Param doc, equals, hashCode, isValid, name, parent, toString, w Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail IntArrayParam public IntArrayParam(Params parent, String name, String doc, scala.Function1<int[],Object> isValid) IntArrayParam public IntArrayParam(Params parent, String name, String doc) Method Detail w public ParamPair<int[]> w(java.util.List<Integer> value) Creates a param pair with a List of values (for Java and Python). jsonEncode public String jsonEncode(int[] value) Description copied from class: Param Encodes a param value into JSON, which can be decoded by jsonDecode(). Overrides: jsonEncode in class Param<int[]> jsonDecode public int[] jsonDecode(String json) Description copied from class: Param Decodes a param value from JSON. Overrides: jsonDecode in class Param<int[]> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IntParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IntParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class IntParam Object org.apache.spark.util.IntParam public class IntParam extends Object An extractor object for parsing strings into integers. Constructor Summary Constructors  Constructor and Description IntParam()  Method Summary Methods  Modifier and Type Method and Description static scala.Option<Object> unapply(String str)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail IntParam public IntParam() Method Detail unapply public static scala.Option<Object> unapply(String str) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IntegerType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IntegerType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class IntegerType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.NumericType org.apache.spark.sql.types.IntegerType public class IntegerType extends NumericType :: DeveloperApi :: The data type representing Int values. Please use the singleton DataTypes.IntegerType. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of the IntegerType is 4 bytes. static String json()  static String prettyJson()  String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() The default size of a value of the IntegerType is 4 bytes. Returns:(undocumented) simpleString public String simpleString() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Interaction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Interaction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class Interaction Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.feature.Interaction All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class Interaction extends Transformer implements DefaultParamsWritable Implements the feature interaction transform. This transformer takes in Double and Vector type columns and outputs a flattened vector of their feature interactions. To handle interaction, we first one-hot encode any nominal features. Then, a vector of the feature cross-products is produced. For example, given the input feature values Double(2) and Vector(3, 4), the output would be Vector(6, 8) if all input features were numeric. If the first feature was instead nominal with four categories, the output would then be Vector(0, 0, 0, 0, 3, 4, 0, 0). See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Interaction()  Interaction(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  Interaction copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String[] getInputCols()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static StringArrayParam inputCols()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Interaction load(String path)  static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  Interaction setInputCols(String[] values)  Interaction setOutputCol(String value)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail Interaction public Interaction(String uid) Interaction public Interaction() Method Detail load public static Interaction load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() inputCols public static final StringArrayParam inputCols() getInputCols public static final String[] getInputCols() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setInputCols public Interaction setInputCols(String[] values) setOutputCol public Interaction setOutputCol(String value) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) copy public Interaction copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Transformer Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalAccumulator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalAccumulator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class InternalAccumulator Object org.apache.spark.InternalAccumulator public class InternalAccumulator extends Object A collection of fields and methods concerned with internal accumulators that represent task level metrics. Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  InternalAccumulator.input$  static class  InternalAccumulator.output$  static class  InternalAccumulator.shuffleRead$  static class  InternalAccumulator.shuffleWrite$  Constructor Summary Constructors  Constructor and Description InternalAccumulator()  Method Summary Methods  Modifier and Type Method and Description static String DISK_BYTES_SPILLED()  static String EXECUTOR_DESERIALIZE_TIME()  static String EXECUTOR_RUN_TIME()  static String INPUT_METRICS_PREFIX()  static String JVM_GC_TIME()  static String MEMORY_BYTES_SPILLED()  static String METRICS_PREFIX()  static String OUTPUT_METRICS_PREFIX()  static String PEAK_EXECUTION_MEMORY()  static String RESULT_SERIALIZATION_TIME()  static String RESULT_SIZE()  static String SHUFFLE_READ_METRICS_PREFIX()  static String SHUFFLE_WRITE_METRICS_PREFIX()  static String TEST_ACCUM()  static String UPDATED_BLOCK_STATUSES()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail InternalAccumulator public InternalAccumulator() Method Detail METRICS_PREFIX public static String METRICS_PREFIX() SHUFFLE_READ_METRICS_PREFIX public static String SHUFFLE_READ_METRICS_PREFIX() SHUFFLE_WRITE_METRICS_PREFIX public static String SHUFFLE_WRITE_METRICS_PREFIX() OUTPUT_METRICS_PREFIX public static String OUTPUT_METRICS_PREFIX() INPUT_METRICS_PREFIX public static String INPUT_METRICS_PREFIX() EXECUTOR_DESERIALIZE_TIME public static String EXECUTOR_DESERIALIZE_TIME() EXECUTOR_RUN_TIME public static String EXECUTOR_RUN_TIME() RESULT_SIZE public static String RESULT_SIZE() JVM_GC_TIME public static String JVM_GC_TIME() RESULT_SERIALIZATION_TIME public static String RESULT_SERIALIZATION_TIME() MEMORY_BYTES_SPILLED public static String MEMORY_BYTES_SPILLED() DISK_BYTES_SPILLED public static String DISK_BYTES_SPILLED() PEAK_EXECUTION_MEMORY public static String PEAK_EXECUTION_MEMORY() UPDATED_BLOCK_STATUSES public static String UPDATED_BLOCK_STATUSES() TEST_ACCUM public static String TEST_ACCUM() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalAccumulator.input$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalAccumulator.input$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class InternalAccumulator.input$ Object org.apache.spark.InternalAccumulator.input$ Enclosing class: InternalAccumulator public static class InternalAccumulator.input$ extends Object Field Summary Fields  Modifier and Type Field and Description static InternalAccumulator.input$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description InternalAccumulator.input$()  Method Summary Methods  Modifier and Type Method and Description String BYTES_READ()  String RECORDS_READ()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final InternalAccumulator.input$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail InternalAccumulator.input$ public InternalAccumulator.input$() Method Detail BYTES_READ public String BYTES_READ() RECORDS_READ public String RECORDS_READ() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalAccumulator.output$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalAccumulator.output$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class InternalAccumulator.output$ Object org.apache.spark.InternalAccumulator.output$ Enclosing class: InternalAccumulator public static class InternalAccumulator.output$ extends Object Field Summary Fields  Modifier and Type Field and Description static InternalAccumulator.output$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description InternalAccumulator.output$()  Method Summary Methods  Modifier and Type Method and Description String BYTES_WRITTEN()  String RECORDS_WRITTEN()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final InternalAccumulator.output$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail InternalAccumulator.output$ public InternalAccumulator.output$() Method Detail BYTES_WRITTEN public String BYTES_WRITTEN() RECORDS_WRITTEN public String RECORDS_WRITTEN() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalAccumulator.shuffleRead$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalAccumulator.shuffleRead$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class InternalAccumulator.shuffleRead$ Object org.apache.spark.InternalAccumulator.shuffleRead$ Enclosing class: InternalAccumulator public static class InternalAccumulator.shuffleRead$ extends Object Field Summary Fields  Modifier and Type Field and Description static InternalAccumulator.shuffleRead$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description InternalAccumulator.shuffleRead$()  Method Summary Methods  Modifier and Type Method and Description String FETCH_WAIT_TIME()  String LOCAL_BLOCKS_FETCHED()  String LOCAL_BYTES_READ()  String RECORDS_READ()  String REMOTE_BLOCKS_FETCHED()  String REMOTE_BYTES_READ()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final InternalAccumulator.shuffleRead$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail InternalAccumulator.shuffleRead$ public InternalAccumulator.shuffleRead$() Method Detail REMOTE_BLOCKS_FETCHED public String REMOTE_BLOCKS_FETCHED() LOCAL_BLOCKS_FETCHED public String LOCAL_BLOCKS_FETCHED() REMOTE_BYTES_READ public String REMOTE_BYTES_READ() LOCAL_BYTES_READ public String LOCAL_BYTES_READ() FETCH_WAIT_TIME public String FETCH_WAIT_TIME() RECORDS_READ public String RECORDS_READ() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalAccumulator.shuffleWrite$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalAccumulator.shuffleWrite$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class InternalAccumulator.shuffleWrite$ Object org.apache.spark.InternalAccumulator.shuffleWrite$ Enclosing class: InternalAccumulator public static class InternalAccumulator.shuffleWrite$ extends Object Field Summary Fields  Modifier and Type Field and Description static InternalAccumulator.shuffleWrite$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description InternalAccumulator.shuffleWrite$()  Method Summary Methods  Modifier and Type Method and Description String BYTES_WRITTEN()  String RECORDS_WRITTEN()  String WRITE_TIME()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final InternalAccumulator.shuffleWrite$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail InternalAccumulator.shuffleWrite$ public InternalAccumulator.shuffleWrite$() Method Detail BYTES_WRITTEN public String BYTES_WRITTEN() RECORDS_WRITTEN public String RECORDS_WRITTEN() WRITE_TIME public String WRITE_TIME() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalNode (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalNode (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class InternalNode Object org.apache.spark.ml.tree.Node org.apache.spark.ml.tree.InternalNode All Implemented Interfaces: java.io.Serializable public class InternalNode extends Node Internal Decision Tree node. param: prediction Prediction this node would make if it were a leaf node param: impurity Impurity measure at this node (for training data) param: gain Information gain value. Values less than 0 indicate missing values; this quirk will be removed with future updates. param: leftChild Left-hand child node param: rightChild Right-hand child node param: split Information about the test used to split to the left or right child. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description double gain()  double impurity() Impurity measure at this node (for training data) Node leftChild()  double prediction() Prediction a leaf node makes, or which an internal node would make if it were a leaf node Node rightChild()  Split split()  static int subtreeToString$default$1()  String toString()  Methods inherited from class org.apache.spark.ml.tree.Node fromOld Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Method Detail subtreeToString$default$1 public static int subtreeToString$default$1() prediction public double prediction() Description copied from class: Node Prediction a leaf node makes, or which an internal node would make if it were a leaf node Specified by: prediction in class Node impurity public double impurity() Description copied from class: Node Impurity measure at this node (for training data) Specified by: impurity in class Node gain public double gain() leftChild public Node leftChild() rightChild public Node rightChild() split public Split split() toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalOutputModes.Append$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalOutputModes.Append$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class InternalOutputModes.Append$ Object org.apache.spark.sql.streaming.OutputMode org.apache.spark.sql.InternalOutputModes.Append$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: InternalOutputModes public static class InternalOutputModes.Append$ extends OutputMode implements scala.Product, scala.Serializable OutputMode in which only the new rows in the streaming DataFrame/Dataset will be written to the sink. This output mode can be only be used in queries that do not contain any aggregation. See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static InternalOutputModes.Append$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description InternalOutputModes.Append$()  Method Summary Methods inherited from class org.apache.spark.sql.streaming.OutputMode Append, Complete Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final InternalOutputModes.Append$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail InternalOutputModes.Append$ public InternalOutputModes.Append$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalOutputModes.Complete$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalOutputModes.Complete$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class InternalOutputModes.Complete$ Object org.apache.spark.sql.streaming.OutputMode org.apache.spark.sql.InternalOutputModes.Complete$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: InternalOutputModes public static class InternalOutputModes.Complete$ extends OutputMode implements scala.Product, scala.Serializable OutputMode in which all the rows in the streaming DataFrame/Dataset will be written to the sink every time these is some updates. This output mode can only be used in queries that contain aggregations. See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static InternalOutputModes.Complete$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description InternalOutputModes.Complete$()  Method Summary Methods inherited from class org.apache.spark.sql.streaming.OutputMode Append, Complete Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final InternalOutputModes.Complete$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail InternalOutputModes.Complete$ public InternalOutputModes.Complete$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalOutputModes.Update$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalOutputModes.Update$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class InternalOutputModes.Update$ Object org.apache.spark.sql.streaming.OutputMode org.apache.spark.sql.InternalOutputModes.Update$ All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: InternalOutputModes public static class InternalOutputModes.Update$ extends OutputMode implements scala.Product, scala.Serializable OutputMode in which only the rows in the streaming DataFrame/Dataset that were updated will be written to the sink every time these is some updates. This output mode can only be used in queries that contain aggregations. See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static InternalOutputModes.Update$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description InternalOutputModes.Update$()  Method Summary Methods inherited from class org.apache.spark.sql.streaming.OutputMode Append, Complete Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final InternalOutputModes.Update$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail InternalOutputModes.Update$ public InternalOutputModes.Update$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InternalOutputModes (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InternalOutputModes (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class InternalOutputModes Object org.apache.spark.sql.InternalOutputModes public class InternalOutputModes extends Object Internal helper class to generate objects representing various OutputModes, Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  InternalOutputModes.Append$ OutputMode in which only the new rows in the streaming DataFrame/Dataset will be written to the sink. static class  InternalOutputModes.Complete$ OutputMode in which all the rows in the streaming DataFrame/Dataset will be written to the sink every time these is some updates. static class  InternalOutputModes.Update$ OutputMode in which only the rows in the streaming DataFrame/Dataset that were updated will be written to the sink every time these is some updates. Constructor Summary Constructors  Constructor and Description InternalOutputModes()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail InternalOutputModes public InternalOutputModes() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method InterruptibleIterator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="InterruptibleIterator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class InterruptibleIterator<T> Object org.apache.spark.InterruptibleIterator<T> All Implemented Interfaces: scala.collection.GenTraversableOnce<T>, scala.collection.Iterator<T>, scala.collection.TraversableOnce<T> public class InterruptibleIterator<T> extends Object implements scala.collection.Iterator<T> :: DeveloperApi :: An iterator that wraps around an existing iterator to provide task killing functionality. It works by checking the interrupted flag in TaskContext. Nested Class Summary Nested classes/interfaces inherited from interface scala.collection.Iterator scala.collection.Iterator.ConcatIterator<A>, scala.collection.Iterator.GroupedIterator<B>, scala.collection.Iterator.JoinIterator<A> Nested classes/interfaces inherited from interface scala.collection.TraversableOnce scala.collection.TraversableOnce.BufferedCanBuildFrom<A,CC extends scala.collection.TraversableOnce<Object>>, scala.collection.TraversableOnce.FlattenOps<A>, scala.collection.TraversableOnce.ForceImplicitAmbiguity, scala.collection.TraversableOnce.MonadOps<A>, scala.collection.TraversableOnce.OnceCanBuildFrom<A> Constructor Summary Constructors  Constructor and Description InterruptibleIterator(TaskContext context, scala.collection.Iterator<T> delegate)  Method Summary Methods  Modifier and Type Method and Description TaskContext context()  scala.collection.Iterator<T> delegate()  boolean hasNext()  T next()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.collection.Iterator $plus$plus, buffered, collect, contains, copyToArray, corresponds, drop, dropWhile, duplicate, exists, filter, filterNot, find, flatMap, forall, foreach, grouped, hasDefiniteSize, indexOf, indexWhere, isEmpty, isTraversableAgain, length, map, padTo, partition, patch, sameElements, scanLeft, scanRight, seq, slice, sliding, sliding$default$2, span, take, takeWhile, toIterator, toStream, toString, toTraversable, withFilter, zip, zipAll, zipWithIndex Methods inherited from interface scala.collection.TraversableOnce $colon$bslash, $div$colon, addString, addString, addString, aggregate, collectFirst, copyToArray, copyToArray, copyToBuffer, count, fold, foldLeft, foldRight, max, maxBy, min, minBy, mkString, mkString, mkString, nonEmpty, product, reduce, reduceLeft, reduceLeftOption, reduceOption, reduceRight, reduceRightOption, reversed, size, sum, to, toArray, toBuffer, toIndexedSeq, toIterable, toList, toMap, toSeq, toSet, toVector Constructor Detail InterruptibleIterator public InterruptibleIterator(TaskContext context, scala.collection.Iterator<T> delegate) Method Detail context public TaskContext context() delegate public scala.collection.Iterator<T> delegate() hasNext public boolean hasNext() Specified by: hasNext in interface scala.collection.Iterator<T> next public T next() Specified by: next in interface scala.collection.Iterator<T> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IsNotNull (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IsNotNull (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class IsNotNull Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.IsNotNull All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class IsNotNull extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff the attribute evaluates to a non-null value. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description IsNotNull(String attribute)  Method Summary Methods  Modifier and Type Method and Description String attribute()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail IsNotNull public IsNotNull(String attribute) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() attribute public String attribute() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IsNull (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IsNull (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class IsNull Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.IsNull All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class IsNull extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff the attribute evaluates to null. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description IsNull(String attribute)  Method Summary Methods  Modifier and Type Method and Description String attribute()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail IsNull public IsNull(String attribute) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() attribute public String attribute() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IsotonicRegression (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IsotonicRegression (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression Class IsotonicRegression Object org.apache.spark.mllib.regression.IsotonicRegression All Implemented Interfaces: java.io.Serializable public class IsotonicRegression extends Object implements java.io.Serializable Isotonic regression. Currently implemented using parallelized pool adjacent violators algorithm. Only univariate (single feature) algorithm supported. Sequential PAV implementation based on: Tibshirani, Ryan J., Holger Hoefling, and Robert Tibshirani. "Nearly-isotonic regression." Technometrics 53.1 (2011): 54-61. Available from http://www.stat.cmu.edu/~ryantibs/papers/neariso.pdf Sequential PAV parallelization based on: Kearsley, Anthony J., Richard A. Tapia, and Michael W. Trosset. "An approach to parallelizing isotonic regression." Applied Mathematics and Parallel Computing. Physica-Verlag HD, 1996. 141-147. Available from http://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR96640.pdf See Also:, Serialized Form Constructor Summary Constructors  Constructor and Description IsotonicRegression() Constructs IsotonicRegression instance with default parameter isotonic = true. Method Summary Methods  Modifier and Type Method and Description IsotonicRegressionModel run(JavaRDD<scala.Tuple3<Double,Double,Double>> input) Run pool adjacent violators algorithm to obtain isotonic regression model. IsotonicRegressionModel run(RDD<scala.Tuple3<Object,Object,Object>> input) Run IsotonicRegression algorithm to obtain isotonic regression model. IsotonicRegression setIsotonic(boolean isotonic) Sets the isotonic parameter. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail IsotonicRegression public IsotonicRegression() Constructs IsotonicRegression instance with default parameter isotonic = true. Method Detail setIsotonic public IsotonicRegression setIsotonic(boolean isotonic) Sets the isotonic parameter. Parameters:isotonic - Isotonic (increasing) or antitonic (decreasing) sequence. Returns:This instance of IsotonicRegression. run public IsotonicRegressionModel run(RDD<scala.Tuple3<Object,Object,Object>> input) Run IsotonicRegression algorithm to obtain isotonic regression model. Parameters:input - RDD of tuples (label, feature, weight) where label is dependent variable for which we calculate isotonic regression, feature is independent variable and weight represents number of measures with default 1. If multiple labels share the same feature value then they are ordered before the algorithm is executed. Returns:Isotonic regression model. run public IsotonicRegressionModel run(JavaRDD<scala.Tuple3<Double,Double,Double>> input) Run pool adjacent violators algorithm to obtain isotonic regression model. Parameters:input - JavaRDD of tuples (label, feature, weight) where label is dependent variable for which we calculate isotonic regression, feature is independent variable and weight represents number of measures with default 1. If multiple labels share the same feature value then they are ordered before the algorithm is executed. Returns:Isotonic regression model. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method IsotonicRegressionModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="IsotonicRegressionModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression Class IsotonicRegressionModel Object org.apache.spark.mllib.regression.IsotonicRegressionModel All Implemented Interfaces: java.io.Serializable, Saveable public class IsotonicRegressionModel extends Object implements java.io.Serializable, Saveable Regression model for isotonic regression. param: boundaries Array of boundaries for which predictions are known. Boundaries must be sorted in increasing order. param: predictions Array of predictions associated to the boundaries at the same index. Results of isotonic regression and therefore monotone. param: isotonic indicates whether this is isotonic or antitonic. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description IsotonicRegressionModel(double[] boundaries, double[] predictions, boolean isotonic)  IsotonicRegressionModel(Iterable<Object> boundaries, Iterable<Object> predictions, Boolean isotonic) A Java-friendly constructor that takes two Iterable parameters and one Boolean parameter. Method Summary Methods  Modifier and Type Method and Description double[] boundaries()  boolean isotonic()  static IsotonicRegressionModel load(SparkContext sc, String path)  double predict(double testData) Predict a single label. JavaDoubleRDD predict(JavaDoubleRDD testData) Predict labels for provided features. RDD<Object> predict(RDD<Object> testData) Predict labels for provided features. double[] predictions()  void save(SparkContext sc, String path) Save this model to the given path. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail IsotonicRegressionModel public IsotonicRegressionModel(double[] boundaries, double[] predictions, boolean isotonic) IsotonicRegressionModel public IsotonicRegressionModel(Iterable<Object> boundaries, Iterable<Object> predictions, Boolean isotonic) A Java-friendly constructor that takes two Iterable parameters and one Boolean parameter. Parameters:boundaries - (undocumented)predictions - (undocumented)isotonic - (undocumented) Method Detail load public static IsotonicRegressionModel load(SparkContext sc, String path) boundaries public double[] boundaries() predictions public double[] predictions() isotonic public boolean isotonic() predict public RDD<Object> predict(RDD<Object> testData) Predict labels for provided features. Using a piecewise linear function. Parameters:testData - Features to be labeled. Returns:Predicted labels. predict public JavaDoubleRDD predict(JavaDoubleRDD testData) Predict labels for provided features. Using a piecewise linear function. Parameters:testData - Features to be labeled. Returns:Predicted labels. predict public double predict(double testData) Predict a single label. Using a piecewise linear function. Parameters:testData - Feature to be labeled. Returns:Predicted label. 1) If testData exactly matches a boundary then associated prediction is returned. In case there are multiple predictions with the same boundary then one of them is returned. Which one is undefined (same as java.util.Arrays.binarySearch). 2) If testData is lower or higher than all boundaries then first or last prediction is returned respectively. In case there are multiple predictions with the same boundary then the lowest or highest is returned respectively. 3) If testData falls between two values in boundary array then prediction is treated as piecewise linear function and interpolated value is returned. In case there are multiple values with the same boundary then the same rules as in 2) are used. save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JVMObjectTracker (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JVMObjectTracker (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.r Class JVMObjectTracker Object org.apache.spark.api.r.JVMObjectTracker public class JVMObjectTracker extends Object Helper singleton that tracks JVM objects returned to R. This is useful for referencing these objects in RPC calls. Constructor Summary Constructors  Constructor and Description JVMObjectTracker()  Method Summary Methods  Modifier and Type Method and Description static scala.Option<Object> get(String id)  static Object getObject(String id)  static String put(Object obj)  static scala.Option<Object> remove(String id)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JVMObjectTracker public JVMObjectTracker() Method Detail getObject public static Object getObject(String id) get public static scala.Option<Object> get(String id) put public static String put(Object obj) remove public static scala.Option<Object> remove(String id) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.api.java Class JavaDStream<T> Object org.apache.spark.streaming.api.java.JavaDStream<T> All Implemented Interfaces: java.io.Serializable, JavaDStreamLike<T,JavaDStream<T>,JavaRDD<T>> Direct Known Subclasses: JavaInputDStream, JavaMapWithStateDStream public class JavaDStream<T> extends Object A Java-friendly interface to DStream, the basic abstraction in Spark Streaming that represents a continuous stream of data. DStreams can either be created from live data (such as, data from TCP sockets, Kafka, Flume, etc.) or it can be generated by transforming existing DStreams using operations such as map, window. For operations applicable to key-value pair DStreams, see JavaPairDStream. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaDStream(DStream<T> dstream, scala.reflect.ClassTag<T> classTag)  Method Summary Methods  Modifier and Type Method and Description JavaDStream<T> cache() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) static DStream<T> checkpoint(Duration interval)  scala.reflect.ClassTag<T> classTag()  JavaRDD<T> compute(Time validTime) Generate an RDD for the given duration static StreamingContext context()  static JavaDStream<Long> count()  static JavaPairDStream<T,Long> countByValue()  static JavaPairDStream<T,Long> countByValue(int numPartitions)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration)  DStream<T> dstream()  JavaDStream<T> filter(Function<T,Boolean> f) Return a new DStream containing only the elements that satisfy a predicate. static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f)  static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f)  static void foreachRDD(VoidFunction<R> foreachFunc)  static void foreachRDD(VoidFunction2<R,Time> foreachFunc)  static <T> JavaDStream<T> fromDStream(DStream<T> dstream, scala.reflect.ClassTag<T> evidence$1) Convert a scala DStream to a Java-friendly JavaDStream. static JavaDStream<java.util.List<T>> glom()  static <R> JavaDStream<R> map(Function<T,R> f)  static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f)  static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f)  static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f)  JavaDStream<T> persist() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) JavaDStream<T> persist(StorageLevel storageLevel) Persist the RDDs of this DStream with the given storage level static void print()  static void print(int num)  static JavaDStream<T> reduce(Function2<T,T,T> f)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration)  JavaDStream<T> repartition(int numPartitions) Return a new DStream with an increased or decreased level of parallelism. static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in)  static java.util.List<R> slice(Time fromTime, Time toTime)  static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc)  static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc)  static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc)  static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc)  static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc)  JavaDStream<T> union(JavaDStream<T> that) Return a new DStream by unifying data of another DStream with this DStream. JavaDStream<T> window(Duration windowDuration) Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream. JavaDStream<T> window(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream. JavaRDD<T> wrapRDD(RDD<T> rdd)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.streaming.api.java.JavaDStreamLike checkpoint, context, count, countByValue, countByValue, countByValueAndWindow, countByValueAndWindow, countByWindow, flatMap, flatMapToPair, foreachRDD, foreachRDD, glom, map, mapPartitions, mapPartitionsToPair, mapToPair, print, print, reduce, reduceByWindow, reduceByWindow, scalaIntToJavaLong, slice, transform, transform, transformToPair, transformToPair, transformWith, transformWith, transformWithToPair, transformWithToPair Constructor Detail JavaDStream public JavaDStream(DStream<T> dstream, scala.reflect.ClassTag<T> classTag) Method Detail fromDStream public static <T> JavaDStream<T> fromDStream(DStream<T> dstream, scala.reflect.ClassTag<T> evidence$1) Convert a scala DStream to a Java-friendly JavaDStream. Parameters:dstream - (undocumented)evidence$1 - (undocumented) Returns:(undocumented) scalaIntToJavaLong public static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in) print public static void print() print public static void print(int num) count public static JavaDStream<Long> count() countByValue public static JavaPairDStream<T,Long> countByValue() countByValue public static JavaPairDStream<T,Long> countByValue(int numPartitions) countByWindow public static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) glom public static JavaDStream<java.util.List<T>> glom() context public static StreamingContext context() map public static <R> JavaDStream<R> map(Function<T,R> f) mapToPair public static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f) flatMap public static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f) flatMapToPair public static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) mapPartitions public static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) mapPartitionsToPair public static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) reduce public static JavaDStream<T> reduce(Function2<T,T,T> f) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) slice public static java.util.List<R> slice(Time fromTime, Time toTime) foreachRDD public static void foreachRDD(VoidFunction<R> foreachFunc) foreachRDD public static void foreachRDD(VoidFunction2<R,Time> foreachFunc) transform public static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc) transform public static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc) checkpoint public static DStream<T> checkpoint(Duration interval) dstream public DStream<T> dstream() classTag public scala.reflect.ClassTag<T> classTag() wrapRDD public JavaRDD<T> wrapRDD(RDD<T> rdd) filter public JavaDStream<T> filter(Function<T,Boolean> f) Return a new DStream containing only the elements that satisfy a predicate. cache public JavaDStream<T> cache() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) persist public JavaDStream<T> persist() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) persist public JavaDStream<T> persist(StorageLevel storageLevel) Persist the RDDs of this DStream with the given storage level compute public JavaRDD<T> compute(Time validTime) Generate an RDD for the given duration window public JavaDStream<T> window(Duration windowDuration) Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream. The new DStream generates RDDs with the same interval as this DStream. Parameters:windowDuration - width of the window; must be a multiple of this DStream's interval. Returns:(undocumented) window public JavaDStream<T> window(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD contains all the elements in seen in a sliding window of time over this DStream. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) union public JavaDStream<T> union(JavaDStream<T> that) Return a new DStream by unifying data of another DStream with this DStream. Parameters:that - Another DStream having the same interval (i.e., slideDuration) as this DStream. Returns:(undocumented) repartition public JavaDStream<T> repartition(int numPartitions) Return a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaDStreamLike (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaDStreamLike (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.api.java Interface JavaDStreamLike<T,This extends JavaDStreamLike<T,This,R>,R extends JavaRDDLike<T,R>> All Superinterfaces: java.io.Serializable All Known Implementing Classes: JavaDStream, JavaInputDStream, JavaMapWithStateDStream, JavaPairDStream, JavaPairInputDStream, JavaPairReceiverInputDStream, JavaReceiverInputDStream public interface JavaDStreamLike<T,This extends JavaDStreamLike<T,This,R>,R extends JavaRDDLike<T,R>> extends scala.Serializable Method Summary Methods  Modifier and Type Method and Description DStream<T> checkpoint(Duration interval) Enable periodic checkpointing of RDDs of this DStream. scala.reflect.ClassTag<T> classTag()  StreamingContext context() Return the StreamingContext associated with this DStream JavaDStream<Long> count() Return a new DStream in which each RDD has a single element generated by counting each RDD of this DStream. JavaPairDStream<T,Long> countByValue() Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream. JavaPairDStream<T,Long> countByValue(int numPartitions) Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream. JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD contains the count of distinct elements in RDDs in a sliding window over this DStream. JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream in which each RDD contains the count of distinct elements in RDDs in a sliding window over this DStream. JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by counting the number of elements in a window over this DStream. DStream<T> dstream()  <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f) Return a new DStream by applying a function to all elements of this DStream, and then flattening the results <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) Return a new DStream by applying a function to all elements of this DStream, and then flattening the results void foreachRDD(VoidFunction<R> foreachFunc) Apply a function to each RDD in this DStream. void foreachRDD(VoidFunction2<R,Time> foreachFunc) Apply a function to each RDD in this DStream. JavaDStream<java.util.List<T>> glom() Return a new DStream in which each RDD is generated by applying glom() to each RDD of this DStream. <R> JavaDStream<R> map(Function<T,R> f) Return a new DStream by applying a function to all elements of this DStream. <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) Return a new DStream in which each RDD is generated by applying mapPartitions() to each RDDs of this DStream. <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) Return a new DStream in which each RDD is generated by applying mapPartitions() to each RDDs of this DStream. <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f) Return a new DStream by applying a function to all elements of this DStream. void print() Print the first ten elements of each RDD generated in this DStream. void print(int num) Print the first num elements of each RDD generated in this DStream. JavaDStream<T> reduce(Function2<T,T,T> f) Return a new DStream in which each RDD has a single element generated by reducing each RDD of this DStream. JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream. JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream. JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in)  java.util.List<R> slice(Time fromTime, Time toTime) Return all the RDDs between 'fromDuration' to 'toDuration' (both included) <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. R wrapRDD(RDD<T> in)  Method Detail classTag scala.reflect.ClassTag<T> classTag() dstream DStream<T> dstream() wrapRDD R wrapRDD(RDD<T> in) scalaIntToJavaLong JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in) print void print() Print the first ten elements of each RDD generated in this DStream. This is an output operator, so this DStream will be registered as an output stream and there materialized. print void print(int num) Print the first num elements of each RDD generated in this DStream. This is an output operator, so this DStream will be registered as an output stream and there materialized. Parameters:num - (undocumented) count JavaDStream<Long> count() Return a new DStream in which each RDD has a single element generated by counting each RDD of this DStream. Returns:(undocumented) countByValue JavaPairDStream<T,Long> countByValue() Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Returns:(undocumented) countByValue JavaPairDStream<T,Long> countByValue(int numPartitions) Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:numPartitions - number of partitions of each RDD in the new DStream. Returns:(undocumented) countByWindow JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by counting the number of elements in a window over this DStream. windowDuration and slideDuration are as defined in the window() operation. This is equivalent to window(windowDuration, slideDuration).count() Parameters:windowDuration - (undocumented)slideDuration - (undocumented) Returns:(undocumented) countByValueAndWindow JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD contains the count of distinct elements in RDDs in a sliding window over this DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) countByValueAndWindow JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream in which each RDD contains the count of distinct elements in RDDs in a sliding window over this DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalnumPartitions - number of partitions of each RDD in the new DStream. Returns:(undocumented) glom JavaDStream<java.util.List<T>> glom() Return a new DStream in which each RDD is generated by applying glom() to each RDD of this DStream. Applying glom() to an RDD coalesces all elements within each partition into an array. Returns:(undocumented) context StreamingContext context() Return the StreamingContext associated with this DStream map <R> JavaDStream<R> map(Function<T,R> f) Return a new DStream by applying a function to all elements of this DStream. mapToPair <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f) Return a new DStream by applying a function to all elements of this DStream. flatMap <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f) Return a new DStream by applying a function to all elements of this DStream, and then flattening the results Parameters:f - (undocumented) Returns:(undocumented) flatMapToPair <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) Return a new DStream by applying a function to all elements of this DStream, and then flattening the results Parameters:f - (undocumented) Returns:(undocumented) mapPartitions <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) Return a new DStream in which each RDD is generated by applying mapPartitions() to each RDDs of this DStream. Applying mapPartitions() to an RDD applies a function to each partition of the RDD. Parameters:f - (undocumented) Returns:(undocumented) mapPartitionsToPair <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) Return a new DStream in which each RDD is generated by applying mapPartitions() to each RDDs of this DStream. Applying mapPartitions() to an RDD applies a function to each partition of the RDD. Parameters:f - (undocumented) Returns:(undocumented) reduce JavaDStream<T> reduce(Function2<T,T,T> f) Return a new DStream in which each RDD has a single element generated by reducing each RDD of this DStream. Parameters:f - (undocumented) Returns:(undocumented) reduceByWindow JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream. Parameters:reduceFunc - associative and commutative reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) reduceByWindow JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream in which each RDD has a single element generated by reducing all elements in a sliding window over this DStream. However, the reduction is done incrementally using the old window's reduced value : 1. reduce the new values that entered the window (e.g., adding new counts) 2. "inverse reduce" the old values that left the window (e.g., subtracting old counts) This is more efficient than reduceByWindow without "inverse reduce" function. However, it is applicable to only "invertible reduce functions". Parameters:reduceFunc - associative and commutative reduce functioninvReduceFunc - inverse reduce function; such that for all y, invertible x: invReduceFunc(reduceFunc(x, y), x) = ywindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) slice java.util.List<R> slice(Time fromTime, Time toTime) Return all the RDDs between 'fromDuration' to 'toDuration' (both included) Parameters:fromTime - (undocumented)toTime - (undocumented) Returns:(undocumented) foreachRDD void foreachRDD(VoidFunction<R> foreachFunc) Apply a function to each RDD in this DStream. This is an output operator, so 'this' DStream will be registered as an output stream and therefore materialized. Parameters:foreachFunc - (undocumented) foreachRDD void foreachRDD(VoidFunction2<R,Time> foreachFunc) Apply a function to each RDD in this DStream. This is an output operator, so 'this' DStream will be registered as an output stream and therefore materialized. Parameters:foreachFunc - (undocumented) transform <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. Parameters:transformFunc - (undocumented) Returns:(undocumented) transform <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. Parameters:transformFunc - (undocumented) Returns:(undocumented) transformToPair <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. Parameters:transformFunc - (undocumented) Returns:(undocumented) transformToPair <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream. Parameters:transformFunc - (undocumented) Returns:(undocumented) transformWith <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. Parameters:other - (undocumented)transformFunc - (undocumented) Returns:(undocumented) transformWithToPair <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. Parameters:other - (undocumented)transformFunc - (undocumented) Returns:(undocumented) transformWith <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. Parameters:other - (undocumented)transformFunc - (undocumented) Returns:(undocumented) transformWithToPair <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc) Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream and 'other' DStream. Parameters:other - (undocumented)transformFunc - (undocumented) Returns:(undocumented) checkpoint DStream<T> checkpoint(Duration interval) Enable periodic checkpointing of RDDs of this DStream. Parameters:interval - Time interval after which generated RDD will be checkpointed Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaDoubleRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaDoubleRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class JavaDoubleRDD Object org.apache.spark.api.java.JavaDoubleRDD All Implemented Interfaces: java.io.Serializable, JavaRDDLike<Double,JavaDoubleRDD> public class JavaDoubleRDD extends Object See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaDoubleRDD(RDD<Object> srdd)  Method Summary Methods  Modifier and Type Method and Description static <U> U aggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp)  JavaDoubleRDD cache() Persist this RDD with the default storage level (`MEMORY_ONLY`). static <U> JavaPairRDD<T,U> cartesian(JavaRDDLike<U,?> other)  static void checkpoint()  scala.reflect.ClassTag<Double> classTag()  JavaDoubleRDD coalesce(int numPartitions) Return a new RDD that is reduced into numPartitions partitions. JavaDoubleRDD coalesce(int numPartitions, boolean shuffle) Return a new RDD that is reduced into numPartitions partitions. static java.util.List<T> collect()  static JavaFutureAction<java.util.List<T>> collectAsync()  static java.util.List<T>[] collectPartitions(int[] partitionIds)  static SparkContext context()  static long count()  static PartialResult<BoundedDouble> countApprox(long timeout)  static PartialResult<BoundedDouble> countApprox(long timeout, double confidence)  static long countApproxDistinct(double relativeSD)  static JavaFutureAction<Long> countAsync()  static java.util.Map<T,Long> countByValue()  static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout)  static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence)  JavaDoubleRDD distinct() Return a new RDD containing the distinct elements in this RDD. JavaDoubleRDD distinct(int numPartitions) Return a new RDD containing the distinct elements in this RDD. JavaDoubleRDD filter(Function<Double,Boolean> f) Return a new RDD containing only the elements that satisfy a predicate. Double first() Return the first element in this RDD. static <U> JavaRDD<U> flatMap(FlatMapFunction<T,U> f)  static JavaDoubleRDD flatMapToDouble(DoubleFlatMapFunction<T> f)  static <K2,V2> JavaPairRDD<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f)  static T fold(T zeroValue, Function2<T,T,T> f)  static void foreach(VoidFunction<T> f)  static JavaFutureAction<Void> foreachAsync(VoidFunction<T> f)  static void foreachPartition(VoidFunction<java.util.Iterator<T>> f)  static JavaFutureAction<Void> foreachPartitionAsync(VoidFunction<java.util.Iterator<T>> f)  static JavaDoubleRDD fromRDD(RDD<Object> rdd)  static Optional<String> getCheckpointFile()  static int getNumPartitions()  static StorageLevel getStorageLevel()  static JavaRDD<java.util.List<T>> glom()  static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f)  static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f, int numPartitions)  long[] histogram(double[] buckets) Compute a histogram using the provided buckets. long[] histogram(Double[] buckets, boolean evenBuckets)  scala.Tuple2<double[],long[]> histogram(int bucketCount) Compute a histogram of the data using bucketCount number of buckets evenly spaced between the minimum and maximum of the RDD. static int id()  JavaDoubleRDD intersection(JavaDoubleRDD other) Return the intersection of this RDD and another one. static boolean isCheckpointed()  static boolean isEmpty()  static java.util.Iterator<T> iterator(Partition split, TaskContext taskContext)  static <U> JavaPairRDD<U,T> keyBy(Function<T,U> f)  static <R> JavaRDD<R> map(Function<T,R> f)  static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f)  static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f, boolean preservesPartitioning)  static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f)  static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f, boolean preservesPartitioning)  static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f)  static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f, boolean preservesPartitioning)  static <R> JavaRDD<R> mapPartitionsWithIndex(Function2<Integer,java.util.Iterator<T>,java.util.Iterator<R>> f, boolean preservesPartitioning)  static <R> boolean mapPartitionsWithIndex$default$2()  static <R> JavaDoubleRDD mapToDouble(DoubleFunction<T> f)  static <K2,V2> JavaPairRDD<K2,V2> mapToPair(PairFunction<T,K2,V2> f)  Double max() Returns the maximum element from this RDD as defined by the default comparator natural order. Double mean() Compute the mean of this RDD's elements. PartialResult<BoundedDouble> meanApprox(long timeout) Approximate operation to return the mean within a timeout. PartialResult<BoundedDouble> meanApprox(long timeout, Double confidence) Return the approximate mean of the elements in this RDD. Double min() Returns the minimum element from this RDD as defined by the default comparator natural order. static String name()  static Optional<Partitioner> partitioner()  static java.util.List<Partition> partitions()  JavaDoubleRDD persist(StorageLevel newLevel) Set this RDD's storage level to persist its values across operations after the first time it is computed. static JavaRDD<String> pipe(java.util.List<String> command)  static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env)  static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize)  static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize, String encoding)  static JavaRDD<String> pipe(String command)  RDD<Double> rdd()  static T reduce(Function2<T,T,T> f)  JavaDoubleRDD repartition(int numPartitions) Return a new RDD that has exactly numPartitions partitions. JavaDoubleRDD sample(boolean withReplacement, Double fraction) Return a sampled subset of this RDD. JavaDoubleRDD sample(boolean withReplacement, Double fraction, long seed) Return a sampled subset of this RDD. Double sampleStdev() Compute the sample standard deviation of this RDD's elements (which corrects for bias in estimating the standard deviation by dividing by N-1 instead of N). Double sampleVariance() Compute the sample variance of this RDD's elements (which corrects for bias in estimating the standard variance by dividing by N-1 instead of N). static void saveAsObjectFile(String path)  static void saveAsTextFile(String path)  static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec)  JavaDoubleRDD setName(String name) Assign a name to this RDD RDD<Object> srdd()  StatCounter stats() Return a StatCounter object that captures the mean, variance and count of the RDD's elements in one operation. Double stdev() Compute the standard deviation of this RDD's elements. JavaDoubleRDD subtract(JavaDoubleRDD other) Return an RDD with the elements from this that are not in other. JavaDoubleRDD subtract(JavaDoubleRDD other, int numPartitions) Return an RDD with the elements from this that are not in other. JavaDoubleRDD subtract(JavaDoubleRDD other, Partitioner p) Return an RDD with the elements from this that are not in other. Double sum() Add up the elements in this RDD. PartialResult<BoundedDouble> sumApprox(long timeout) Approximate operation to return the sum within a timeout. PartialResult<BoundedDouble> sumApprox(long timeout, Double confidence) Approximate operation to return the sum within a timeout. static java.util.List<T> take(int num)  static JavaFutureAction<java.util.List<T>> takeAsync(int num)  static java.util.List<T> takeOrdered(int num)  static java.util.List<T> takeOrdered(int num, java.util.Comparator<T> comp)  static java.util.List<T> takeSample(boolean withReplacement, int num)  static java.util.List<T> takeSample(boolean withReplacement, int num, long seed)  static String toDebugString()  static java.util.Iterator<T> toLocalIterator()  static java.util.List<T> top(int num)  static java.util.List<T> top(int num, java.util.Comparator<T> comp)  static RDD<Object> toRDD(JavaDoubleRDD rdd)  static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp)  static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp, int depth)  static T treeReduce(Function2<T,T,T> f)  static T treeReduce(Function2<T,T,T> f, int depth)  JavaDoubleRDD union(JavaDoubleRDD other) Return the union of this RDD and another one. JavaDoubleRDD unpersist() Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. JavaDoubleRDD unpersist(boolean blocking) Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. Double variance() Compute the variance of this RDD's elements. JavaDoubleRDD wrapRDD(RDD<Double> rdd)  static <U> JavaPairRDD<T,U> zip(JavaRDDLike<U,?> other)  static <U,V> JavaRDD<V> zipPartitions(JavaRDDLike<U,?> other, FlatMapFunction2<java.util.Iterator<T>,java.util.Iterator<U>,V> f)  static JavaPairRDD<T,Long> zipWithIndex()  static JavaPairRDD<T,Long> zipWithUniqueId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.api.java.JavaRDDLike aggregate, cartesian, checkpoint, collect, collectAsync, collectPartitions, context, count, countApprox, countApprox, countApproxDistinct, countAsync, countByValue, countByValueApprox, countByValueApprox, flatMap, flatMapToDouble, flatMapToPair, fold, foreach, foreachAsync, foreachPartition, foreachPartitionAsync, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, id, isCheckpointed, isEmpty, iterator, keyBy, map, mapPartitions, mapPartitions, mapPartitionsToDouble, mapPartitionsToDouble, mapPartitionsToPair, mapPartitionsToPair, mapPartitionsWithIndex, mapToDouble, mapToPair, max, min, name, partitioner, partitions, pipe, pipe, pipe, pipe, pipe, reduce, saveAsObjectFile, saveAsTextFile, saveAsTextFile, take, takeAsync, takeOrdered, takeOrdered, takeSample, takeSample, toDebugString, toLocalIterator, top, top, treeAggregate, treeAggregate, treeReduce, treeReduce, zip, zipPartitions, zipWithIndex, zipWithUniqueId Constructor Detail JavaDoubleRDD public JavaDoubleRDD(RDD<Object> srdd) Method Detail fromRDD public static JavaDoubleRDD fromRDD(RDD<Object> rdd) toRDD public static RDD<Object> toRDD(JavaDoubleRDD rdd) partitions public static java.util.List<Partition> partitions() getNumPartitions public static int getNumPartitions() partitioner public static Optional<Partitioner> partitioner() context public static SparkContext context() id public static int id() getStorageLevel public static StorageLevel getStorageLevel() iterator public static java.util.Iterator<T> iterator(Partition split, TaskContext taskContext) map public static <R> JavaRDD<R> map(Function<T,R> f) mapPartitionsWithIndex public static <R> JavaRDD<R> mapPartitionsWithIndex(Function2<Integer,java.util.Iterator<T>,java.util.Iterator<R>> f, boolean preservesPartitioning) mapToDouble public static <R> JavaDoubleRDD mapToDouble(DoubleFunction<T> f) mapToPair public static <K2,V2> JavaPairRDD<K2,V2> mapToPair(PairFunction<T,K2,V2> f) flatMap public static <U> JavaRDD<U> flatMap(FlatMapFunction<T,U> f) flatMapToDouble public static JavaDoubleRDD flatMapToDouble(DoubleFlatMapFunction<T> f) flatMapToPair public static <K2,V2> JavaPairRDD<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) mapPartitions public static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) mapPartitions public static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f, boolean preservesPartitioning) mapPartitionsToDouble public static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f) mapPartitionsToPair public static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) mapPartitionsToDouble public static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f, boolean preservesPartitioning) mapPartitionsToPair public static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f, boolean preservesPartitioning) foreachPartition public static void foreachPartition(VoidFunction<java.util.Iterator<T>> f) glom public static JavaRDD<java.util.List<T>> glom() cartesian public static <U> JavaPairRDD<T,U> cartesian(JavaRDDLike<U,?> other) groupBy public static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f) groupBy public static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f, int numPartitions) pipe public static JavaRDD<String> pipe(String command) pipe public static JavaRDD<String> pipe(java.util.List<String> command) pipe public static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env) pipe public static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize) pipe public static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize, String encoding) zip public static <U> JavaPairRDD<T,U> zip(JavaRDDLike<U,?> other) zipPartitions public static <U,V> JavaRDD<V> zipPartitions(JavaRDDLike<U,?> other, FlatMapFunction2<java.util.Iterator<T>,java.util.Iterator<U>,V> f) zipWithUniqueId public static JavaPairRDD<T,Long> zipWithUniqueId() zipWithIndex public static JavaPairRDD<T,Long> zipWithIndex() foreach public static void foreach(VoidFunction<T> f) collect public static java.util.List<T> collect() toLocalIterator public static java.util.Iterator<T> toLocalIterator() collectPartitions public static java.util.List<T>[] collectPartitions(int[] partitionIds) reduce public static T reduce(Function2<T,T,T> f) treeReduce public static T treeReduce(Function2<T,T,T> f, int depth) treeReduce public static T treeReduce(Function2<T,T,T> f) fold public static T fold(T zeroValue, Function2<T,T,T> f) aggregate public static <U> U aggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) treeAggregate public static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp, int depth) treeAggregate public static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) count public static long count() countApprox public static PartialResult<BoundedDouble> countApprox(long timeout, double confidence) countApprox public static PartialResult<BoundedDouble> countApprox(long timeout) countByValue public static java.util.Map<T,Long> countByValue() countByValueApprox public static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence) countByValueApprox public static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout) take public static java.util.List<T> take(int num) takeSample public static java.util.List<T> takeSample(boolean withReplacement, int num) takeSample public static java.util.List<T> takeSample(boolean withReplacement, int num, long seed) isEmpty public static boolean isEmpty() saveAsTextFile public static void saveAsTextFile(String path) saveAsTextFile public static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) saveAsObjectFile public static void saveAsObjectFile(String path) keyBy public static <U> JavaPairRDD<U,T> keyBy(Function<T,U> f) checkpoint public static void checkpoint() isCheckpointed public static boolean isCheckpointed() getCheckpointFile public static Optional<String> getCheckpointFile() toDebugString public static String toDebugString() top public static java.util.List<T> top(int num, java.util.Comparator<T> comp) top public static java.util.List<T> top(int num) takeOrdered public static java.util.List<T> takeOrdered(int num, java.util.Comparator<T> comp) takeOrdered public static java.util.List<T> takeOrdered(int num) countApproxDistinct public static long countApproxDistinct(double relativeSD) name public static String name() countAsync public static JavaFutureAction<Long> countAsync() collectAsync public static JavaFutureAction<java.util.List<T>> collectAsync() takeAsync public static JavaFutureAction<java.util.List<T>> takeAsync(int num) foreachAsync public static JavaFutureAction<Void> foreachAsync(VoidFunction<T> f) foreachPartitionAsync public static JavaFutureAction<Void> foreachPartitionAsync(VoidFunction<java.util.Iterator<T>> f) mapPartitionsWithIndex$default$2 public static <R> boolean mapPartitionsWithIndex$default$2() srdd public RDD<Object> srdd() classTag public scala.reflect.ClassTag<Double> classTag() rdd public RDD<Double> rdd() wrapRDD public JavaDoubleRDD wrapRDD(RDD<Double> rdd) cache public JavaDoubleRDD cache() Persist this RDD with the default storage level (`MEMORY_ONLY`). persist public JavaDoubleRDD persist(StorageLevel newLevel) Set this RDD's storage level to persist its values across operations after the first time it is computed. Can only be called once on each RDD. Parameters:newLevel - (undocumented) Returns:(undocumented) unpersist public JavaDoubleRDD unpersist() Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. This method blocks until all blocks are deleted. Returns:(undocumented) unpersist public JavaDoubleRDD unpersist(boolean blocking) Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. Parameters:blocking - Whether to block until all blocks are deleted. Returns:(undocumented) first public Double first() Description copied from interface: JavaRDDLike Return the first element in this RDD. Returns:(undocumented) distinct public JavaDoubleRDD distinct() Return a new RDD containing the distinct elements in this RDD. Returns:(undocumented) distinct public JavaDoubleRDD distinct(int numPartitions) Return a new RDD containing the distinct elements in this RDD. Parameters:numPartitions - (undocumented) Returns:(undocumented) filter public JavaDoubleRDD filter(Function<Double,Boolean> f) Return a new RDD containing only the elements that satisfy a predicate. Parameters:f - (undocumented) Returns:(undocumented) coalesce public JavaDoubleRDD coalesce(int numPartitions) Return a new RDD that is reduced into numPartitions partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented) coalesce public JavaDoubleRDD coalesce(int numPartitions, boolean shuffle) Return a new RDD that is reduced into numPartitions partitions. Parameters:numPartitions - (undocumented)shuffle - (undocumented) Returns:(undocumented) repartition public JavaDoubleRDD repartition(int numPartitions) Return a new RDD that has exactly numPartitions partitions. Can increase or decrease the level of parallelism in this RDD. Internally, this uses a shuffle to redistribute data. If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle. Parameters:numPartitions - (undocumented) Returns:(undocumented) subtract public JavaDoubleRDD subtract(JavaDoubleRDD other) Return an RDD with the elements from this that are not in other. Uses this partitioner/partition size, because even if other is huge, the resulting RDD will be &lt;= us. Parameters:other - (undocumented) Returns:(undocumented) subtract public JavaDoubleRDD subtract(JavaDoubleRDD other, int numPartitions) Return an RDD with the elements from this that are not in other. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) subtract public JavaDoubleRDD subtract(JavaDoubleRDD other, Partitioner p) Return an RDD with the elements from this that are not in other. Parameters:other - (undocumented)p - (undocumented) Returns:(undocumented) sample public JavaDoubleRDD sample(boolean withReplacement, Double fraction) Return a sampled subset of this RDD. Parameters:withReplacement - (undocumented)fraction - (undocumented) Returns:(undocumented) sample public JavaDoubleRDD sample(boolean withReplacement, Double fraction, long seed) Return a sampled subset of this RDD. Parameters:withReplacement - (undocumented)fraction - (undocumented)seed - (undocumented) Returns:(undocumented) union public JavaDoubleRDD union(JavaDoubleRDD other) Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them). Parameters:other - (undocumented) Returns:(undocumented) intersection public JavaDoubleRDD intersection(JavaDoubleRDD other) Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did. Note that this method performs a shuffle internally. Parameters:other - (undocumented) Returns:(undocumented) sum public Double sum() Add up the elements in this RDD. min public Double min() Returns the minimum element from this RDD as defined by the default comparator natural order. Returns:the minimum of the RDD max public Double max() Returns the maximum element from this RDD as defined by the default comparator natural order. Returns:the maximum of the RDD stats public StatCounter stats() Return a StatCounter object that captures the mean, variance and count of the RDD's elements in one operation. Returns:(undocumented) mean public Double mean() Compute the mean of this RDD's elements. variance public Double variance() Compute the variance of this RDD's elements. stdev public Double stdev() Compute the standard deviation of this RDD's elements. sampleStdev public Double sampleStdev() Compute the sample standard deviation of this RDD's elements (which corrects for bias in estimating the standard deviation by dividing by N-1 instead of N). Returns:(undocumented) sampleVariance public Double sampleVariance() Compute the sample variance of this RDD's elements (which corrects for bias in estimating the standard variance by dividing by N-1 instead of N). Returns:(undocumented) meanApprox public PartialResult<BoundedDouble> meanApprox(long timeout, Double confidence) Return the approximate mean of the elements in this RDD. meanApprox public PartialResult<BoundedDouble> meanApprox(long timeout) Approximate operation to return the mean within a timeout. Parameters:timeout - (undocumented) Returns:(undocumented) sumApprox public PartialResult<BoundedDouble> sumApprox(long timeout, Double confidence) Approximate operation to return the sum within a timeout. Parameters:timeout - (undocumented)confidence - (undocumented) Returns:(undocumented) sumApprox public PartialResult<BoundedDouble> sumApprox(long timeout) Approximate operation to return the sum within a timeout. Parameters:timeout - (undocumented) Returns:(undocumented) histogram public scala.Tuple2<double[],long[]> histogram(int bucketCount) Compute a histogram of the data using bucketCount number of buckets evenly spaced between the minimum and maximum of the RDD. For example if the min value is 0 and the max is 100 and there are two buckets the resulting buckets will be [0,50) [50,100]. bucketCount must be at least 1 If the RDD contains infinity, NaN throws an exception If the elements in RDD do not vary (max == min) always returns a single bucket. Parameters:bucketCount - (undocumented) Returns:(undocumented) histogram public long[] histogram(double[] buckets) Compute a histogram using the provided buckets. The buckets are all open to the left except for the last which is closed e.g. for the array [1,10,20,50] the buckets are [1,10) [10,20) [20,50] e.g 1&lt;=x&lt;10 , 10&lt;=x&lt;20, 20&lt;=x&lt;50 And on the input of 1 and 50 we would have a histogram of 1,0,0 Note: if your histogram is evenly spaced (e.g. [0, 10, 20, 30]) this can be switched from an O(log n) insertion to O(1) per element. (where n = # buckets) if you set evenBuckets to true. buckets must be sorted and not contain any duplicates. buckets array must be at least two elements All NaN entries are treated the same. If you have a NaN bucket it must be the maximum value of the last position and all NaN entries will be counted in that bucket. Parameters:buckets - (undocumented) Returns:(undocumented) histogram public long[] histogram(Double[] buckets, boolean evenBuckets) setName public JavaDoubleRDD setName(String name) Assign a name to this RDD Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaFutureAction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaFutureAction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Interface JavaFutureAction<T> All Superinterfaces: java.util.concurrent.Future<T> public interface JavaFutureAction<T> extends java.util.concurrent.Future<T> Method Summary Methods  Modifier and Type Method and Description java.util.List<Integer> jobIds() Returns the job IDs run by the underlying async operation. Methods inherited from interface java.util.concurrent.Future cancel, get, get, isCancelled, isDone Method Detail jobIds java.util.List<Integer> jobIds() Returns the job IDs run by the underlying async operation. This returns the current snapshot of the job list. Certain operations may run multiple jobs, so multiple calls to this method may return different lists. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaHadoopRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaHadoopRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class JavaHadoopRDD<K,V> Object org.apache.spark.api.java.JavaPairRDD<K,V> org.apache.spark.api.java.JavaHadoopRDD<K,V> All Implemented Interfaces: java.io.Serializable, JavaRDDLike<scala.Tuple2<K,V>,JavaPairRDD<K,V>> public class JavaHadoopRDD<K,V> extends JavaPairRDD<K,V> See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaHadoopRDD(HadoopRDD<K,V> rdd, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag)  Method Summary Methods  Modifier and Type Method and Description scala.reflect.ClassTag<K> kClassTag()  <R> JavaRDD<R> mapPartitionsWithInputSplit(Function2<org.apache.hadoop.mapred.InputSplit,java.util.Iterator<scala.Tuple2<K,V>>,java.util.Iterator<R>> f, boolean preservesPartitioning) Maps over a partition, providing the InputSplit that was used as the base of the partition. scala.reflect.ClassTag<V> vClassTag()  Methods inherited from class org.apache.spark.api.java.JavaPairRDD aggregate, aggregateByKey, aggregateByKey, aggregateByKey, cache, cartesian, checkpoint, classTag, coalesce, coalesce, cogroup, cogroup, cogroup, cogroup, cogroup, cogroup, cogroup, cogroup, cogroup, collect, collectAsMap, collectAsync, collectPartitions, combineByKey, combineByKey, combineByKey, combineByKey, context, count, countApprox, countApprox, countApproxDistinct, countApproxDistinctByKey, countApproxDistinctByKey, countApproxDistinctByKey, countAsync, countByKey, countByKeyApprox, countByKeyApprox, countByValue, countByValueApprox, countByValueApprox, distinct, distinct, filter, first, flatMap, flatMapToDouble, flatMapToPair, flatMapValues, fold, foldByKey, foldByKey, foldByKey, foreach, foreachAsync, foreachPartition, foreachPartitionAsync, fromJavaRDD, fromRDD, fullOuterJoin, fullOuterJoin, fullOuterJoin, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupByKey, groupByKey, groupByKey, groupWith, groupWith, groupWith, id, intersection, isCheckpointed, isEmpty, iterator, join, join, join, keyBy, keys, leftOuterJoin, leftOuterJoin, leftOuterJoin, lookup, map, mapPartitions, mapPartitions, mapPartitionsToDouble, mapPartitionsToDouble, mapPartitionsToPair, mapPartitionsToPair, mapPartitionsWithIndex, mapPartitionsWithIndex$default$2, mapToDouble, mapToPair, mapValues, max, min, name, partitionBy, partitioner, partitions, persist, pipe, pipe, pipe, pipe, pipe, rdd, reduce, reduceByKey, reduceByKey, reduceByKey, reduceByKeyLocally, repartition, repartitionAndSortWithinPartitions, repartitionAndSortWithinPartitions, rightOuterJoin, rightOuterJoin, rightOuterJoin, sample, sample, sampleByKey, sampleByKey, sampleByKeyExact, sampleByKeyExact, saveAsHadoopDataset, saveAsHadoopFile, saveAsHadoopFile, saveAsHadoopFile, saveAsNewAPIHadoopDataset, saveAsNewAPIHadoopFile, saveAsNewAPIHadoopFile, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortByKey, sortByKey, sortByKey, sortByKey, sortByKey, sortByKey, subtract, subtract, subtract, subtractByKey, subtractByKey, subtractByKey, take, takeAsync, takeOrdered, takeOrdered, takeSample, takeSample, toDebugString, toLocalIterator, top, top, toRDD, treeAggregate, treeAggregate, treeReduce, treeReduce, union, unpersist, unpersist, values, wrapRDD, zip, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.api.java.JavaRDDLike aggregate, cartesian, checkpoint, collect, collectAsync, collectPartitions, context, count, countApprox, countApprox, countApproxDistinct, countAsync, countByValue, countByValueApprox, countByValueApprox, flatMap, flatMapToDouble, flatMapToPair, fold, foreach, foreachAsync, foreachPartition, foreachPartitionAsync, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, id, isCheckpointed, isEmpty, iterator, keyBy, map, mapPartitions, mapPartitions, mapPartitionsToDouble, mapPartitionsToDouble, mapPartitionsToPair, mapPartitionsToPair, mapPartitionsWithIndex, mapToDouble, mapToPair, max, min, name, partitioner, partitions, pipe, pipe, pipe, pipe, pipe, reduce, saveAsObjectFile, saveAsTextFile, saveAsTextFile, take, takeAsync, takeOrdered, takeOrdered, takeSample, takeSample, toDebugString, toLocalIterator, top, top, treeAggregate, treeAggregate, treeReduce, treeReduce, zip, zipPartitions, zipWithIndex, zipWithUniqueId Constructor Detail JavaHadoopRDD public JavaHadoopRDD(HadoopRDD<K,V> rdd, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag) Method Detail kClassTag public scala.reflect.ClassTag<K> kClassTag() Overrides: kClassTag in class JavaPairRDD<K,V> vClassTag public scala.reflect.ClassTag<V> vClassTag() Overrides: vClassTag in class JavaPairRDD<K,V> mapPartitionsWithInputSplit public <R> JavaRDD<R> mapPartitionsWithInputSplit(Function2<org.apache.hadoop.mapred.InputSplit,java.util.Iterator<scala.Tuple2<K,V>>,java.util.Iterator<R>> f, boolean preservesPartitioning) Maps over a partition, providing the InputSplit that was used as the base of the partition. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaInputDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaInputDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.api.java Class JavaInputDStream<T> Object org.apache.spark.streaming.api.java.JavaDStream<T> org.apache.spark.streaming.api.java.JavaInputDStream<T> All Implemented Interfaces: java.io.Serializable, JavaDStreamLike<T,JavaDStream<T>,JavaRDD<T>> Direct Known Subclasses: JavaReceiverInputDStream public class JavaInputDStream<T> extends JavaDStream<T> A Java-friendly interface to InputDStream. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaInputDStream(InputDStream<T> inputDStream, scala.reflect.ClassTag<T> classTag)  Method Summary Methods  Modifier and Type Method and Description static JavaDStream<T> cache()  static DStream<T> checkpoint(Duration interval)  scala.reflect.ClassTag<T> classTag()  static JavaRDD<T> compute(Time validTime)  static StreamingContext context()  static JavaDStream<Long> count()  static JavaPairDStream<T,Long> countByValue()  static JavaPairDStream<T,Long> countByValue(int numPartitions)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration)  static DStream<T> dstream()  static JavaDStream<T> filter(Function<T,Boolean> f)  static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f)  static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f)  static void foreachRDD(VoidFunction<R> foreachFunc)  static void foreachRDD(VoidFunction2<R,Time> foreachFunc)  static <T> JavaInputDStream<T> fromInputDStream(InputDStream<T> inputDStream, scala.reflect.ClassTag<T> evidence$1) Convert a scala InputDStream to a Java-friendly JavaInputDStream. static JavaDStream<java.util.List<T>> glom()  InputDStream<T> inputDStream()  static <R> JavaDStream<R> map(Function<T,R> f)  static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f)  static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f)  static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f)  static JavaDStream<T> persist()  static JavaDStream<T> persist(StorageLevel storageLevel)  static void print()  static void print(int num)  static JavaDStream<T> reduce(Function2<T,T,T> f)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration)  static JavaDStream<T> repartition(int numPartitions)  static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in)  static java.util.List<R> slice(Time fromTime, Time toTime)  static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc)  static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc)  static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc)  static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc)  static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc)  static JavaDStream<T> union(JavaDStream<T> that)  static JavaDStream<T> window(Duration windowDuration)  static JavaDStream<T> window(Duration windowDuration, Duration slideDuration)  static JavaRDD<T> wrapRDD(RDD<T> rdd)  Methods inherited from class org.apache.spark.streaming.api.java.JavaDStream cache, compute, dstream, filter, fromDStream, persist, persist, repartition, union, window, window, wrapRDD Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.streaming.api.java.JavaDStreamLike checkpoint, context, count, countByValue, countByValue, countByValueAndWindow, countByValueAndWindow, countByWindow, flatMap, flatMapToPair, foreachRDD, foreachRDD, glom, map, mapPartitions, mapPartitionsToPair, mapToPair, print, print, reduce, reduceByWindow, reduceByWindow, scalaIntToJavaLong, slice, transform, transform, transformToPair, transformToPair, transformWith, transformWith, transformWithToPair, transformWithToPair Constructor Detail JavaInputDStream public JavaInputDStream(InputDStream<T> inputDStream, scala.reflect.ClassTag<T> classTag) Method Detail fromInputDStream public static <T> JavaInputDStream<T> fromInputDStream(InputDStream<T> inputDStream, scala.reflect.ClassTag<T> evidence$1) Convert a scala InputDStream to a Java-friendly JavaInputDStream. Parameters:inputDStream - (undocumented)evidence$1 - (undocumented) Returns:(undocumented) scalaIntToJavaLong public static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in) print public static void print() print public static void print(int num) count public static JavaDStream<Long> count() countByValue public static JavaPairDStream<T,Long> countByValue() countByValue public static JavaPairDStream<T,Long> countByValue(int numPartitions) countByWindow public static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) glom public static JavaDStream<java.util.List<T>> glom() context public static StreamingContext context() map public static <R> JavaDStream<R> map(Function<T,R> f) mapToPair public static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f) flatMap public static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f) flatMapToPair public static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) mapPartitions public static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) mapPartitionsToPair public static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) reduce public static JavaDStream<T> reduce(Function2<T,T,T> f) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) slice public static java.util.List<R> slice(Time fromTime, Time toTime) foreachRDD public static void foreachRDD(VoidFunction<R> foreachFunc) foreachRDD public static void foreachRDD(VoidFunction2<R,Time> foreachFunc) transform public static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc) transform public static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc) checkpoint public static DStream<T> checkpoint(Duration interval) dstream public static DStream<T> dstream() wrapRDD public static JavaRDD<T> wrapRDD(RDD<T> rdd) filter public static JavaDStream<T> filter(Function<T,Boolean> f) cache public static JavaDStream<T> cache() persist public static JavaDStream<T> persist() persist public static JavaDStream<T> persist(StorageLevel storageLevel) compute public static JavaRDD<T> compute(Time validTime) window public static JavaDStream<T> window(Duration windowDuration) window public static JavaDStream<T> window(Duration windowDuration, Duration slideDuration) union public static JavaDStream<T> union(JavaDStream<T> that) repartition public static JavaDStream<T> repartition(int numPartitions) inputDStream public InputDStream<T> inputDStream() classTag public scala.reflect.ClassTag<T> classTag() Specified by: classTag in interface JavaDStreamLike<T,JavaDStream<T>,JavaRDD<T>> Overrides: classTag in class JavaDStream<T> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaIterableWrapperSerializer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaIterableWrapperSerializer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.serializer Class JavaIterableWrapperSerializer Object com.esotericsoftware.kryo.Serializer<Iterable<?>> org.apache.spark.serializer.JavaIterableWrapperSerializer public class JavaIterableWrapperSerializer extends com.esotericsoftware.kryo.Serializer<Iterable<?>> A Kryo serializer for serializing results returned by asJavaIterable. The underlying object is scala.collection.convert.Wrappers$IterableWrapper. Kryo deserializes this into an AbstractCollection, which unfortunately doesn't work. Constructor Summary Constructors  Constructor and Description JavaIterableWrapperSerializer()  Method Summary Methods  Modifier and Type Method and Description static T copy(com.esotericsoftware.kryo.Kryo x$1, T x$2)  static boolean getAcceptsNull()  static boolean isImmutable()  Iterable<?> read(com.esotericsoftware.kryo.Kryo kryo, com.esotericsoftware.kryo.io.Input in, Class<Iterable<?>> clz)  static void setAcceptsNull(boolean x$1)  static void setGenerics(com.esotericsoftware.kryo.Kryo x$1, Class<?>[] x$2)  static void setImmutable(boolean x$1)  static Object wrapperClass()  void write(com.esotericsoftware.kryo.Kryo kryo, com.esotericsoftware.kryo.io.Output out, Iterable<?> obj)  Methods inherited from class com.esotericsoftware.kryo.Serializer copy, getAcceptsNull, isImmutable, setAcceptsNull, setGenerics, setImmutable Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JavaIterableWrapperSerializer public JavaIterableWrapperSerializer() Method Detail wrapperClass public static Object wrapperClass() getAcceptsNull public static boolean getAcceptsNull() setAcceptsNull public static void setAcceptsNull(boolean x$1) isImmutable public static boolean isImmutable() setImmutable public static void setImmutable(boolean x$1) setGenerics public static void setGenerics(com.esotericsoftware.kryo.Kryo x$1, Class<?>[] x$2) copy public static T copy(com.esotericsoftware.kryo.Kryo x$1, T x$2) write public void write(com.esotericsoftware.kryo.Kryo kryo, com.esotericsoftware.kryo.io.Output out, Iterable<?> obj) Specified by: write in class com.esotericsoftware.kryo.Serializer<Iterable<?>> read public Iterable<?> read(com.esotericsoftware.kryo.Kryo kryo, com.esotericsoftware.kryo.io.Input in, Class<Iterable<?>> clz) Specified by: read in class com.esotericsoftware.kryo.Serializer<Iterable<?>> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaMapWithStateDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaMapWithStateDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.api.java Class JavaMapWithStateDStream<KeyType,ValueType,StateType,MappedType> Object org.apache.spark.streaming.api.java.JavaDStream<MappedType> org.apache.spark.streaming.api.java.JavaMapWithStateDStream<KeyType,ValueType,StateType,MappedType> All Implemented Interfaces: java.io.Serializable, JavaDStreamLike<MappedType,JavaDStream<MappedType>,JavaRDD<MappedType>> public class JavaMapWithStateDStream<KeyType,ValueType,StateType,MappedType> extends JavaDStream<MappedType> :: Experimental :: DStream representing the stream of data generated by mapWithState operation on a JavaPairDStream. Additionally, it also gives access to the stream of state snapshots, that is, the state data of all keys after a batch has updated them. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description JavaPairDStream<KeyType,StateType> stateSnapshots()  Methods inherited from class org.apache.spark.streaming.api.java.JavaDStream cache, checkpoint, classTag, compute, context, count, countByValue, countByValue, countByValueAndWindow, countByValueAndWindow, countByWindow, dstream, filter, flatMap, flatMapToPair, foreachRDD, foreachRDD, fromDStream, glom, map, mapPartitions, mapPartitionsToPair, mapToPair, persist, persist, print, print, reduce, reduceByWindow, reduceByWindow, repartition, scalaIntToJavaLong, slice, transform, transform, transformToPair, transformToPair, transformWith, transformWith, transformWithToPair, transformWithToPair, union, window, window, wrapRDD Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.streaming.api.java.JavaDStreamLike checkpoint, context, count, countByValue, countByValue, countByValueAndWindow, countByValueAndWindow, countByWindow, flatMap, flatMapToPair, foreachRDD, foreachRDD, glom, map, mapPartitions, mapPartitionsToPair, mapToPair, print, print, reduce, reduceByWindow, reduceByWindow, scalaIntToJavaLong, slice, transform, transform, transformToPair, transformToPair, transformWith, transformWith, transformWithToPair, transformWithToPair Method Detail stateSnapshots public JavaPairDStream<KeyType,StateType> stateSnapshots() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaNewHadoopRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaNewHadoopRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class JavaNewHadoopRDD<K,V> Object org.apache.spark.api.java.JavaPairRDD<K,V> org.apache.spark.api.java.JavaNewHadoopRDD<K,V> All Implemented Interfaces: java.io.Serializable, JavaRDDLike<scala.Tuple2<K,V>,JavaPairRDD<K,V>> public class JavaNewHadoopRDD<K,V> extends JavaPairRDD<K,V> See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaNewHadoopRDD(NewHadoopRDD<K,V> rdd, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag)  Method Summary Methods  Modifier and Type Method and Description scala.reflect.ClassTag<K> kClassTag()  <R> JavaRDD<R> mapPartitionsWithInputSplit(Function2<org.apache.hadoop.mapreduce.InputSplit,java.util.Iterator<scala.Tuple2<K,V>>,java.util.Iterator<R>> f, boolean preservesPartitioning) Maps over a partition, providing the InputSplit that was used as the base of the partition. scala.reflect.ClassTag<V> vClassTag()  Methods inherited from class org.apache.spark.api.java.JavaPairRDD aggregate, aggregateByKey, aggregateByKey, aggregateByKey, cache, cartesian, checkpoint, classTag, coalesce, coalesce, cogroup, cogroup, cogroup, cogroup, cogroup, cogroup, cogroup, cogroup, cogroup, collect, collectAsMap, collectAsync, collectPartitions, combineByKey, combineByKey, combineByKey, combineByKey, context, count, countApprox, countApprox, countApproxDistinct, countApproxDistinctByKey, countApproxDistinctByKey, countApproxDistinctByKey, countAsync, countByKey, countByKeyApprox, countByKeyApprox, countByValue, countByValueApprox, countByValueApprox, distinct, distinct, filter, first, flatMap, flatMapToDouble, flatMapToPair, flatMapValues, fold, foldByKey, foldByKey, foldByKey, foreach, foreachAsync, foreachPartition, foreachPartitionAsync, fromJavaRDD, fromRDD, fullOuterJoin, fullOuterJoin, fullOuterJoin, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupByKey, groupByKey, groupByKey, groupWith, groupWith, groupWith, id, intersection, isCheckpointed, isEmpty, iterator, join, join, join, keyBy, keys, leftOuterJoin, leftOuterJoin, leftOuterJoin, lookup, map, mapPartitions, mapPartitions, mapPartitionsToDouble, mapPartitionsToDouble, mapPartitionsToPair, mapPartitionsToPair, mapPartitionsWithIndex, mapPartitionsWithIndex$default$2, mapToDouble, mapToPair, mapValues, max, min, name, partitionBy, partitioner, partitions, persist, pipe, pipe, pipe, pipe, pipe, rdd, reduce, reduceByKey, reduceByKey, reduceByKey, reduceByKeyLocally, repartition, repartitionAndSortWithinPartitions, repartitionAndSortWithinPartitions, rightOuterJoin, rightOuterJoin, rightOuterJoin, sample, sample, sampleByKey, sampleByKey, sampleByKeyExact, sampleByKeyExact, saveAsHadoopDataset, saveAsHadoopFile, saveAsHadoopFile, saveAsHadoopFile, saveAsNewAPIHadoopDataset, saveAsNewAPIHadoopFile, saveAsNewAPIHadoopFile, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortByKey, sortByKey, sortByKey, sortByKey, sortByKey, sortByKey, subtract, subtract, subtract, subtractByKey, subtractByKey, subtractByKey, take, takeAsync, takeOrdered, takeOrdered, takeSample, takeSample, toDebugString, toLocalIterator, top, top, toRDD, treeAggregate, treeAggregate, treeReduce, treeReduce, union, unpersist, unpersist, values, wrapRDD, zip, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.api.java.JavaRDDLike aggregate, cartesian, checkpoint, collect, collectAsync, collectPartitions, context, count, countApprox, countApprox, countApproxDistinct, countAsync, countByValue, countByValueApprox, countByValueApprox, flatMap, flatMapToDouble, flatMapToPair, fold, foreach, foreachAsync, foreachPartition, foreachPartitionAsync, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, id, isCheckpointed, isEmpty, iterator, keyBy, map, mapPartitions, mapPartitions, mapPartitionsToDouble, mapPartitionsToDouble, mapPartitionsToPair, mapPartitionsToPair, mapPartitionsWithIndex, mapToDouble, mapToPair, max, min, name, partitioner, partitions, pipe, pipe, pipe, pipe, pipe, reduce, saveAsObjectFile, saveAsTextFile, saveAsTextFile, take, takeAsync, takeOrdered, takeOrdered, takeSample, takeSample, toDebugString, toLocalIterator, top, top, treeAggregate, treeAggregate, treeReduce, treeReduce, zip, zipPartitions, zipWithIndex, zipWithUniqueId Constructor Detail JavaNewHadoopRDD public JavaNewHadoopRDD(NewHadoopRDD<K,V> rdd, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag) Method Detail kClassTag public scala.reflect.ClassTag<K> kClassTag() Overrides: kClassTag in class JavaPairRDD<K,V> vClassTag public scala.reflect.ClassTag<V> vClassTag() Overrides: vClassTag in class JavaPairRDD<K,V> mapPartitionsWithInputSplit public <R> JavaRDD<R> mapPartitionsWithInputSplit(Function2<org.apache.hadoop.mapreduce.InputSplit,java.util.Iterator<scala.Tuple2<K,V>>,java.util.Iterator<R>> f, boolean preservesPartitioning) Maps over a partition, providing the InputSplit that was used as the base of the partition. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaPackage (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaPackage (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib Class JavaPackage Object org.apache.spark.mllib.JavaPackage public class JavaPackage extends Object A dummy class as a workaround to show the package doc of spark.mllib in generated Java API docs. See Also: JDK-4492654 Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaPairDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaPairDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.api.java Class JavaPairDStream<K,V> Object org.apache.spark.streaming.api.java.JavaPairDStream<K,V> All Implemented Interfaces: java.io.Serializable, JavaDStreamLike<scala.Tuple2<K,V>,JavaPairDStream<K,V>,JavaPairRDD<K,V>> Direct Known Subclasses: JavaPairInputDStream public class JavaPairDStream<K,V> extends Object A Java-friendly interface to a DStream of key-value pairs, which provides extra methods like reduceByKey and join. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaPairDStream(DStream<scala.Tuple2<K,V>> dstream, scala.reflect.ClassTag<K> kManifest, scala.reflect.ClassTag<V> vManifest)  Method Summary Methods  Modifier and Type Method and Description JavaPairDStream<K,V> cache() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) static DStream<T> checkpoint(Duration interval)  scala.reflect.ClassTag<scala.Tuple2<K,V>> classTag()  <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner) Combine elements of each key in DStream's RDDs using custom function. <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine) Combine elements of each key in DStream's RDDs using custom function. JavaPairRDD<K,V> compute(Time validTime) Method that generates a RDD for the given Duration static StreamingContext context()  static JavaDStream<Long> count()  static JavaPairDStream<T,Long> countByValue()  static JavaPairDStream<T,Long> countByValue(int numPartitions)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration)  DStream<scala.Tuple2<K,V>> dstream()  JavaPairDStream<K,V> filter(Function<scala.Tuple2<K,V>,Boolean> f) Return a new DStream containing only the elements that satisfy a predicate. static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f)  static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f)  <U> JavaPairDStream<K,U> flatMapValues(Function<V,Iterable<U>> f) Return a new DStream by applying a flatmap function to the value of each key-value pairs in 'this' DStream without changing the key. static void foreachRDD(VoidFunction<R> foreachFunc)  static void foreachRDD(VoidFunction2<R,Time> foreachFunc)  static <K,V> JavaPairDStream<K,V> fromJavaDStream(JavaDStream<scala.Tuple2<K,V>> dstream)  static <K,V> JavaPairDStream<K,V> fromPairDStream(DStream<scala.Tuple2<K,V>> dstream, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2)  <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. static JavaDStream<java.util.List<T>> glom()  JavaPairDStream<K,Iterable<V>> groupByKey() Return a new DStream by applying groupByKey to each RDD. JavaPairDStream<K,Iterable<V>> groupByKey(int numPartitions) Return a new DStream by applying groupByKey to each RDD. JavaPairDStream<K,Iterable<V>> groupByKey(Partitioner partitioner) Return a new DStream by applying groupByKey on each RDD of this DStream. JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration) Return a new DStream by applying groupByKey over a sliding window. JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration) Return a new DStream by applying groupByKey over a sliding window. JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream by applying groupByKey over a sliding window on this DStream. JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, Partitioner partitioner) Return a new DStream by applying groupByKey over a sliding window on this DStream. <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. scala.reflect.ClassTag<K> kManifest()  <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. static <R> JavaDStream<R> map(Function<T,R> f)  static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f)  static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f)  static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f)  <U> JavaPairDStream<K,U> mapValues(Function<V,U> f) Return a new DStream by applying a map function to the value of each key-value pairs in 'this' DStream without changing the key. <StateType,MappedType> JavaMapWithStateDStream<K,V,StateType,MappedType> mapWithState(StateSpec<K,V,StateType,MappedType> spec) :: Experimental :: Return a JavaMapWithStateDStream by applying a function to every key-value element of this stream, while maintaining some state data for each unique key. JavaPairDStream<K,V> persist() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) JavaPairDStream<K,V> persist(StorageLevel storageLevel) Persist the RDDs of this DStream with the given storage level static void print()  static void print(int num)  static JavaDStream<T> reduce(Function2<T,T,T> f)  JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func) Return a new DStream by applying reduceByKey to each RDD. JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, int numPartitions) Return a new DStream by applying reduceByKey to each RDD. JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, Partitioner partitioner) Return a new DStream by applying reduceByKey to each RDD. JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration) Create a new DStream by applying reduceByKey over a sliding window on this DStream. JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream by applying reduceByKey over a sliding window. JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream by applying reduceByKey over a sliding window. JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner) Return a new DStream by applying reduceByKey over a sliding window. JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream by reducing over a using incremental computation. JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions, Function<scala.Tuple2<K,V>,Boolean> filterFunc) Return a new DStream by applying incremental reduceByKey over a sliding window. JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner, Function<scala.Tuple2<K,V>,Boolean> filterFunc) Return a new DStream by applying incremental reduceByKey over a sliding window. static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration)  JavaPairDStream<K,V> repartition(int numPartitions) Return a new DStream with an increased or decreased level of parallelism. <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. void saveAsHadoopFiles(String prefix, String suffix) Save each RDD in this DStream as a Hadoop file. <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) Save each RDD in this DStream as a Hadoop file. <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.mapred.JobConf conf) Save each RDD in this DStream as a Hadoop file. void saveAsNewAPIHadoopFiles(String prefix, String suffix) Save each RDD in this DStream as a Hadoop file. <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) Save each RDD in this DStream as a Hadoop file. <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.conf.Configuration conf) Save each RDD in this DStream as a Hadoop file. static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in)  static <K> JavaPairDStream<K,Long> scalaToJavaLong(JavaPairDStream<K,Object> dstream, scala.reflect.ClassTag<K> evidence$3)  static java.util.List<R> slice(Time fromTime, Time toTime)  JavaDStream<scala.Tuple2<K,V>> toJavaDStream() Convert to a JavaDStream static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc)  static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc)  static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc)  static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc)  static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc)  JavaPairDStream<K,V> union(JavaPairDStream<K,V> that) Return a new DStream by unifying data of another DStream with this DStream. <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, int numPartitions) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key. <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner, JavaPairRDD<K,S> initialRDD) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key. scala.reflect.ClassTag<V> vManifest()  JavaPairDStream<K,V> window(Duration windowDuration) Return a new DStream which is computed based on windowed batches of this DStream. JavaPairDStream<K,V> window(Duration windowDuration, Duration slideDuration) Return a new DStream which is computed based on windowed batches of this DStream. JavaPairRDD<K,V> wrapRDD(RDD<scala.Tuple2<K,V>> rdd)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.streaming.api.java.JavaDStreamLike checkpoint, context, count, countByValue, countByValue, countByValueAndWindow, countByValueAndWindow, countByWindow, flatMap, flatMapToPair, foreachRDD, foreachRDD, glom, map, mapPartitions, mapPartitionsToPair, mapToPair, print, print, reduce, reduceByWindow, reduceByWindow, scalaIntToJavaLong, slice, transform, transform, transformToPair, transformToPair, transformWith, transformWith, transformWithToPair, transformWithToPair Constructor Detail JavaPairDStream public JavaPairDStream(DStream<scala.Tuple2<K,V>> dstream, scala.reflect.ClassTag<K> kManifest, scala.reflect.ClassTag<V> vManifest) Method Detail fromPairDStream public static <K,V> JavaPairDStream<K,V> fromPairDStream(DStream<scala.Tuple2<K,V>> dstream, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2) fromJavaDStream public static <K,V> JavaPairDStream<K,V> fromJavaDStream(JavaDStream<scala.Tuple2<K,V>> dstream) scalaToJavaLong public static <K> JavaPairDStream<K,Long> scalaToJavaLong(JavaPairDStream<K,Object> dstream, scala.reflect.ClassTag<K> evidence$3) scalaIntToJavaLong public static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in) print public static void print() print public static void print(int num) count public static JavaDStream<Long> count() countByValue public static JavaPairDStream<T,Long> countByValue() countByValue public static JavaPairDStream<T,Long> countByValue(int numPartitions) countByWindow public static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) glom public static JavaDStream<java.util.List<T>> glom() context public static StreamingContext context() map public static <R> JavaDStream<R> map(Function<T,R> f) mapToPair public static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f) flatMap public static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f) flatMapToPair public static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) mapPartitions public static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) mapPartitionsToPair public static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) reduce public static JavaDStream<T> reduce(Function2<T,T,T> f) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) slice public static java.util.List<R> slice(Time fromTime, Time toTime) foreachRDD public static void foreachRDD(VoidFunction<R> foreachFunc) foreachRDD public static void foreachRDD(VoidFunction2<R,Time> foreachFunc) transform public static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc) transform public static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc) checkpoint public static DStream<T> checkpoint(Duration interval) dstream public DStream<scala.Tuple2<K,V>> dstream() kManifest public scala.reflect.ClassTag<K> kManifest() vManifest public scala.reflect.ClassTag<V> vManifest() wrapRDD public JavaPairRDD<K,V> wrapRDD(RDD<scala.Tuple2<K,V>> rdd) filter public JavaPairDStream<K,V> filter(Function<scala.Tuple2<K,V>,Boolean> f) Return a new DStream containing only the elements that satisfy a predicate. cache public JavaPairDStream<K,V> cache() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) persist public JavaPairDStream<K,V> persist() Persist RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) persist public JavaPairDStream<K,V> persist(StorageLevel storageLevel) Persist the RDDs of this DStream with the given storage level repartition public JavaPairDStream<K,V> repartition(int numPartitions) Return a new DStream with an increased or decreased level of parallelism. Each RDD in the returned DStream has exactly numPartitions partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented) compute public JavaPairRDD<K,V> compute(Time validTime) Method that generates a RDD for the given Duration window public JavaPairDStream<K,V> window(Duration windowDuration) Return a new DStream which is computed based on windowed batches of this DStream. The new DStream generates RDDs with the same interval as this DStream. Parameters:windowDuration - width of the window; must be a multiple of this DStream's interval. Returns: window public JavaPairDStream<K,V> window(Duration windowDuration, Duration slideDuration) Return a new DStream which is computed based on windowed batches of this DStream. Parameters:windowDuration - duration (i.e., width) of the window; must be a multiple of this DStream's intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's interval Returns:(undocumented) union public JavaPairDStream<K,V> union(JavaPairDStream<K,V> that) Return a new DStream by unifying data of another DStream with this DStream. Parameters:that - Another DStream having the same interval (i.e., slideDuration) as this DStream. Returns:(undocumented) groupByKey public JavaPairDStream<K,Iterable<V>> groupByKey() Return a new DStream by applying groupByKey to each RDD. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Returns:(undocumented) groupByKey public JavaPairDStream<K,Iterable<V>> groupByKey(int numPartitions) Return a new DStream by applying groupByKey to each RDD. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented) groupByKey public JavaPairDStream<K,Iterable<V>> groupByKey(Partitioner partitioner) Return a new DStream by applying groupByKey on each RDD of this DStream. Therefore, the values for each key in this DStream's RDDs are grouped into a single sequence to generate the RDDs of the new DStream. org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:partitioner - (undocumented) Returns:(undocumented) reduceByKey public JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func) Return a new DStream by applying reduceByKey to each RDD. The values for each key are merged using the associative and commutative reduce function. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:func - (undocumented) Returns:(undocumented) reduceByKey public JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, int numPartitions) Return a new DStream by applying reduceByKey to each RDD. The values for each key are merged using the supplied reduce function. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:func - (undocumented)numPartitions - (undocumented) Returns:(undocumented) reduceByKey public JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, Partitioner partitioner) Return a new DStream by applying reduceByKey to each RDD. The values for each key are merged using the supplied reduce function. org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:func - (undocumented)partitioner - (undocumented) Returns:(undocumented) combineByKey public <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner) Combine elements of each key in DStream's RDDs using custom function. This is similar to the combineByKey for RDDs. Please refer to combineByKey in org.apache.spark.rdd.PairRDDFunctions for more information. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)partitioner - (undocumented) Returns:(undocumented) combineByKey public <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine) Combine elements of each key in DStream's RDDs using custom function. This is similar to the combineByKey for RDDs. Please refer to combineByKey in org.apache.spark.rdd.PairRDDFunctions for more information. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)partitioner - (undocumented)mapSideCombine - (undocumented) Returns:(undocumented) groupByKeyAndWindow public JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration) Return a new DStream by applying groupByKey over a sliding window. This is similar to DStream.groupByKey() but applies it over a sliding window. The new DStream generates RDDs with the same interval as this DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching interval Returns:(undocumented) groupByKeyAndWindow public JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration) Return a new DStream by applying groupByKey over a sliding window. Similar to DStream.groupByKey(), but applies it over a sliding window. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) groupByKeyAndWindow public JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream by applying groupByKey over a sliding window on this DStream. Similar to DStream.groupByKey(), but applies it over a sliding window. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalnumPartitions - Number of partitions of each RDD in the new DStream. Returns:(undocumented) groupByKeyAndWindow public JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, Partitioner partitioner) Return a new DStream by applying groupByKey over a sliding window on this DStream. Similar to DStream.groupByKey(), but applies it over a sliding window. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalpartitioner - Partitioner for controlling the partitioning of each RDD in the new DStream. Returns:(undocumented) reduceByKeyAndWindow public JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration) Create a new DStream by applying reduceByKey over a sliding window on this DStream. Similar to DStream.reduceByKey(), but applies it over a sliding window. The new DStream generates RDDs with the same interval as this DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:reduceFunc - associative and commutative reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching interval Returns:(undocumented) reduceByKeyAndWindow public JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream by applying reduceByKey over a sliding window. This is similar to DStream.reduceByKey() but applies it over a sliding window. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:reduceFunc - associative and commutative reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) reduceByKeyAndWindow public JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream by applying reduceByKey over a sliding window. This is similar to DStream.reduceByKey() but applies it over a sliding window. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:reduceFunc - associative and commutative reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalnumPartitions - Number of partitions of each RDD in the new DStream. Returns:(undocumented) reduceByKeyAndWindow public JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner) Return a new DStream by applying reduceByKey over a sliding window. Similar to DStream.reduceByKey(), but applies it over a sliding window. Parameters:reduceFunc - associative rand commutative educe functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalpartitioner - Partitioner for controlling the partitioning of each RDD in the new DStream. Returns:(undocumented) reduceByKeyAndWindow public JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream by reducing over a using incremental computation. The reduced value of over a new window is calculated using the old window's reduce value : 1. reduce the new values that entered the window (e.g., adding new counts) 2. "inverse reduce" the old values that left the window (e.g., subtracting old counts) This is more efficient that reduceByKeyAndWindow without "inverse reduce" function. However, it is applicable to only "invertible reduce functions". Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:reduceFunc - associative and commutative reduce functioninvReduceFunc - inverse function; such that for all y, invertible x: invReduceFunc(reduceFunc(x, y), x) = ywindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) reduceByKeyAndWindow public JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions, Function<scala.Tuple2<K,V>,Boolean> filterFunc) Return a new DStream by applying incremental reduceByKey over a sliding window. The reduced value of over a new window is calculated using the old window's reduce value : 1. reduce the new values that entered the window (e.g., adding new counts) 2. "inverse reduce" the old values that left the window (e.g., subtracting old counts) This is more efficient that reduceByKeyAndWindow without "inverse reduce" function. However, it is applicable to only "invertible reduce functions". Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:reduceFunc - associative and commutative reduce functioninvReduceFunc - inverse functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalnumPartitions - number of partitions of each RDD in the new DStream.filterFunc - function to filter expired key-value pairs; only pairs that satisfy the function are retained set this to null if you do not want to filter Returns:(undocumented) reduceByKeyAndWindow public JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner, Function<scala.Tuple2<K,V>,Boolean> filterFunc) Return a new DStream by applying incremental reduceByKey over a sliding window. The reduced value of over a new window is calculated using the old window's reduce value : 1. reduce the new values that entered the window (e.g., adding new counts) 2. "inverse reduce" the old values that left the window (e.g., subtracting old counts) This is more efficient that reduceByKeyAndWindow without "inverse reduce" function. However, it is applicable to only "invertible reduce functions". Parameters:reduceFunc - associative and commutative reduce functioninvReduceFunc - inverse functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalpartitioner - Partitioner for controlling the partitioning of each RDD in the new DStream.filterFunc - function to filter expired key-value pairs; only pairs that satisfy the function are retained set this to null if you do not want to filter Returns:(undocumented) mapWithState public <StateType,MappedType> JavaMapWithStateDStream<K,V,StateType,MappedType> mapWithState(StateSpec<K,V,StateType,MappedType> spec) :: Experimental :: Return a JavaMapWithStateDStream by applying a function to every key-value element of this stream, while maintaining some state data for each unique key. The mapping function and other specification (e.g. partitioners, timeouts, initial state data, etc.) of this transformation can be specified using StateSpec class. The state data is accessible in as a parameter of type State in the mapping function. Example of using mapWithState: // A mapping function that maintains an integer state and return a string Function3<String, Optional<Integer>, State<Integer>, String> mappingFunction = new Function3<String, Optional<Integer>, State<Integer>, String>() { @Override public Optional<String> call(Optional<Integer> value, State<Integer> state) { // Use state.exists(), state.get(), state.update() and state.remove() // to manage state, and return the necessary string } }; JavaMapWithStateDStream<String, Integer, Integer, String> mapWithStateDStream = keyValueDStream.mapWithState(StateSpec.function(mappingFunc)); Parameters:spec - Specification of this transformation Returns:(undocumented) updateStateByKey public <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:updateFunc - State update function. If this function returns None, then corresponding state key-value pair will be eliminated. Returns:(undocumented) updateStateByKey public <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, int numPartitions) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:updateFunc - State update function. If this function returns None, then corresponding state key-value pair will be eliminated.numPartitions - Number of partitions of each RDD in the new DStream. Returns:(undocumented) updateStateByKey public <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key. org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:updateFunc - State update function. If this function returns None, then corresponding state key-value pair will be eliminated.partitioner - Partitioner for controlling the partitioning of each RDD in the new DStream. Returns:(undocumented) updateStateByKey public <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner, JavaPairRDD<K,S> initialRDD) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key. org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:updateFunc - State update function. If this function returns None, then corresponding state key-value pair will be eliminated.partitioner - Partitioner for controlling the partitioning of each RDD in the new DStream.initialRDD - initial state value of each key. Returns:(undocumented) mapValues public <U> JavaPairDStream<K,U> mapValues(Function<V,U> f) Return a new DStream by applying a map function to the value of each key-value pairs in 'this' DStream without changing the key. Parameters:f - (undocumented) Returns:(undocumented) flatMapValues public <U> JavaPairDStream<K,U> flatMapValues(Function<V,Iterable<U>> f) Return a new DStream by applying a flatmap function to the value of each key-value pairs in 'this' DStream without changing the key. Parameters:f - (undocumented) Returns:(undocumented) cogroup public <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented) Returns:(undocumented) cogroup public <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) cogroup public <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) join public <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented) Returns:(undocumented) join public <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) join public <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. The supplied org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) leftOuterJoin public <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented) Returns:(undocumented) leftOuterJoin public <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) leftOuterJoin public <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. The supplied org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) rightOuterJoin public <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented) Returns:(undocumented) rightOuterJoin public <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) rightOuterJoin public <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. The supplied org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) fullOuterJoin public <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented) Returns:(undocumented) fullOuterJoin public <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, int numPartitions) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) fullOuterJoin public <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. The supplied org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) saveAsHadoopFiles public void saveAsHadoopFiles(String prefix, String suffix) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented) saveAsHadoopFiles public <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented)keyClass - (undocumented)valueClass - (undocumented)outputFormatClass - (undocumented) saveAsHadoopFiles public <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.mapred.JobConf conf) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented)keyClass - (undocumented)valueClass - (undocumented)outputFormatClass - (undocumented)conf - (undocumented) saveAsNewAPIHadoopFiles public void saveAsNewAPIHadoopFiles(String prefix, String suffix) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented) saveAsNewAPIHadoopFiles public <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented)keyClass - (undocumented)valueClass - (undocumented)outputFormatClass - (undocumented) saveAsNewAPIHadoopFiles public <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.conf.Configuration conf) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented)keyClass - (undocumented)valueClass - (undocumented)outputFormatClass - (undocumented)conf - (undocumented) toJavaDStream public JavaDStream<scala.Tuple2<K,V>> toJavaDStream() Convert to a JavaDStream classTag public scala.reflect.ClassTag<scala.Tuple2<K,V>> classTag() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaPairInputDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaPairInputDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.api.java Class JavaPairInputDStream<K,V> Object org.apache.spark.streaming.api.java.JavaPairDStream<K,V> org.apache.spark.streaming.api.java.JavaPairInputDStream<K,V> All Implemented Interfaces: java.io.Serializable, JavaDStreamLike<scala.Tuple2<K,V>,JavaPairDStream<K,V>,JavaPairRDD<K,V>> Direct Known Subclasses: JavaPairReceiverInputDStream public class JavaPairInputDStream<K,V> extends JavaPairDStream<K,V> A Java-friendly interface to InputDStream of key-value pairs. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaPairInputDStream(InputDStream<scala.Tuple2<K,V>> inputDStream, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag)  Method Summary Methods  Modifier and Type Method and Description static JavaPairDStream<K,V> cache()  static DStream<T> checkpoint(Duration interval)  static scala.reflect.ClassTag<scala.Tuple2<K,V>> classTag()  static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, Partitioner partitioner)  static <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner)  static <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine)  static JavaPairRDD<K,V> compute(Time validTime)  static StreamingContext context()  static JavaDStream<Long> count()  static JavaPairDStream<T,Long> countByValue()  static JavaPairDStream<T,Long> countByValue(int numPartitions)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration)  static DStream<scala.Tuple2<K,V>> dstream()  static JavaPairDStream<K,V> filter(Function<scala.Tuple2<K,V>,Boolean> f)  static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f)  static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f)  static <U> JavaPairDStream<K,U> flatMapValues(Function<V,Iterable<U>> f)  static void foreachRDD(VoidFunction<R> foreachFunc)  static void foreachRDD(VoidFunction2<R,Time> foreachFunc)  static <K,V> JavaPairInputDStream<K,V> fromInputDStream(InputDStream<scala.Tuple2<K,V>> inputDStream, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2) Convert a scala InputDStream of pairs to a Java-friendly JavaPairInputDStream. static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner)  static JavaDStream<java.util.List<T>> glom()  static JavaPairDStream<K,Iterable<V>> groupByKey()  static JavaPairDStream<K,Iterable<V>> groupByKey(int numPartitions)  static JavaPairDStream<K,Iterable<V>> groupByKey(Partitioner partitioner)  static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration)  static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration)  static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, Partitioner partitioner)  InputDStream<scala.Tuple2<K,V>> inputDStream()  static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, Partitioner partitioner)  scala.reflect.ClassTag<K> kClassTag()  static scala.reflect.ClassTag<K> kManifest()  static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner)  static <R> JavaDStream<R> map(Function<T,R> f)  static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f)  static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f)  static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f)  static <U> JavaPairDStream<K,U> mapValues(Function<V,U> f)  static <StateType,MappedType> JavaMapWithStateDStream<K,V,StateType,MappedType> mapWithState(StateSpec<K,V,StateType,MappedType> spec)  static JavaPairDStream<K,V> persist()  static JavaPairDStream<K,V> persist(StorageLevel storageLevel)  static void print()  static void print(int num)  static JavaDStream<T> reduce(Function2<T,T,T> f)  static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func)  static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, int numPartitions)  static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, Partitioner partitioner)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions, Function<scala.Tuple2<K,V>,Boolean> filterFunc)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner, Function<scala.Tuple2<K,V>,Boolean> filterFunc)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration)  static JavaPairDStream<K,V> repartition(int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner)  static void saveAsHadoopFiles(String prefix, String suffix)  static <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass)  static <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.mapred.JobConf conf)  static void saveAsNewAPIHadoopFiles(String prefix, String suffix)  static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass)  static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.conf.Configuration conf)  static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> org.apache.hadoop.conf.Configuration saveAsNewAPIHadoopFiles$default$6()  static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in)  static java.util.List<R> slice(Time fromTime, Time toTime)  static JavaDStream<scala.Tuple2<K,V>> toJavaDStream()  static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc)  static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc)  static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc)  static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc)  static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc)  static JavaPairDStream<K,V> union(JavaPairDStream<K,V> that)  static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc)  static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, int numPartitions)  static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner)  static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner, JavaPairRDD<K,S> initialRDD)  scala.reflect.ClassTag<V> vClassTag()  static scala.reflect.ClassTag<V> vManifest()  static JavaPairDStream<K,V> window(Duration windowDuration)  static JavaPairDStream<K,V> window(Duration windowDuration, Duration slideDuration)  static JavaPairRDD<K,V> wrapRDD(RDD<scala.Tuple2<K,V>> rdd)  Methods inherited from class org.apache.spark.streaming.api.java.JavaPairDStream cache, classTag, cogroup, cogroup, cogroup, combineByKey, combineByKey, compute, dstream, filter, flatMapValues, fromJavaDStream, fromPairDStream, fullOuterJoin, fullOuterJoin, fullOuterJoin, groupByKey, groupByKey, groupByKey, groupByKeyAndWindow, groupByKeyAndWindow, groupByKeyAndWindow, groupByKeyAndWindow, join, join, join, kManifest, leftOuterJoin, leftOuterJoin, leftOuterJoin, mapValues, mapWithState, persist, persist, reduceByKey, reduceByKey, reduceByKey, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, repartition, rightOuterJoin, rightOuterJoin, rightOuterJoin, saveAsHadoopFiles, saveAsHadoopFiles, saveAsHadoopFiles, saveAsNewAPIHadoopFiles, saveAsNewAPIHadoopFiles, saveAsNewAPIHadoopFiles, scalaToJavaLong, toJavaDStream, union, updateStateByKey, updateStateByKey, updateStateByKey, updateStateByKey, vManifest, window, window, wrapRDD Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.streaming.api.java.JavaDStreamLike checkpoint, context, count, countByValue, countByValue, countByValueAndWindow, countByValueAndWindow, countByWindow, flatMap, flatMapToPair, foreachRDD, foreachRDD, glom, map, mapPartitions, mapPartitionsToPair, mapToPair, print, print, reduce, reduceByWindow, reduceByWindow, scalaIntToJavaLong, slice, transform, transform, transformToPair, transformToPair, transformWith, transformWith, transformWithToPair, transformWithToPair Constructor Detail JavaPairInputDStream public JavaPairInputDStream(InputDStream<scala.Tuple2<K,V>> inputDStream, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag) Method Detail fromInputDStream public static <K,V> JavaPairInputDStream<K,V> fromInputDStream(InputDStream<scala.Tuple2<K,V>> inputDStream, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2) Convert a scala InputDStream of pairs to a Java-friendly JavaPairInputDStream. Parameters:inputDStream - (undocumented)evidence$1 - (undocumented)evidence$2 - (undocumented) Returns:(undocumented) scalaIntToJavaLong public static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in) print public static void print() print public static void print(int num) count public static JavaDStream<Long> count() countByValue public static JavaPairDStream<T,Long> countByValue() countByValue public static JavaPairDStream<T,Long> countByValue(int numPartitions) countByWindow public static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) glom public static JavaDStream<java.util.List<T>> glom() context public static StreamingContext context() map public static <R> JavaDStream<R> map(Function<T,R> f) mapToPair public static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f) flatMap public static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f) flatMapToPair public static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) mapPartitions public static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) mapPartitionsToPair public static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) reduce public static JavaDStream<T> reduce(Function2<T,T,T> f) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) slice public static java.util.List<R> slice(Time fromTime, Time toTime) foreachRDD public static void foreachRDD(VoidFunction<R> foreachFunc) foreachRDD public static void foreachRDD(VoidFunction2<R,Time> foreachFunc) transform public static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc) transform public static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc) checkpoint public static DStream<T> checkpoint(Duration interval) dstream public static DStream<scala.Tuple2<K,V>> dstream() kManifest public static scala.reflect.ClassTag<K> kManifest() vManifest public static scala.reflect.ClassTag<V> vManifest() wrapRDD public static JavaPairRDD<K,V> wrapRDD(RDD<scala.Tuple2<K,V>> rdd) filter public static JavaPairDStream<K,V> filter(Function<scala.Tuple2<K,V>,Boolean> f) cache public static JavaPairDStream<K,V> cache() persist public static JavaPairDStream<K,V> persist() persist public static JavaPairDStream<K,V> persist(StorageLevel storageLevel) repartition public static JavaPairDStream<K,V> repartition(int numPartitions) compute public static JavaPairRDD<K,V> compute(Time validTime) window public static JavaPairDStream<K,V> window(Duration windowDuration) window public static JavaPairDStream<K,V> window(Duration windowDuration, Duration slideDuration) union public static JavaPairDStream<K,V> union(JavaPairDStream<K,V> that) groupByKey public static JavaPairDStream<K,Iterable<V>> groupByKey() groupByKey public static JavaPairDStream<K,Iterable<V>> groupByKey(int numPartitions) groupByKey public static JavaPairDStream<K,Iterable<V>> groupByKey(Partitioner partitioner) reduceByKey public static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func) reduceByKey public static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, int numPartitions) reduceByKey public static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, Partitioner partitioner) combineByKey public static <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner) combineByKey public static <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine) groupByKeyAndWindow public static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration) groupByKeyAndWindow public static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration) groupByKeyAndWindow public static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) groupByKeyAndWindow public static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, Partitioner partitioner) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions, Function<scala.Tuple2<K,V>,Boolean> filterFunc) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner, Function<scala.Tuple2<K,V>,Boolean> filterFunc) mapWithState public static <StateType,MappedType> JavaMapWithStateDStream<K,V,StateType,MappedType> mapWithState(StateSpec<K,V,StateType,MappedType> spec) updateStateByKey public static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc) updateStateByKey public static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, int numPartitions) updateStateByKey public static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner) updateStateByKey public static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner, JavaPairRDD<K,S> initialRDD) mapValues public static <U> JavaPairDStream<K,U> mapValues(Function<V,U> f) flatMapValues public static <U> JavaPairDStream<K,U> flatMapValues(Function<V,Iterable<U>> f) cogroup public static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other) cogroup public static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, int numPartitions) cogroup public static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, Partitioner partitioner) join public static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other) join public static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, int numPartitions) join public static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, Partitioner partitioner) leftOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other) leftOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, int numPartitions) leftOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) rightOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other) rightOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, int numPartitions) rightOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) fullOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other) fullOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, int numPartitions) fullOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) saveAsHadoopFiles public static void saveAsHadoopFiles(String prefix, String suffix) saveAsHadoopFiles public static <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) saveAsHadoopFiles public static <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.mapred.JobConf conf) saveAsNewAPIHadoopFiles public static void saveAsNewAPIHadoopFiles(String prefix, String suffix) saveAsNewAPIHadoopFiles public static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) saveAsNewAPIHadoopFiles public static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.conf.Configuration conf) toJavaDStream public static JavaDStream<scala.Tuple2<K,V>> toJavaDStream() classTag public static scala.reflect.ClassTag<scala.Tuple2<K,V>> classTag() saveAsNewAPIHadoopFiles$default$6 public static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> org.apache.hadoop.conf.Configuration saveAsNewAPIHadoopFiles$default$6() inputDStream public InputDStream<scala.Tuple2<K,V>> inputDStream() kClassTag public scala.reflect.ClassTag<K> kClassTag() vClassTag public scala.reflect.ClassTag<V> vClassTag() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaPairRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaPairRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class JavaPairRDD<K,V> Object org.apache.spark.api.java.JavaPairRDD<K,V> All Implemented Interfaces: java.io.Serializable, JavaRDDLike<scala.Tuple2<K,V>,JavaPairRDD<K,V>> Direct Known Subclasses: JavaHadoopRDD, JavaNewHadoopRDD public class JavaPairRDD<K,V> extends Object See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaPairRDD(RDD<scala.Tuple2<K,V>> rdd, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag)  Method Summary Methods  Modifier and Type Method and Description static <U> U aggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp)  <U> JavaPairRDD<K,U> aggregateByKey(U zeroValue, Function2<U,V,U> seqFunc, Function2<U,U,U> combFunc) Aggregate the values of each key, using given combine functions and a neutral "zero value". <U> JavaPairRDD<K,U> aggregateByKey(U zeroValue, int numPartitions, Function2<U,V,U> seqFunc, Function2<U,U,U> combFunc) Aggregate the values of each key, using given combine functions and a neutral "zero value". <U> JavaPairRDD<K,U> aggregateByKey(U zeroValue, Partitioner partitioner, Function2<U,V,U> seqFunc, Function2<U,U,U> combFunc) Aggregate the values of each key, using given combine functions and a neutral "zero value". JavaPairRDD<K,V> cache() Persist this RDD with the default storage level (`MEMORY_ONLY`). static <U> JavaPairRDD<T,U> cartesian(JavaRDDLike<U,?> other)  static void checkpoint()  scala.reflect.ClassTag<scala.Tuple2<K,V>> classTag()  JavaPairRDD<K,V> coalesce(int numPartitions) Return a new RDD that is reduced into numPartitions partitions. JavaPairRDD<K,V> coalesce(int numPartitions, boolean shuffle) Return a new RDD that is reduced into numPartitions partitions. <W> JavaPairRDD<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairRDD<K,W> other) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. <W> JavaPairRDD<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairRDD<K,W> other, int numPartitions) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. <W> JavaPairRDD<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairRDD<K,W> other, Partitioner partitioner) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. <W1,W2> JavaPairRDD<K,scala.Tuple3<Iterable<V>,Iterable<W1>,Iterable<W2>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. <W1,W2> JavaPairRDD<K,scala.Tuple3<Iterable<V>,Iterable<W1>,Iterable<W2>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, int numPartitions) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. <W1,W2,W3> JavaPairRDD<K,scala.Tuple4<Iterable<V>,Iterable<W1>,Iterable<W2>,Iterable<W3>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, JavaPairRDD<K,W3> other3) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. <W1,W2,W3> JavaPairRDD<K,scala.Tuple4<Iterable<V>,Iterable<W1>,Iterable<W2>,Iterable<W3>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, JavaPairRDD<K,W3> other3, int numPartitions) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. <W1,W2,W3> JavaPairRDD<K,scala.Tuple4<Iterable<V>,Iterable<W1>,Iterable<W2>,Iterable<W3>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, JavaPairRDD<K,W3> other3, Partitioner partitioner) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. <W1,W2> JavaPairRDD<K,scala.Tuple3<Iterable<V>,Iterable<W1>,Iterable<W2>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, Partitioner partitioner) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. static java.util.List<T> collect()  java.util.Map<K,V> collectAsMap() Return the key-value pairs in this RDD to the master as a Map. static JavaFutureAction<java.util.List<T>> collectAsync()  static java.util.List<T>[] collectPartitions(int[] partitionIds)  <C> JavaPairRDD<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners) Simplified version of combineByKey that hash-partitions the resulting RDD using the existing partitioner/parallelism level and using map-side aggregation. <C> JavaPairRDD<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, int numPartitions) Simplified version of combineByKey that hash-partitions the output RDD and uses map-side aggregation. <C> JavaPairRDD<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner) Generic function to combine the elements for each key using a custom set of aggregation functions. <C> JavaPairRDD<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine, Serializer serializer) Generic function to combine the elements for each key using a custom set of aggregation functions. static SparkContext context()  static long count()  static PartialResult<BoundedDouble> countApprox(long timeout)  static PartialResult<BoundedDouble> countApprox(long timeout, double confidence)  static long countApproxDistinct(double relativeSD)  JavaPairRDD<K,Long> countApproxDistinctByKey(double relativeSD) Return approximate number of distinct values for each key in this RDD. JavaPairRDD<K,Long> countApproxDistinctByKey(double relativeSD, int numPartitions) Return approximate number of distinct values for each key in this RDD. JavaPairRDD<K,Long> countApproxDistinctByKey(double relativeSD, Partitioner partitioner) Return approximate number of distinct values for each key in this RDD. static JavaFutureAction<Long> countAsync()  java.util.Map<K,Long> countByKey() Count the number of elements for each key, and return the result to the master as a Map. PartialResult<java.util.Map<K,BoundedDouble>> countByKeyApprox(long timeout) Approximate version of countByKey that can return a partial result if it does not finish within a timeout. PartialResult<java.util.Map<K,BoundedDouble>> countByKeyApprox(long timeout, double confidence) Approximate version of countByKey that can return a partial result if it does not finish within a timeout. static java.util.Map<T,Long> countByValue()  static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout)  static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence)  JavaPairRDD<K,V> distinct() Return a new RDD containing the distinct elements in this RDD. JavaPairRDD<K,V> distinct(int numPartitions) Return a new RDD containing the distinct elements in this RDD. JavaPairRDD<K,V> filter(Function<scala.Tuple2<K,V>,Boolean> f) Return a new RDD containing only the elements that satisfy a predicate. scala.Tuple2<K,V> first() Return the first element in this RDD. static <U> JavaRDD<U> flatMap(FlatMapFunction<T,U> f)  static JavaDoubleRDD flatMapToDouble(DoubleFlatMapFunction<T> f)  static <K2,V2> JavaPairRDD<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f)  <U> JavaPairRDD<K,U> flatMapValues(Function<V,Iterable<U>> f) Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD's partitioning. static T fold(T zeroValue, Function2<T,T,T> f)  JavaPairRDD<K,V> foldByKey(V zeroValue, Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). JavaPairRDD<K,V> foldByKey(V zeroValue, int numPartitions, Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g ., Nil for list concatenation, 0 for addition, or 1 for multiplication.). JavaPairRDD<K,V> foldByKey(V zeroValue, Partitioner partitioner, Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g ., Nil for list concatenation, 0 for addition, or 1 for multiplication.). static void foreach(VoidFunction<T> f)  static JavaFutureAction<Void> foreachAsync(VoidFunction<T> f)  static void foreachPartition(VoidFunction<java.util.Iterator<T>> f)  static JavaFutureAction<Void> foreachPartitionAsync(VoidFunction<java.util.Iterator<T>> f)  static <K,V> JavaPairRDD<K,V> fromJavaRDD(JavaRDD<scala.Tuple2<K,V>> rdd) Convert a JavaRDD of key-value pairs to JavaPairRDD. static <K,V> JavaPairRDD<K,V> fromRDD(RDD<scala.Tuple2<K,V>> rdd, scala.reflect.ClassTag<K> evidence$5, scala.reflect.ClassTag<V> evidence$6)  <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairRDD<K,W> other) Perform a full outer join of this and other. <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairRDD<K,W> other, int numPartitions) Perform a full outer join of this and other. <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairRDD<K,W> other, Partitioner partitioner) Perform a full outer join of this and other. static Optional<String> getCheckpointFile()  static int getNumPartitions()  static StorageLevel getStorageLevel()  static JavaRDD<java.util.List<T>> glom()  static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f)  static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f, int numPartitions)  JavaPairRDD<K,Iterable<V>> groupByKey() Group the values for each key in the RDD into a single sequence. JavaPairRDD<K,Iterable<V>> groupByKey(int numPartitions) Group the values for each key in the RDD into a single sequence. JavaPairRDD<K,Iterable<V>> groupByKey(Partitioner partitioner) Group the values for each key in the RDD into a single sequence. <W> JavaPairRDD<K,scala.Tuple2<Iterable<V>,Iterable<W>>> groupWith(JavaPairRDD<K,W> other) Alias for cogroup. <W1,W2> JavaPairRDD<K,scala.Tuple3<Iterable<V>,Iterable<W1>,Iterable<W2>>> groupWith(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2) Alias for cogroup. <W1,W2,W3> JavaPairRDD<K,scala.Tuple4<Iterable<V>,Iterable<W1>,Iterable<W2>,Iterable<W3>>> groupWith(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, JavaPairRDD<K,W3> other3) Alias for cogroup. static int id()  JavaPairRDD<K,V> intersection(JavaPairRDD<K,V> other) Return the intersection of this RDD and another one. static boolean isCheckpointed()  static boolean isEmpty()  static java.util.Iterator<T> iterator(Partition split, TaskContext taskContext)  <W> JavaPairRDD<K,scala.Tuple2<V,W>> join(JavaPairRDD<K,W> other) Return an RDD containing all pairs of elements with matching keys in this and other. <W> JavaPairRDD<K,scala.Tuple2<V,W>> join(JavaPairRDD<K,W> other, int numPartitions) Return an RDD containing all pairs of elements with matching keys in this and other. <W> JavaPairRDD<K,scala.Tuple2<V,W>> join(JavaPairRDD<K,W> other, Partitioner partitioner) Return an RDD containing all pairs of elements with matching keys in this and other. scala.reflect.ClassTag<K> kClassTag()  static <U> JavaPairRDD<U,T> keyBy(Function<T,U> f)  JavaRDD<K> keys() Return an RDD with the keys of each tuple. <W> JavaPairRDD<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairRDD<K,W> other) Perform a left outer join of this and other. <W> JavaPairRDD<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairRDD<K,W> other, int numPartitions) Perform a left outer join of this and other. <W> JavaPairRDD<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairRDD<K,W> other, Partitioner partitioner) Perform a left outer join of this and other. java.util.List<V> lookup(K key) Return the list of values in the RDD for key key. static <R> JavaRDD<R> map(Function<T,R> f)  static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f)  static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f, boolean preservesPartitioning)  static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f)  static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f, boolean preservesPartitioning)  static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f)  static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f, boolean preservesPartitioning)  static <R> JavaRDD<R> mapPartitionsWithIndex(Function2<Integer,java.util.Iterator<T>,java.util.Iterator<R>> f, boolean preservesPartitioning)  static <R> boolean mapPartitionsWithIndex$default$2()  static <R> JavaDoubleRDD mapToDouble(DoubleFunction<T> f)  static <K2,V2> JavaPairRDD<K2,V2> mapToPair(PairFunction<T,K2,V2> f)  <U> JavaPairRDD<K,U> mapValues(Function<V,U> f) Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD's partitioning. static T max(java.util.Comparator<T> comp)  static T min(java.util.Comparator<T> comp)  static String name()  JavaPairRDD<K,V> partitionBy(Partitioner partitioner) Return a copy of the RDD partitioned using the specified partitioner. static Optional<Partitioner> partitioner()  static java.util.List<Partition> partitions()  JavaPairRDD<K,V> persist(StorageLevel newLevel) Set this RDD's storage level to persist its values across operations after the first time it is computed. static JavaRDD<String> pipe(java.util.List<String> command)  static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env)  static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize)  static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize, String encoding)  static JavaRDD<String> pipe(String command)  RDD<scala.Tuple2<K,V>> rdd()  static T reduce(Function2<T,T,T> f)  JavaPairRDD<K,V> reduceByKey(Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function. JavaPairRDD<K,V> reduceByKey(Function2<V,V,V> func, int numPartitions) Merge the values for each key using an associative and commutative reduce function. JavaPairRDD<K,V> reduceByKey(Partitioner partitioner, Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function. java.util.Map<K,V> reduceByKeyLocally(Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function, but return the result immediately to the master as a Map. JavaPairRDD<K,V> repartition(int numPartitions) Return a new RDD that has exactly numPartitions partitions. JavaPairRDD<K,V> repartitionAndSortWithinPartitions(Partitioner partitioner) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. JavaPairRDD<K,V> repartitionAndSortWithinPartitions(Partitioner partitioner, java.util.Comparator<K> comp) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairRDD<K,W> other) Perform a right outer join of this and other. <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairRDD<K,W> other, int numPartitions) Perform a right outer join of this and other. <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairRDD<K,W> other, Partitioner partitioner) Perform a right outer join of this and other. JavaPairRDD<K,V> sample(boolean withReplacement, double fraction) Return a sampled subset of this RDD. JavaPairRDD<K,V> sample(boolean withReplacement, double fraction, long seed) Return a sampled subset of this RDD. JavaPairRDD<K,V> sampleByKey(boolean withReplacement, java.util.Map<K,Double> fractions) Return a subset of this RDD sampled by key (via stratified sampling). JavaPairRDD<K,V> sampleByKey(boolean withReplacement, java.util.Map<K,Double> fractions, long seed) Return a subset of this RDD sampled by key (via stratified sampling). JavaPairRDD<K,V> sampleByKeyExact(boolean withReplacement, java.util.Map<K,Double> fractions) Return a subset of this RDD sampled by key (via stratified sampling) containing exactly math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). JavaPairRDD<K,V> sampleByKeyExact(boolean withReplacement, java.util.Map<K,Double> fractions, long seed) Return a subset of this RDD sampled by key (via stratified sampling) containing exactly math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). void saveAsHadoopDataset(org.apache.hadoop.mapred.JobConf conf) Output the RDD to any Hadoop-supported storage system, using a Hadoop JobConf object for that storage system. <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) Output the RDD to any Hadoop-supported file system. <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) Output the RDD to any Hadoop-supported file system, compressing with the supplied codec. <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.mapred.JobConf conf) Output the RDD to any Hadoop-supported file system. void saveAsNewAPIHadoopDataset(org.apache.hadoop.conf.Configuration conf) Output the RDD to any Hadoop-supported storage system, using a Configuration object for that storage system. <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) Output the RDD to any Hadoop-supported file system. <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.conf.Configuration conf) Output the RDD to any Hadoop-supported file system. static void saveAsObjectFile(String path)  static void saveAsTextFile(String path)  static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec)  JavaPairRDD<K,V> setName(String name) Assign a name to this RDD JavaPairRDD<K,V> sortByKey() Sort the RDD by key, so that each partition contains a sorted range of the elements in ascending order. JavaPairRDD<K,V> sortByKey(boolean ascending) Sort the RDD by key, so that each partition contains a sorted range of the elements. JavaPairRDD<K,V> sortByKey(boolean ascending, int numPartitions) Sort the RDD by key, so that each partition contains a sorted range of the elements. JavaPairRDD<K,V> sortByKey(java.util.Comparator<K> comp) Sort the RDD by key, so that each partition contains a sorted range of the elements. JavaPairRDD<K,V> sortByKey(java.util.Comparator<K> comp, boolean ascending) Sort the RDD by key, so that each partition contains a sorted range of the elements. JavaPairRDD<K,V> sortByKey(java.util.Comparator<K> comp, boolean ascending, int numPartitions) Sort the RDD by key, so that each partition contains a sorted range of the elements. JavaPairRDD<K,V> subtract(JavaPairRDD<K,V> other) Return an RDD with the elements from this that are not in other. JavaPairRDD<K,V> subtract(JavaPairRDD<K,V> other, int numPartitions) Return an RDD with the elements from this that are not in other. JavaPairRDD<K,V> subtract(JavaPairRDD<K,V> other, Partitioner p) Return an RDD with the elements from this that are not in other. <W> JavaPairRDD<K,V> subtractByKey(JavaPairRDD<K,W> other) Return an RDD with the pairs from this whose keys are not in other. <W> JavaPairRDD<K,V> subtractByKey(JavaPairRDD<K,W> other, int numPartitions) Return an RDD with the pairs from `this` whose keys are not in `other`. <W> JavaPairRDD<K,V> subtractByKey(JavaPairRDD<K,W> other, Partitioner p) Return an RDD with the pairs from `this` whose keys are not in `other`. static java.util.List<T> take(int num)  static JavaFutureAction<java.util.List<T>> takeAsync(int num)  static java.util.List<T> takeOrdered(int num)  static java.util.List<T> takeOrdered(int num, java.util.Comparator<T> comp)  static java.util.List<T> takeSample(boolean withReplacement, int num)  static java.util.List<T> takeSample(boolean withReplacement, int num, long seed)  static String toDebugString()  static java.util.Iterator<T> toLocalIterator()  static java.util.List<T> top(int num)  static java.util.List<T> top(int num, java.util.Comparator<T> comp)  static <K,V> RDD<scala.Tuple2<K,V>> toRDD(JavaPairRDD<K,V> rdd)  static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp)  static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp, int depth)  static T treeReduce(Function2<T,T,T> f)  static T treeReduce(Function2<T,T,T> f, int depth)  JavaPairRDD<K,V> union(JavaPairRDD<K,V> other) Return the union of this RDD and another one. JavaPairRDD<K,V> unpersist() Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. JavaPairRDD<K,V> unpersist(boolean blocking) Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. JavaRDD<V> values() Return an RDD with the values of each tuple. scala.reflect.ClassTag<V> vClassTag()  JavaPairRDD<K,V> wrapRDD(RDD<scala.Tuple2<K,V>> rdd)  static <U> JavaPairRDD<T,U> zip(JavaRDDLike<U,?> other)  static <U,V> JavaRDD<V> zipPartitions(JavaRDDLike<U,?> other, FlatMapFunction2<java.util.Iterator<T>,java.util.Iterator<U>,V> f)  static JavaPairRDD<T,Long> zipWithIndex()  static JavaPairRDD<T,Long> zipWithUniqueId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.api.java.JavaRDDLike aggregate, cartesian, checkpoint, collect, collectAsync, collectPartitions, context, count, countApprox, countApprox, countApproxDistinct, countAsync, countByValue, countByValueApprox, countByValueApprox, flatMap, flatMapToDouble, flatMapToPair, fold, foreach, foreachAsync, foreachPartition, foreachPartitionAsync, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, id, isCheckpointed, isEmpty, iterator, keyBy, map, mapPartitions, mapPartitions, mapPartitionsToDouble, mapPartitionsToDouble, mapPartitionsToPair, mapPartitionsToPair, mapPartitionsWithIndex, mapToDouble, mapToPair, max, min, name, partitioner, partitions, pipe, pipe, pipe, pipe, pipe, reduce, saveAsObjectFile, saveAsTextFile, saveAsTextFile, take, takeAsync, takeOrdered, takeOrdered, takeSample, takeSample, toDebugString, toLocalIterator, top, top, treeAggregate, treeAggregate, treeReduce, treeReduce, zip, zipPartitions, zipWithIndex, zipWithUniqueId Constructor Detail JavaPairRDD public JavaPairRDD(RDD<scala.Tuple2<K,V>> rdd, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag) Method Detail fromRDD public static <K,V> JavaPairRDD<K,V> fromRDD(RDD<scala.Tuple2<K,V>> rdd, scala.reflect.ClassTag<K> evidence$5, scala.reflect.ClassTag<V> evidence$6) toRDD public static <K,V> RDD<scala.Tuple2<K,V>> toRDD(JavaPairRDD<K,V> rdd) fromJavaRDD public static <K,V> JavaPairRDD<K,V> fromJavaRDD(JavaRDD<scala.Tuple2<K,V>> rdd) Convert a JavaRDD of key-value pairs to JavaPairRDD. partitions public static java.util.List<Partition> partitions() getNumPartitions public static int getNumPartitions() partitioner public static Optional<Partitioner> partitioner() context public static SparkContext context() id public static int id() getStorageLevel public static StorageLevel getStorageLevel() iterator public static java.util.Iterator<T> iterator(Partition split, TaskContext taskContext) map public static <R> JavaRDD<R> map(Function<T,R> f) mapPartitionsWithIndex public static <R> JavaRDD<R> mapPartitionsWithIndex(Function2<Integer,java.util.Iterator<T>,java.util.Iterator<R>> f, boolean preservesPartitioning) mapToDouble public static <R> JavaDoubleRDD mapToDouble(DoubleFunction<T> f) mapToPair public static <K2,V2> JavaPairRDD<K2,V2> mapToPair(PairFunction<T,K2,V2> f) flatMap public static <U> JavaRDD<U> flatMap(FlatMapFunction<T,U> f) flatMapToDouble public static JavaDoubleRDD flatMapToDouble(DoubleFlatMapFunction<T> f) flatMapToPair public static <K2,V2> JavaPairRDD<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) mapPartitions public static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) mapPartitions public static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f, boolean preservesPartitioning) mapPartitionsToDouble public static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f) mapPartitionsToPair public static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) mapPartitionsToDouble public static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f, boolean preservesPartitioning) mapPartitionsToPair public static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f, boolean preservesPartitioning) foreachPartition public static void foreachPartition(VoidFunction<java.util.Iterator<T>> f) glom public static JavaRDD<java.util.List<T>> glom() cartesian public static <U> JavaPairRDD<T,U> cartesian(JavaRDDLike<U,?> other) groupBy public static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f) groupBy public static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f, int numPartitions) pipe public static JavaRDD<String> pipe(String command) pipe public static JavaRDD<String> pipe(java.util.List<String> command) pipe public static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env) pipe public static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize) pipe public static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize, String encoding) zip public static <U> JavaPairRDD<T,U> zip(JavaRDDLike<U,?> other) zipPartitions public static <U,V> JavaRDD<V> zipPartitions(JavaRDDLike<U,?> other, FlatMapFunction2<java.util.Iterator<T>,java.util.Iterator<U>,V> f) zipWithUniqueId public static JavaPairRDD<T,Long> zipWithUniqueId() zipWithIndex public static JavaPairRDD<T,Long> zipWithIndex() foreach public static void foreach(VoidFunction<T> f) collect public static java.util.List<T> collect() toLocalIterator public static java.util.Iterator<T> toLocalIterator() collectPartitions public static java.util.List<T>[] collectPartitions(int[] partitionIds) reduce public static T reduce(Function2<T,T,T> f) treeReduce public static T treeReduce(Function2<T,T,T> f, int depth) treeReduce public static T treeReduce(Function2<T,T,T> f) fold public static T fold(T zeroValue, Function2<T,T,T> f) aggregate public static <U> U aggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) treeAggregate public static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp, int depth) treeAggregate public static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) count public static long count() countApprox public static PartialResult<BoundedDouble> countApprox(long timeout, double confidence) countApprox public static PartialResult<BoundedDouble> countApprox(long timeout) countByValue public static java.util.Map<T,Long> countByValue() countByValueApprox public static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence) countByValueApprox public static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout) take public static java.util.List<T> take(int num) takeSample public static java.util.List<T> takeSample(boolean withReplacement, int num) takeSample public static java.util.List<T> takeSample(boolean withReplacement, int num, long seed) isEmpty public static boolean isEmpty() saveAsTextFile public static void saveAsTextFile(String path) saveAsTextFile public static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) saveAsObjectFile public static void saveAsObjectFile(String path) keyBy public static <U> JavaPairRDD<U,T> keyBy(Function<T,U> f) checkpoint public static void checkpoint() isCheckpointed public static boolean isCheckpointed() getCheckpointFile public static Optional<String> getCheckpointFile() toDebugString public static String toDebugString() top public static java.util.List<T> top(int num, java.util.Comparator<T> comp) top public static java.util.List<T> top(int num) takeOrdered public static java.util.List<T> takeOrdered(int num, java.util.Comparator<T> comp) max public static T max(java.util.Comparator<T> comp) min public static T min(java.util.Comparator<T> comp) takeOrdered public static java.util.List<T> takeOrdered(int num) countApproxDistinct public static long countApproxDistinct(double relativeSD) name public static String name() countAsync public static JavaFutureAction<Long> countAsync() collectAsync public static JavaFutureAction<java.util.List<T>> collectAsync() takeAsync public static JavaFutureAction<java.util.List<T>> takeAsync(int num) foreachAsync public static JavaFutureAction<Void> foreachAsync(VoidFunction<T> f) foreachPartitionAsync public static JavaFutureAction<Void> foreachPartitionAsync(VoidFunction<java.util.Iterator<T>> f) mapPartitionsWithIndex$default$2 public static <R> boolean mapPartitionsWithIndex$default$2() rdd public RDD<scala.Tuple2<K,V>> rdd() kClassTag public scala.reflect.ClassTag<K> kClassTag() vClassTag public scala.reflect.ClassTag<V> vClassTag() wrapRDD public JavaPairRDD<K,V> wrapRDD(RDD<scala.Tuple2<K,V>> rdd) classTag public scala.reflect.ClassTag<scala.Tuple2<K,V>> classTag() cache public JavaPairRDD<K,V> cache() Persist this RDD with the default storage level (`MEMORY_ONLY`). persist public JavaPairRDD<K,V> persist(StorageLevel newLevel) Set this RDD's storage level to persist its values across operations after the first time it is computed. Can only be called once on each RDD. Parameters:newLevel - (undocumented) Returns:(undocumented) unpersist public JavaPairRDD<K,V> unpersist() Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. This method blocks until all blocks are deleted. Returns:(undocumented) unpersist public JavaPairRDD<K,V> unpersist(boolean blocking) Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. Parameters:blocking - Whether to block until all blocks are deleted. Returns:(undocumented) distinct public JavaPairRDD<K,V> distinct() Return a new RDD containing the distinct elements in this RDD. Returns:(undocumented) distinct public JavaPairRDD<K,V> distinct(int numPartitions) Return a new RDD containing the distinct elements in this RDD. Parameters:numPartitions - (undocumented) Returns:(undocumented) filter public JavaPairRDD<K,V> filter(Function<scala.Tuple2<K,V>,Boolean> f) Return a new RDD containing only the elements that satisfy a predicate. Parameters:f - (undocumented) Returns:(undocumented) coalesce public JavaPairRDD<K,V> coalesce(int numPartitions) Return a new RDD that is reduced into numPartitions partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented) coalesce public JavaPairRDD<K,V> coalesce(int numPartitions, boolean shuffle) Return a new RDD that is reduced into numPartitions partitions. Parameters:numPartitions - (undocumented)shuffle - (undocumented) Returns:(undocumented) repartition public JavaPairRDD<K,V> repartition(int numPartitions) Return a new RDD that has exactly numPartitions partitions. Can increase or decrease the level of parallelism in this RDD. Internally, this uses a shuffle to redistribute data. If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle. Parameters:numPartitions - (undocumented) Returns:(undocumented) sample public JavaPairRDD<K,V> sample(boolean withReplacement, double fraction) Return a sampled subset of this RDD. Parameters:withReplacement - (undocumented)fraction - (undocumented) Returns:(undocumented) sample public JavaPairRDD<K,V> sample(boolean withReplacement, double fraction, long seed) Return a sampled subset of this RDD. Parameters:withReplacement - (undocumented)fraction - (undocumented)seed - (undocumented) Returns:(undocumented) sampleByKey public JavaPairRDD<K,V> sampleByKey(boolean withReplacement, java.util.Map<K,Double> fractions, long seed) Return a subset of this RDD sampled by key (via stratified sampling). Create a sample of this RDD using variable sampling rates for different keys as specified by fractions, a key to sampling rate map, via simple random sampling with one pass over the RDD, to produce a sample of size that's approximately equal to the sum of math.ceil(numItems * samplingRate) over all key values. Parameters:withReplacement - (undocumented)fractions - (undocumented)seed - (undocumented) Returns:(undocumented) sampleByKey public JavaPairRDD<K,V> sampleByKey(boolean withReplacement, java.util.Map<K,Double> fractions) Return a subset of this RDD sampled by key (via stratified sampling). Create a sample of this RDD using variable sampling rates for different keys as specified by fractions, a key to sampling rate map, via simple random sampling with one pass over the RDD, to produce a sample of size that's approximately equal to the sum of math.ceil(numItems * samplingRate) over all key values. Use Utils.random.nextLong as the default seed for the random number generator. Parameters:withReplacement - (undocumented)fractions - (undocumented) Returns:(undocumented) sampleByKeyExact public JavaPairRDD<K,V> sampleByKeyExact(boolean withReplacement, java.util.Map<K,Double> fractions, long seed) Return a subset of this RDD sampled by key (via stratified sampling) containing exactly math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). This method differs from sampleByKey in that we make additional passes over the RDD to create a sample size that's exactly equal to the sum of math.ceil(numItems * samplingRate) over all key values with a 99.99% confidence. When sampling without replacement, we need one additional pass over the RDD to guarantee sample size; when sampling with replacement, we need two additional passes. Parameters:withReplacement - (undocumented)fractions - (undocumented)seed - (undocumented) Returns:(undocumented) sampleByKeyExact public JavaPairRDD<K,V> sampleByKeyExact(boolean withReplacement, java.util.Map<K,Double> fractions) Return a subset of this RDD sampled by key (via stratified sampling) containing exactly math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). This method differs from sampleByKey in that we make additional passes over the RDD to create a sample size that's exactly equal to the sum of math.ceil(numItems * samplingRate) over all key values with a 99.99% confidence. When sampling without replacement, we need one additional pass over the RDD to guarantee sample size; when sampling with replacement, we need two additional passes. Use Utils.random.nextLong as the default seed for the random number generator. Parameters:withReplacement - (undocumented)fractions - (undocumented) Returns:(undocumented) union public JavaPairRDD<K,V> union(JavaPairRDD<K,V> other) Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them). Parameters:other - (undocumented) Returns:(undocumented) intersection public JavaPairRDD<K,V> intersection(JavaPairRDD<K,V> other) Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did. Note that this method performs a shuffle internally. Parameters:other - (undocumented) Returns:(undocumented) first public scala.Tuple2<K,V> first() Description copied from interface: JavaRDDLike Return the first element in this RDD. Returns:(undocumented) combineByKey public <C> JavaPairRDD<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine, Serializer serializer) Generic function to combine the elements for each key using a custom set of aggregation functions. Turns a JavaPairRDD[(K, V)] into a result of type JavaPairRDD[(K, C)], for a "combined type" C. Note that V and C can be different -- for example, one might group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]). Users provide three functions: - createCombiner, which turns a V into a C (e.g., creates a one-element list) - mergeValue, to merge a V into a C (e.g., adds it to the end of a list) - mergeCombiners, to combine two C's into a single one. In addition, users can control the partitioning of the output RDD, the serializer that is use for the shuffle, and whether to perform map-side aggregation (if a mapper can produce multiple items with the same key). Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)partitioner - (undocumented)mapSideCombine - (undocumented)serializer - (undocumented) Returns:(undocumented) combineByKey public <C> JavaPairRDD<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner) Generic function to combine the elements for each key using a custom set of aggregation functions. Turns a JavaPairRDD[(K, V)] into a result of type JavaPairRDD[(K, C)], for a "combined type" C. Note that V and C can be different -- for example, one might group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]). Users provide three functions: - createCombiner, which turns a V into a C (e.g., creates a one-element list) - mergeValue, to merge a V into a C (e.g., adds it to the end of a list) - mergeCombiners, to combine two C's into a single one. In addition, users can control the partitioning of the output RDD. This method automatically uses map-side aggregation in shuffling the RDD. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)partitioner - (undocumented) Returns:(undocumented) combineByKey public <C> JavaPairRDD<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, int numPartitions) Simplified version of combineByKey that hash-partitions the output RDD and uses map-side aggregation. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)numPartitions - (undocumented) Returns:(undocumented) reduceByKey public JavaPairRDD<K,V> reduceByKey(Partitioner partitioner, Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Parameters:partitioner - (undocumented)func - (undocumented) Returns:(undocumented) reduceByKeyLocally public java.util.Map<K,V> reduceByKeyLocally(Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function, but return the result immediately to the master as a Map. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Parameters:func - (undocumented) Returns:(undocumented) countByKey public java.util.Map<K,Long> countByKey() Count the number of elements for each key, and return the result to the master as a Map. countByKeyApprox public PartialResult<java.util.Map<K,BoundedDouble>> countByKeyApprox(long timeout) Approximate version of countByKey that can return a partial result if it does not finish within a timeout. Parameters:timeout - (undocumented) Returns:(undocumented) countByKeyApprox public PartialResult<java.util.Map<K,BoundedDouble>> countByKeyApprox(long timeout, double confidence) Approximate version of countByKey that can return a partial result if it does not finish within a timeout. Parameters:timeout - (undocumented)confidence - (undocumented) Returns:(undocumented) aggregateByKey public <U> JavaPairRDD<K,U> aggregateByKey(U zeroValue, Partitioner partitioner, Function2<U,V,U> seqFunc, Function2<U,U,U> combFunc) Aggregate the values of each key, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U. Parameters:zeroValue - (undocumented)partitioner - (undocumented)seqFunc - (undocumented)combFunc - (undocumented) Returns:(undocumented) aggregateByKey public <U> JavaPairRDD<K,U> aggregateByKey(U zeroValue, int numPartitions, Function2<U,V,U> seqFunc, Function2<U,U,U> combFunc) Aggregate the values of each key, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U. Parameters:zeroValue - (undocumented)numPartitions - (undocumented)seqFunc - (undocumented)combFunc - (undocumented) Returns:(undocumented) aggregateByKey public <U> JavaPairRDD<K,U> aggregateByKey(U zeroValue, Function2<U,V,U> seqFunc, Function2<U,U,U> combFunc) Aggregate the values of each key, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U's. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U. Parameters:zeroValue - (undocumented)seqFunc - (undocumented)combFunc - (undocumented) Returns:(undocumented) foldByKey public JavaPairRDD<K,V> foldByKey(V zeroValue, Partitioner partitioner, Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g ., Nil for list concatenation, 0 for addition, or 1 for multiplication.). Parameters:zeroValue - (undocumented)partitioner - (undocumented)func - (undocumented) Returns:(undocumented) foldByKey public JavaPairRDD<K,V> foldByKey(V zeroValue, int numPartitions, Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g ., Nil for list concatenation, 0 for addition, or 1 for multiplication.). Parameters:zeroValue - (undocumented)numPartitions - (undocumented)func - (undocumented) Returns:(undocumented) foldByKey public JavaPairRDD<K,V> foldByKey(V zeroValue, Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). Parameters:zeroValue - (undocumented)func - (undocumented) Returns:(undocumented) reduceByKey public JavaPairRDD<K,V> reduceByKey(Function2<V,V,V> func, int numPartitions) Merge the values for each key using an associative and commutative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Output will be hash-partitioned with numPartitions partitions. Parameters:func - (undocumented)numPartitions - (undocumented) Returns:(undocumented) groupByKey public JavaPairRDD<K,Iterable<V>> groupByKey(Partitioner partitioner) Group the values for each key in the RDD into a single sequence. Allows controlling the partitioning of the resulting key-value pair RDD by passing a Partitioner. Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using JavaPairRDD.reduceByKey or JavaPairRDD.combineByKey will provide much better performance. Parameters:partitioner - (undocumented) Returns:(undocumented) groupByKey public JavaPairRDD<K,Iterable<V>> groupByKey(int numPartitions) Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with into numPartitions partitions. Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using JavaPairRDD.reduceByKey or JavaPairRDD.combineByKey will provide much better performance. Parameters:numPartitions - (undocumented) Returns:(undocumented) subtract public JavaPairRDD<K,V> subtract(JavaPairRDD<K,V> other) Return an RDD with the elements from this that are not in other. Uses this partitioner/partition size, because even if other is huge, the resulting RDD will be &lt;= us. Parameters:other - (undocumented) Returns:(undocumented) subtract public JavaPairRDD<K,V> subtract(JavaPairRDD<K,V> other, int numPartitions) Return an RDD with the elements from this that are not in other. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) subtract public JavaPairRDD<K,V> subtract(JavaPairRDD<K,V> other, Partitioner p) Return an RDD with the elements from this that are not in other. Parameters:other - (undocumented)p - (undocumented) Returns:(undocumented) subtractByKey public <W> JavaPairRDD<K,V> subtractByKey(JavaPairRDD<K,W> other) Return an RDD with the pairs from this whose keys are not in other. Uses this partitioner/partition size, because even if other is huge, the resulting RDD will be &lt;= us. Parameters:other - (undocumented) Returns:(undocumented) subtractByKey public <W> JavaPairRDD<K,V> subtractByKey(JavaPairRDD<K,W> other, int numPartitions) Return an RDD with the pairs from `this` whose keys are not in `other`. subtractByKey public <W> JavaPairRDD<K,V> subtractByKey(JavaPairRDD<K,W> other, Partitioner p) Return an RDD with the pairs from `this` whose keys are not in `other`. partitionBy public JavaPairRDD<K,V> partitionBy(Partitioner partitioner) Return a copy of the RDD partitioned using the specified partitioner. Parameters:partitioner - (undocumented) Returns:(undocumented) join public <W> JavaPairRDD<K,scala.Tuple2<V,W>> join(JavaPairRDD<K,W> other, Partitioner partitioner) Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Uses the given Partitioner to partition the output RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) leftOuterJoin public <W> JavaPairRDD<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairRDD<K,W> other, Partitioner partitioner) Perform a left outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (v, Some(w))) for w in other, or the pair (k, (v, None)) if no elements in other have key k. Uses the given Partitioner to partition the output RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) rightOuterJoin public <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairRDD<K,W> other, Partitioner partitioner) Perform a right outer join of this and other. For each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), w)) for v in this, or the pair (k, (None, w)) if no elements in this have key k. Uses the given Partitioner to partition the output RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) fullOuterJoin public <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairRDD<K,W> other, Partitioner partitioner) Perform a full outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in other, or the pair (k, (Some(v), None)) if no elements in other have key k. Similarly, for each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for v in this, or the pair (k, (None, Some(w))) if no elements in this have key k. Uses the given Partitioner to partition the output RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) combineByKey public <C> JavaPairRDD<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners) Simplified version of combineByKey that hash-partitions the resulting RDD using the existing partitioner/parallelism level and using map-side aggregation. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented) Returns:(undocumented) reduceByKey public JavaPairRDD<K,V> reduceByKey(Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/ parallelism level. Parameters:func - (undocumented) Returns:(undocumented) groupByKey public JavaPairRDD<K,Iterable<V>> groupByKey() Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with the existing partitioner/parallelism level. Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using JavaPairRDD.reduceByKey or JavaPairRDD.combineByKey will provide much better performance. Returns:(undocumented) join public <W> JavaPairRDD<K,scala.Tuple2<V,W>> join(JavaPairRDD<K,W> other) Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster. Parameters:other - (undocumented) Returns:(undocumented) join public <W> JavaPairRDD<K,scala.Tuple2<V,W>> join(JavaPairRDD<K,W> other, int numPartitions) Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) leftOuterJoin public <W> JavaPairRDD<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairRDD<K,W> other) Perform a left outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (v, Some(w))) for w in other, or the pair (k, (v, None)) if no elements in other have key k. Hash-partitions the output using the existing partitioner/parallelism level. Parameters:other - (undocumented) Returns:(undocumented) leftOuterJoin public <W> JavaPairRDD<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairRDD<K,W> other, int numPartitions) Perform a left outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (v, Some(w))) for w in other, or the pair (k, (v, None)) if no elements in other have key k. Hash-partitions the output into numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) rightOuterJoin public <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairRDD<K,W> other) Perform a right outer join of this and other. For each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), w)) for v in this, or the pair (k, (None, w)) if no elements in this have key k. Hash-partitions the resulting RDD using the existing partitioner/parallelism level. Parameters:other - (undocumented) Returns:(undocumented) rightOuterJoin public <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairRDD<K,W> other, int numPartitions) Perform a right outer join of this and other. For each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), w)) for v in this, or the pair (k, (None, w)) if no elements in this have key k. Hash-partitions the resulting RDD into the given number of partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) fullOuterJoin public <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairRDD<K,W> other) Perform a full outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in other, or the pair (k, (Some(v), None)) if no elements in other have key k. Similarly, for each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for v in this, or the pair (k, (None, Some(w))) if no elements in this have key k. Hash-partitions the resulting RDD using the existing partitioner/ parallelism level. Parameters:other - (undocumented) Returns:(undocumented) fullOuterJoin public <W> JavaPairRDD<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairRDD<K,W> other, int numPartitions) Perform a full outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in other, or the pair (k, (Some(v), None)) if no elements in other have key k. Similarly, for each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for v in this, or the pair (k, (None, Some(w))) if no elements in this have key k. Hash-partitions the resulting RDD into the given number of partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) collectAsMap public java.util.Map<K,V> collectAsMap() Return the key-value pairs in this RDD to the master as a Map. Returns:(undocumented) mapValues public <U> JavaPairRDD<K,U> mapValues(Function<V,U> f) Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD's partitioning. Parameters:f - (undocumented) Returns:(undocumented) flatMapValues public <U> JavaPairRDD<K,U> flatMapValues(Function<V,Iterable<U>> f) Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD's partitioning. Parameters:f - (undocumented) Returns:(undocumented) cogroup public <W> JavaPairRDD<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairRDD<K,W> other, Partitioner partitioner) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) cogroup public <W1,W2> JavaPairRDD<K,scala.Tuple3<Iterable<V>,Iterable<W1>,Iterable<W2>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, Partitioner partitioner) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. Parameters:other1 - (undocumented)other2 - (undocumented)partitioner - (undocumented) Returns:(undocumented) cogroup public <W1,W2,W3> JavaPairRDD<K,scala.Tuple4<Iterable<V>,Iterable<W1>,Iterable<W2>,Iterable<W3>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, JavaPairRDD<K,W3> other3, Partitioner partitioner) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. Parameters:other1 - (undocumented)other2 - (undocumented)other3 - (undocumented)partitioner - (undocumented) Returns:(undocumented) cogroup public <W> JavaPairRDD<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairRDD<K,W> other) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. Parameters:other - (undocumented) Returns:(undocumented) cogroup public <W1,W2> JavaPairRDD<K,scala.Tuple3<Iterable<V>,Iterable<W1>,Iterable<W2>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. Parameters:other1 - (undocumented)other2 - (undocumented) Returns:(undocumented) cogroup public <W1,W2,W3> JavaPairRDD<K,scala.Tuple4<Iterable<V>,Iterable<W1>,Iterable<W2>,Iterable<W3>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, JavaPairRDD<K,W3> other3) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. Parameters:other1 - (undocumented)other2 - (undocumented)other3 - (undocumented) Returns:(undocumented) cogroup public <W> JavaPairRDD<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairRDD<K,W> other, int numPartitions) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) cogroup public <W1,W2> JavaPairRDD<K,scala.Tuple3<Iterable<V>,Iterable<W1>,Iterable<W2>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, int numPartitions) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. Parameters:other1 - (undocumented)other2 - (undocumented)numPartitions - (undocumented) Returns:(undocumented) cogroup public <W1,W2,W3> JavaPairRDD<K,scala.Tuple4<Iterable<V>,Iterable<W1>,Iterable<W2>,Iterable<W3>>> cogroup(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, JavaPairRDD<K,W3> other3, int numPartitions) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. Parameters:other1 - (undocumented)other2 - (undocumented)other3 - (undocumented)numPartitions - (undocumented) Returns:(undocumented) groupWith public <W> JavaPairRDD<K,scala.Tuple2<Iterable<V>,Iterable<W>>> groupWith(JavaPairRDD<K,W> other) Alias for cogroup. groupWith public <W1,W2> JavaPairRDD<K,scala.Tuple3<Iterable<V>,Iterable<W1>,Iterable<W2>>> groupWith(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2) Alias for cogroup. groupWith public <W1,W2,W3> JavaPairRDD<K,scala.Tuple4<Iterable<V>,Iterable<W1>,Iterable<W2>,Iterable<W3>>> groupWith(JavaPairRDD<K,W1> other1, JavaPairRDD<K,W2> other2, JavaPairRDD<K,W3> other3) Alias for cogroup. lookup public java.util.List<V> lookup(K key) Return the list of values in the RDD for key key. This operation is done efficiently if the RDD has a known partitioner by only searching the partition that the key maps to. Parameters:key - (undocumented) Returns:(undocumented) saveAsHadoopFile public <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.mapred.JobConf conf) Output the RDD to any Hadoop-supported file system. saveAsHadoopFile public <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) Output the RDD to any Hadoop-supported file system. saveAsHadoopFile public <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) Output the RDD to any Hadoop-supported file system, compressing with the supplied codec. saveAsNewAPIHadoopFile public <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.conf.Configuration conf) Output the RDD to any Hadoop-supported file system. saveAsNewAPIHadoopDataset public void saveAsNewAPIHadoopDataset(org.apache.hadoop.conf.Configuration conf) Output the RDD to any Hadoop-supported storage system, using a Configuration object for that storage system. Parameters:conf - (undocumented) saveAsNewAPIHadoopFile public <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) Output the RDD to any Hadoop-supported file system. saveAsHadoopDataset public void saveAsHadoopDataset(org.apache.hadoop.mapred.JobConf conf) Output the RDD to any Hadoop-supported storage system, using a Hadoop JobConf object for that storage system. The JobConf should set an OutputFormat and any output paths required (e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. Parameters:conf - (undocumented) repartitionAndSortWithinPartitions public JavaPairRDD<K,V> repartitionAndSortWithinPartitions(Partitioner partitioner) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery. Parameters:partitioner - (undocumented) Returns:(undocumented) repartitionAndSortWithinPartitions public JavaPairRDD<K,V> repartitionAndSortWithinPartitions(Partitioner partitioner, java.util.Comparator<K> comp) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery. Parameters:partitioner - (undocumented)comp - (undocumented) Returns:(undocumented) sortByKey public JavaPairRDD<K,V> sortByKey() Sort the RDD by key, so that each partition contains a sorted range of the elements in ascending order. Calling collect or save on the resulting RDD will return or output an ordered list of records (in the save case, they will be written to multiple part-X files in the filesystem, in order of the keys). Returns:(undocumented) sortByKey public JavaPairRDD<K,V> sortByKey(boolean ascending) Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling collect or save on the resulting RDD will return or output an ordered list of records (in the save case, they will be written to multiple part-X files in the filesystem, in order of the keys). Parameters:ascending - (undocumented) Returns:(undocumented) sortByKey public JavaPairRDD<K,V> sortByKey(boolean ascending, int numPartitions) Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling collect or save on the resulting RDD will return or output an ordered list of records (in the save case, they will be written to multiple part-X files in the filesystem, in order of the keys). Parameters:ascending - (undocumented)numPartitions - (undocumented) Returns:(undocumented) sortByKey public JavaPairRDD<K,V> sortByKey(java.util.Comparator<K> comp) Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling collect or save on the resulting RDD will return or output an ordered list of records (in the save case, they will be written to multiple part-X files in the filesystem, in order of the keys). Parameters:comp - (undocumented) Returns:(undocumented) sortByKey public JavaPairRDD<K,V> sortByKey(java.util.Comparator<K> comp, boolean ascending) Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling collect or save on the resulting RDD will return or output an ordered list of records (in the save case, they will be written to multiple part-X files in the filesystem, in order of the keys). Parameters:comp - (undocumented)ascending - (undocumented) Returns:(undocumented) sortByKey public JavaPairRDD<K,V> sortByKey(java.util.Comparator<K> comp, boolean ascending, int numPartitions) Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling collect or save on the resulting RDD will return or output an ordered list of records (in the save case, they will be written to multiple part-X files in the filesystem, in order of the keys). Parameters:comp - (undocumented)ascending - (undocumented)numPartitions - (undocumented) Returns:(undocumented) keys public JavaRDD<K> keys() Return an RDD with the keys of each tuple. Returns:(undocumented) values public JavaRDD<V> values() Return an RDD with the values of each tuple. Returns:(undocumented) countApproxDistinctByKey public JavaPairRDD<K,Long> countApproxDistinctByKey(double relativeSD, Partitioner partitioner) Return approximate number of distinct values for each key in this RDD. The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here. Parameters:relativeSD - Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017.partitioner - partitioner of the resulting RDD. Returns:(undocumented) countApproxDistinctByKey public JavaPairRDD<K,Long> countApproxDistinctByKey(double relativeSD, int numPartitions) Return approximate number of distinct values for each key in this RDD. The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here. Parameters:relativeSD - Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017.numPartitions - number of partitions of the resulting RDD. Returns:(undocumented) countApproxDistinctByKey public JavaPairRDD<K,Long> countApproxDistinctByKey(double relativeSD) Return approximate number of distinct values for each key in this RDD. The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here. Parameters:relativeSD - Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017. Returns:(undocumented) setName public JavaPairRDD<K,V> setName(String name) Assign a name to this RDD Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaPairReceiverInputDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaPairReceiverInputDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.api.java Class JavaPairReceiverInputDStream<K,V> Object org.apache.spark.streaming.api.java.JavaPairDStream<K,V> org.apache.spark.streaming.api.java.JavaPairInputDStream<K,V> org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream<K,V> All Implemented Interfaces: java.io.Serializable, JavaDStreamLike<scala.Tuple2<K,V>,JavaPairDStream<K,V>,JavaPairRDD<K,V>> public class JavaPairReceiverInputDStream<K,V> extends JavaPairInputDStream<K,V> A Java-friendly interface to ReceiverInputDStream, the abstract class for defining any input stream that receives data over the network. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaPairReceiverInputDStream(ReceiverInputDStream<scala.Tuple2<K,V>> receiverInputDStream, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag)  Method Summary Methods  Modifier and Type Method and Description static JavaPairDStream<K,V> cache()  static DStream<T> checkpoint(Duration interval)  static scala.reflect.ClassTag<scala.Tuple2<K,V>> classTag()  static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, Partitioner partitioner)  static <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner)  static <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine)  static JavaPairRDD<K,V> compute(Time validTime)  static StreamingContext context()  static JavaDStream<Long> count()  static JavaPairDStream<T,Long> countByValue()  static JavaPairDStream<T,Long> countByValue(int numPartitions)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration)  static DStream<scala.Tuple2<K,V>> dstream()  static JavaPairDStream<K,V> filter(Function<scala.Tuple2<K,V>,Boolean> f)  static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f)  static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f)  static <U> JavaPairDStream<K,U> flatMapValues(Function<V,Iterable<U>> f)  static void foreachRDD(VoidFunction<R> foreachFunc)  static void foreachRDD(VoidFunction2<R,Time> foreachFunc)  static <K,V> JavaPairReceiverInputDStream<K,V> fromReceiverInputDStream(ReceiverInputDStream<scala.Tuple2<K,V>> receiverInputDStream, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2) Convert a scala ReceiverInputDStream to a Java-friendly JavaReceiverInputDStream. static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner)  static JavaDStream<java.util.List<T>> glom()  static JavaPairDStream<K,Iterable<V>> groupByKey()  static JavaPairDStream<K,Iterable<V>> groupByKey(int numPartitions)  static JavaPairDStream<K,Iterable<V>> groupByKey(Partitioner partitioner)  static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration)  static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration)  static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, Partitioner partitioner)  static InputDStream<scala.Tuple2<K,V>> inputDStream()  static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, Partitioner partitioner)  scala.reflect.ClassTag<K> kClassTag()  static scala.reflect.ClassTag<K> kManifest()  static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner)  static <R> JavaDStream<R> map(Function<T,R> f)  static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f)  static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f)  static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f)  static <U> JavaPairDStream<K,U> mapValues(Function<V,U> f)  static <StateType,MappedType> JavaMapWithStateDStream<K,V,StateType,MappedType> mapWithState(StateSpec<K,V,StateType,MappedType> spec)  static JavaPairDStream<K,V> persist()  static JavaPairDStream<K,V> persist(StorageLevel storageLevel)  static void print()  static void print(int num)  ReceiverInputDStream<scala.Tuple2<K,V>> receiverInputDStream()  static JavaDStream<T> reduce(Function2<T,T,T> f)  static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func)  static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, int numPartitions)  static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, Partitioner partitioner)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions, Function<scala.Tuple2<K,V>,Boolean> filterFunc)  static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner, Function<scala.Tuple2<K,V>,Boolean> filterFunc)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration)  static JavaPairDStream<K,V> repartition(int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, int numPartitions)  static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner)  static void saveAsHadoopFiles(String prefix, String suffix)  static <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass)  static <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.mapred.JobConf conf)  static void saveAsNewAPIHadoopFiles(String prefix, String suffix)  static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass)  static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.conf.Configuration conf)  static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> org.apache.hadoop.conf.Configuration saveAsNewAPIHadoopFiles$default$6()  static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in)  static java.util.List<R> slice(Time fromTime, Time toTime)  static JavaDStream<scala.Tuple2<K,V>> toJavaDStream()  static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc)  static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc)  static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc)  static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc)  static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc)  static JavaPairDStream<K,V> union(JavaPairDStream<K,V> that)  static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc)  static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, int numPartitions)  static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner)  static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner, JavaPairRDD<K,S> initialRDD)  scala.reflect.ClassTag<V> vClassTag()  static scala.reflect.ClassTag<V> vManifest()  static JavaPairDStream<K,V> window(Duration windowDuration)  static JavaPairDStream<K,V> window(Duration windowDuration, Duration slideDuration)  static JavaPairRDD<K,V> wrapRDD(RDD<scala.Tuple2<K,V>> rdd)  Methods inherited from class org.apache.spark.streaming.api.java.JavaPairInputDStream fromInputDStream, inputDStream Methods inherited from class org.apache.spark.streaming.api.java.JavaPairDStream cache, classTag, cogroup, cogroup, cogroup, combineByKey, combineByKey, compute, dstream, filter, flatMapValues, fromJavaDStream, fromPairDStream, fullOuterJoin, fullOuterJoin, fullOuterJoin, groupByKey, groupByKey, groupByKey, groupByKeyAndWindow, groupByKeyAndWindow, groupByKeyAndWindow, groupByKeyAndWindow, join, join, join, kManifest, leftOuterJoin, leftOuterJoin, leftOuterJoin, mapValues, mapWithState, persist, persist, reduceByKey, reduceByKey, reduceByKey, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, reduceByKeyAndWindow, repartition, rightOuterJoin, rightOuterJoin, rightOuterJoin, saveAsHadoopFiles, saveAsHadoopFiles, saveAsHadoopFiles, saveAsNewAPIHadoopFiles, saveAsNewAPIHadoopFiles, saveAsNewAPIHadoopFiles, scalaToJavaLong, toJavaDStream, union, updateStateByKey, updateStateByKey, updateStateByKey, updateStateByKey, vManifest, window, window, wrapRDD Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.streaming.api.java.JavaDStreamLike checkpoint, context, count, countByValue, countByValue, countByValueAndWindow, countByValueAndWindow, countByWindow, flatMap, flatMapToPair, foreachRDD, foreachRDD, glom, map, mapPartitions, mapPartitionsToPair, mapToPair, print, print, reduce, reduceByWindow, reduceByWindow, scalaIntToJavaLong, slice, transform, transform, transformToPair, transformToPair, transformWith, transformWith, transformWithToPair, transformWithToPair Constructor Detail JavaPairReceiverInputDStream public JavaPairReceiverInputDStream(ReceiverInputDStream<scala.Tuple2<K,V>> receiverInputDStream, scala.reflect.ClassTag<K> kClassTag, scala.reflect.ClassTag<V> vClassTag) Method Detail fromReceiverInputDStream public static <K,V> JavaPairReceiverInputDStream<K,V> fromReceiverInputDStream(ReceiverInputDStream<scala.Tuple2<K,V>> receiverInputDStream, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2) Convert a scala ReceiverInputDStream to a Java-friendly JavaReceiverInputDStream. Parameters:receiverInputDStream - (undocumented)evidence$1 - (undocumented)evidence$2 - (undocumented) Returns:(undocumented) scalaIntToJavaLong public static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in) print public static void print() print public static void print(int num) count public static JavaDStream<Long> count() countByValue public static JavaPairDStream<T,Long> countByValue() countByValue public static JavaPairDStream<T,Long> countByValue(int numPartitions) countByWindow public static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) glom public static JavaDStream<java.util.List<T>> glom() context public static StreamingContext context() map public static <R> JavaDStream<R> map(Function<T,R> f) mapToPair public static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f) flatMap public static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f) flatMapToPair public static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) mapPartitions public static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) mapPartitionsToPair public static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) reduce public static JavaDStream<T> reduce(Function2<T,T,T> f) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) slice public static java.util.List<R> slice(Time fromTime, Time toTime) foreachRDD public static void foreachRDD(VoidFunction<R> foreachFunc) foreachRDD public static void foreachRDD(VoidFunction2<R,Time> foreachFunc) transform public static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc) transform public static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc) checkpoint public static DStream<T> checkpoint(Duration interval) dstream public static DStream<scala.Tuple2<K,V>> dstream() kManifest public static scala.reflect.ClassTag<K> kManifest() vManifest public static scala.reflect.ClassTag<V> vManifest() wrapRDD public static JavaPairRDD<K,V> wrapRDD(RDD<scala.Tuple2<K,V>> rdd) filter public static JavaPairDStream<K,V> filter(Function<scala.Tuple2<K,V>,Boolean> f) cache public static JavaPairDStream<K,V> cache() persist public static JavaPairDStream<K,V> persist() persist public static JavaPairDStream<K,V> persist(StorageLevel storageLevel) repartition public static JavaPairDStream<K,V> repartition(int numPartitions) compute public static JavaPairRDD<K,V> compute(Time validTime) window public static JavaPairDStream<K,V> window(Duration windowDuration) window public static JavaPairDStream<K,V> window(Duration windowDuration, Duration slideDuration) union public static JavaPairDStream<K,V> union(JavaPairDStream<K,V> that) groupByKey public static JavaPairDStream<K,Iterable<V>> groupByKey() groupByKey public static JavaPairDStream<K,Iterable<V>> groupByKey(int numPartitions) groupByKey public static JavaPairDStream<K,Iterable<V>> groupByKey(Partitioner partitioner) reduceByKey public static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func) reduceByKey public static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, int numPartitions) reduceByKey public static JavaPairDStream<K,V> reduceByKey(Function2<V,V,V> func, Partitioner partitioner) combineByKey public static <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner) combineByKey public static <C> JavaPairDStream<K,C> combineByKey(Function<V,C> createCombiner, Function2<C,V,C> mergeValue, Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine) groupByKeyAndWindow public static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration) groupByKeyAndWindow public static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration) groupByKeyAndWindow public static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) groupByKeyAndWindow public static JavaPairDStream<K,Iterable<V>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, Partitioner partitioner) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions, Function<scala.Tuple2<K,V>,Boolean> filterFunc) reduceByKeyAndWindow public static JavaPairDStream<K,V> reduceByKeyAndWindow(Function2<V,V,V> reduceFunc, Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner, Function<scala.Tuple2<K,V>,Boolean> filterFunc) mapWithState public static <StateType,MappedType> JavaMapWithStateDStream<K,V,StateType,MappedType> mapWithState(StateSpec<K,V,StateType,MappedType> spec) updateStateByKey public static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc) updateStateByKey public static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, int numPartitions) updateStateByKey public static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner) updateStateByKey public static <S> JavaPairDStream<K,S> updateStateByKey(Function2<java.util.List<V>,Optional<S>,Optional<S>> updateFunc, Partitioner partitioner, JavaPairRDD<K,S> initialRDD) mapValues public static <U> JavaPairDStream<K,U> mapValues(Function<V,U> f) flatMapValues public static <U> JavaPairDStream<K,U> flatMapValues(Function<V,Iterable<U>> f) cogroup public static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other) cogroup public static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, int numPartitions) cogroup public static <W> JavaPairDStream<K,scala.Tuple2<Iterable<V>,Iterable<W>>> cogroup(JavaPairDStream<K,W> other, Partitioner partitioner) join public static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other) join public static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, int numPartitions) join public static <W> JavaPairDStream<K,scala.Tuple2<V,W>> join(JavaPairDStream<K,W> other, Partitioner partitioner) leftOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other) leftOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, int numPartitions) leftOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<V,Optional<W>>> leftOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) rightOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other) rightOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, int numPartitions) rightOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,W>> rightOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) fullOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other) fullOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, int numPartitions) fullOuterJoin public static <W> JavaPairDStream<K,scala.Tuple2<Optional<V>,Optional<W>>> fullOuterJoin(JavaPairDStream<K,W> other, Partitioner partitioner) saveAsHadoopFiles public static void saveAsHadoopFiles(String prefix, String suffix) saveAsHadoopFiles public static <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) saveAsHadoopFiles public static <F extends org.apache.hadoop.mapred.OutputFormat<?,?>> void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.mapred.JobConf conf) saveAsNewAPIHadoopFiles public static void saveAsNewAPIHadoopFiles(String prefix, String suffix) saveAsNewAPIHadoopFiles public static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass) saveAsNewAPIHadoopFiles public static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<F> outputFormatClass, org.apache.hadoop.conf.Configuration conf) toJavaDStream public static JavaDStream<scala.Tuple2<K,V>> toJavaDStream() classTag public static scala.reflect.ClassTag<scala.Tuple2<K,V>> classTag() saveAsNewAPIHadoopFiles$default$6 public static <F extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> org.apache.hadoop.conf.Configuration saveAsNewAPIHadoopFiles$default$6() inputDStream public static InputDStream<scala.Tuple2<K,V>> inputDStream() receiverInputDStream public ReceiverInputDStream<scala.Tuple2<K,V>> receiverInputDStream() kClassTag public scala.reflect.ClassTag<K> kClassTag() Overrides: kClassTag in class JavaPairInputDStream<K,V> vClassTag public scala.reflect.ClassTag<V> vClassTag() Overrides: vClassTag in class JavaPairInputDStream<K,V> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaParams (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaParams (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class JavaParams Object org.apache.spark.ml.param.JavaParams All Implemented Interfaces: java.io.Serializable, Params, Identifiable public abstract class JavaParams extends Object implements Params :: DeveloperApi :: Java-friendly wrapper for Params. Java developers who need to extend Params should use this class instead. If you need to extend an abstract class which already extends Params, then that abstract class should be Java-friendly as well. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaParams()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copy, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail JavaParams public JavaParams() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class JavaRDD<T> Object org.apache.spark.api.java.JavaRDD<T> All Implemented Interfaces: java.io.Serializable, JavaRDDLike<T,JavaRDD<T>> public class JavaRDD<T> extends Object See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaRDD(RDD<T> rdd, scala.reflect.ClassTag<T> classTag)  Method Summary Methods  Modifier and Type Method and Description static <U> U aggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp)  JavaRDD<T> cache() Persist this RDD with the default storage level (`MEMORY_ONLY`). static <U> JavaPairRDD<T,U> cartesian(JavaRDDLike<U,?> other)  static void checkpoint()  scala.reflect.ClassTag<T> classTag()  JavaRDD<T> coalesce(int numPartitions) Return a new RDD that is reduced into numPartitions partitions. JavaRDD<T> coalesce(int numPartitions, boolean shuffle) Return a new RDD that is reduced into numPartitions partitions. static java.util.List<T> collect()  static JavaFutureAction<java.util.List<T>> collectAsync()  static java.util.List<T>[] collectPartitions(int[] partitionIds)  static SparkContext context()  static long count()  static PartialResult<BoundedDouble> countApprox(long timeout)  static PartialResult<BoundedDouble> countApprox(long timeout, double confidence)  static long countApproxDistinct(double relativeSD)  static JavaFutureAction<Long> countAsync()  static java.util.Map<T,Long> countByValue()  static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout)  static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence)  JavaRDD<T> distinct() Return a new RDD containing the distinct elements in this RDD. JavaRDD<T> distinct(int numPartitions) Return a new RDD containing the distinct elements in this RDD. JavaRDD<T> filter(Function<T,Boolean> f) Return a new RDD containing only the elements that satisfy a predicate. static T first()  static <U> JavaRDD<U> flatMap(FlatMapFunction<T,U> f)  static JavaDoubleRDD flatMapToDouble(DoubleFlatMapFunction<T> f)  static <K2,V2> JavaPairRDD<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f)  static T fold(T zeroValue, Function2<T,T,T> f)  static void foreach(VoidFunction<T> f)  static JavaFutureAction<Void> foreachAsync(VoidFunction<T> f)  static void foreachPartition(VoidFunction<java.util.Iterator<T>> f)  static JavaFutureAction<Void> foreachPartitionAsync(VoidFunction<java.util.Iterator<T>> f)  static <T> JavaRDD<T> fromRDD(RDD<T> rdd, scala.reflect.ClassTag<T> evidence$1)  static Optional<String> getCheckpointFile()  static int getNumPartitions()  static StorageLevel getStorageLevel()  static JavaRDD<java.util.List<T>> glom()  static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f)  static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f, int numPartitions)  static int id()  JavaRDD<T> intersection(JavaRDD<T> other) Return the intersection of this RDD and another one. static boolean isCheckpointed()  static boolean isEmpty()  static java.util.Iterator<T> iterator(Partition split, TaskContext taskContext)  static <U> JavaPairRDD<U,T> keyBy(Function<T,U> f)  static <R> JavaRDD<R> map(Function<T,R> f)  static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f)  static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f, boolean preservesPartitioning)  static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f)  static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f, boolean preservesPartitioning)  static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f)  static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f, boolean preservesPartitioning)  static <R> JavaRDD<R> mapPartitionsWithIndex(Function2<Integer,java.util.Iterator<T>,java.util.Iterator<R>> f, boolean preservesPartitioning)  static <R> boolean mapPartitionsWithIndex$default$2()  static <R> JavaDoubleRDD mapToDouble(DoubleFunction<T> f)  static <K2,V2> JavaPairRDD<K2,V2> mapToPair(PairFunction<T,K2,V2> f)  static T max(java.util.Comparator<T> comp)  static T min(java.util.Comparator<T> comp)  static String name()  static Optional<Partitioner> partitioner()  static java.util.List<Partition> partitions()  JavaRDD<T> persist(StorageLevel newLevel) Set this RDD's storage level to persist its values across operations after the first time it is computed. static JavaRDD<String> pipe(java.util.List<String> command)  static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env)  static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize)  static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize, String encoding)  static JavaRDD<String> pipe(String command)  JavaRDD<T>[] randomSplit(double[] weights) Randomly splits this RDD with the provided weights. JavaRDD<T>[] randomSplit(double[] weights, long seed) Randomly splits this RDD with the provided weights. RDD<T> rdd()  static T reduce(Function2<T,T,T> f)  JavaRDD<T> repartition(int numPartitions) Return a new RDD that has exactly numPartitions partitions. JavaRDD<T> sample(boolean withReplacement, double fraction) Return a sampled subset of this RDD. JavaRDD<T> sample(boolean withReplacement, double fraction, long seed) Return a sampled subset of this RDD. static void saveAsObjectFile(String path)  static void saveAsTextFile(String path)  static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec)  JavaRDD<T> setName(String name) Assign a name to this RDD <S> JavaRDD<T> sortBy(Function<T,S> f, boolean ascending, int numPartitions) Return this RDD sorted by the given key function. JavaRDD<T> subtract(JavaRDD<T> other) Return an RDD with the elements from this that are not in other. JavaRDD<T> subtract(JavaRDD<T> other, int numPartitions) Return an RDD with the elements from this that are not in other. JavaRDD<T> subtract(JavaRDD<T> other, Partitioner p) Return an RDD with the elements from this that are not in other. static java.util.List<T> take(int num)  static JavaFutureAction<java.util.List<T>> takeAsync(int num)  static java.util.List<T> takeOrdered(int num)  static java.util.List<T> takeOrdered(int num, java.util.Comparator<T> comp)  static java.util.List<T> takeSample(boolean withReplacement, int num)  static java.util.List<T> takeSample(boolean withReplacement, int num, long seed)  static String toDebugString()  static java.util.Iterator<T> toLocalIterator()  static java.util.List<T> top(int num)  static java.util.List<T> top(int num, java.util.Comparator<T> comp)  static <T> RDD<T> toRDD(JavaRDD<T> rdd)  String toString()  static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp)  static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp, int depth)  static T treeReduce(Function2<T,T,T> f)  static T treeReduce(Function2<T,T,T> f, int depth)  JavaRDD<T> union(JavaRDD<T> other) Return the union of this RDD and another one. JavaRDD<T> unpersist() Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. JavaRDD<T> unpersist(boolean blocking) Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. JavaRDD<T> wrapRDD(RDD<T> rdd)  static <U> JavaPairRDD<T,U> zip(JavaRDDLike<U,?> other)  static <U,V> JavaRDD<V> zipPartitions(JavaRDDLike<U,?> other, FlatMapFunction2<java.util.Iterator<T>,java.util.Iterator<U>,V> f)  static JavaPairRDD<T,Long> zipWithIndex()  static JavaPairRDD<T,Long> zipWithUniqueId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.api.java.JavaRDDLike aggregate, cartesian, checkpoint, collect, collectAsync, collectPartitions, context, count, countApprox, countApprox, countApproxDistinct, countAsync, countByValue, countByValueApprox, countByValueApprox, first, flatMap, flatMapToDouble, flatMapToPair, fold, foreach, foreachAsync, foreachPartition, foreachPartitionAsync, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, id, isCheckpointed, isEmpty, iterator, keyBy, map, mapPartitions, mapPartitions, mapPartitionsToDouble, mapPartitionsToDouble, mapPartitionsToPair, mapPartitionsToPair, mapPartitionsWithIndex, mapToDouble, mapToPair, max, min, name, partitioner, partitions, pipe, pipe, pipe, pipe, pipe, reduce, saveAsObjectFile, saveAsTextFile, saveAsTextFile, take, takeAsync, takeOrdered, takeOrdered, takeSample, takeSample, toDebugString, toLocalIterator, top, top, treeAggregate, treeAggregate, treeReduce, treeReduce, zip, zipPartitions, zipWithIndex, zipWithUniqueId Constructor Detail JavaRDD public JavaRDD(RDD<T> rdd, scala.reflect.ClassTag<T> classTag) Method Detail fromRDD public static <T> JavaRDD<T> fromRDD(RDD<T> rdd, scala.reflect.ClassTag<T> evidence$1) toRDD public static <T> RDD<T> toRDD(JavaRDD<T> rdd) partitions public static java.util.List<Partition> partitions() getNumPartitions public static int getNumPartitions() partitioner public static Optional<Partitioner> partitioner() context public static SparkContext context() id public static int id() getStorageLevel public static StorageLevel getStorageLevel() iterator public static java.util.Iterator<T> iterator(Partition split, TaskContext taskContext) map public static <R> JavaRDD<R> map(Function<T,R> f) mapPartitionsWithIndex public static <R> JavaRDD<R> mapPartitionsWithIndex(Function2<Integer,java.util.Iterator<T>,java.util.Iterator<R>> f, boolean preservesPartitioning) mapToDouble public static <R> JavaDoubleRDD mapToDouble(DoubleFunction<T> f) mapToPair public static <K2,V2> JavaPairRDD<K2,V2> mapToPair(PairFunction<T,K2,V2> f) flatMap public static <U> JavaRDD<U> flatMap(FlatMapFunction<T,U> f) flatMapToDouble public static JavaDoubleRDD flatMapToDouble(DoubleFlatMapFunction<T> f) flatMapToPair public static <K2,V2> JavaPairRDD<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) mapPartitions public static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) mapPartitions public static <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f, boolean preservesPartitioning) mapPartitionsToDouble public static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f) mapPartitionsToPair public static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) mapPartitionsToDouble public static JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f, boolean preservesPartitioning) mapPartitionsToPair public static <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f, boolean preservesPartitioning) foreachPartition public static void foreachPartition(VoidFunction<java.util.Iterator<T>> f) glom public static JavaRDD<java.util.List<T>> glom() cartesian public static <U> JavaPairRDD<T,U> cartesian(JavaRDDLike<U,?> other) groupBy public static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f) groupBy public static <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f, int numPartitions) pipe public static JavaRDD<String> pipe(String command) pipe public static JavaRDD<String> pipe(java.util.List<String> command) pipe public static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env) pipe public static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize) pipe public static JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize, String encoding) zip public static <U> JavaPairRDD<T,U> zip(JavaRDDLike<U,?> other) zipPartitions public static <U,V> JavaRDD<V> zipPartitions(JavaRDDLike<U,?> other, FlatMapFunction2<java.util.Iterator<T>,java.util.Iterator<U>,V> f) zipWithUniqueId public static JavaPairRDD<T,Long> zipWithUniqueId() zipWithIndex public static JavaPairRDD<T,Long> zipWithIndex() foreach public static void foreach(VoidFunction<T> f) collect public static java.util.List<T> collect() toLocalIterator public static java.util.Iterator<T> toLocalIterator() collectPartitions public static java.util.List<T>[] collectPartitions(int[] partitionIds) reduce public static T reduce(Function2<T,T,T> f) treeReduce public static T treeReduce(Function2<T,T,T> f, int depth) treeReduce public static T treeReduce(Function2<T,T,T> f) fold public static T fold(T zeroValue, Function2<T,T,T> f) aggregate public static <U> U aggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) treeAggregate public static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp, int depth) treeAggregate public static <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) count public static long count() countApprox public static PartialResult<BoundedDouble> countApprox(long timeout, double confidence) countApprox public static PartialResult<BoundedDouble> countApprox(long timeout) countByValue public static java.util.Map<T,Long> countByValue() countByValueApprox public static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence) countByValueApprox public static PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout) take public static java.util.List<T> take(int num) takeSample public static java.util.List<T> takeSample(boolean withReplacement, int num) takeSample public static java.util.List<T> takeSample(boolean withReplacement, int num, long seed) first public static T first() isEmpty public static boolean isEmpty() saveAsTextFile public static void saveAsTextFile(String path) saveAsTextFile public static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) saveAsObjectFile public static void saveAsObjectFile(String path) keyBy public static <U> JavaPairRDD<U,T> keyBy(Function<T,U> f) checkpoint public static void checkpoint() isCheckpointed public static boolean isCheckpointed() getCheckpointFile public static Optional<String> getCheckpointFile() toDebugString public static String toDebugString() top public static java.util.List<T> top(int num, java.util.Comparator<T> comp) top public static java.util.List<T> top(int num) takeOrdered public static java.util.List<T> takeOrdered(int num, java.util.Comparator<T> comp) max public static T max(java.util.Comparator<T> comp) min public static T min(java.util.Comparator<T> comp) takeOrdered public static java.util.List<T> takeOrdered(int num) countApproxDistinct public static long countApproxDistinct(double relativeSD) name public static String name() countAsync public static JavaFutureAction<Long> countAsync() collectAsync public static JavaFutureAction<java.util.List<T>> collectAsync() takeAsync public static JavaFutureAction<java.util.List<T>> takeAsync(int num) foreachAsync public static JavaFutureAction<Void> foreachAsync(VoidFunction<T> f) foreachPartitionAsync public static JavaFutureAction<Void> foreachPartitionAsync(VoidFunction<java.util.Iterator<T>> f) mapPartitionsWithIndex$default$2 public static <R> boolean mapPartitionsWithIndex$default$2() rdd public RDD<T> rdd() classTag public scala.reflect.ClassTag<T> classTag() wrapRDD public JavaRDD<T> wrapRDD(RDD<T> rdd) cache public JavaRDD<T> cache() Persist this RDD with the default storage level (`MEMORY_ONLY`). persist public JavaRDD<T> persist(StorageLevel newLevel) Set this RDD's storage level to persist its values across operations after the first time it is computed. This can only be used to assign a new storage level if the RDD does not have a storage level set yet.. Parameters:newLevel - (undocumented) Returns:(undocumented) unpersist public JavaRDD<T> unpersist() Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. This method blocks until all blocks are deleted. Returns:(undocumented) unpersist public JavaRDD<T> unpersist(boolean blocking) Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. Parameters:blocking - Whether to block until all blocks are deleted. Returns:(undocumented) distinct public JavaRDD<T> distinct() Return a new RDD containing the distinct elements in this RDD. Returns:(undocumented) distinct public JavaRDD<T> distinct(int numPartitions) Return a new RDD containing the distinct elements in this RDD. Parameters:numPartitions - (undocumented) Returns:(undocumented) filter public JavaRDD<T> filter(Function<T,Boolean> f) Return a new RDD containing only the elements that satisfy a predicate. Parameters:f - (undocumented) Returns:(undocumented) coalesce public JavaRDD<T> coalesce(int numPartitions) Return a new RDD that is reduced into numPartitions partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented) coalesce public JavaRDD<T> coalesce(int numPartitions, boolean shuffle) Return a new RDD that is reduced into numPartitions partitions. Parameters:numPartitions - (undocumented)shuffle - (undocumented) Returns:(undocumented) repartition public JavaRDD<T> repartition(int numPartitions) Return a new RDD that has exactly numPartitions partitions. Can increase or decrease the level of parallelism in this RDD. Internally, this uses a shuffle to redistribute data. If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle. Parameters:numPartitions - (undocumented) Returns:(undocumented) sample public JavaRDD<T> sample(boolean withReplacement, double fraction) Return a sampled subset of this RDD. Parameters:withReplacement - can elements be sampled multiple times (replaced when sampled out)fraction - expected size of the sample as a fraction of this RDD's size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0 Returns:(undocumented) sample public JavaRDD<T> sample(boolean withReplacement, double fraction, long seed) Return a sampled subset of this RDD. Parameters:withReplacement - can elements be sampled multiple times (replaced when sampled out)fraction - expected size of the sample as a fraction of this RDD's size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0seed - seed for the random number generator Returns:(undocumented) randomSplit public JavaRDD<T>[] randomSplit(double[] weights) Randomly splits this RDD with the provided weights. Parameters:weights - weights for splits, will be normalized if they don't sum to 1 Returns:split RDDs in an array randomSplit public JavaRDD<T>[] randomSplit(double[] weights, long seed) Randomly splits this RDD with the provided weights. Parameters:weights - weights for splits, will be normalized if they don't sum to 1seed - random seed Returns:split RDDs in an array union public JavaRDD<T> union(JavaRDD<T> other) Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them). Parameters:other - (undocumented) Returns:(undocumented) intersection public JavaRDD<T> intersection(JavaRDD<T> other) Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did. Note that this method performs a shuffle internally. Parameters:other - (undocumented) Returns:(undocumented) subtract public JavaRDD<T> subtract(JavaRDD<T> other) Return an RDD with the elements from this that are not in other. Uses this partitioner/partition size, because even if other is huge, the resulting RDD will be <= us. Parameters:other - (undocumented) Returns:(undocumented) subtract public JavaRDD<T> subtract(JavaRDD<T> other, int numPartitions) Return an RDD with the elements from this that are not in other. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) subtract public JavaRDD<T> subtract(JavaRDD<T> other, Partitioner p) Return an RDD with the elements from this that are not in other. Parameters:other - (undocumented)p - (undocumented) Returns:(undocumented) toString public String toString() Overrides: toString in class Object setName public JavaRDD<T> setName(String name) Assign a name to this RDD sortBy public <S> JavaRDD<T> sortBy(Function<T,S> f, boolean ascending, int numPartitions) Return this RDD sorted by the given key function. Parameters:f - (undocumented)ascending - (undocumented)numPartitions - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaRDDLike (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaRDDLike (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Interface JavaRDDLike<T,This extends JavaRDDLike<T,This>> All Superinterfaces: java.io.Serializable All Known Implementing Classes: JavaDoubleRDD, JavaHadoopRDD, JavaNewHadoopRDD, JavaPairRDD, JavaRDD public interface JavaRDDLike<T,This extends JavaRDDLike<T,This>> extends scala.Serializable Defines operations common to several Java RDD implementations. Note that this trait is not intended to be implemented by user code. Method Summary Methods  Modifier and Type Method and Description <U> U aggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) Aggregate the elements of each partition, and then the results for all the partitions, using given combine functions and a neutral "zero value". <U> JavaPairRDD<T,U> cartesian(JavaRDDLike<U,?> other) Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in this and b is in other. void checkpoint() Mark this RDD for checkpointing. scala.reflect.ClassTag<T> classTag()  java.util.List<T> collect() Return an array that contains all of the elements in this RDD. JavaFutureAction<java.util.List<T>> collectAsync() The asynchronous version of collect, which returns a future for retrieving an array containing all of the elements in this RDD. java.util.List<T>[] collectPartitions(int[] partitionIds) Return an array that contains all of the elements in a specific partition of this RDD. SparkContext context() The SparkContext that this RDD was created on. long count() Return the number of elements in the RDD. PartialResult<BoundedDouble> countApprox(long timeout) Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished. PartialResult<BoundedDouble> countApprox(long timeout, double confidence) Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished. long countApproxDistinct(double relativeSD) Return approximate number of distinct elements in the RDD. JavaFutureAction<Long> countAsync() The asynchronous version of count, which returns a future for counting the number of elements in this RDD. java.util.Map<T,Long> countByValue() Return the count of each unique value in this RDD as a map of (value, count) pairs. PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout) Approximate version of countByValue(). PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence) Approximate version of countByValue(). T first() Return the first element in this RDD. <U> JavaRDD<U> flatMap(FlatMapFunction<T,U> f) Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. JavaDoubleRDD flatMapToDouble(DoubleFlatMapFunction<T> f) Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. <K2,V2> JavaPairRDD<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. T fold(T zeroValue, Function2<T,T,T> f) Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral "zero value". void foreach(VoidFunction<T> f) Applies a function f to all elements of this RDD. JavaFutureAction<Void> foreachAsync(VoidFunction<T> f) The asynchronous version of the foreach action, which applies a function f to all the elements of this RDD. void foreachPartition(VoidFunction<java.util.Iterator<T>> f) Applies a function f to each partition of this RDD. JavaFutureAction<Void> foreachPartitionAsync(VoidFunction<java.util.Iterator<T>> f) The asynchronous version of the foreachPartition action, which applies a function f to each partition of this RDD. Optional<String> getCheckpointFile() Gets the name of the file to which this RDD was checkpointed int getNumPartitions() Return the number of partitions in this RDD. StorageLevel getStorageLevel() Get the RDD's current storage level, or StorageLevel.NONE if none is set. JavaRDD<java.util.List<T>> glom() Return an RDD created by coalescing all elements within each partition into an array. <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f) Return an RDD of grouped elements. <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f, int numPartitions) Return an RDD of grouped elements. int id() A unique ID for this RDD (within its SparkContext). boolean isCheckpointed() Return whether this RDD has been checkpointed or not boolean isEmpty()  java.util.Iterator<T> iterator(Partition split, TaskContext taskContext) Internal method to this RDD; will read from cache if applicable, or otherwise compute it. <U> JavaPairRDD<U,T> keyBy(Function<T,U> f) Creates tuples of the elements in this RDD by applying f. <R> JavaRDD<R> map(Function<T,R> f) Return a new RDD by applying a function to all elements of this RDD. <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) Return a new RDD by applying a function to each partition of this RDD. <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f, boolean preservesPartitioning) Return a new RDD by applying a function to each partition of this RDD. JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f) Return a new RDD by applying a function to each partition of this RDD. JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f, boolean preservesPartitioning) Return a new RDD by applying a function to each partition of this RDD. <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) Return a new RDD by applying a function to each partition of this RDD. <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f, boolean preservesPartitioning) Return a new RDD by applying a function to each partition of this RDD. <R> JavaRDD<R> mapPartitionsWithIndex(Function2<Integer,java.util.Iterator<T>,java.util.Iterator<R>> f, boolean preservesPartitioning) Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition. <R> JavaDoubleRDD mapToDouble(DoubleFunction<T> f) Return a new RDD by applying a function to all elements of this RDD. <K2,V2> JavaPairRDD<K2,V2> mapToPair(PairFunction<T,K2,V2> f) Return a new RDD by applying a function to all elements of this RDD. T max(java.util.Comparator<T> comp) Returns the maximum element from this RDD as defined by the specified Comparator[T]. T min(java.util.Comparator<T> comp) Returns the minimum element from this RDD as defined by the specified Comparator[T]. String name()  Optional<Partitioner> partitioner() The partitioner of this RDD. java.util.List<Partition> partitions() Set of partitions in this RDD. JavaRDD<String> pipe(java.util.List<String> command) Return an RDD created by piping elements to a forked external process. JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env) Return an RDD created by piping elements to a forked external process. JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize) Return an RDD created by piping elements to a forked external process. JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize, String encoding) Return an RDD created by piping elements to a forked external process. JavaRDD<String> pipe(String command) Return an RDD created by piping elements to a forked external process. RDD<T> rdd()  T reduce(Function2<T,T,T> f) Reduces the elements of this RDD using the specified commutative and associative binary operator. void saveAsObjectFile(String path) Save this RDD as a SequenceFile of serialized objects. void saveAsTextFile(String path) Save this RDD as a text file, using string representations of elements. void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) Save this RDD as a compressed text file, using string representations of elements. java.util.List<T> take(int num) Take the first num elements of the RDD. JavaFutureAction<java.util.List<T>> takeAsync(int num) The asynchronous version of the take action, which returns a future for retrieving the first num elements of this RDD. java.util.List<T> takeOrdered(int num) Returns the first k (smallest) elements from this RDD using the natural ordering for T while maintain the order. java.util.List<T> takeOrdered(int num, java.util.Comparator<T> comp) Returns the first k (smallest) elements from this RDD as defined by the specified Comparator[T] and maintains the order. java.util.List<T> takeSample(boolean withReplacement, int num)  java.util.List<T> takeSample(boolean withReplacement, int num, long seed)  String toDebugString() A description of this RDD and its recursive dependencies for debugging. java.util.Iterator<T> toLocalIterator() Return an iterator that contains all of the elements in this RDD. java.util.List<T> top(int num) Returns the top k (largest) elements from this RDD using the natural ordering for T and maintains the order. java.util.List<T> top(int num, java.util.Comparator<T> comp) Returns the top k (largest) elements from this RDD as defined by the specified Comparator[T] and maintains the order. <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) treeAggregate(U, org.apache.spark.api.java.function.Function2<U, T, U>, org.apache.spark.api.java.function.Function2<U, U, U>, int) with suggested depth 2. <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp, int depth) Aggregates the elements of this RDD in a multi-level tree pattern. T treeReduce(Function2<T,T,T> f) treeReduce(org.apache.spark.api.java.function.Function2<T, T, T>, int) with suggested depth 2. T treeReduce(Function2<T,T,T> f, int depth) Reduces the elements of this RDD in a multi-level tree pattern. This wrapRDD(RDD<T> rdd)  <U> JavaPairRDD<T,U> zip(JavaRDDLike<U,?> other) Zips this RDD with another one, returning key-value pairs with the first element in each RDD, second element in each RDD, etc. <U,V> JavaRDD<V> zipPartitions(JavaRDDLike<U,?> other, FlatMapFunction2<java.util.Iterator<T>,java.util.Iterator<U>,V> f) Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by applying a function to the zipped partitions. JavaPairRDD<T,Long> zipWithIndex() Zips this RDD with its element indices. JavaPairRDD<T,Long> zipWithUniqueId() Zips this RDD with generated unique Long ids. Method Detail wrapRDD This wrapRDD(RDD<T> rdd) classTag scala.reflect.ClassTag<T> classTag() rdd RDD<T> rdd() partitions java.util.List<Partition> partitions() Set of partitions in this RDD. getNumPartitions int getNumPartitions() Return the number of partitions in this RDD. partitioner Optional<Partitioner> partitioner() The partitioner of this RDD. context SparkContext context() The SparkContext that this RDD was created on. id int id() A unique ID for this RDD (within its SparkContext). getStorageLevel StorageLevel getStorageLevel() Get the RDD's current storage level, or StorageLevel.NONE if none is set. iterator java.util.Iterator<T> iterator(Partition split, TaskContext taskContext) Internal method to this RDD; will read from cache if applicable, or otherwise compute it. This should ''not'' be called by users directly, but is available for implementors of custom subclasses of RDD. Parameters:split - (undocumented)taskContext - (undocumented) Returns:(undocumented) map <R> JavaRDD<R> map(Function<T,R> f) Return a new RDD by applying a function to all elements of this RDD. Parameters:f - (undocumented) Returns:(undocumented) mapPartitionsWithIndex <R> JavaRDD<R> mapPartitionsWithIndex(Function2<Integer,java.util.Iterator<T>,java.util.Iterator<R>> f, boolean preservesPartitioning) Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition. Parameters:f - (undocumented)preservesPartitioning - (undocumented) Returns:(undocumented) mapToDouble <R> JavaDoubleRDD mapToDouble(DoubleFunction<T> f) Return a new RDD by applying a function to all elements of this RDD. Parameters:f - (undocumented) Returns:(undocumented) mapToPair <K2,V2> JavaPairRDD<K2,V2> mapToPair(PairFunction<T,K2,V2> f) Return a new RDD by applying a function to all elements of this RDD. Parameters:f - (undocumented) Returns:(undocumented) flatMap <U> JavaRDD<U> flatMap(FlatMapFunction<T,U> f) Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. Parameters:f - (undocumented) Returns:(undocumented) flatMapToDouble JavaDoubleRDD flatMapToDouble(DoubleFlatMapFunction<T> f) Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. Parameters:f - (undocumented) Returns:(undocumented) flatMapToPair <K2,V2> JavaPairRDD<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. Parameters:f - (undocumented) Returns:(undocumented) mapPartitions <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) Return a new RDD by applying a function to each partition of this RDD. Parameters:f - (undocumented) Returns:(undocumented) mapPartitions <U> JavaRDD<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f, boolean preservesPartitioning) Return a new RDD by applying a function to each partition of this RDD. Parameters:f - (undocumented)preservesPartitioning - (undocumented) Returns:(undocumented) mapPartitionsToDouble JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f) Return a new RDD by applying a function to each partition of this RDD. Parameters:f - (undocumented) Returns:(undocumented) mapPartitionsToPair <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) Return a new RDD by applying a function to each partition of this RDD. Parameters:f - (undocumented) Returns:(undocumented) mapPartitionsToDouble JavaDoubleRDD mapPartitionsToDouble(DoubleFlatMapFunction<java.util.Iterator<T>> f, boolean preservesPartitioning) Return a new RDD by applying a function to each partition of this RDD. Parameters:f - (undocumented)preservesPartitioning - (undocumented) Returns:(undocumented) mapPartitionsToPair <K2,V2> JavaPairRDD<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f, boolean preservesPartitioning) Return a new RDD by applying a function to each partition of this RDD. Parameters:f - (undocumented)preservesPartitioning - (undocumented) Returns:(undocumented) foreachPartition void foreachPartition(VoidFunction<java.util.Iterator<T>> f) Applies a function f to each partition of this RDD. Parameters:f - (undocumented) glom JavaRDD<java.util.List<T>> glom() Return an RDD created by coalescing all elements within each partition into an array. Returns:(undocumented) cartesian <U> JavaPairRDD<T,U> cartesian(JavaRDDLike<U,?> other) Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in this and b is in other. Parameters:other - (undocumented) Returns:(undocumented) groupBy <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f) Return an RDD of grouped elements. Each group consists of a key and a sequence of elements mapping to that key. Parameters:f - (undocumented) Returns:(undocumented) groupBy <U> JavaPairRDD<U,Iterable<T>> groupBy(Function<T,U> f, int numPartitions) Return an RDD of grouped elements. Each group consists of a key and a sequence of elements mapping to that key. Parameters:f - (undocumented)numPartitions - (undocumented) Returns:(undocumented) pipe JavaRDD<String> pipe(String command) Return an RDD created by piping elements to a forked external process. Parameters:command - (undocumented) Returns:(undocumented) pipe JavaRDD<String> pipe(java.util.List<String> command) Return an RDD created by piping elements to a forked external process. Parameters:command - (undocumented) Returns:(undocumented) pipe JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env) Return an RDD created by piping elements to a forked external process. Parameters:command - (undocumented)env - (undocumented) Returns:(undocumented) pipe JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize) Return an RDD created by piping elements to a forked external process. Parameters:command - (undocumented)env - (undocumented)separateWorkingDir - (undocumented)bufferSize - (undocumented) Returns:(undocumented) pipe JavaRDD<String> pipe(java.util.List<String> command, java.util.Map<String,String> env, boolean separateWorkingDir, int bufferSize, String encoding) Return an RDD created by piping elements to a forked external process. Parameters:command - (undocumented)env - (undocumented)separateWorkingDir - (undocumented)bufferSize - (undocumented)encoding - (undocumented) Returns:(undocumented) zip <U> JavaPairRDD<T,U> zip(JavaRDDLike<U,?> other) Zips this RDD with another one, returning key-value pairs with the first element in each RDD, second element in each RDD, etc. Assumes that the two RDDs have the *same number of partitions* and the *same number of elements in each partition* (e.g. one was made through a map on the other). Parameters:other - (undocumented) Returns:(undocumented) zipPartitions <U,V> JavaRDD<V> zipPartitions(JavaRDDLike<U,?> other, FlatMapFunction2<java.util.Iterator<T>,java.util.Iterator<U>,V> f) Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by applying a function to the zipped partitions. Assumes that all the RDDs have the *same number of partitions*, but does *not* require them to have the same number of elements in each partition. Parameters:other - (undocumented)f - (undocumented) Returns:(undocumented) zipWithUniqueId JavaPairRDD<T,Long> zipWithUniqueId() Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method won't trigger a spark job, which is different from RDD.zipWithIndex(). Returns:(undocumented) zipWithIndex JavaPairRDD<T,Long> zipWithIndex() Zips this RDD with its element indices. The ordering is first based on the partition index and then the ordering of items within each partition. So the first item in the first partition gets index 0, and the last item in the last partition receives the largest index. This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. This method needs to trigger a spark job when this RDD contains more than one partitions. Returns:(undocumented) foreach void foreach(VoidFunction<T> f) Applies a function f to all elements of this RDD. Parameters:f - (undocumented) collect java.util.List<T> collect() Return an array that contains all of the elements in this RDD. Returns:(undocumented) toLocalIterator java.util.Iterator<T> toLocalIterator() Return an iterator that contains all of the elements in this RDD. The iterator will consume as much memory as the largest partition in this RDD. Returns:(undocumented) collectPartitions java.util.List<T>[] collectPartitions(int[] partitionIds) Return an array that contains all of the elements in a specific partition of this RDD. Parameters:partitionIds - (undocumented) Returns:(undocumented) reduce T reduce(Function2<T,T,T> f) Reduces the elements of this RDD using the specified commutative and associative binary operator. Parameters:f - (undocumented) Returns:(undocumented) treeReduce T treeReduce(Function2<T,T,T> f, int depth) Reduces the elements of this RDD in a multi-level tree pattern. Parameters:depth - suggested depth of the treef - (undocumented) Returns:(undocumented)See Also:reduce(org.apache.spark.api.java.function.Function2<T, T, T>) treeReduce T treeReduce(Function2<T,T,T> f) treeReduce(org.apache.spark.api.java.function.Function2<T, T, T>, int) with suggested depth 2. Parameters:f - (undocumented) Returns:(undocumented) fold T fold(T zeroValue, Function2<T,T,T> f) Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral "zero value". The function op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2. This behaves somewhat differently from fold operations implemented for non-distributed collections in functional languages like Scala. This fold operation may be applied to partitions individually, and then fold those results into the final result, rather than apply the fold to each element sequentially in some defined ordering. For functions that are not commutative, the result may differ from that of a fold applied to a non-distributed collection. Parameters:zeroValue - (undocumented)f - (undocumented) Returns:(undocumented) aggregate <U> U aggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) Aggregate the elements of each partition, and then the results for all the partitions, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are allowed to modify and return their first argument instead of creating a new U to avoid memory allocation. Parameters:zeroValue - (undocumented)seqOp - (undocumented)combOp - (undocumented) Returns:(undocumented) treeAggregate <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp, int depth) Aggregates the elements of this RDD in a multi-level tree pattern. Parameters:depth - suggested depth of the treezeroValue - (undocumented)seqOp - (undocumented)combOp - (undocumented) Returns:(undocumented)See Also:aggregate(U, org.apache.spark.api.java.function.Function2<U, T, U>, org.apache.spark.api.java.function.Function2<U, U, U>) treeAggregate <U> U treeAggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) treeAggregate(U, org.apache.spark.api.java.function.Function2<U, T, U>, org.apache.spark.api.java.function.Function2<U, U, U>, int) with suggested depth 2. Parameters:zeroValue - (undocumented)seqOp - (undocumented)combOp - (undocumented) Returns:(undocumented) count long count() Return the number of elements in the RDD. Returns:(undocumented) countApprox PartialResult<BoundedDouble> countApprox(long timeout, double confidence) Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished. The confidence is the probability that the error bounds of the result will contain the true value. That is, if countApprox were called repeatedly with confidence 0.9, we would expect 90% of the results to contain the true count. The confidence must be in the range [0,1] or an exception will be thrown. Parameters:timeout - maximum time to wait for the job, in millisecondsconfidence - the desired statistical confidence in the result Returns:a potentially incomplete result, with error bounds countApprox PartialResult<BoundedDouble> countApprox(long timeout) Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished. Parameters:timeout - maximum time to wait for the job, in milliseconds Returns:(undocumented) countByValue java.util.Map<T,Long> countByValue() Return the count of each unique value in this RDD as a map of (value, count) pairs. The final combine step happens locally on the master, equivalent to running a single reduce task. Returns:(undocumented) countByValueApprox PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence) Approximate version of countByValue(). The confidence is the probability that the error bounds of the result will contain the true value. That is, if countApprox were called repeatedly with confidence 0.9, we would expect 90% of the results to contain the true count. The confidence must be in the range [0,1] or an exception will be thrown. Parameters:timeout - maximum time to wait for the job, in millisecondsconfidence - the desired statistical confidence in the result Returns:a potentially incomplete result, with error bounds countByValueApprox PartialResult<java.util.Map<T,BoundedDouble>> countByValueApprox(long timeout) Approximate version of countByValue(). Parameters:timeout - maximum time to wait for the job, in milliseconds Returns:a potentially incomplete result, with error bounds take java.util.List<T> take(int num) Take the first num elements of the RDD. This currently scans the partitions *one by one*, so it will be slow if a lot of partitions are required. In that case, use collect() to get the whole RDD instead. Parameters:num - (undocumented) Returns:(undocumented) takeSample java.util.List<T> takeSample(boolean withReplacement, int num) takeSample java.util.List<T> takeSample(boolean withReplacement, int num, long seed) first T first() Return the first element in this RDD. Returns:(undocumented) isEmpty boolean isEmpty() Returns:true if and only if the RDD contains no elements at all. Note that an RDD may be empty even when it has at least 1 partition. saveAsTextFile void saveAsTextFile(String path) Save this RDD as a text file, using string representations of elements. Parameters:path - (undocumented) saveAsTextFile void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) Save this RDD as a compressed text file, using string representations of elements. Parameters:path - (undocumented)codec - (undocumented) saveAsObjectFile void saveAsObjectFile(String path) Save this RDD as a SequenceFile of serialized objects. Parameters:path - (undocumented) keyBy <U> JavaPairRDD<U,T> keyBy(Function<T,U> f) Creates tuples of the elements in this RDD by applying f. Parameters:f - (undocumented) Returns:(undocumented) checkpoint void checkpoint() Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint directory set with SparkContext.setCheckpointDir() and all references to its parent RDDs will be removed. This function must be called before any job has been executed on this RDD. It is strongly recommended that this RDD is persisted in memory, otherwise saving it on a file will require recomputation. isCheckpointed boolean isCheckpointed() Return whether this RDD has been checkpointed or not Returns:(undocumented) getCheckpointFile Optional<String> getCheckpointFile() Gets the name of the file to which this RDD was checkpointed Returns:(undocumented) toDebugString String toDebugString() A description of this RDD and its recursive dependencies for debugging. top java.util.List<T> top(int num, java.util.Comparator<T> comp) Returns the top k (largest) elements from this RDD as defined by the specified Comparator[T] and maintains the order. Parameters:num - k, the number of top elements to returncomp - the comparator that defines the order Returns:an array of top elements top java.util.List<T> top(int num) Returns the top k (largest) elements from this RDD using the natural ordering for T and maintains the order. Parameters:num - k, the number of top elements to return Returns:an array of top elements takeOrdered java.util.List<T> takeOrdered(int num, java.util.Comparator<T> comp) Returns the first k (smallest) elements from this RDD as defined by the specified Comparator[T] and maintains the order. Parameters:num - k, the number of elements to returncomp - the comparator that defines the order Returns:an array of top elements max T max(java.util.Comparator<T> comp) Returns the maximum element from this RDD as defined by the specified Comparator[T]. Parameters:comp - the comparator that defines ordering Returns:the maximum of the RDD min T min(java.util.Comparator<T> comp) Returns the minimum element from this RDD as defined by the specified Comparator[T]. Parameters:comp - the comparator that defines ordering Returns:the minimum of the RDD takeOrdered java.util.List<T> takeOrdered(int num) Returns the first k (smallest) elements from this RDD using the natural ordering for T while maintain the order. Parameters:num - k, the number of top elements to return Returns:an array of top elements countApproxDistinct long countApproxDistinct(double relativeSD) Return approximate number of distinct elements in the RDD. The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here. Parameters:relativeSD - Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017. Returns:(undocumented) name String name() countAsync JavaFutureAction<Long> countAsync() The asynchronous version of count, which returns a future for counting the number of elements in this RDD. Returns:(undocumented) collectAsync JavaFutureAction<java.util.List<T>> collectAsync() The asynchronous version of collect, which returns a future for retrieving an array containing all of the elements in this RDD. Returns:(undocumented) takeAsync JavaFutureAction<java.util.List<T>> takeAsync(int num) The asynchronous version of the take action, which returns a future for retrieving the first num elements of this RDD. Parameters:num - (undocumented) Returns:(undocumented) foreachAsync JavaFutureAction<Void> foreachAsync(VoidFunction<T> f) The asynchronous version of the foreach action, which applies a function f to all the elements of this RDD. Parameters:f - (undocumented) Returns:(undocumented) foreachPartitionAsync JavaFutureAction<Void> foreachPartitionAsync(VoidFunction<java.util.Iterator<T>> f) The asynchronous version of the foreachPartition action, which applies a function f to each partition of this RDD. Parameters:f - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaReceiverInputDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaReceiverInputDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.api.java Class JavaReceiverInputDStream<T> Object org.apache.spark.streaming.api.java.JavaDStream<T> org.apache.spark.streaming.api.java.JavaInputDStream<T> org.apache.spark.streaming.api.java.JavaReceiverInputDStream<T> All Implemented Interfaces: java.io.Serializable, JavaDStreamLike<T,JavaDStream<T>,JavaRDD<T>> public class JavaReceiverInputDStream<T> extends JavaInputDStream<T> A Java-friendly interface to ReceiverInputDStream, the abstract class for defining any input stream that receives data over the network. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaReceiverInputDStream(ReceiverInputDStream<T> receiverInputDStream, scala.reflect.ClassTag<T> classTag)  Method Summary Methods  Modifier and Type Method and Description static JavaDStream<T> cache()  static DStream<T> checkpoint(Duration interval)  scala.reflect.ClassTag<T> classTag()  static JavaRDD<T> compute(Time validTime)  static StreamingContext context()  static JavaDStream<Long> count()  static JavaPairDStream<T,Long> countByValue()  static JavaPairDStream<T,Long> countByValue(int numPartitions)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration)  static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions)  static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration)  static DStream<T> dstream()  static JavaDStream<T> filter(Function<T,Boolean> f)  static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f)  static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f)  static void foreachRDD(VoidFunction<R> foreachFunc)  static void foreachRDD(VoidFunction2<R,Time> foreachFunc)  static <T> JavaReceiverInputDStream<T> fromReceiverInputDStream(ReceiverInputDStream<T> receiverInputDStream, scala.reflect.ClassTag<T> evidence$1) Convert a scala ReceiverInputDStream to a Java-friendly JavaReceiverInputDStream. static JavaDStream<java.util.List<T>> glom()  static InputDStream<T> inputDStream()  static <R> JavaDStream<R> map(Function<T,R> f)  static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f)  static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f)  static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f)  static JavaDStream<T> persist()  static JavaDStream<T> persist(StorageLevel storageLevel)  static void print()  static void print(int num)  ReceiverInputDStream<T> receiverInputDStream()  static JavaDStream<T> reduce(Function2<T,T,T> f)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration)  static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration)  static JavaDStream<T> repartition(int numPartitions)  static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in)  static java.util.List<R> slice(Time fromTime, Time toTime)  static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc)  static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc)  static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc)  static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc)  static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc)  static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc)  static JavaDStream<T> union(JavaDStream<T> that)  static JavaDStream<T> window(Duration windowDuration)  static JavaDStream<T> window(Duration windowDuration, Duration slideDuration)  static JavaRDD<T> wrapRDD(RDD<T> rdd)  Methods inherited from class org.apache.spark.streaming.api.java.JavaInputDStream fromInputDStream, inputDStream Methods inherited from class org.apache.spark.streaming.api.java.JavaDStream cache, compute, dstream, filter, fromDStream, persist, persist, repartition, union, window, window, wrapRDD Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.streaming.api.java.JavaDStreamLike checkpoint, context, count, countByValue, countByValue, countByValueAndWindow, countByValueAndWindow, countByWindow, flatMap, flatMapToPair, foreachRDD, foreachRDD, glom, map, mapPartitions, mapPartitionsToPair, mapToPair, print, print, reduce, reduceByWindow, reduceByWindow, scalaIntToJavaLong, slice, transform, transform, transformToPair, transformToPair, transformWith, transformWith, transformWithToPair, transformWithToPair Constructor Detail JavaReceiverInputDStream public JavaReceiverInputDStream(ReceiverInputDStream<T> receiverInputDStream, scala.reflect.ClassTag<T> classTag) Method Detail fromReceiverInputDStream public static <T> JavaReceiverInputDStream<T> fromReceiverInputDStream(ReceiverInputDStream<T> receiverInputDStream, scala.reflect.ClassTag<T> evidence$1) Convert a scala ReceiverInputDStream to a Java-friendly JavaReceiverInputDStream. Parameters:receiverInputDStream - (undocumented)evidence$1 - (undocumented) Returns:(undocumented) scalaIntToJavaLong public static JavaDStream<Long> scalaIntToJavaLong(DStream<Object> in) print public static void print() print public static void print(int num) count public static JavaDStream<Long> count() countByValue public static JavaPairDStream<T,Long> countByValue() countByValue public static JavaPairDStream<T,Long> countByValue(int numPartitions) countByWindow public static JavaDStream<Long> countByWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration) countByValueAndWindow public static JavaPairDStream<T,Long> countByValueAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) glom public static JavaDStream<java.util.List<T>> glom() context public static StreamingContext context() map public static <R> JavaDStream<R> map(Function<T,R> f) mapToPair public static <K2,V2> JavaPairDStream<K2,V2> mapToPair(PairFunction<T,K2,V2> f) flatMap public static <U> JavaDStream<U> flatMap(FlatMapFunction<T,U> f) flatMapToPair public static <K2,V2> JavaPairDStream<K2,V2> flatMapToPair(PairFlatMapFunction<T,K2,V2> f) mapPartitions public static <U> JavaDStream<U> mapPartitions(FlatMapFunction<java.util.Iterator<T>,U> f) mapPartitionsToPair public static <K2,V2> JavaPairDStream<K2,V2> mapPartitionsToPair(PairFlatMapFunction<java.util.Iterator<T>,K2,V2> f) reduce public static JavaDStream<T> reduce(Function2<T,T,T> f) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Duration windowDuration, Duration slideDuration) reduceByWindow public static JavaDStream<T> reduceByWindow(Function2<T,T,T> reduceFunc, Function2<T,T,T> invReduceFunc, Duration windowDuration, Duration slideDuration) slice public static java.util.List<R> slice(Time fromTime, Time toTime) foreachRDD public static void foreachRDD(VoidFunction<R> foreachFunc) foreachRDD public static void foreachRDD(VoidFunction2<R,Time> foreachFunc) transform public static <U> JavaDStream<U> transform(Function<R,JavaRDD<U>> transformFunc) transform public static <U> JavaDStream<U> transform(Function2<R,Time,JavaRDD<U>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function<R,JavaPairRDD<K2,V2>> transformFunc) transformToPair public static <K2,V2> JavaPairDStream<K2,V2> transformToPair(Function2<R,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <U,W> JavaDStream<W> transformWith(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <U,K2,V2> JavaPairDStream<K2,V2> transformWithToPair(JavaDStream<U> other, Function3<R,JavaRDD<U>,Time,JavaPairRDD<K2,V2>> transformFunc) transformWith public static <K2,V2,W> JavaDStream<W> transformWith(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaRDD<W>> transformFunc) transformWithToPair public static <K2,V2,K3,V3> JavaPairDStream<K3,V3> transformWithToPair(JavaPairDStream<K2,V2> other, Function3<R,JavaPairRDD<K2,V2>,Time,JavaPairRDD<K3,V3>> transformFunc) checkpoint public static DStream<T> checkpoint(Duration interval) dstream public static DStream<T> dstream() wrapRDD public static JavaRDD<T> wrapRDD(RDD<T> rdd) filter public static JavaDStream<T> filter(Function<T,Boolean> f) cache public static JavaDStream<T> cache() persist public static JavaDStream<T> persist() persist public static JavaDStream<T> persist(StorageLevel storageLevel) compute public static JavaRDD<T> compute(Time validTime) window public static JavaDStream<T> window(Duration windowDuration) window public static JavaDStream<T> window(Duration windowDuration, Duration slideDuration) union public static JavaDStream<T> union(JavaDStream<T> that) repartition public static JavaDStream<T> repartition(int numPartitions) inputDStream public static InputDStream<T> inputDStream() receiverInputDStream public ReceiverInputDStream<T> receiverInputDStream() classTag public scala.reflect.ClassTag<T> classTag() Specified by: classTag in interface JavaDStreamLike<T,JavaDStream<T>,JavaRDD<T>> Overrides: classTag in class JavaInputDStream<T> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaSerializer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaSerializer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.serializer Class JavaSerializer Object org.apache.spark.serializer.Serializer org.apache.spark.serializer.JavaSerializer All Implemented Interfaces: java.io.Externalizable, java.io.Serializable public class JavaSerializer extends Serializer implements java.io.Externalizable :: DeveloperApi :: A Spark serializer that uses Java's built-in serialization. Note that this serializer is not guaranteed to be wire-compatible across different versions of Spark. It is intended to be used to serialize/de-serialize data within a single Spark application. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JavaSerializer(SparkConf conf)  Method Summary Methods  Modifier and Type Method and Description SerializerInstance newInstance() Creates a new SerializerInstance. void readExternal(java.io.ObjectInput in)  void writeExternal(java.io.ObjectOutput out)  Methods inherited from class org.apache.spark.serializer.Serializer setDefaultClassLoader Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JavaSerializer public JavaSerializer(SparkConf conf) Method Detail newInstance public SerializerInstance newInstance() Description copied from class: Serializer Creates a new SerializerInstance. Specified by: newInstance in class Serializer writeExternal public void writeExternal(java.io.ObjectOutput out) Specified by: writeExternal in interface java.io.Externalizable readExternal public void readExternal(java.io.ObjectInput in) Specified by: readExternal in interface java.io.Externalizable Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaSparkContext (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaSparkContext (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class JavaSparkContext Object org.apache.spark.api.java.JavaSparkContext All Implemented Interfaces: java.io.Closeable, AutoCloseable public class JavaSparkContext extends Object implements java.io.Closeable A Java-friendly version of SparkContext that returns JavaRDDs and works with Java collections instead of Scala ones. Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one. This limitation may eventually be removed; see SPARK-2243 for more details. Constructor Summary Constructors  Constructor and Description JavaSparkContext() Create a JavaSparkContext that loads settings from system properties (for instance, when launching with ./bin/spark-submit). JavaSparkContext(SparkConf conf)  JavaSparkContext(SparkContext sc)  JavaSparkContext(String master, String appName)  JavaSparkContext(String master, String appName, SparkConf conf)  JavaSparkContext(String master, String appName, String sparkHome, String jarFile)  JavaSparkContext(String master, String appName, String sparkHome, String[] jars)  JavaSparkContext(String master, String appName, String sparkHome, String[] jars, java.util.Map<String,String> environment)  Method Summary Methods  Modifier and Type Method and Description <T,R> Accumulable<T,R> accumulable(T initialValue, AccumulableParam<T,R> param) Deprecated.  use AccumulatorV2. Since 2.0.0. <T,R> Accumulable<T,R> accumulable(T initialValue, String name, AccumulableParam<T,R> param) Deprecated.  use AccumulatorV2. Since 2.0.0. Accumulator<Double> accumulator(double initialValue) Deprecated.  use sc().doubleAccumulator(). Since 2.0.0. Accumulator<Double> accumulator(double initialValue, String name) Deprecated.  use sc().doubleAccumulator(String). Since 2.0.0. Accumulator<Integer> accumulator(int initialValue) Deprecated.  use sc().longAccumulator(). Since 2.0.0. Accumulator<Integer> accumulator(int initialValue, String name) Deprecated.  use sc().longAccumulator(String). Since 2.0.0. <T> Accumulator<T> accumulator(T initialValue, AccumulatorParam<T> accumulatorParam) Deprecated.  use AccumulatorV2. Since 2.0.0. <T> Accumulator<T> accumulator(T initialValue, String name, AccumulatorParam<T> accumulatorParam) Deprecated.  use AccumulatorV2. Since 2.0.0. void addFile(String path) Add a file to be downloaded with this Spark job on every node. void addJar(String path) Adds a JAR dependency for all tasks to be executed on this SparkContext in the future. String appName()  JavaPairRDD<String,PortableDataStream> binaryFiles(String path) Read a directory of binary files from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI as a byte array. JavaPairRDD<String,PortableDataStream> binaryFiles(String path, int minPartitions) Read a directory of binary files from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI as a byte array. JavaRDD<byte[]> binaryRecords(String path, int recordLength) Load data from a flat binary file, assuming the length of each record is constant. <T> Broadcast<T> broadcast(T value) Broadcast a read-only variable to the cluster, returning a Broadcast object for reading it in distributed functions. void cancelAllJobs() Cancel all jobs that have been scheduled or are running. void cancelJobGroup(String groupId) Cancel active jobs for the specified group. void clearCallSite() Pass-through to SparkContext.setCallSite. void clearJobGroup() Clear the current thread's job group ID and its description. void close()  Integer defaultMinPartitions() Default min number of partitions for Hadoop RDDs when not given by user Integer defaultParallelism() Default level of parallelism to use when not given by user (e.g. Accumulator<Double> doubleAccumulator(double initialValue) Deprecated.  use sc().doubleAccumulator(). Since 2.0.0. Accumulator<Double> doubleAccumulator(double initialValue, String name) Deprecated.  use sc().doubleAccumulator(String). Since 2.0.0. <T> JavaRDD<T> emptyRDD() Get an RDD that has no partitions or elements. static JavaSparkContext fromSparkContext(SparkContext sc)  Optional<String> getCheckpointDir()  SparkConf getConf() Return a copy of this JavaSparkContext's configuration. String getLocalProperty(String key) Get a local property set in this thread, or null if it is missing. java.util.Map<Integer,JavaRDD<?>> getPersistentRDDs() Returns a Java map of JavaRDDs that have marked themselves as persistent via cache() call. Optional<String> getSparkHome() Get Spark's home location from either a value set through the constructor, or the spark.home Java property, or the SPARK_HOME environment variable (in that order of preference). org.apache.hadoop.conf.Configuration hadoopConfiguration() Returns the Hadoop configuration used for the Hadoop code (e.g. <K,V,F extends org.apache.hadoop.mapred.InputFormat<K,V>> JavaPairRDD<K,V> hadoopFile(String path, Class<F> inputFormatClass, Class<K> keyClass, Class<V> valueClass) Get an RDD for a Hadoop file with an arbitrary InputFormat <K,V,F extends org.apache.hadoop.mapred.InputFormat<K,V>> JavaPairRDD<K,V> hadoopFile(String path, Class<F> inputFormatClass, Class<K> keyClass, Class<V> valueClass, int minPartitions) Get an RDD for a Hadoop file with an arbitrary InputFormat. <K,V,F extends org.apache.hadoop.mapred.InputFormat<K,V>> JavaPairRDD<K,V> hadoopRDD(org.apache.hadoop.mapred.JobConf conf, Class<F> inputFormatClass, Class<K> keyClass, Class<V> valueClass) Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf giving its InputFormat and any other necessary info (e.g. <K,V,F extends org.apache.hadoop.mapred.InputFormat<K,V>> JavaPairRDD<K,V> hadoopRDD(org.apache.hadoop.mapred.JobConf conf, Class<F> inputFormatClass, Class<K> keyClass, Class<V> valueClass, int minPartitions) Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf giving its InputFormat and any other necessary info (e.g. Accumulator<Integer> intAccumulator(int initialValue) Deprecated.  use sc().longAccumulator(). Since 2.0.0. Accumulator<Integer> intAccumulator(int initialValue, String name) Deprecated.  use sc().longAccumulator(String). Since 2.0.0. Boolean isLocal()  static String[] jarOfClass(Class<?> cls) Find the JAR from which a given class was loaded, to make it easy for users to pass their JARs to SparkContext. static String[] jarOfObject(Object obj) Find the JAR that contains the class of a particular object, to make it easy for users to pass their JARs to SparkContext. java.util.List<String> jars()  String master()  <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairRDD<K,V> newAPIHadoopFile(String path, Class<F> fClass, Class<K> kClass, Class<V> vClass, org.apache.hadoop.conf.Configuration conf) Get an RDD for a given Hadoop file with an arbitrary new API InputFormat and extra configuration options to pass to the input format. <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairRDD<K,V> newAPIHadoopRDD(org.apache.hadoop.conf.Configuration conf, Class<F> fClass, Class<K> kClass, Class<V> vClass) Get an RDD for a given Hadoop file with an arbitrary new API InputFormat and extra configuration options to pass to the input format. <T> JavaRDD<T> objectFile(String path) Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and BytesWritable values that contain a serialized partition. <T> JavaRDD<T> objectFile(String path, int minPartitions) Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and BytesWritable values that contain a serialized partition. <T> JavaRDD<T> parallelize(java.util.List<T> list) Distribute a local Scala collection to form an RDD. <T> JavaRDD<T> parallelize(java.util.List<T> list, int numSlices) Distribute a local Scala collection to form an RDD. JavaDoubleRDD parallelizeDoubles(java.util.List<Double> list) Distribute a local Scala collection to form an RDD. JavaDoubleRDD parallelizeDoubles(java.util.List<Double> list, int numSlices) Distribute a local Scala collection to form an RDD. <K,V> JavaPairRDD<K,V> parallelizePairs(java.util.List<scala.Tuple2<K,V>> list) Distribute a local Scala collection to form an RDD. <K,V> JavaPairRDD<K,V> parallelizePairs(java.util.List<scala.Tuple2<K,V>> list, int numSlices) Distribute a local Scala collection to form an RDD. SparkContext sc()  <K,V> JavaPairRDD<K,V> sequenceFile(String path, Class<K> keyClass, Class<V> valueClass) Get an RDD for a Hadoop SequenceFile. <K,V> JavaPairRDD<K,V> sequenceFile(String path, Class<K> keyClass, Class<V> valueClass, int minPartitions) Get an RDD for a Hadoop SequenceFile with given key and value types. void setCallSite(String site) Pass-through to SparkContext.setCallSite. void setCheckpointDir(String dir) Set the directory under which RDDs are going to be checkpointed. void setJobGroup(String groupId, String description) Assigns a group ID to all the jobs started by this thread until the group ID is set to a different value or cleared. void setJobGroup(String groupId, String description, boolean interruptOnCancel) Assigns a group ID to all the jobs started by this thread until the group ID is set to a different value or cleared. void setLocalProperty(String key, String value) Set a local property that affects jobs submitted from this thread, and all child threads, such as the Spark fair scheduler pool. void setLogLevel(String logLevel) Control our logLevel. String sparkUser()  Long startTime()  JavaSparkStatusTracker statusTracker()  void stop() Shut down the SparkContext. JavaRDD<String> textFile(String path) Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings. JavaRDD<String> textFile(String path, int minPartitions) Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings. static SparkContext toSparkContext(JavaSparkContext jsc)  JavaDoubleRDD union(JavaDoubleRDD... rdds)  JavaDoubleRDD union(JavaDoubleRDD first, java.util.List<JavaDoubleRDD> rest) Build the union of two or more RDDs. <K,V> JavaPairRDD<K,V> union(JavaPairRDD<K,V>... rdds)  <K,V> JavaPairRDD<K,V> union(JavaPairRDD<K,V> first, java.util.List<JavaPairRDD<K,V>> rest) Build the union of two or more RDDs. <T> JavaRDD<T> union(JavaRDD<T>... rdds)  <T> JavaRDD<T> union(JavaRDD<T> first, java.util.List<JavaRDD<T>> rest) Build the union of two or more RDDs. String version() The version of Spark on which this application is running. JavaPairRDD<String,String> wholeTextFiles(String path) Read a directory of text files from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI. JavaPairRDD<String,String> wholeTextFiles(String path, int minPartitions) Read a directory of text files from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JavaSparkContext public JavaSparkContext(SparkContext sc) JavaSparkContext public JavaSparkContext() Create a JavaSparkContext that loads settings from system properties (for instance, when launching with ./bin/spark-submit). JavaSparkContext public JavaSparkContext(SparkConf conf) Parameters:conf - a SparkConf object specifying Spark parameters JavaSparkContext public JavaSparkContext(String master, String appName) Parameters:master - Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).appName - A name for your application, to display on the cluster web UI JavaSparkContext public JavaSparkContext(String master, String appName, SparkConf conf) Parameters:master - Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).appName - A name for your application, to display on the cluster web UIconf - a SparkConf object specifying other Spark parameters JavaSparkContext public JavaSparkContext(String master, String appName, String sparkHome, String jarFile) Parameters:master - Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).appName - A name for your application, to display on the cluster web UIsparkHome - The SPARK_HOME directory on the slave nodesjarFile - JAR file to send to the cluster. This can be a path on the local file system or an HDFS, HTTP, HTTPS, or FTP URL. JavaSparkContext public JavaSparkContext(String master, String appName, String sparkHome, String[] jars) Parameters:master - Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).appName - A name for your application, to display on the cluster web UIsparkHome - The SPARK_HOME directory on the slave nodesjars - Collection of JARs to send to the cluster. These can be paths on the local file system or HDFS, HTTP, HTTPS, or FTP URLs. JavaSparkContext public JavaSparkContext(String master, String appName, String sparkHome, String[] jars, java.util.Map<String,String> environment) Parameters:master - Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).appName - A name for your application, to display on the cluster web UIsparkHome - The SPARK_HOME directory on the slave nodesjars - Collection of JARs to send to the cluster. These can be paths on the local file system or HDFS, HTTP, HTTPS, or FTP URLs.environment - Environment variables to set on worker nodes Method Detail fromSparkContext public static JavaSparkContext fromSparkContext(SparkContext sc) toSparkContext public static SparkContext toSparkContext(JavaSparkContext jsc) jarOfClass public static String[] jarOfClass(Class<?> cls) Find the JAR from which a given class was loaded, to make it easy for users to pass their JARs to SparkContext. Parameters:cls - (undocumented) Returns:(undocumented) jarOfObject public static String[] jarOfObject(Object obj) Find the JAR that contains the class of a particular object, to make it easy for users to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in your driver program. Parameters:obj - (undocumented) Returns:(undocumented) sc public SparkContext sc() statusTracker public JavaSparkStatusTracker statusTracker() isLocal public Boolean isLocal() sparkUser public String sparkUser() master public String master() appName public String appName() jars public java.util.List<String> jars() startTime public Long startTime() version public String version() The version of Spark on which this application is running. defaultParallelism public Integer defaultParallelism() Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD). defaultMinPartitions public Integer defaultMinPartitions() Default min number of partitions for Hadoop RDDs when not given by user parallelize public <T> JavaRDD<T> parallelize(java.util.List<T> list, int numSlices) Distribute a local Scala collection to form an RDD. emptyRDD public <T> JavaRDD<T> emptyRDD() Get an RDD that has no partitions or elements. parallelize public <T> JavaRDD<T> parallelize(java.util.List<T> list) Distribute a local Scala collection to form an RDD. parallelizePairs public <K,V> JavaPairRDD<K,V> parallelizePairs(java.util.List<scala.Tuple2<K,V>> list, int numSlices) Distribute a local Scala collection to form an RDD. parallelizePairs public <K,V> JavaPairRDD<K,V> parallelizePairs(java.util.List<scala.Tuple2<K,V>> list) Distribute a local Scala collection to form an RDD. parallelizeDoubles public JavaDoubleRDD parallelizeDoubles(java.util.List<Double> list, int numSlices) Distribute a local Scala collection to form an RDD. parallelizeDoubles public JavaDoubleRDD parallelizeDoubles(java.util.List<Double> list) Distribute a local Scala collection to form an RDD. textFile public JavaRDD<String> textFile(String path) Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings. Parameters:path - (undocumented) Returns:(undocumented) textFile public JavaRDD<String> textFile(String path, int minPartitions) Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings. Parameters:path - (undocumented)minPartitions - (undocumented) Returns:(undocumented) wholeTextFiles public JavaPairRDD<String,String> wholeTextFiles(String path, int minPartitions) Read a directory of text files from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI. Each file is read as a single record and returned in a key-value pair, where the key is the path of each file, the value is the content of each file. For example, if you have the following files: hdfs://a-hdfs-path/part-00000 hdfs://a-hdfs-path/part-00001 ... hdfs://a-hdfs-path/part-nnnnn Do JavaPairRDD<String, String> rdd = sparkContext.wholeTextFiles("hdfs://a-hdfs-path") then rdd contains (a-hdfs-path/part-00000, its content) (a-hdfs-path/part-00001, its content) ... (a-hdfs-path/part-nnnnn, its content) Parameters:minPartitions - A suggestion value of the minimal splitting number for input data.path - (undocumented) Returns:(undocumented) wholeTextFiles public JavaPairRDD<String,String> wholeTextFiles(String path) Read a directory of text files from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI. Each file is read as a single record and returned in a key-value pair, where the key is the path of each file, the value is the content of each file. Parameters:path - (undocumented) Returns:(undocumented)See Also:wholeTextFiles(path: String, minPartitions: Int). binaryFiles public JavaPairRDD<String,PortableDataStream> binaryFiles(String path, int minPartitions) Read a directory of binary files from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI as a byte array. Each file is read as a single record and returned in a key-value pair, where the key is the path of each file, the value is the content of each file. For example, if you have the following files: hdfs://a-hdfs-path/part-00000 hdfs://a-hdfs-path/part-00001 ... hdfs://a-hdfs-path/part-nnnnn Do JavaPairRDD rdd = sparkContext.dataStreamFiles("hdfs://a-hdfs-path"), then rdd contains (a-hdfs-path/part-00000, its content) (a-hdfs-path/part-00001, its content) ... (a-hdfs-path/part-nnnnn, its content) Parameters:minPartitions - A suggestion value of the minimal splitting number for input data.path - (undocumented) Returns:(undocumented) binaryFiles public JavaPairRDD<String,PortableDataStream> binaryFiles(String path) Read a directory of binary files from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI as a byte array. Each file is read as a single record and returned in a key-value pair, where the key is the path of each file, the value is the content of each file. For example, if you have the following files: hdfs://a-hdfs-path/part-00000 hdfs://a-hdfs-path/part-00001 ... hdfs://a-hdfs-path/part-nnnnn Do JavaPairRDD rdd = sparkContext.dataStreamFiles("hdfs://a-hdfs-path"), then rdd contains (a-hdfs-path/part-00000, its content) (a-hdfs-path/part-00001, its content) ... (a-hdfs-path/part-nnnnn, its content) Parameters:path - (undocumented) Returns:(undocumented) binaryRecords public JavaRDD<byte[]> binaryRecords(String path, int recordLength) Load data from a flat binary file, assuming the length of each record is constant. Parameters:path - Directory to the input data filesrecordLength - (undocumented) Returns:An RDD of data with values, represented as byte arrays sequenceFile public <K,V> JavaPairRDD<K,V> sequenceFile(String path, Class<K> keyClass, Class<V> valueClass, int minPartitions) Get an RDD for a Hadoop SequenceFile with given key and value types. '''Note:''' Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function. Parameters:path - (undocumented)keyClass - (undocumented)valueClass - (undocumented)minPartitions - (undocumented) Returns:(undocumented) sequenceFile public <K,V> JavaPairRDD<K,V> sequenceFile(String path, Class<K> keyClass, Class<V> valueClass) Get an RDD for a Hadoop SequenceFile. '''Note:''' Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function. Parameters:path - (undocumented)keyClass - (undocumented)valueClass - (undocumented) Returns:(undocumented) objectFile public <T> JavaRDD<T> objectFile(String path, int minPartitions) Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and BytesWritable values that contain a serialized partition. This is still an experimental storage format and may not be supported exactly as is in future Spark releases. It will also be pretty slow if you use the default serializer (Java serialization), though the nice thing about it is that there's very little effort required to save arbitrary objects. Parameters:path - (undocumented)minPartitions - (undocumented) Returns:(undocumented) objectFile public <T> JavaRDD<T> objectFile(String path) Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and BytesWritable values that contain a serialized partition. This is still an experimental storage format and may not be supported exactly as is in future Spark releases. It will also be pretty slow if you use the default serializer (Java serialization), though the nice thing about it is that there's very little effort required to save arbitrary objects. Parameters:path - (undocumented) Returns:(undocumented) hadoopRDD public <K,V,F extends org.apache.hadoop.mapred.InputFormat<K,V>> JavaPairRDD<K,V> hadoopRDD(org.apache.hadoop.mapred.JobConf conf, Class<F> inputFormatClass, Class<K> keyClass, Class<V> valueClass, int minPartitions) Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf giving its InputFormat and any other necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable, etc). Parameters:conf - JobConf for setting up the dataset. Note: This will be put into a Broadcast. Therefore if you plan to reuse this conf to create multiple RDDs, you need to make sure you won't modify the conf. A safe approach is always creating a new conf for a new RDD.inputFormatClass - Class of the InputFormatkeyClass - Class of the keysvalueClass - Class of the valuesminPartitions - Minimum number of Hadoop Splits to generate. '''Note:''' Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function. Returns:(undocumented) hadoopRDD public <K,V,F extends org.apache.hadoop.mapred.InputFormat<K,V>> JavaPairRDD<K,V> hadoopRDD(org.apache.hadoop.mapred.JobConf conf, Class<F> inputFormatClass, Class<K> keyClass, Class<V> valueClass) Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf giving its InputFormat and any other necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable, Parameters:conf - JobConf for setting up the dataset. Note: This will be put into a Broadcast. Therefore if you plan to reuse this conf to create multiple RDDs, you need to make sure you won't modify the conf. A safe approach is always creating a new conf for a new RDD.inputFormatClass - Class of the InputFormatkeyClass - Class of the keysvalueClass - Class of the values '''Note:''' Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function. Returns:(undocumented) hadoopFile public <K,V,F extends org.apache.hadoop.mapred.InputFormat<K,V>> JavaPairRDD<K,V> hadoopFile(String path, Class<F> inputFormatClass, Class<K> keyClass, Class<V> valueClass, int minPartitions) Get an RDD for a Hadoop file with an arbitrary InputFormat. '''Note:''' Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function. Parameters:path - (undocumented)inputFormatClass - (undocumented)keyClass - (undocumented)valueClass - (undocumented)minPartitions - (undocumented) Returns:(undocumented) hadoopFile public <K,V,F extends org.apache.hadoop.mapred.InputFormat<K,V>> JavaPairRDD<K,V> hadoopFile(String path, Class<F> inputFormatClass, Class<K> keyClass, Class<V> valueClass) Get an RDD for a Hadoop file with an arbitrary InputFormat '''Note:''' Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function. Parameters:path - (undocumented)inputFormatClass - (undocumented)keyClass - (undocumented)valueClass - (undocumented) Returns:(undocumented) newAPIHadoopFile public <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairRDD<K,V> newAPIHadoopFile(String path, Class<F> fClass, Class<K> kClass, Class<V> vClass, org.apache.hadoop.conf.Configuration conf) Get an RDD for a given Hadoop file with an arbitrary new API InputFormat and extra configuration options to pass to the input format. '''Note:''' Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function. Parameters:path - (undocumented)fClass - (undocumented)kClass - (undocumented)vClass - (undocumented)conf - (undocumented) Returns:(undocumented) newAPIHadoopRDD public <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairRDD<K,V> newAPIHadoopRDD(org.apache.hadoop.conf.Configuration conf, Class<F> fClass, Class<K> kClass, Class<V> vClass) Get an RDD for a given Hadoop file with an arbitrary new API InputFormat and extra configuration options to pass to the input format. Parameters:conf - Configuration for setting up the dataset. Note: This will be put into a Broadcast. Therefore if you plan to reuse this conf to create multiple RDDs, you need to make sure you won't modify the conf. A safe approach is always creating a new conf for a new RDD.fClass - Class of the InputFormatkClass - Class of the keysvClass - Class of the values '''Note:''' Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function. Returns:(undocumented) union public <T> JavaRDD<T> union(JavaRDD<T> first, java.util.List<JavaRDD<T>> rest) Build the union of two or more RDDs. union public <K,V> JavaPairRDD<K,V> union(JavaPairRDD<K,V> first, java.util.List<JavaPairRDD<K,V>> rest) Build the union of two or more RDDs. union public JavaDoubleRDD union(JavaDoubleRDD first, java.util.List<JavaDoubleRDD> rest) Build the union of two or more RDDs. intAccumulator public Accumulator<Integer> intAccumulator(int initialValue) Deprecated. use sc().longAccumulator(). Since 2.0.0. Create an Accumulator integer variable, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. Parameters:initialValue - (undocumented) Returns:(undocumented) intAccumulator public Accumulator<Integer> intAccumulator(int initialValue, String name) Deprecated. use sc().longAccumulator(String). Since 2.0.0. Create an Accumulator integer variable, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. This version supports naming the accumulator for display in Spark's web UI. Parameters:initialValue - (undocumented)name - (undocumented) Returns:(undocumented) doubleAccumulator public Accumulator<Double> doubleAccumulator(double initialValue) Deprecated. use sc().doubleAccumulator(). Since 2.0.0. Create an Accumulator double variable, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. Parameters:initialValue - (undocumented) Returns:(undocumented) doubleAccumulator public Accumulator<Double> doubleAccumulator(double initialValue, String name) Deprecated. use sc().doubleAccumulator(String). Since 2.0.0. Create an Accumulator double variable, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. This version supports naming the accumulator for display in Spark's web UI. Parameters:initialValue - (undocumented)name - (undocumented) Returns:(undocumented) accumulator public Accumulator<Integer> accumulator(int initialValue) Deprecated. use sc().longAccumulator(). Since 2.0.0. Create an Accumulator integer variable, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. Parameters:initialValue - (undocumented) Returns:(undocumented) accumulator public Accumulator<Integer> accumulator(int initialValue, String name) Deprecated. use sc().longAccumulator(String). Since 2.0.0. Create an Accumulator integer variable, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. This version supports naming the accumulator for display in Spark's web UI. Parameters:initialValue - (undocumented)name - (undocumented) Returns:(undocumented) accumulator public Accumulator<Double> accumulator(double initialValue) Deprecated. use sc().doubleAccumulator(). Since 2.0.0. Create an Accumulator double variable, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. Parameters:initialValue - (undocumented) Returns:(undocumented) accumulator public Accumulator<Double> accumulator(double initialValue, String name) Deprecated. use sc().doubleAccumulator(String). Since 2.0.0. Create an Accumulator double variable, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. This version supports naming the accumulator for display in Spark's web UI. Parameters:initialValue - (undocumented)name - (undocumented) Returns:(undocumented) accumulator public <T> Accumulator<T> accumulator(T initialValue, AccumulatorParam<T> accumulatorParam) Deprecated. use AccumulatorV2. Since 2.0.0. Create an Accumulator variable of a given type, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. Parameters:initialValue - (undocumented)accumulatorParam - (undocumented) Returns:(undocumented) accumulator public <T> Accumulator<T> accumulator(T initialValue, String name, AccumulatorParam<T> accumulatorParam) Deprecated. use AccumulatorV2. Since 2.0.0. Create an Accumulator variable of a given type, which tasks can "add" values to using the add method. Only the master can access the accumulator's value. This version supports naming the accumulator for display in Spark's web UI. Parameters:initialValue - (undocumented)name - (undocumented)accumulatorParam - (undocumented) Returns:(undocumented) accumulable public <T,R> Accumulable<T,R> accumulable(T initialValue, AccumulableParam<T,R> param) Deprecated. use AccumulatorV2. Since 2.0.0. Create an Accumulable shared variable of the given type, to which tasks can "add" values with add. Only the master can access the accumulable's value. Parameters:initialValue - (undocumented)param - (undocumented) Returns:(undocumented) accumulable public <T,R> Accumulable<T,R> accumulable(T initialValue, String name, AccumulableParam<T,R> param) Deprecated. use AccumulatorV2. Since 2.0.0. Create an Accumulable shared variable of the given type, to which tasks can "add" values with add. Only the master can access the accumulable's value. This version supports naming the accumulator for display in Spark's web UI. Parameters:initialValue - (undocumented)name - (undocumented)param - (undocumented) Returns:(undocumented) broadcast public <T> Broadcast<T> broadcast(T value) Broadcast a read-only variable to the cluster, returning a Broadcast object for reading it in distributed functions. The variable will be sent to each cluster only once. Parameters:value - (undocumented) Returns:(undocumented) stop public void stop() Shut down the SparkContext. close public void close() Specified by: close in interface java.io.Closeable Specified by: close in interface AutoCloseable getSparkHome public Optional<String> getSparkHome() Get Spark's home location from either a value set through the constructor, or the spark.home Java property, or the SPARK_HOME environment variable (in that order of preference). If neither of these is set, return None. Returns:(undocumented) addFile public void addFile(String path) Add a file to be downloaded with this Spark job on every node. The path passed can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, use SparkFiles.get(fileName) to find its download location. Parameters:path - (undocumented) addJar public void addJar(String path) Adds a JAR dependency for all tasks to be executed on this SparkContext in the future. The path passed can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), or an HTTP, HTTPS or FTP URI. Parameters:path - (undocumented) hadoopConfiguration public org.apache.hadoop.conf.Configuration hadoopConfiguration() Returns the Hadoop configuration used for the Hadoop code (e.g. file systems) we reuse. '''Note:''' As it will be reused in all Hadoop RDDs, it's better not to modify it unless you plan to set some global configurations for all Hadoop RDDs. Returns:(undocumented) setCheckpointDir public void setCheckpointDir(String dir) Set the directory under which RDDs are going to be checkpointed. The directory must be a HDFS path if running on a cluster. Parameters:dir - (undocumented) getCheckpointDir public Optional<String> getCheckpointDir() getConf public SparkConf getConf() Return a copy of this JavaSparkContext's configuration. The configuration ''cannot'' be changed at runtime. Returns:(undocumented) setCallSite public void setCallSite(String site) Pass-through to SparkContext.setCallSite. For API support only. Parameters:site - (undocumented) clearCallSite public void clearCallSite() Pass-through to SparkContext.setCallSite. For API support only. setLocalProperty public void setLocalProperty(String key, String value) Set a local property that affects jobs submitted from this thread, and all child threads, such as the Spark fair scheduler pool. These properties are inherited by child threads spawned from this thread. This may have unexpected consequences when working with thread pools. The standard java implementation of thread pools have worker threads spawn other worker threads. As a result, local properties may propagate unpredictably. Parameters:key - (undocumented)value - (undocumented) getLocalProperty public String getLocalProperty(String key) Get a local property set in this thread, or null if it is missing. See org.apache.spark.api.java.JavaSparkContext.setLocalProperty. Parameters:key - (undocumented) Returns:(undocumented) setLogLevel public void setLogLevel(String logLevel) Control our logLevel. This overrides any user-defined log settings. Parameters:logLevel - The desired log level as a string. Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN setJobGroup public void setJobGroup(String groupId, String description, boolean interruptOnCancel) Assigns a group ID to all the jobs started by this thread until the group ID is set to a different value or cleared. Often, a unit of execution in an application consists of multiple Spark actions or jobs. Application programmers can use this method to group all those jobs together and give a group description. Once set, the Spark web UI will associate such jobs with this group. The application can also use org.apache.spark.api.java.JavaSparkContext.cancelJobGroup to cancel all running jobs in this group. For example, // In the main thread: sc.setJobGroup("some_job_to_cancel", "some job description"); rdd.map(...).count(); // In a separate thread: sc.cancelJobGroup("some_job_to_cancel"); If interruptOnCancel is set to true for the job group, then job cancellation will result in Thread.interrupt() being called on the job's executor threads. This is useful to help ensure that the tasks are actually stopped in a timely manner, but is off by default due to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead. Parameters:groupId - (undocumented)description - (undocumented)interruptOnCancel - (undocumented) setJobGroup public void setJobGroup(String groupId, String description) Assigns a group ID to all the jobs started by this thread until the group ID is set to a different value or cleared. Parameters:groupId - (undocumented)description - (undocumented)See Also:setJobGroup(groupId: String, description: String, interruptThread: Boolean). This method sets interruptOnCancel to false. clearJobGroup public void clearJobGroup() Clear the current thread's job group ID and its description. cancelJobGroup public void cancelJobGroup(String groupId) Cancel active jobs for the specified group. See org.apache.spark.api.java.JavaSparkContext.setJobGroup for more information. Parameters:groupId - (undocumented) cancelAllJobs public void cancelAllJobs() Cancel all jobs that have been scheduled or are running. getPersistentRDDs public java.util.Map<Integer,JavaRDD<?>> getPersistentRDDs() Returns a Java map of JavaRDDs that have marked themselves as persistent via cache() call. Note that this does not necessarily mean the caching or computation was successful. Returns:(undocumented) union @SafeVarargs public final <T> JavaRDD<T> union(JavaRDD<T>... rdds) union public JavaDoubleRDD union(JavaDoubleRDD... rdds) union @SafeVarargs public final <K,V> JavaPairRDD<K,V> union(JavaPairRDD<K,V>... rdds) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaSparkStatusTracker (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaSparkStatusTracker (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class JavaSparkStatusTracker Object org.apache.spark.api.java.JavaSparkStatusTracker public class JavaSparkStatusTracker extends Object Low-level status reporting APIs for monitoring job and stage progress. These APIs intentionally provide very weak consistency semantics; consumers of these APIs should be prepared to handle empty / missing information. For example, a job's stage ids may be known but the status API may not have any information about the details of those stages, so getStageInfo could potentially return null for a valid stage id. To limit memory usage, these APIs only provide information on recent jobs / stages. These APIs will provide information for the last spark.ui.retainedStages stages and spark.ui.retainedJobs jobs. NOTE: this class's constructor should be considered private and may be subject to change. Method Summary Methods  Modifier and Type Method and Description int[] getActiveJobIds() Returns an array containing the ids of all active jobs. int[] getActiveStageIds() Returns an array containing the ids of all active stages. int[] getJobIdsForGroup(String jobGroup) Return a list of all known jobs in a particular job group. SparkJobInfo getJobInfo(int jobId) Returns job information, or null if the job info could not be found or was garbage collected. SparkStageInfo getStageInfo(int stageId) Returns stage information, or null if the stage info could not be found or was garbage collected. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail getJobIdsForGroup public int[] getJobIdsForGroup(String jobGroup) Return a list of all known jobs in a particular job group. If jobGroup is null, then returns all known jobs that are not associated with a job group. The returned list may contain running, failed, and completed jobs, and may vary across invocations of this method. This method does not guarantee the order of the elements in its result. Parameters:jobGroup - (undocumented) Returns:(undocumented) getActiveStageIds public int[] getActiveStageIds() Returns an array containing the ids of all active stages. This method does not guarantee the order of the elements in its result. Returns:(undocumented) getActiveJobIds public int[] getActiveJobIds() Returns an array containing the ids of all active jobs. This method does not guarantee the order of the elements in its result. Returns:(undocumented) getJobInfo public SparkJobInfo getJobInfo(int jobId) Returns job information, or null if the job info could not be found or was garbage collected. Parameters:jobId - (undocumented) Returns:(undocumented) getStageInfo public SparkStageInfo getStageInfo(int stageId) Returns stage information, or null if the stage info could not be found or was garbage collected. Parameters:stageId - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaStreamingContext (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaStreamingContext (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.api.java Class JavaStreamingContext Object org.apache.spark.streaming.api.java.JavaStreamingContext All Implemented Interfaces: java.io.Closeable, AutoCloseable public class JavaStreamingContext extends Object implements java.io.Closeable A Java-friendly version of StreamingContext which is the main entry point for Spark Streaming functionality. It provides methods to create JavaDStream and JavaPairDStream$ from input sources. The internal org.apache.spark.api.java.JavaSparkContext (see core Spark documentation) can be accessed using context.sparkContext. After creating and transforming DStreams, the streaming computation can be started and stopped using context.start() and context.stop(), respectively. context.awaitTermination() allows the current thread to wait for the termination of a context by stop() or by an exception. Constructor Summary Constructors  Constructor and Description JavaStreamingContext(JavaSparkContext sparkContext, Duration batchDuration) Create a JavaStreamingContext using an existing JavaSparkContext. JavaStreamingContext(SparkConf conf, Duration batchDuration) Create a JavaStreamingContext using a SparkConf configuration. JavaStreamingContext(StreamingContext ssc)  JavaStreamingContext(String path) Recreate a JavaStreamingContext from a checkpoint file. JavaStreamingContext(String path, org.apache.hadoop.conf.Configuration hadoopConf) Re-creates a JavaStreamingContext from a checkpoint file. JavaStreamingContext(String master, String appName, Duration batchDuration) Create a StreamingContext. JavaStreamingContext(String master, String appName, Duration batchDuration, String sparkHome, String jarFile) Create a StreamingContext. JavaStreamingContext(String master, String appName, Duration batchDuration, String sparkHome, String[] jars) Create a StreamingContext. JavaStreamingContext(String master, String appName, Duration batchDuration, String sparkHome, String[] jars, java.util.Map<String,String> environment) Create a StreamingContext. Method Summary Methods  Modifier and Type Method and Description void addStreamingListener(StreamingListener streamingListener) Add a StreamingListener object for receiving system events related to streaming. void awaitTermination() Wait for the execution to stop. boolean awaitTerminationOrTimeout(long timeout) Wait for the execution to stop. JavaDStream<byte[]> binaryRecordsStream(String directory, int recordLength) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them as flat binary files with fixed record lengths, yielding byte arrays void checkpoint(String directory) Sets the context to periodically checkpoint the DStream operations for master fault-tolerance. void close()  <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairInputDStream<K,V> fileStream(String directory, Class<K> kClass, Class<V> vClass, Class<F> fClass) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them using the given key-value types and input format. <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairInputDStream<K,V> fileStream(String directory, Class<K> kClass, Class<V> vClass, Class<F> fClass, Function<org.apache.hadoop.fs.Path,Boolean> filter, boolean newFilesOnly) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them using the given key-value types and input format. <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairInputDStream<K,V> fileStream(String directory, Class<K> kClass, Class<V> vClass, Class<F> fClass, Function<org.apache.hadoop.fs.Path,Boolean> filter, boolean newFilesOnly, org.apache.hadoop.conf.Configuration conf) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them using the given key-value types and input format. static JavaStreamingContext getOrCreate(String checkpointPath, Function0<JavaStreamingContext> creatingFunc) Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. static JavaStreamingContext getOrCreate(String checkpointPath, Function0<JavaStreamingContext> creatingFunc, org.apache.hadoop.conf.Configuration hadoopConf) Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. static JavaStreamingContext getOrCreate(String checkpointPath, Function0<JavaStreamingContext> creatingFunc, org.apache.hadoop.conf.Configuration hadoopConf, boolean createOnError) Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. StreamingContextState getState() :: DeveloperApi :: static String[] jarOfClass(Class<?> cls) Find the JAR from which a given class was loaded, to make it easy for users to pass their JARs to StreamingContext. <T> JavaDStream<T> queueStream(java.util.Queue<JavaRDD<T>> queue) Create an input stream from a queue of RDDs. <T> JavaInputDStream<T> queueStream(java.util.Queue<JavaRDD<T>> queue, boolean oneAtATime) Create an input stream from a queue of RDDs. <T> JavaInputDStream<T> queueStream(java.util.Queue<JavaRDD<T>> queue, boolean oneAtATime, JavaRDD<T> defaultRDD) Create an input stream from a queue of RDDs. <T> JavaReceiverInputDStream<T> rawSocketStream(String hostname, int port) Create an input stream from network source hostname:port, where data is received as serialized blocks (serialized using the Spark's serializer) that can be directly pushed into the block manager without deserializing them. <T> JavaReceiverInputDStream<T> rawSocketStream(String hostname, int port, StorageLevel storageLevel) Create an input stream from network source hostname:port, where data is received as serialized blocks (serialized using the Spark's serializer) that can be directly pushed into the block manager without deserializing them. <T> JavaReceiverInputDStream<T> receiverStream(Receiver<T> receiver) Create an input stream with any arbitrary user implemented receiver. void remember(Duration duration) Sets each DStreams in this context to remember RDDs it generated in the last given duration. <T> JavaReceiverInputDStream<T> socketStream(String hostname, int port, Function<java.io.InputStream,Iterable<T>> converter, StorageLevel storageLevel) Create an input stream from network source hostname:port. JavaReceiverInputDStream<String> socketTextStream(String hostname, int port) Create an input stream from network source hostname:port. JavaReceiverInputDStream<String> socketTextStream(String hostname, int port, StorageLevel storageLevel) Create an input stream from network source hostname:port. JavaSparkContext sparkContext() The underlying SparkContext StreamingContext ssc()  void start() Start the execution of the streams. void stop() Stop the execution of the streams. void stop(boolean stopSparkContext) Stop the execution of the streams. void stop(boolean stopSparkContext, boolean stopGracefully) Stop the execution of the streams. JavaDStream<String> textFileStream(String directory) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them as text files (using key as LongWritable, value as Text and input format as TextInputFormat). <T> JavaDStream<T> transform(java.util.List<JavaDStream<?>> dstreams, Function2<java.util.List<JavaRDD<?>>,Time,JavaRDD<T>> transformFunc) Create a new DStream in which each RDD is generated by applying a function on RDDs of the DStreams. <K,V> JavaPairDStream<K,V> transformToPair(java.util.List<JavaDStream<?>> dstreams, Function2<java.util.List<JavaRDD<?>>,Time,JavaPairRDD<K,V>> transformFunc) Create a new DStream in which each RDD is generated by applying a function on RDDs of the DStreams. <T> JavaDStream<T> union(JavaDStream<T> first, java.util.List<JavaDStream<T>> rest) Create a unified DStream from multiple DStreams of the same type and same slide duration. <K,V> JavaPairDStream<K,V> union(JavaPairDStream<K,V> first, java.util.List<JavaPairDStream<K,V>> rest) Create a unified DStream from multiple DStreams of the same type and same slide duration. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JavaStreamingContext public JavaStreamingContext(StreamingContext ssc) JavaStreamingContext public JavaStreamingContext(String master, String appName, Duration batchDuration) Create a StreamingContext. Parameters:master - Name of the Spark MasterappName - Name to be used when registering with the schedulerbatchDuration - The time interval at which streaming data will be divided into batches JavaStreamingContext public JavaStreamingContext(String master, String appName, Duration batchDuration, String sparkHome, String jarFile) Create a StreamingContext. Parameters:master - Name of the Spark MasterappName - Name to be used when registering with the schedulerbatchDuration - The time interval at which streaming data will be divided into batchessparkHome - The SPARK_HOME directory on the slave nodesjarFile - JAR file containing job code, to ship to cluster. This can be a path on the local file system or an HDFS, HTTP, HTTPS, or FTP URL. JavaStreamingContext public JavaStreamingContext(String master, String appName, Duration batchDuration, String sparkHome, String[] jars) Create a StreamingContext. Parameters:master - Name of the Spark MasterappName - Name to be used when registering with the schedulerbatchDuration - The time interval at which streaming data will be divided into batchessparkHome - The SPARK_HOME directory on the slave nodesjars - Collection of JARs to send to the cluster. These can be paths on the local file system or HDFS, HTTP, HTTPS, or FTP URLs. JavaStreamingContext public JavaStreamingContext(String master, String appName, Duration batchDuration, String sparkHome, String[] jars, java.util.Map<String,String> environment) Create a StreamingContext. Parameters:master - Name of the Spark MasterappName - Name to be used when registering with the schedulerbatchDuration - The time interval at which streaming data will be divided into batchessparkHome - The SPARK_HOME directory on the slave nodesjars - Collection of JARs to send to the cluster. These can be paths on the local file system or HDFS, HTTP, HTTPS, or FTP URLs.environment - Environment variables to set on worker nodes JavaStreamingContext public JavaStreamingContext(JavaSparkContext sparkContext, Duration batchDuration) Create a JavaStreamingContext using an existing JavaSparkContext. Parameters:sparkContext - The underlying JavaSparkContext to usebatchDuration - The time interval at which streaming data will be divided into batches JavaStreamingContext public JavaStreamingContext(SparkConf conf, Duration batchDuration) Create a JavaStreamingContext using a SparkConf configuration. Parameters:conf - A Spark application configurationbatchDuration - The time interval at which streaming data will be divided into batches JavaStreamingContext public JavaStreamingContext(String path) Recreate a JavaStreamingContext from a checkpoint file. Parameters:path - Path to the directory that was specified as the checkpoint directory JavaStreamingContext public JavaStreamingContext(String path, org.apache.hadoop.conf.Configuration hadoopConf) Re-creates a JavaStreamingContext from a checkpoint file. Parameters:path - Path to the directory that was specified as the checkpoint directory hadoopConf - (undocumented) Method Detail getOrCreate public static JavaStreamingContext getOrCreate(String checkpointPath, Function0<JavaStreamingContext> creatingFunc) Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. If checkpoint data exists in the provided checkpointPath, then StreamingContext will be recreated from the checkpoint data. If the data does not exist, then the provided factory will be used to create a JavaStreamingContext. Parameters:checkpointPath - Checkpoint directory used in an earlier JavaStreamingContext programcreatingFunc - Function to create a new JavaStreamingContext Returns:(undocumented) getOrCreate public static JavaStreamingContext getOrCreate(String checkpointPath, Function0<JavaStreamingContext> creatingFunc, org.apache.hadoop.conf.Configuration hadoopConf) Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. If checkpoint data exists in the provided checkpointPath, then StreamingContext will be recreated from the checkpoint data. If the data does not exist, then the provided factory will be used to create a JavaStreamingContext. Parameters:checkpointPath - Checkpoint directory used in an earlier StreamingContext programcreatingFunc - Function to create a new JavaStreamingContexthadoopConf - Hadoop configuration if necessary for reading from any HDFS compatible file system Returns:(undocumented) getOrCreate public static JavaStreamingContext getOrCreate(String checkpointPath, Function0<JavaStreamingContext> creatingFunc, org.apache.hadoop.conf.Configuration hadoopConf, boolean createOnError) Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. If checkpoint data exists in the provided checkpointPath, then StreamingContext will be recreated from the checkpoint data. If the data does not exist, then the provided factory will be used to create a JavaStreamingContext. Parameters:checkpointPath - Checkpoint directory used in an earlier StreamingContext programcreatingFunc - Function to create a new JavaStreamingContexthadoopConf - Hadoop configuration if necessary for reading from any HDFS compatible file systemcreateOnError - Whether to create a new JavaStreamingContext if there is an error in reading checkpoint data. Returns:(undocumented) jarOfClass public static String[] jarOfClass(Class<?> cls) Find the JAR from which a given class was loaded, to make it easy for users to pass their JARs to StreamingContext. Parameters:cls - (undocumented) Returns:(undocumented) ssc public StreamingContext ssc() sparkContext public JavaSparkContext sparkContext() The underlying SparkContext socketTextStream public JavaReceiverInputDStream<String> socketTextStream(String hostname, int port, StorageLevel storageLevel) Create an input stream from network source hostname:port. Data is received using a TCP socket and the receive bytes is interpreted as UTF8 encoded \n delimited lines. Parameters:hostname - Hostname to connect to for receiving dataport - Port to connect to for receiving datastorageLevel - Storage level to use for storing the received objects Returns:(undocumented) socketTextStream public JavaReceiverInputDStream<String> socketTextStream(String hostname, int port) Create an input stream from network source hostname:port. Data is received using a TCP socket and the receive bytes is interpreted as UTF8 encoded \n delimited lines. Storage level of the data will be the default StorageLevel.MEMORY_AND_DISK_SER_2. Parameters:hostname - Hostname to connect to for receiving dataport - Port to connect to for receiving data Returns:(undocumented) socketStream public <T> JavaReceiverInputDStream<T> socketStream(String hostname, int port, Function<java.io.InputStream,Iterable<T>> converter, StorageLevel storageLevel) Create an input stream from network source hostname:port. Data is received using a TCP socket and the receive bytes it interpreted as object using the given converter. Parameters:hostname - Hostname to connect to for receiving dataport - Port to connect to for receiving dataconverter - Function to convert the byte stream to objectsstorageLevel - Storage level to use for storing the received objects Returns:(undocumented) textFileStream public JavaDStream<String> textFileStream(String directory) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them as text files (using key as LongWritable, value as Text and input format as TextInputFormat). Files must be written to the monitored directory by "moving" them from another location within the same file system. File names starting with . are ignored. Parameters:directory - HDFS directory to monitor for new file Returns:(undocumented) binaryRecordsStream public JavaDStream<byte[]> binaryRecordsStream(String directory, int recordLength) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them as flat binary files with fixed record lengths, yielding byte arrays '''Note:''' We ensure that the byte array for each record in the resulting RDDs of the DStream has the provided record length. Parameters:directory - HDFS directory to monitor for new filesrecordLength - The length at which to split the records Returns:(undocumented) rawSocketStream public <T> JavaReceiverInputDStream<T> rawSocketStream(String hostname, int port, StorageLevel storageLevel) Create an input stream from network source hostname:port, where data is received as serialized blocks (serialized using the Spark's serializer) that can be directly pushed into the block manager without deserializing them. This is the most efficient way to receive data. Parameters:hostname - Hostname to connect to for receiving dataport - Port to connect to for receiving datastorageLevel - Storage level to use for storing the received objects Returns:(undocumented) rawSocketStream public <T> JavaReceiverInputDStream<T> rawSocketStream(String hostname, int port) Create an input stream from network source hostname:port, where data is received as serialized blocks (serialized using the Spark's serializer) that can be directly pushed into the block manager without deserializing them. This is the most efficient way to receive data. Parameters:hostname - Hostname to connect to for receiving dataport - Port to connect to for receiving data Returns:(undocumented) fileStream public <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairInputDStream<K,V> fileStream(String directory, Class<K> kClass, Class<V> vClass, Class<F> fClass) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them using the given key-value types and input format. Files must be written to the monitored directory by "moving" them from another location within the same file system. File names starting with . are ignored. Parameters:directory - HDFS directory to monitor for new filekClass - class of key for reading HDFS filevClass - class of value for reading HDFS filefClass - class of input format for reading HDFS file Returns:(undocumented) fileStream public <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairInputDStream<K,V> fileStream(String directory, Class<K> kClass, Class<V> vClass, Class<F> fClass, Function<org.apache.hadoop.fs.Path,Boolean> filter, boolean newFilesOnly) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them using the given key-value types and input format. Files must be written to the monitored directory by "moving" them from another location within the same file system. File names starting with . are ignored. Parameters:directory - HDFS directory to monitor for new filekClass - class of key for reading HDFS filevClass - class of value for reading HDFS filefClass - class of input format for reading HDFS filefilter - Function to filter paths to processnewFilesOnly - Should process only new files and ignore existing files in the directory Returns:(undocumented) fileStream public <K,V,F extends org.apache.hadoop.mapreduce.InputFormat<K,V>> JavaPairInputDStream<K,V> fileStream(String directory, Class<K> kClass, Class<V> vClass, Class<F> fClass, Function<org.apache.hadoop.fs.Path,Boolean> filter, boolean newFilesOnly, org.apache.hadoop.conf.Configuration conf) Create an input stream that monitors a Hadoop-compatible filesystem for new files and reads them using the given key-value types and input format. Files must be written to the monitored directory by "moving" them from another location within the same file system. File names starting with . are ignored. Parameters:directory - HDFS directory to monitor for new filekClass - class of key for reading HDFS filevClass - class of value for reading HDFS filefClass - class of input format for reading HDFS filefilter - Function to filter paths to processnewFilesOnly - Should process only new files and ignore existing files in the directoryconf - Hadoop configuration Returns:(undocumented) queueStream public <T> JavaDStream<T> queueStream(java.util.Queue<JavaRDD<T>> queue) Create an input stream from a queue of RDDs. In each batch, it will process either one or all of the RDDs returned by the queue. NOTE: 1. Changes to the queue after the stream is created will not be recognized. 2. Arbitrary RDDs can be added to queueStream, there is no way to recover data of those RDDs, so queueStream doesn't support checkpointing. Parameters:queue - Queue of RDDs Returns:(undocumented) queueStream public <T> JavaInputDStream<T> queueStream(java.util.Queue<JavaRDD<T>> queue, boolean oneAtATime) Create an input stream from a queue of RDDs. In each batch, it will process either one or all of the RDDs returned by the queue. NOTE: 1. Changes to the queue after the stream is created will not be recognized. 2. Arbitrary RDDs can be added to queueStream, there is no way to recover data of those RDDs, so queueStream doesn't support checkpointing. Parameters:queue - Queue of RDDsoneAtATime - Whether only one RDD should be consumed from the queue in every interval Returns:(undocumented) queueStream public <T> JavaInputDStream<T> queueStream(java.util.Queue<JavaRDD<T>> queue, boolean oneAtATime, JavaRDD<T> defaultRDD) Create an input stream from a queue of RDDs. In each batch, it will process either one or all of the RDDs returned by the queue. NOTE: 1. Changes to the queue after the stream is created will not be recognized. 2. Arbitrary RDDs can be added to queueStream, there is no way to recover data of those RDDs, so queueStream doesn't support checkpointing. Parameters:queue - Queue of RDDsoneAtATime - Whether only one RDD should be consumed from the queue in every intervaldefaultRDD - Default RDD is returned by the DStream when the queue is empty Returns:(undocumented) receiverStream public <T> JavaReceiverInputDStream<T> receiverStream(Receiver<T> receiver) Create an input stream with any arbitrary user implemented receiver. Find more details at: http://spark.apache.org/docs/latest/streaming-custom-receivers.html Parameters:receiver - Custom implementation of Receiver Returns:(undocumented) union public <T> JavaDStream<T> union(JavaDStream<T> first, java.util.List<JavaDStream<T>> rest) Create a unified DStream from multiple DStreams of the same type and same slide duration. Parameters:first - (undocumented)rest - (undocumented) Returns:(undocumented) union public <K,V> JavaPairDStream<K,V> union(JavaPairDStream<K,V> first, java.util.List<JavaPairDStream<K,V>> rest) Create a unified DStream from multiple DStreams of the same type and same slide duration. Parameters:first - (undocumented)rest - (undocumented) Returns:(undocumented) transform public <T> JavaDStream<T> transform(java.util.List<JavaDStream<?>> dstreams, Function2<java.util.List<JavaRDD<?>>,Time,JavaRDD<T>> transformFunc) Create a new DStream in which each RDD is generated by applying a function on RDDs of the DStreams. The order of the JavaRDDs in the transform function parameter will be the same as the order of corresponding DStreams in the list. Note that for adding a JavaPairDStream in the list of JavaDStreams, convert it to a JavaDStream using JavaPairDStream.toJavaDStream(). In the transform function, convert the JavaRDD corresponding to that JavaDStream to a JavaPairRDD using org.apache.spark.api.java.JavaPairRDD.fromJavaRDD(). Parameters:dstreams - (undocumented)transformFunc - (undocumented) Returns:(undocumented) transformToPair public <K,V> JavaPairDStream<K,V> transformToPair(java.util.List<JavaDStream<?>> dstreams, Function2<java.util.List<JavaRDD<?>>,Time,JavaPairRDD<K,V>> transformFunc) Create a new DStream in which each RDD is generated by applying a function on RDDs of the DStreams. The order of the JavaRDDs in the transform function parameter will be the same as the order of corresponding DStreams in the list. Note that for adding a JavaPairDStream in the list of JavaDStreams, convert it to a JavaDStream using JavaPairDStream.toJavaDStream(). In the transform function, convert the JavaRDD corresponding to that JavaDStream to a JavaPairRDD using org.apache.spark.api.java.JavaPairRDD.fromJavaRDD(). Parameters:dstreams - (undocumented)transformFunc - (undocumented) Returns:(undocumented) checkpoint public void checkpoint(String directory) Sets the context to periodically checkpoint the DStream operations for master fault-tolerance. The graph will be checkpointed every batch interval. Parameters:directory - HDFS-compatible directory where the checkpoint data will be reliably stored remember public void remember(Duration duration) Sets each DStreams in this context to remember RDDs it generated in the last given duration. DStreams remember RDDs only for a limited duration of duration and releases them for garbage collection. This method allows the developer to specify how long to remember the RDDs ( if the developer wishes to query old data outside the DStream computation). Parameters:duration - Minimum duration that each DStream should remember its RDDs addStreamingListener public void addStreamingListener(StreamingListener streamingListener) Add a StreamingListener object for receiving system events related to streaming. Parameters:streamingListener - (undocumented) getState public StreamingContextState getState() :: DeveloperApi :: Return the current state of the context. The context can be in three possible states - StreamingContextState.INITIALIZED - The context has been created, but not been started yet. Input DStreams, transformations and output operations can be created on the context. StreamingContextState.ACTIVE - The context has been started, and been not stopped. Input DStreams, transformations and output operations cannot be created on the context. StreamingContextState.STOPPED - The context has been stopped and cannot be used any more. Returns:(undocumented) start public void start() Start the execution of the streams. awaitTermination public void awaitTermination() throws InterruptedException Wait for the execution to stop. Any exceptions that occurs during the execution will be thrown in this thread. Throws: InterruptedException awaitTerminationOrTimeout public boolean awaitTerminationOrTimeout(long timeout) throws InterruptedException Wait for the execution to stop. Any exceptions that occurs during the execution will be thrown in this thread. Parameters:timeout - time to wait in milliseconds Returns:true if it's stopped; or throw the reported error during the execution; or false if the waiting time elapsed before returning from the method. Throws: InterruptedException stop public void stop() Stop the execution of the streams. Will stop the associated JavaSparkContext as well. stop public void stop(boolean stopSparkContext) Stop the execution of the streams. Parameters:stopSparkContext - Stop the associated SparkContext or not stop public void stop(boolean stopSparkContext, boolean stopGracefully) Stop the execution of the streams. Parameters:stopSparkContext - Stop the associated SparkContext or notstopGracefully - Stop gracefully by waiting for the processing of all received data to be completed close public void close() Specified by: close in interface java.io.Closeable Specified by: close in interface AutoCloseable Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaUtils.SerializableMapWrapper (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaUtils.SerializableMapWrapper (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class JavaUtils.SerializableMapWrapper<A,B> Object java.util.AbstractMap<A,B> org.apache.spark.api.java.JavaUtils.SerializableMapWrapper<A,B> All Implemented Interfaces: java.io.Serializable, java.util.Map<A,B> Enclosing class: JavaUtils public static class JavaUtils.SerializableMapWrapper<A,B> extends java.util.AbstractMap<A,B> implements java.io.Serializable See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from class java.util.AbstractMap java.util.AbstractMap.SimpleEntry<K,V>, java.util.AbstractMap.SimpleImmutableEntry<K,V> Nested classes/interfaces inherited from interface java.util.Map java.util.Map.Entry<K,V> Constructor Summary Constructors  Constructor and Description JavaUtils.SerializableMapWrapper(scala.collection.Map<A,B> underlying)  Method Summary Methods  Modifier and Type Method and Description java.util.Set<java.util.Map.Entry<A,B>> entrySet()  B get(Object key)  int size()  Methods inherited from class java.util.AbstractMap clear, containsKey, containsValue, equals, hashCode, isEmpty, keySet, put, putAll, remove, toString, values Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail JavaUtils.SerializableMapWrapper public JavaUtils.SerializableMapWrapper(scala.collection.Map<A,B> underlying) Method Detail size public int size() Specified by: size in interface java.util.Map<A,B> Overrides: size in class java.util.AbstractMap<A,B> get public B get(Object key) Specified by: get in interface java.util.Map<A,B> Overrides: get in class java.util.AbstractMap<A,B> entrySet public java.util.Set<java.util.Map.Entry<A,B>> entrySet() Specified by: entrySet in interface java.util.Map<A,B> Specified by: entrySet in class java.util.AbstractMap<A,B> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JavaUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JavaUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class JavaUtils Object org.apache.spark.api.java.JavaUtils public class JavaUtils extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  JavaUtils.SerializableMapWrapper<A,B>  Constructor Summary Constructors  Constructor and Description JavaUtils()  Method Summary Methods  Modifier and Type Method and Description static <A,B> JavaUtils.SerializableMapWrapper<A,B> mapAsSerializableJavaMap(scala.collection.Map<A,B> underlying)  static <T> Optional<T> optionToOptional(scala.Option<T> option)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JavaUtils public JavaUtils() Method Detail optionToOptional public static <T> Optional<T> optionToOptional(scala.Option<T> option) mapAsSerializableJavaMap public static <A,B> JavaUtils.SerializableMapWrapper<A,B> mapAsSerializableJavaMap(scala.collection.Map<A,B> underlying) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JdbcDialect (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JdbcDialect (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class JdbcDialect Object org.apache.spark.sql.jdbc.JdbcDialect All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: AggregatedDialect public abstract class JdbcDialect extends Object implements scala.Serializable :: DeveloperApi :: Encapsulates everything (extensions, workarounds, quirks) to handle the SQL dialect of a certain database or jdbc driver. Lots of databases define types that aren't explicitly supported by the JDBC spec. Some JDBC drivers also report inaccurate information---for instance, BIT(n>1) being reported as a BIT type is quite common, even though BIT in JDBC is meant for single-bit values. Also, there does not appear to be a standard name for an unbounded string or binary type; we use BLOB and CLOB by default but override with database-specific alternatives when these are absent or do not behave correctly. Currently, the only thing done by the dialect is type mapping. getCatalystType is used when reading from a JDBC table and getJDBCType is used when writing to a JDBC table. If getCatalystType returns null, the default type handling is used for the given JDBC type. Similarly, if getJDBCType returns (null, None), the default type handling is used for the given Catalyst type. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JdbcDialect()  Method Summary Methods  Modifier and Type Method and Description void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties) Override connection specific properties to run before a select is made. abstract boolean canHandle(String url) Check if this dialect instance can handle a certain jdbc url. scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) Get the custom datatype mapping for the given jdbc meta information. scala.Option<JdbcType> getJDBCType(DataType dt) Retrieve the jdbc / sql type for a given datatype. String getTableExistsQuery(String table) Get the SQL query that should be used to find if the given table exists. String quoteIdentifier(String colName) Quotes the identifier. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JdbcDialect public JdbcDialect() Method Detail canHandle public abstract boolean canHandle(String url) Check if this dialect instance can handle a certain jdbc url. Parameters:url - the jdbc url. Returns:True if the dialect can be applied on the given jdbc url. Throws: NullPointerException - if the url is null. getCatalystType public scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) Get the custom datatype mapping for the given jdbc meta information. Parameters:sqlType - The sql type (see java.sql.Types)typeName - The sql type name (e.g. "BIGINT UNSIGNED")size - The size of the type.md - Result metadata associated with this type. Returns:The actual DataType (subclasses of DataType) or null if the default type mapping should be used. getJDBCType public scala.Option<JdbcType> getJDBCType(DataType dt) Retrieve the jdbc / sql type for a given datatype. Parameters:dt - The datatype (e.g. StringType) Returns:The new JdbcType if there is an override for this DataType quoteIdentifier public String quoteIdentifier(String colName) Quotes the identifier. This is used to put quotes around the identifier in case the column name is a reserved keyword, or in case it contains characters that require quotes (e.g. space). Parameters:colName - (undocumented) Returns:(undocumented) getTableExistsQuery public String getTableExistsQuery(String table) Get the SQL query that should be used to find if the given table exists. Dialects can override this method to return a query that works best in a particular database. Parameters:table - The name of the table. Returns:The SQL query to use for checking the table. beforeFetch public void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties) Override connection specific properties to run before a select is made. This is in place to allow dialects that need special treatment to optimize behavior. Parameters:connection - The connection objectproperties - The connection properties. This is passed through from the relation. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JdbcDialects (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JdbcDialects (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class JdbcDialects Object org.apache.spark.sql.jdbc.JdbcDialects public class JdbcDialects extends Object :: DeveloperApi :: Registry of dialects that apply to every new jdbc org.apache.spark.sql.DataFrame. If multiple matching dialects are registered then all matching ones will be tried in reverse order. A user-added dialect will thus be applied first, overwriting the defaults. Note that all new dialects are applied to new jdbc DataFrames only. Make sure to register your dialects first. Constructor Summary Constructors  Constructor and Description JdbcDialects()  Method Summary Methods  Modifier and Type Method and Description static void registerDialect(JdbcDialect dialect) Register a dialect for use on all new matching jdbc org.apache.spark.sql.DataFrame. static void unregisterDialect(JdbcDialect dialect) Unregister a dialect. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JdbcDialects public JdbcDialects() Method Detail registerDialect public static void registerDialect(JdbcDialect dialect) Register a dialect for use on all new matching jdbc org.apache.spark.sql.DataFrame. Reading an existing dialect will cause a move-to-front. Parameters:dialect - The new dialect. unregisterDialect public static void unregisterDialect(JdbcDialect dialect) Unregister a dialect. Does nothing if the dialect is not registered. Parameters:dialect - The jdbc dialect. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JdbcRDD.ConnectionFactory (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JdbcRDD.ConnectionFactory (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Interface JdbcRDD.ConnectionFactory All Superinterfaces: java.io.Serializable Enclosing class: JdbcRDD<T> public static interface JdbcRDD.ConnectionFactory extends scala.Serializable Method Summary Methods  Modifier and Type Method and Description java.sql.Connection getConnection()  Method Detail getConnection java.sql.Connection getConnection() throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JdbcRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JdbcRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class JdbcRDD<T> Object org.apache.spark.rdd.RDD<T> org.apache.spark.rdd.JdbcRDD<T> All Implemented Interfaces: java.io.Serializable public class JdbcRDD<T> extends RDD<T> An RDD that executes a SQL query on a JDBC connection and reads results. For usage example, see test case JdbcRDDSuite. param: getConnection a function that returns an open Connection. The RDD takes care of closing the connection. param: sql the text of the query. The query must contain two ? placeholders for parameters used to partition the results. E.g. "select title, author from books where ? <= id and id <= ?" param: lowerBound the minimum value of the first placeholder param: upperBound the maximum value of the second placeholder The lower and upper bounds are inclusive. param: numPartitions the number of partitions. Given a lowerBound of 1, an upperBound of 20, and a numPartitions of 2, the query would be executed twice, once with (1, 10) and once with (11, 20) param: mapRow a function from a ResultSet to a single row of the desired result type(s). This should only call getInt, getString, etc; the RDD takes care of calling next. The default maps a ResultSet to an array of Object. See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static interface  JdbcRDD.ConnectionFactory  Constructor Summary Constructors  Constructor and Description JdbcRDD(SparkContext sc, scala.Function0<java.sql.Connection> getConnection, String sql, long lowerBound, long upperBound, int numPartitions, scala.Function1<java.sql.ResultSet,T> mapRow, scala.reflect.ClassTag<T> evidence$1)  Method Summary Methods  Modifier and Type Method and Description static RDD<T> $plus$plus(RDD<T> other)  static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29)  static RDD<T> cache()  static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5)  static void checkpoint()  static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord)  static boolean coalesce$default$2()  static scala.Option<PartitionCoalescer> coalesce$default$3()  static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer)  static Object collect()  static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28)  scala.collection.Iterator<T> compute(Partition thePart, TaskContext context) :: DeveloperApi :: Implemented by subclasses to compute a given partition. static SparkContext context()  static long count()  static PartialResult<BoundedDouble> countApprox(long timeout, double confidence)  static double countApprox$default$2()  static long countApproxDistinct(double relativeSD)  static long countApproxDistinct(int p, int sp)  static double countApproxDistinct$default$1()  static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord)  static scala.math.Ordering<T> countByValue$default$1()  static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord)  static double countByValueApprox$default$2()  static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence)  static JavaRDD<Object[]> create(JavaSparkContext sc, JdbcRDD.ConnectionFactory connectionFactory, String sql, long lowerBound, long upperBound, int numPartitions) Create an RDD that executes a SQL query on a JDBC connection and reads results. static <T> JavaRDD<T> create(JavaSparkContext sc, JdbcRDD.ConnectionFactory connectionFactory, String sql, long lowerBound, long upperBound, int numPartitions, Function<java.sql.ResultSet,T> mapRow) Create an RDD that executes a SQL query on a JDBC connection and reads results. static scala.collection.Seq<Dependency<?>> dependencies()  static RDD<T> distinct()  static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> distinct$default$2(int numPartitions)  static RDD<T> filter(scala.Function1<T,Object> f)  static T first()  static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4)  static T fold(T zeroValue, scala.Function2<T,T,T> op)  static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f)  static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f)  static scala.Option<String> getCheckpointFile()  static int getNumPartitions()  Partition[] getPartitions() Implemented by subclasses to return the set of partitions in this RDD. static StorageLevel getStorageLevel()  static RDD<Object> glom()  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord)  static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p)  static int id()  static RDD<T> intersection(RDD<T> other)  static RDD<T> intersection(RDD<T> other, int numPartitions)  static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner)  static boolean isCheckpointed()  static boolean isEmpty()  static scala.collection.Iterator<T> iterator(Partition split, TaskContext context)  static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f)  static RDD<T> localCheckpoint()  static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3)  static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6)  static <U> boolean mapPartitions$default$2()  static <U> boolean mapPartitionsInternal$default$2()  static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8)  static <U> boolean mapPartitionsWithIndex$default$2()  static T max(scala.math.Ordering<T> ord)  static T min(scala.math.Ordering<T> ord)  static void name_$eq(String x$1)  static String name()  static scala.Option<Partitioner> partitioner()  static Partition[] partitions()  static RDD<T> persist()  static RDD<T> persist(StorageLevel newLevel)  static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding)  static RDD<String> pipe(String command)  static RDD<String> pipe(String command, scala.collection.Map<String,String> env)  static scala.collection.Map<String,String> pipe$default$2()  static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3()  static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4()  static boolean pipe$default$5()  static int pipe$default$6()  static String pipe$default$7()  static scala.collection.Seq<String> preferredLocations(Partition split)  static RDD<T>[] randomSplit(double[] weights, long seed)  static long randomSplit$default$2()  static T reduce(scala.Function2<T,T,T> f)  static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> repartition$default$2(int numPartitions)  static Object[] resultSetToObjectArray(java.sql.ResultSet rs)  static RDD<T> sample(boolean withReplacement, double fraction, long seed)  static long sample$default$3()  static void saveAsObjectFile(String path)  static void saveAsTextFile(String path)  static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec)  static RDD<T> setName(String _name)  static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag)  static <K> boolean sortBy$default$2()  static <K> int sortBy$default$3()  static SparkContext sparkContext()  static RDD<T> subtract(RDD<T> other)  static RDD<T> subtract(RDD<T> other, int numPartitions)  static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p)  static Object take(int num)  static Object takeOrdered(int num, scala.math.Ordering<T> ord)  static Object takeSample(boolean withReplacement, int num, long seed)  static long takeSample$default$3()  static String toDebugString()  static JavaRDD<T> toJavaRDD()  static scala.collection.Iterator<T> toLocalIterator()  static Object top(int num, scala.math.Ordering<T> ord)  static String toString()  static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30)  static <U> int treeAggregate$default$4(U zeroValue)  static T treeReduce(scala.Function2<T,T,T> f, int depth)  static int treeReduce$default$2()  static RDD<T> union(RDD<T> other)  static RDD<T> unpersist(boolean blocking)  static boolean unpersist$default$1()  static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27)  static RDD<scala.Tuple2<T,Object>> zipWithIndex()  static RDD<scala.Tuple2<T,Object>> zipWithUniqueId()  Methods inherited from class org.apache.spark.rdd.RDD aggregate, cache, cartesian, checkpoint, coalesce, collect, collect, context, count, countApprox, countApproxDistinct, countApproxDistinct, countByValue, countByValueApprox, dependencies, distinct, distinct, doubleRDDToDoubleRDDFunctions, filter, first, flatMap, fold, foreach, foreachPartition, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupBy, id, intersection, intersection, intersection, isCheckpointed, isEmpty, iterator, keyBy, localCheckpoint, map, mapPartitions, mapPartitionsWithIndex, max, min, name, numericRDDToDoubleRDDFunctions, partitioner, partitions, persist, persist, pipe, pipe, pipe, preferredLocations, randomSplit, rddToAsyncRDDActions, rddToOrderedRDDFunctions, rddToPairRDDFunctions, rddToSequenceFileRDDFunctions, reduce, repartition, sample, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortBy, sparkContext, subtract, subtract, subtract, take, takeOrdered, takeSample, toDebugString, toJavaRDD, toLocalIterator, top, toString, treeAggregate, treeReduce, union, unpersist, zip, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail JdbcRDD public JdbcRDD(SparkContext sc, scala.Function0<java.sql.Connection> getConnection, String sql, long lowerBound, long upperBound, int numPartitions, scala.Function1<java.sql.ResultSet,T> mapRow, scala.reflect.ClassTag<T> evidence$1) Method Detail resultSetToObjectArray public static Object[] resultSetToObjectArray(java.sql.ResultSet rs) create public static <T> JavaRDD<T> create(JavaSparkContext sc, JdbcRDD.ConnectionFactory connectionFactory, String sql, long lowerBound, long upperBound, int numPartitions, Function<java.sql.ResultSet,T> mapRow) Create an RDD that executes a SQL query on a JDBC connection and reads results. For usage example, see test case JavaAPISuite.testJavaJdbcRDD. Parameters:connectionFactory - a factory that returns an open Connection. The RDD takes care of closing the connection.sql - the text of the query. The query must contain two ? placeholders for parameters used to partition the results. E.g. "select title, author from books where ? <= id and id <= ?"lowerBound - the minimum value of the first placeholderupperBound - the maximum value of the second placeholder The lower and upper bounds are inclusive.numPartitions - the number of partitions. Given a lowerBound of 1, an upperBound of 20, and a numPartitions of 2, the query would be executed twice, once with (1, 10) and once with (11, 20)mapRow - a function from a ResultSet to a single row of the desired result type(s). This should only call getInt, getString, etc; the RDD takes care of calling next. The default maps a ResultSet to an array of Object.sc - (undocumented) Returns:(undocumented) create public static JavaRDD<Object[]> create(JavaSparkContext sc, JdbcRDD.ConnectionFactory connectionFactory, String sql, long lowerBound, long upperBound, int numPartitions) Create an RDD that executes a SQL query on a JDBC connection and reads results. Each row is converted into a Object array. For usage example, see test case JavaAPISuite.testJavaJdbcRDD. Parameters:connectionFactory - a factory that returns an open Connection. The RDD takes care of closing the connection.sql - the text of the query. The query must contain two ? placeholders for parameters used to partition the results. E.g. "select title, author from books where ? <= id and id <= ?"lowerBound - the minimum value of the first placeholderupperBound - the maximum value of the second placeholder The lower and upper bounds are inclusive.numPartitions - the number of partitions. Given a lowerBound of 1, an upperBound of 20, and a numPartitions of 2, the query would be executed twice, once with (1, 10) and once with (11, 20)sc - (undocumented) Returns:(undocumented) partitioner public static scala.Option<Partitioner> partitioner() sparkContext public static SparkContext sparkContext() id public static int id() name public static String name() name_$eq public static void name_$eq(String x$1) setName public static RDD<T> setName(String _name) persist public static RDD<T> persist(StorageLevel newLevel) persist public static RDD<T> persist() cache public static RDD<T> cache() unpersist public static RDD<T> unpersist(boolean blocking) getStorageLevel public static StorageLevel getStorageLevel() dependencies public static final scala.collection.Seq<Dependency<?>> dependencies() partitions public static final Partition[] partitions() getNumPartitions public static final int getNumPartitions() preferredLocations public static final scala.collection.Seq<String> preferredLocations(Partition split) iterator public static final scala.collection.Iterator<T> iterator(Partition split, TaskContext context) map public static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3) flatMap public static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4) filter public static RDD<T> filter(scala.Function1<T,Object> f) distinct public static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord) distinct public static RDD<T> distinct() repartition public static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord) coalesce public static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord) sample public static RDD<T> sample(boolean withReplacement, double fraction, long seed) randomSplit public static RDD<T>[] randomSplit(double[] weights, long seed) takeSample public static Object takeSample(boolean withReplacement, int num, long seed) union public static RDD<T> union(RDD<T> other) $plus$plus public static RDD<T> $plus$plus(RDD<T> other) sortBy public static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag) intersection public static RDD<T> intersection(RDD<T> other) intersection public static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord) intersection public static RDD<T> intersection(RDD<T> other, int numPartitions) glom public static RDD<Object> glom() cartesian public static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord) pipe public static RDD<String> pipe(String command) pipe public static RDD<String> pipe(String command, scala.collection.Map<String,String> env) pipe public static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding) mapPartitions public static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6) mapPartitionsWithIndex public static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8) zip public static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27) foreach public static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f) foreachPartition public static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) collect public static Object collect() toLocalIterator public static scala.collection.Iterator<T> toLocalIterator() collect public static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28) subtract public static RDD<T> subtract(RDD<T> other) subtract public static RDD<T> subtract(RDD<T> other, int numPartitions) subtract public static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord) reduce public static T reduce(scala.Function2<T,T,T> f) treeReduce public static T treeReduce(scala.Function2<T,T,T> f, int depth) fold public static T fold(T zeroValue, scala.Function2<T,T,T> op) aggregate public static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29) treeAggregate public static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30) count public static long count() countApprox public static PartialResult<BoundedDouble> countApprox(long timeout, double confidence) countByValue public static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord) countByValueApprox public static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord) countApproxDistinct public static long countApproxDistinct(int p, int sp) countApproxDistinct public static long countApproxDistinct(double relativeSD) zipWithIndex public static RDD<scala.Tuple2<T,Object>> zipWithIndex() zipWithUniqueId public static RDD<scala.Tuple2<T,Object>> zipWithUniqueId() take public static Object take(int num) first public static T first() top public static Object top(int num, scala.math.Ordering<T> ord) takeOrdered public static Object takeOrdered(int num, scala.math.Ordering<T> ord) max public static T max(scala.math.Ordering<T> ord) min public static T min(scala.math.Ordering<T> ord) isEmpty public static boolean isEmpty() saveAsTextFile public static void saveAsTextFile(String path) saveAsTextFile public static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) saveAsObjectFile public static void saveAsObjectFile(String path) keyBy public static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f) checkpoint public static void checkpoint() localCheckpoint public static RDD<T> localCheckpoint() isCheckpointed public static boolean isCheckpointed() getCheckpointFile public static scala.Option<String> getCheckpointFile() context public static SparkContext context() toDebugString public static String toDebugString() toString public static String toString() toJavaRDD public static JavaRDD<T> toJavaRDD() sample$default$3 public static long sample$default$3() mapPartitionsWithIndex$default$2 public static <U> boolean mapPartitionsWithIndex$default$2() unpersist$default$1 public static boolean unpersist$default$1() distinct$default$2 public static scala.math.Ordering<T> distinct$default$2(int numPartitions) coalesce$default$2 public static boolean coalesce$default$2() coalesce$default$3 public static scala.Option<PartitionCoalescer> coalesce$default$3() coalesce$default$4 public static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer) repartition$default$2 public static scala.math.Ordering<T> repartition$default$2(int numPartitions) subtract$default$3 public static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p) intersection$default$3 public static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner) randomSplit$default$2 public static long randomSplit$default$2() sortBy$default$2 public static <K> boolean sortBy$default$2() sortBy$default$3 public static <K> int sortBy$default$3() mapPartitions$default$2 public static <U> boolean mapPartitions$default$2() groupBy$default$4 public static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p) pipe$default$2 public static scala.collection.Map<String,String> pipe$default$2() pipe$default$3 public static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3() pipe$default$4 public static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4() pipe$default$5 public static boolean pipe$default$5() pipe$default$6 public static int pipe$default$6() pipe$default$7 public static String pipe$default$7() treeReduce$default$2 public static int treeReduce$default$2() treeAggregate$default$4 public static <U> int treeAggregate$default$4(U zeroValue) countApprox$default$2 public static double countApprox$default$2() countByValue$default$1 public static scala.math.Ordering<T> countByValue$default$1() countByValueApprox$default$2 public static double countByValueApprox$default$2() countByValueApprox$default$3 public static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence) takeSample$default$3 public static long takeSample$default$3() countApproxDistinct$default$1 public static double countApproxDistinct$default$1() mapPartitionsInternal$default$2 public static <U> boolean mapPartitionsInternal$default$2() getPartitions public Partition[] getPartitions() Description copied from class: RDD Implemented by subclasses to return the set of partitions in this RDD. This method will only be called once, so it is safe to implement a time-consuming computation in it. The partitions in this array must satisfy the following property: rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index } Returns:(undocumented) compute public scala.collection.Iterator<T> compute(Partition thePart, TaskContext context) Description copied from class: RDD :: DeveloperApi :: Implemented by subclasses to compute a given partition. Specified by: compute in class RDD<T> Parameters:thePart - (undocumented)context - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JdbcType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JdbcType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class JdbcType Object org.apache.spark.sql.jdbc.JdbcType All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class JdbcType extends Object implements scala.Product, scala.Serializable :: DeveloperApi :: A database type definition coupled with the jdbc type needed to send null values to the database. param: databaseTypeDefinition The database type definition param: jdbcNullType The jdbc type (as defined in java.sql.Types) used to send a null value to the database. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description JdbcType(String databaseTypeDefinition, int jdbcNullType)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  String databaseTypeDefinition()  abstract static boolean equals(Object that)  int jdbcNullType()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail JdbcType public JdbcType(String databaseTypeDefinition, int jdbcNullType) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() databaseTypeDefinition public String databaseTypeDefinition() jdbcNullType public int jdbcNullType() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JettyUtils.ServletParams$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JettyUtils.ServletParams$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ui Class JettyUtils.ServletParams$ Object org.apache.spark.ui.JettyUtils.ServletParams$ Enclosing class: JettyUtils public static class JettyUtils.ServletParams$ extends Object Field Summary Fields  Modifier and Type Field and Description static JettyUtils.ServletParams$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description JettyUtils.ServletParams$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final JettyUtils.ServletParams$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail JettyUtils.ServletParams$ public JettyUtils.ServletParams$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JettyUtils.ServletParams (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JettyUtils.ServletParams (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ui Class JettyUtils.ServletParams<T> Object org.apache.spark.ui.JettyUtils.ServletParams<T> Enclosing class: JettyUtils public static class JettyUtils.ServletParams<T> extends Object Constructor Summary Constructors  Constructor and Description JettyUtils.ServletParams(scala.Function1<javax.servlet.http.HttpServletRequest,T> responder, String contentType, scala.Function1<T,String> extractFn, scala.Function1<T,Object> evidence$1)  Method Summary Methods  Modifier and Type Method and Description String contentType()  scala.Function1<T,String> extractFn()  scala.Function1<javax.servlet.http.HttpServletRequest,T> responder()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JettyUtils.ServletParams public JettyUtils.ServletParams(scala.Function1<javax.servlet.http.HttpServletRequest,T> responder, String contentType, scala.Function1<T,String> extractFn, scala.Function1<T,Object> evidence$1) Method Detail responder public scala.Function1<javax.servlet.http.HttpServletRequest,T> responder() contentType public String contentType() extractFn public scala.Function1<T,String> extractFn() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JettyUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JettyUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ui Class JettyUtils Object org.apache.spark.ui.JettyUtils public class JettyUtils extends Object Utilities for launching a web server using Jetty's HTTP Server class Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  JettyUtils.ServletParams<T>  static class  JettyUtils.ServletParams$  Constructor Summary Constructors  Constructor and Description JettyUtils()  Method Summary Methods  Modifier and Type Method and Description static void addFilters(scala.collection.Seq<org.eclipse.jetty.servlet.ServletContextHandler> handlers, SparkConf conf) Add filters, if any, to the given list of ServletContextHandlers static org.eclipse.jetty.servlet.ServletContextHandler createRedirectHandler(String srcPath, String destPath, scala.Function1<javax.servlet.http.HttpServletRequest,scala.runtime.BoxedUnit> beforeRedirect, String basePath, scala.collection.immutable.Set<String> httpMethods) Create a handler that always redirects the user to the given path static <T> javax.servlet.http.HttpServlet createServlet(JettyUtils.ServletParams<T> servletParams, org.apache.spark.SecurityManager securityMgr, SparkConf conf, scala.Function1<T,Object> evidence$2)  static org.eclipse.jetty.servlet.ServletContextHandler createServletHandler(String path, javax.servlet.http.HttpServlet servlet, String basePath) Create a context handler that responds to a request with the given path prefix static <T> org.eclipse.jetty.servlet.ServletContextHandler createServletHandler(String path, JettyUtils.ServletParams<T> servletParams, org.apache.spark.SecurityManager securityMgr, SparkConf conf, String basePath, scala.Function1<T,Object> evidence$3) Create a context handler that responds to a request with the given path prefix static org.eclipse.jetty.servlet.ServletContextHandler createStaticHandler(String resourceBase, String path) Create a handler for serving files from a static directory static JettyUtils.ServletParams<scala.collection.Seq<scala.xml.Node>> htmlResponderToServlet(scala.Function1<javax.servlet.http.HttpServletRequest,scala.collection.Seq<scala.xml.Node>> responder)  static JettyUtils.ServletParams<org.json4s.JsonAST.JValue> jsonResponderToServlet(scala.Function1<javax.servlet.http.HttpServletRequest,org.json4s.JsonAST.JValue> responder)  static org.apache.spark.ui.ServerInfo startJettyServer(String hostName, int port, org.apache.spark.SSLOptions sslOptions, scala.collection.Seq<org.eclipse.jetty.servlet.ServletContextHandler> handlers, SparkConf conf, String serverName) Attempt to start a Jetty server bound to the supplied hostName:port using the given context handlers. static JettyUtils.ServletParams<String> textResponderToServlet(scala.Function1<javax.servlet.http.HttpServletRequest,String> responder)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JettyUtils public JettyUtils() Method Detail jsonResponderToServlet public static JettyUtils.ServletParams<org.json4s.JsonAST.JValue> jsonResponderToServlet(scala.Function1<javax.servlet.http.HttpServletRequest,org.json4s.JsonAST.JValue> responder) htmlResponderToServlet public static JettyUtils.ServletParams<scala.collection.Seq<scala.xml.Node>> htmlResponderToServlet(scala.Function1<javax.servlet.http.HttpServletRequest,scala.collection.Seq<scala.xml.Node>> responder) textResponderToServlet public static JettyUtils.ServletParams<String> textResponderToServlet(scala.Function1<javax.servlet.http.HttpServletRequest,String> responder) createServlet public static <T> javax.servlet.http.HttpServlet createServlet(JettyUtils.ServletParams<T> servletParams, org.apache.spark.SecurityManager securityMgr, SparkConf conf, scala.Function1<T,Object> evidence$2) createServletHandler public static <T> org.eclipse.jetty.servlet.ServletContextHandler createServletHandler(String path, JettyUtils.ServletParams<T> servletParams, org.apache.spark.SecurityManager securityMgr, SparkConf conf, String basePath, scala.Function1<T,Object> evidence$3) Create a context handler that responds to a request with the given path prefix createServletHandler public static org.eclipse.jetty.servlet.ServletContextHandler createServletHandler(String path, javax.servlet.http.HttpServlet servlet, String basePath) Create a context handler that responds to a request with the given path prefix createRedirectHandler public static org.eclipse.jetty.servlet.ServletContextHandler createRedirectHandler(String srcPath, String destPath, scala.Function1<javax.servlet.http.HttpServletRequest,scala.runtime.BoxedUnit> beforeRedirect, String basePath, scala.collection.immutable.Set<String> httpMethods) Create a handler that always redirects the user to the given path createStaticHandler public static org.eclipse.jetty.servlet.ServletContextHandler createStaticHandler(String resourceBase, String path) Create a handler for serving files from a static directory addFilters public static void addFilters(scala.collection.Seq<org.eclipse.jetty.servlet.ServletContextHandler> handlers, SparkConf conf) Add filters, if any, to the given list of ServletContextHandlers startJettyServer public static org.apache.spark.ui.ServerInfo startJettyServer(String hostName, int port, org.apache.spark.SSLOptions sslOptions, scala.collection.Seq<org.eclipse.jetty.servlet.ServletContextHandler> handlers, SparkConf conf, String serverName) Attempt to start a Jetty server bound to the supplied hostName:port using the given context handlers. If the desired port number is contended, continues incrementing ports until a free port is found. Return the jetty Server object, the chosen port, and a mutable collection of handlers. Parameters:hostName - (undocumented)port - (undocumented)sslOptions - (undocumented)handlers - (undocumented)conf - (undocumented)serverName - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JobData (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JobData (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class JobData Object org.apache.spark.status.api.v1.JobData public class JobData extends Object Method Summary Methods  Modifier and Type Method and Description scala.Option<java.util.Date> completionTime()  scala.Option<String> description()  scala.Option<String> jobGroup()  int jobId()  String name()  int numActiveStages()  int numActiveTasks()  int numCompletedStages()  int numCompletedTasks()  int numFailedStages()  int numFailedTasks()  int numSkippedStages()  int numSkippedTasks()  int numTasks()  scala.collection.Seq<Object> stageIds()  JobExecutionStatus status()  scala.Option<java.util.Date> submissionTime()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail jobId public int jobId() name public String name() description public scala.Option<String> description() submissionTime public scala.Option<java.util.Date> submissionTime() completionTime public scala.Option<java.util.Date> completionTime() stageIds public scala.collection.Seq<Object> stageIds() jobGroup public scala.Option<String> jobGroup() status public JobExecutionStatus status() numTasks public int numTasks() numActiveTasks public int numActiveTasks() numCompletedTasks public int numCompletedTasks() numSkippedTasks public int numSkippedTasks() numFailedTasks public int numFailedTasks() numActiveStages public int numActiveStages() numCompletedStages public int numCompletedStages() numSkippedStages public int numSkippedStages() numFailedStages public int numFailedStages() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JobExecutionStatus (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JobExecutionStatus (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method org.apache.spark Enum JobExecutionStatus Object Enum<JobExecutionStatus> org.apache.spark.JobExecutionStatus All Implemented Interfaces: java.io.Serializable, Comparable<JobExecutionStatus> public enum JobExecutionStatus extends Enum<JobExecutionStatus> Enum Constant Summary Enum Constants  Enum Constant and Description FAILED  RUNNING  SUCCEEDED  UNKNOWN  Method Summary Methods  Modifier and Type Method and Description static JobExecutionStatus fromString(String str)  static JobExecutionStatus valueOf(String name) Returns the enum constant of this type with the specified name. static JobExecutionStatus[] values() Returns an array containing the constants of this enum type, in the order they are declared. Methods inherited from class Enum compareTo, equals, getDeclaringClass, hashCode, name, ordinal, toString, valueOf Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Enum Constant Detail RUNNING public static final JobExecutionStatus RUNNING SUCCEEDED public static final JobExecutionStatus SUCCEEDED FAILED public static final JobExecutionStatus FAILED UNKNOWN public static final JobExecutionStatus UNKNOWN Method Detail values public static JobExecutionStatus[] values() Returns an array containing the constants of this enum type, in the order they are declared. This method may be used to iterate over the constants as follows: for (JobExecutionStatus c : JobExecutionStatus.values())   System.out.println(c); Returns:an array containing the constants of this enum type, in the order they are declared valueOf public static JobExecutionStatus valueOf(String name) Returns the enum constant of this type with the specified name. The string must match exactly an identifier used to declare an enum constant in this type. (Extraneous whitespace characters are not permitted.) Parameters:name - the name of the enum constant to be returned. Returns:the enum constant with the specified name Throws: IllegalArgumentException - if this enum type has no constant with the specified name NullPointerException - if the argument is null fromString public static JobExecutionStatus fromString(String str) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Enum Constants |  Field |  Method Detail:  Enum Constants |  Field |  Method JobProgressListener (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JobProgressListener (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ui.jobs Class JobProgressListener Object org.apache.spark.scheduler.SparkListener org.apache.spark.ui.jobs.JobProgressListener public class JobProgressListener extends SparkListener :: DeveloperApi :: Tracks task-level information to be displayed in the UI. All access to the data structures in this class must be synchronized on the class, since the UI thread and the EventBus loop may otherwise be reading and updating the internal data structures concurrently. Constructor Summary Constructors  Constructor and Description JobProgressListener(SparkConf conf)  Method Summary Methods  Modifier and Type Method and Description scala.collection.mutable.HashMap<Object,UIData.JobUIData> activeJobs()  scala.collection.mutable.HashMap<Object,StageInfo> activeStages()  scala.collection.Seq<BlockManagerId> blockManagerIds()  scala.collection.mutable.ListBuffer<UIData.JobUIData> completedJobs()  scala.collection.mutable.ListBuffer<StageInfo> completedStages()  long endTime()  scala.collection.mutable.HashMap<String,BlockManagerId> executorIdToBlockManagerId()  scala.collection.mutable.ListBuffer<UIData.JobUIData> failedJobs()  scala.collection.mutable.ListBuffer<StageInfo> failedStages()  scala.collection.mutable.HashMap<String,scala.collection.mutable.HashSet<Object>> jobGroupToJobIds()  scala.collection.mutable.HashMap<Object,UIData.JobUIData> jobIdToData()  int numCompletedJobs()  int numCompletedStages()  int numFailedJobs()  int numFailedStages()  void onApplicationEnd(SparkListenerApplicationEnd appEnded) Called when the application ends void onApplicationStart(SparkListenerApplicationStart appStarted) Called when the application starts void onBlockManagerAdded(SparkListenerBlockManagerAdded blockManagerAdded) Called when a new block manager has joined void onBlockManagerRemoved(SparkListenerBlockManagerRemoved blockManagerRemoved) Called when an existing block manager has been removed void onEnvironmentUpdate(SparkListenerEnvironmentUpdate environmentUpdate) Called when environment properties have been updated void onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate executorMetricsUpdate) Called when the driver receives task metrics from an executor in a heartbeat. void onJobEnd(SparkListenerJobEnd jobEnd) Called when a job ends void onJobStart(SparkListenerJobStart jobStart) Called when a job starts void onStageCompleted(SparkListenerStageCompleted stageCompleted) Called when a stage completes successfully or fails, with information on the completed stage. void onStageSubmitted(SparkListenerStageSubmitted stageSubmitted) For FIFO, all stages are contained by "default" pool but "default" pool here is meaningless void onTaskEnd(SparkListenerTaskEnd taskEnd) Called when a task ends void onTaskGettingResult(SparkListenerTaskGettingResult taskGettingResult) Called when a task begins remotely fetching its result (will not be called for tasks that do not need to fetch the result remotely). void onTaskStart(SparkListenerTaskStart taskStart) Called when a task starts scala.collection.mutable.HashMap<Object,StageInfo> pendingStages()  scala.collection.mutable.HashMap<String,scala.collection.mutable.HashMap<Object,StageInfo>> poolToActiveStages()  int retainedJobs()  int retainedStages()  int retainedTasks()  scala.Option<scala.Enumeration.Value> schedulingMode()  scala.collection.mutable.ListBuffer<StageInfo> skippedStages()  scala.collection.mutable.HashMap<Object,scala.collection.mutable.HashSet<Object>> stageIdToActiveJobIds()  scala.collection.mutable.HashMap<scala.Tuple2<Object,Object>,UIData.StageUIData> stageIdToData()  scala.collection.mutable.HashMap<Object,StageInfo> stageIdToInfo()  long startTime()  void updateAggregateMetrics(UIData.StageUIData stageData, String execId, org.apache.spark.executor.TaskMetrics taskMetrics, scala.Option<UIData.TaskMetricsUIData> oldMetrics) Upon receiving new metrics for a task, updates the per-stage and per-executor-per-stage aggregate metrics by calculating deltas between the currently recorded metrics and the new metrics. Methods inherited from class org.apache.spark.scheduler.SparkListener onBlockUpdated, onExecutorAdded, onExecutorRemoved, onOtherEvent, onUnpersistRDD Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JobProgressListener public JobProgressListener(SparkConf conf) Method Detail startTime public long startTime() endTime public long endTime() activeJobs public scala.collection.mutable.HashMap<Object,UIData.JobUIData> activeJobs() completedJobs public scala.collection.mutable.ListBuffer<UIData.JobUIData> completedJobs() failedJobs public scala.collection.mutable.ListBuffer<UIData.JobUIData> failedJobs() jobIdToData public scala.collection.mutable.HashMap<Object,UIData.JobUIData> jobIdToData() jobGroupToJobIds public scala.collection.mutable.HashMap<String,scala.collection.mutable.HashSet<Object>> jobGroupToJobIds() pendingStages public scala.collection.mutable.HashMap<Object,StageInfo> pendingStages() activeStages public scala.collection.mutable.HashMap<Object,StageInfo> activeStages() completedStages public scala.collection.mutable.ListBuffer<StageInfo> completedStages() skippedStages public scala.collection.mutable.ListBuffer<StageInfo> skippedStages() failedStages public scala.collection.mutable.ListBuffer<StageInfo> failedStages() stageIdToData public scala.collection.mutable.HashMap<scala.Tuple2<Object,Object>,UIData.StageUIData> stageIdToData() stageIdToInfo public scala.collection.mutable.HashMap<Object,StageInfo> stageIdToInfo() stageIdToActiveJobIds public scala.collection.mutable.HashMap<Object,scala.collection.mutable.HashSet<Object>> stageIdToActiveJobIds() poolToActiveStages public scala.collection.mutable.HashMap<String,scala.collection.mutable.HashMap<Object,StageInfo>> poolToActiveStages() numCompletedStages public int numCompletedStages() numFailedStages public int numFailedStages() numCompletedJobs public int numCompletedJobs() numFailedJobs public int numFailedJobs() executorIdToBlockManagerId public scala.collection.mutable.HashMap<String,BlockManagerId> executorIdToBlockManagerId() blockManagerIds public scala.collection.Seq<BlockManagerId> blockManagerIds() schedulingMode public scala.Option<scala.Enumeration.Value> schedulingMode() retainedStages public int retainedStages() retainedJobs public int retainedJobs() retainedTasks public int retainedTasks() onJobStart public void onJobStart(SparkListenerJobStart jobStart) Called when a job starts Overrides: onJobStart in class SparkListener Parameters:jobStart - (undocumented) onJobEnd public void onJobEnd(SparkListenerJobEnd jobEnd) Called when a job ends Overrides: onJobEnd in class SparkListener Parameters:jobEnd - (undocumented) onStageCompleted public void onStageCompleted(SparkListenerStageCompleted stageCompleted) Called when a stage completes successfully or fails, with information on the completed stage. Overrides: onStageCompleted in class SparkListener Parameters:stageCompleted - (undocumented) onStageSubmitted public void onStageSubmitted(SparkListenerStageSubmitted stageSubmitted) For FIFO, all stages are contained by "default" pool but "default" pool here is meaningless Overrides: onStageSubmitted in class SparkListener Parameters:stageSubmitted - (undocumented) onTaskStart public void onTaskStart(SparkListenerTaskStart taskStart) Called when a task starts Overrides: onTaskStart in class SparkListener Parameters:taskStart - (undocumented) onTaskGettingResult public void onTaskGettingResult(SparkListenerTaskGettingResult taskGettingResult) Called when a task begins remotely fetching its result (will not be called for tasks that do not need to fetch the result remotely). Overrides: onTaskGettingResult in class SparkListener Parameters:taskGettingResult - (undocumented) onTaskEnd public void onTaskEnd(SparkListenerTaskEnd taskEnd) Called when a task ends Overrides: onTaskEnd in class SparkListener Parameters:taskEnd - (undocumented) updateAggregateMetrics public void updateAggregateMetrics(UIData.StageUIData stageData, String execId, org.apache.spark.executor.TaskMetrics taskMetrics, scala.Option<UIData.TaskMetricsUIData> oldMetrics) Upon receiving new metrics for a task, updates the per-stage and per-executor-per-stage aggregate metrics by calculating deltas between the currently recorded metrics and the new metrics. Parameters:stageData - (undocumented)execId - (undocumented)taskMetrics - (undocumented)oldMetrics - (undocumented) onExecutorMetricsUpdate public void onExecutorMetricsUpdate(SparkListenerExecutorMetricsUpdate executorMetricsUpdate) Called when the driver receives task metrics from an executor in a heartbeat. Overrides: onExecutorMetricsUpdate in class SparkListener Parameters:executorMetricsUpdate - (undocumented) onEnvironmentUpdate public void onEnvironmentUpdate(SparkListenerEnvironmentUpdate environmentUpdate) Called when environment properties have been updated Overrides: onEnvironmentUpdate in class SparkListener Parameters:environmentUpdate - (undocumented) onBlockManagerAdded public void onBlockManagerAdded(SparkListenerBlockManagerAdded blockManagerAdded) Called when a new block manager has joined Overrides: onBlockManagerAdded in class SparkListener Parameters:blockManagerAdded - (undocumented) onBlockManagerRemoved public void onBlockManagerRemoved(SparkListenerBlockManagerRemoved blockManagerRemoved) Called when an existing block manager has been removed Overrides: onBlockManagerRemoved in class SparkListener Parameters:blockManagerRemoved - (undocumented) onApplicationStart public void onApplicationStart(SparkListenerApplicationStart appStarted) Called when the application starts Overrides: onApplicationStart in class SparkListener Parameters:appStarted - (undocumented) onApplicationEnd public void onApplicationEnd(SparkListenerApplicationEnd appEnded) Called when the application ends Overrides: onApplicationEnd in class SparkListener Parameters:appEnded - (undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JobResult (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JobResult (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler Interface JobResult public interface JobResult :: DeveloperApi :: A result of a job in the DAGScheduler. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JobSubmitter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JobSubmitter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Interface JobSubmitter public interface JobSubmitter Handle via which a "run" function passed to a ComplexFutureAction can submit jobs for execution. Method Summary Methods  Modifier and Type Method and Description <T,U,R> FutureAction<R> submitJob(RDD<T> rdd, scala.Function1<scala.collection.Iterator<T>,U> processPartition, scala.collection.Seq<Object> partitions, scala.Function2<Object,U,scala.runtime.BoxedUnit> resultHandler, scala.Function0<R> resultFunc) Submit a job for execution and return a FutureAction holding the result. Method Detail submitJob <T,U,R> FutureAction<R> submitJob(RDD<T> rdd, scala.Function1<scala.collection.Iterator<T>,U> processPartition, scala.collection.Seq<Object> partitions, scala.Function2<Object,U,scala.runtime.BoxedUnit> resultHandler, scala.Function0<R> resultFunc) Submit a job for execution and return a FutureAction holding the result. This is a wrapper around the same functionality provided by SparkContext to enable cancellation. Parameters:rdd - (undocumented)processPartition - (undocumented)partitions - (undocumented)resultHandler - (undocumented)resultFunc - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JobSucceeded (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JobSucceeded (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler Class JobSucceeded Object org.apache.spark.scheduler.JobSucceeded public class JobSucceeded extends Object Constructor Summary Constructors  Constructor and Description JobSucceeded()  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JobSucceeded public JobSucceeded() Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JsonProtocol (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JsonProtocol (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class JsonProtocol Object org.apache.spark.util.JsonProtocol public class JsonProtocol extends Object Serializes SparkListener events to/from JSON. This protocol provides strong backwards- and forwards-compatibility guarantees: any version of Spark should be able to read JSON output written by any other version, including newer versions. JsonProtocolSuite contains backwards-compatibility tests which check that the current version of JsonProtocol is able to read output written by earlier versions. We do not currently have tests for reading newer JSON output with older Spark versions. To ensure that we provide these guarantees, follow these rules when modifying these methods: - Never delete any JSON fields. - Any new JSON fields should be optional; use Utils.jsonOption when reading these fields in *FromJson methods. Constructor Summary Constructors  Constructor and Description JsonProtocol()  Method Summary Methods  Modifier and Type Method and Description static AccumulableInfo accumulableInfoFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue accumulableInfoToJson(AccumulableInfo accumulableInfo)  static SparkListenerApplicationEnd applicationEndFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue applicationEndToJson(SparkListenerApplicationEnd applicationEnd)  static SparkListenerApplicationStart applicationStartFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue applicationStartToJson(SparkListenerApplicationStart applicationStart)  static SparkListenerBlockManagerAdded blockManagerAddedFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue blockManagerAddedToJson(SparkListenerBlockManagerAdded blockManagerAdded)  static BlockManagerId blockManagerIdFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue blockManagerIdToJson(BlockManagerId blockManagerId)  static SparkListenerBlockManagerRemoved blockManagerRemovedFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue blockManagerRemovedToJson(SparkListenerBlockManagerRemoved blockManagerRemoved)  static BlockStatus blockStatusFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue blockStatusToJson(BlockStatus blockStatus)  static SparkListenerEnvironmentUpdate environmentUpdateFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue environmentUpdateToJson(SparkListenerEnvironmentUpdate environmentUpdate)  static Exception exceptionFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue exceptionToJson(Exception exception)  static SparkListenerExecutorAdded executorAddedFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue executorAddedToJson(SparkListenerExecutorAdded executorAdded)  static ExecutorInfo executorInfoFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue executorInfoToJson(ExecutorInfo executorInfo)  static SparkListenerExecutorMetricsUpdate executorMetricsUpdateFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue executorMetricsUpdateToJson(SparkListenerExecutorMetricsUpdate metricsUpdate)  static SparkListenerExecutorRemoved executorRemovedFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue executorRemovedToJson(SparkListenerExecutorRemoved executorRemoved)  static SparkListenerJobEnd jobEndFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue jobEndToJson(SparkListenerJobEnd jobEnd)  static JobResult jobResultFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue jobResultToJson(JobResult jobResult)  static SparkListenerJobStart jobStartFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue jobStartToJson(SparkListenerJobStart jobStart)  static org.apache.spark.scheduler.SparkListenerLogStart logStartFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue logStartToJson(org.apache.spark.scheduler.SparkListenerLogStart logStart)  static scala.collection.Map<String,String> mapFromJson(org.json4s.JsonAST.JValue json) -------------------------------- * Util JSON deserialization methods | static org.json4s.JsonAST.JValue mapToJson(scala.collection.Map<String,String> m) ------------------------------ * Util JSON serialization methods | static java.util.Properties propertiesFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue propertiesToJson(java.util.Properties properties)  static RDDInfo rddInfoFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue rddInfoToJson(RDDInfo rddInfo)  static SparkListenerEvent sparkEventFromJson(org.json4s.JsonAST.JValue json) --------------------------------------------------- * JSON deserialization methods for SparkListenerEvents | static org.json4s.JsonAST.JValue sparkEventToJson(SparkListenerEvent event) ------------------------------------------------- * JSON serialization methods for SparkListenerEvents | static StackTraceElement[] stackTraceFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue stackTraceToJson(StackTraceElement[] stackTrace)  static SparkListenerStageCompleted stageCompletedFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue stageCompletedToJson(SparkListenerStageCompleted stageCompleted)  static StageInfo stageInfoFromJson(org.json4s.JsonAST.JValue json) --------------------------------------------------------------------- * JSON deserialization methods for classes SparkListenerEvents depend on | static org.json4s.JsonAST.JValue stageInfoToJson(StageInfo stageInfo) ------------------------------------------------------------------- * JSON serialization methods for classes SparkListenerEvents depend on | static SparkListenerStageSubmitted stageSubmittedFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue stageSubmittedToJson(SparkListenerStageSubmitted stageSubmitted)  static StorageLevel storageLevelFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue storageLevelToJson(StorageLevel storageLevel)  static SparkListenerTaskEnd taskEndFromJson(org.json4s.JsonAST.JValue json)  static TaskEndReason taskEndReasonFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue taskEndReasonToJson(TaskEndReason taskEndReason)  static org.json4s.JsonAST.JValue taskEndToJson(SparkListenerTaskEnd taskEnd)  static SparkListenerTaskGettingResult taskGettingResultFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue taskGettingResultToJson(SparkListenerTaskGettingResult taskGettingResult)  static TaskInfo taskInfoFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue taskInfoToJson(TaskInfo taskInfo)  static org.apache.spark.executor.TaskMetrics taskMetricsFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue taskMetricsToJson(org.apache.spark.executor.TaskMetrics taskMetrics)  static SparkListenerTaskStart taskStartFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue taskStartToJson(SparkListenerTaskStart taskStart)  static SparkListenerUnpersistRDD unpersistRDDFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue unpersistRDDToJson(SparkListenerUnpersistRDD unpersistRDD)  static java.util.UUID UUIDFromJson(org.json4s.JsonAST.JValue json)  static org.json4s.JsonAST.JValue UUIDToJson(java.util.UUID id)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JsonProtocol public JsonProtocol() Method Detail sparkEventToJson public static org.json4s.JsonAST.JValue sparkEventToJson(SparkListenerEvent event) ------------------------------------------------- * JSON serialization methods for SparkListenerEvents | Parameters:event - (undocumented) Returns:(undocumented) -------------------------------------------------- stageSubmittedToJson public static org.json4s.JsonAST.JValue stageSubmittedToJson(SparkListenerStageSubmitted stageSubmitted) stageCompletedToJson public static org.json4s.JsonAST.JValue stageCompletedToJson(SparkListenerStageCompleted stageCompleted) taskStartToJson public static org.json4s.JsonAST.JValue taskStartToJson(SparkListenerTaskStart taskStart) taskGettingResultToJson public static org.json4s.JsonAST.JValue taskGettingResultToJson(SparkListenerTaskGettingResult taskGettingResult) taskEndToJson public static org.json4s.JsonAST.JValue taskEndToJson(SparkListenerTaskEnd taskEnd) jobStartToJson public static org.json4s.JsonAST.JValue jobStartToJson(SparkListenerJobStart jobStart) jobEndToJson public static org.json4s.JsonAST.JValue jobEndToJson(SparkListenerJobEnd jobEnd) environmentUpdateToJson public static org.json4s.JsonAST.JValue environmentUpdateToJson(SparkListenerEnvironmentUpdate environmentUpdate) blockManagerAddedToJson public static org.json4s.JsonAST.JValue blockManagerAddedToJson(SparkListenerBlockManagerAdded blockManagerAdded) blockManagerRemovedToJson public static org.json4s.JsonAST.JValue blockManagerRemovedToJson(SparkListenerBlockManagerRemoved blockManagerRemoved) unpersistRDDToJson public static org.json4s.JsonAST.JValue unpersistRDDToJson(SparkListenerUnpersistRDD unpersistRDD) applicationStartToJson public static org.json4s.JsonAST.JValue applicationStartToJson(SparkListenerApplicationStart applicationStart) applicationEndToJson public static org.json4s.JsonAST.JValue applicationEndToJson(SparkListenerApplicationEnd applicationEnd) executorAddedToJson public static org.json4s.JsonAST.JValue executorAddedToJson(SparkListenerExecutorAdded executorAdded) executorRemovedToJson public static org.json4s.JsonAST.JValue executorRemovedToJson(SparkListenerExecutorRemoved executorRemoved) logStartToJson public static org.json4s.JsonAST.JValue logStartToJson(org.apache.spark.scheduler.SparkListenerLogStart logStart) executorMetricsUpdateToJson public static org.json4s.JsonAST.JValue executorMetricsUpdateToJson(SparkListenerExecutorMetricsUpdate metricsUpdate) stageInfoToJson public static org.json4s.JsonAST.JValue stageInfoToJson(StageInfo stageInfo) ------------------------------------------------------------------- * JSON serialization methods for classes SparkListenerEvents depend on | Parameters:stageInfo - (undocumented) Returns:(undocumented) -------------------------------------------------------------------- taskInfoToJson public static org.json4s.JsonAST.JValue taskInfoToJson(TaskInfo taskInfo) accumulableInfoToJson public static org.json4s.JsonAST.JValue accumulableInfoToJson(AccumulableInfo accumulableInfo) taskMetricsToJson public static org.json4s.JsonAST.JValue taskMetricsToJson(org.apache.spark.executor.TaskMetrics taskMetrics) taskEndReasonToJson public static org.json4s.JsonAST.JValue taskEndReasonToJson(TaskEndReason taskEndReason) blockManagerIdToJson public static org.json4s.JsonAST.JValue blockManagerIdToJson(BlockManagerId blockManagerId) jobResultToJson public static org.json4s.JsonAST.JValue jobResultToJson(JobResult jobResult) rddInfoToJson public static org.json4s.JsonAST.JValue rddInfoToJson(RDDInfo rddInfo) storageLevelToJson public static org.json4s.JsonAST.JValue storageLevelToJson(StorageLevel storageLevel) blockStatusToJson public static org.json4s.JsonAST.JValue blockStatusToJson(BlockStatus blockStatus) executorInfoToJson public static org.json4s.JsonAST.JValue executorInfoToJson(ExecutorInfo executorInfo) mapToJson public static org.json4s.JsonAST.JValue mapToJson(scala.collection.Map<String,String> m) ------------------------------ * Util JSON serialization methods | Parameters:m - (undocumented) Returns:(undocumented) ------------------------------- propertiesToJson public static org.json4s.JsonAST.JValue propertiesToJson(java.util.Properties properties) UUIDToJson public static org.json4s.JsonAST.JValue UUIDToJson(java.util.UUID id) stackTraceToJson public static org.json4s.JsonAST.JValue stackTraceToJson(StackTraceElement[] stackTrace) exceptionToJson public static org.json4s.JsonAST.JValue exceptionToJson(Exception exception) sparkEventFromJson public static SparkListenerEvent sparkEventFromJson(org.json4s.JsonAST.JValue json) --------------------------------------------------- * JSON deserialization methods for SparkListenerEvents | Parameters:json - (undocumented) Returns:(undocumented) ---------------------------------------------------- stageSubmittedFromJson public static SparkListenerStageSubmitted stageSubmittedFromJson(org.json4s.JsonAST.JValue json) stageCompletedFromJson public static SparkListenerStageCompleted stageCompletedFromJson(org.json4s.JsonAST.JValue json) taskStartFromJson public static SparkListenerTaskStart taskStartFromJson(org.json4s.JsonAST.JValue json) taskGettingResultFromJson public static SparkListenerTaskGettingResult taskGettingResultFromJson(org.json4s.JsonAST.JValue json) taskEndFromJson public static SparkListenerTaskEnd taskEndFromJson(org.json4s.JsonAST.JValue json) jobStartFromJson public static SparkListenerJobStart jobStartFromJson(org.json4s.JsonAST.JValue json) jobEndFromJson public static SparkListenerJobEnd jobEndFromJson(org.json4s.JsonAST.JValue json) environmentUpdateFromJson public static SparkListenerEnvironmentUpdate environmentUpdateFromJson(org.json4s.JsonAST.JValue json) blockManagerAddedFromJson public static SparkListenerBlockManagerAdded blockManagerAddedFromJson(org.json4s.JsonAST.JValue json) blockManagerRemovedFromJson public static SparkListenerBlockManagerRemoved blockManagerRemovedFromJson(org.json4s.JsonAST.JValue json) unpersistRDDFromJson public static SparkListenerUnpersistRDD unpersistRDDFromJson(org.json4s.JsonAST.JValue json) applicationStartFromJson public static SparkListenerApplicationStart applicationStartFromJson(org.json4s.JsonAST.JValue json) applicationEndFromJson public static SparkListenerApplicationEnd applicationEndFromJson(org.json4s.JsonAST.JValue json) executorAddedFromJson public static SparkListenerExecutorAdded executorAddedFromJson(org.json4s.JsonAST.JValue json) executorRemovedFromJson public static SparkListenerExecutorRemoved executorRemovedFromJson(org.json4s.JsonAST.JValue json) logStartFromJson public static org.apache.spark.scheduler.SparkListenerLogStart logStartFromJson(org.json4s.JsonAST.JValue json) executorMetricsUpdateFromJson public static SparkListenerExecutorMetricsUpdate executorMetricsUpdateFromJson(org.json4s.JsonAST.JValue json) stageInfoFromJson public static StageInfo stageInfoFromJson(org.json4s.JsonAST.JValue json) --------------------------------------------------------------------- * JSON deserialization methods for classes SparkListenerEvents depend on | Parameters:json - (undocumented) Returns:(undocumented) ---------------------------------------------------------------------- taskInfoFromJson public static TaskInfo taskInfoFromJson(org.json4s.JsonAST.JValue json) accumulableInfoFromJson public static AccumulableInfo accumulableInfoFromJson(org.json4s.JsonAST.JValue json) taskMetricsFromJson public static org.apache.spark.executor.TaskMetrics taskMetricsFromJson(org.json4s.JsonAST.JValue json) taskEndReasonFromJson public static TaskEndReason taskEndReasonFromJson(org.json4s.JsonAST.JValue json) blockManagerIdFromJson public static BlockManagerId blockManagerIdFromJson(org.json4s.JsonAST.JValue json) jobResultFromJson public static JobResult jobResultFromJson(org.json4s.JsonAST.JValue json) rddInfoFromJson public static RDDInfo rddInfoFromJson(org.json4s.JsonAST.JValue json) storageLevelFromJson public static StorageLevel storageLevelFromJson(org.json4s.JsonAST.JValue json) blockStatusFromJson public static BlockStatus blockStatusFromJson(org.json4s.JsonAST.JValue json) executorInfoFromJson public static ExecutorInfo executorInfoFromJson(org.json4s.JsonAST.JValue json) mapFromJson public static scala.collection.Map<String,String> mapFromJson(org.json4s.JsonAST.JValue json) -------------------------------- * Util JSON deserialization methods | Parameters:json - (undocumented) Returns:(undocumented) --------------------------------- propertiesFromJson public static java.util.Properties propertiesFromJson(org.json4s.JsonAST.JValue json) UUIDFromJson public static java.util.UUID UUIDFromJson(org.json4s.JsonAST.JValue json) stackTraceFromJson public static StackTraceElement[] stackTraceFromJson(org.json4s.JsonAST.JValue json) exceptionFromJson public static Exception exceptionFromJson(org.json4s.JsonAST.JValue json) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method JsonVectorConverter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="JsonVectorConverter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.linalg Class JsonVectorConverter Object org.apache.spark.ml.linalg.JsonVectorConverter public class JsonVectorConverter extends Object Constructor Summary Constructors  Constructor and Description JsonVectorConverter()  Method Summary Methods  Modifier and Type Method and Description static Vector fromJson(String json) Parses the JSON representation of a vector into a Vector. static String toJson(Vector v) Coverts the vector to a JSON string. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail JsonVectorConverter public JsonVectorConverter() Method Detail fromJson public static Vector fromJson(String json) Parses the JSON representation of a vector into a Vector. Parameters:json - (undocumented) Returns:(undocumented) toJson public static String toJson(Vector v) Coverts the vector to a JSON string. Parameters:v - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KMeans (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KMeans (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class KMeans Object org.apache.spark.mllib.clustering.KMeans All Implemented Interfaces: java.io.Serializable public class KMeans extends Object implements scala.Serializable K-means clustering with a k-means++ like initialization mode (the k-means|| algorithm by Bahmani et al). This is an iterative algorithm that will make multiple passes over the data, so any RDDs given to it should be cached by the user. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description KMeans() Constructs a KMeans instance with default parameters: {k: 2, maxIterations: 20, runs: 1, initializationMode: "k-means||", initializationSteps: 5, epsilon: 1e-4, seed: random}. Method Summary Methods  Modifier and Type Method and Description double getEpsilon() The distance threshold within which we've consider centers to have converged. String getInitializationMode() The initialization algorithm. int getInitializationSteps() Number of steps for the k-means|| initialization mode int getK() Number of clusters to create (k). int getMaxIterations() Maximum number of iterations allowed. int getRuns() This function has no effect since Spark 2.0.0. long getSeed() The random seed for cluster initialization. static String K_MEANS_PARALLEL()  static String RANDOM()  KMeansModel run(RDD<Vector> data) Train a K-means model on the given set of points; data should be cached for high performance, because this is an iterative algorithm. KMeans setEpsilon(double epsilon) Set the distance threshold within which we've consider centers to have converged. KMeans setInitializationMode(String initializationMode) Set the initialization algorithm. KMeans setInitializationSteps(int initializationSteps) Set the number of steps for the k-means|| initialization mode. KMeans setInitialModel(KMeansModel model) Set the initial starting point, bypassing the random initialization or k-means|| The condition model.k == this.k must be met, failure results in an IllegalArgumentException. KMeans setK(int k) Set the number of clusters to create (k). KMeans setMaxIterations(int maxIterations) Set maximum number of iterations allowed. KMeans setRuns(int runs) This function has no effect since Spark 2.0.0. KMeans setSeed(long seed) Set the random seed for cluster initialization. static KMeansModel train(RDD<Vector> data, int k, int maxIterations) Trains a k-means model using specified parameters and the default values for unspecified. static KMeansModel train(RDD<Vector> data, int k, int maxIterations, int runs) Trains a k-means model using specified parameters and the default values for unspecified. static KMeansModel train(RDD<Vector> data, int k, int maxIterations, int runs, String initializationMode) Trains a k-means model using the given set of parameters. static KMeansModel train(RDD<Vector> data, int k, int maxIterations, int runs, String initializationMode, long seed) Trains a k-means model using the given set of parameters. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail KMeans public KMeans() Constructs a KMeans instance with default parameters: {k: 2, maxIterations: 20, runs: 1, initializationMode: "k-means||", initializationSteps: 5, epsilon: 1e-4, seed: random}. Method Detail RANDOM public static String RANDOM() K_MEANS_PARALLEL public static String K_MEANS_PARALLEL() train public static KMeansModel train(RDD<Vector> data, int k, int maxIterations, int runs, String initializationMode, long seed) Trains a k-means model using the given set of parameters. Parameters:data - Training points as an RDD of Vector types.k - Number of clusters to create.maxIterations - Maximum number of iterations allowed.runs - This param has no effect since Spark 2.0.0.initializationMode - The initialization algorithm. This can either be "random" or "k-means||". (default: "k-means||")seed - Random seed for cluster initialization. Default is to generate seed based on system time. Returns:(undocumented) train public static KMeansModel train(RDD<Vector> data, int k, int maxIterations, int runs, String initializationMode) Trains a k-means model using the given set of parameters. Parameters:data - Training points as an RDD of Vector types.k - Number of clusters to create.maxIterations - Maximum number of iterations allowed.runs - This param has no effect since Spark 2.0.0.initializationMode - The initialization algorithm. This can either be "random" or "k-means||". (default: "k-means||") Returns:(undocumented) train public static KMeansModel train(RDD<Vector> data, int k, int maxIterations) Trains a k-means model using specified parameters and the default values for unspecified. Parameters:data - (undocumented)k - (undocumented)maxIterations - (undocumented) Returns:(undocumented) train public static KMeansModel train(RDD<Vector> data, int k, int maxIterations, int runs) Trains a k-means model using specified parameters and the default values for unspecified. Parameters:data - (undocumented)k - (undocumented)maxIterations - (undocumented)runs - (undocumented) Returns:(undocumented) getK public int getK() Number of clusters to create (k). Returns:(undocumented) setK public KMeans setK(int k) Set the number of clusters to create (k). Default: 2. Parameters:k - (undocumented) Returns:(undocumented) getMaxIterations public int getMaxIterations() Maximum number of iterations allowed. Returns:(undocumented) setMaxIterations public KMeans setMaxIterations(int maxIterations) Set maximum number of iterations allowed. Default: 20. Parameters:maxIterations - (undocumented) Returns:(undocumented) getInitializationMode public String getInitializationMode() The initialization algorithm. This can be either "random" or "k-means||". Returns:(undocumented) setInitializationMode public KMeans setInitializationMode(String initializationMode) Set the initialization algorithm. This can be either "random" to choose random points as initial cluster centers, or "k-means||" to use a parallel variant of k-means++ (Bahmani et al., Scalable K-Means++, VLDB 2012). Default: k-means||. Parameters:initializationMode - (undocumented) Returns:(undocumented) getRuns public int getRuns() This function has no effect since Spark 2.0.0. Returns:(undocumented) setRuns public KMeans setRuns(int runs) This function has no effect since Spark 2.0.0. Parameters:runs - (undocumented) Returns:(undocumented) getInitializationSteps public int getInitializationSteps() Number of steps for the k-means|| initialization mode Returns:(undocumented) setInitializationSteps public KMeans setInitializationSteps(int initializationSteps) Set the number of steps for the k-means|| initialization mode. This is an advanced setting -- the default of 5 is almost always enough. Default: 5. Parameters:initializationSteps - (undocumented) Returns:(undocumented) getEpsilon public double getEpsilon() The distance threshold within which we've consider centers to have converged. Returns:(undocumented) setEpsilon public KMeans setEpsilon(double epsilon) Set the distance threshold within which we've consider centers to have converged. If all centers move less than this Euclidean distance, we stop iterating one run. Parameters:epsilon - (undocumented) Returns:(undocumented) getSeed public long getSeed() The random seed for cluster initialization. Returns:(undocumented) setSeed public KMeans setSeed(long seed) Set the random seed for cluster initialization. Parameters:seed - (undocumented) Returns:(undocumented) setInitialModel public KMeans setInitialModel(KMeansModel model) Set the initial starting point, bypassing the random initialization or k-means|| The condition model.k == this.k must be met, failure results in an IllegalArgumentException. Parameters:model - (undocumented) Returns:(undocumented) run public KMeansModel run(RDD<Vector> data) Train a K-means model on the given set of points; data should be cached for high performance, because this is an iterative algorithm. Parameters:data - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KMeansDataGenerator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KMeansDataGenerator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.util Class KMeansDataGenerator Object org.apache.spark.mllib.util.KMeansDataGenerator public class KMeansDataGenerator extends Object :: DeveloperApi :: Generate test data for KMeans. This class first chooses k cluster centers from a d-dimensional Gaussian distribution scaled by factor r and then creates a Gaussian cluster with scale 1 around each center. Constructor Summary Constructors  Constructor and Description KMeansDataGenerator()  Method Summary Methods  Modifier and Type Method and Description static RDD<double[]> generateKMeansRDD(SparkContext sc, int numPoints, int k, int d, double r, int numPartitions) Generate an RDD containing test data for KMeans. static void main(String[] args)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail KMeansDataGenerator public KMeansDataGenerator() Method Detail generateKMeansRDD public static RDD<double[]> generateKMeansRDD(SparkContext sc, int numPoints, int k, int d, double r, int numPartitions) Generate an RDD containing test data for KMeans. Parameters:sc - SparkContext to use for creating the RDDnumPoints - Number of points that will be contained in the RDDk - Number of clustersd - Number of dimensionsr - Scaling factor for the distribution of the initial centersnumPartitions - Number of partitions of the generated RDD; default 2 Returns:(undocumented) main public static void main(String[] args) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KMeansModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KMeansModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class KMeansModel.SaveLoadV1_0$ Object org.apache.spark.mllib.clustering.KMeansModel.SaveLoadV1_0$ Enclosing class: KMeansModel public static class KMeansModel.SaveLoadV1_0$ extends Object Field Summary Fields  Modifier and Type Field and Description static KMeansModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description KMeansModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description KMeansModel load(SparkContext sc, String path)  void save(SparkContext sc, KMeansModel model, String path)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final KMeansModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail KMeansModel.SaveLoadV1_0$ public KMeansModel.SaveLoadV1_0$() Method Detail save public void save(SparkContext sc, KMeansModel model, String path) load public KMeansModel load(SparkContext sc, String path) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KMeansModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KMeansModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class KMeansModel Object org.apache.spark.mllib.clustering.KMeansModel All Implemented Interfaces: java.io.Serializable, PMMLExportable, Saveable Direct Known Subclasses: StreamingKMeansModel public class KMeansModel extends Object implements Saveable, scala.Serializable, PMMLExportable A clustering model for K-means. Each point belongs to the cluster with the closest center. See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  KMeansModel.SaveLoadV1_0$  Constructor Summary Constructors  Constructor and Description KMeansModel(Iterable<Vector> centers) A Java-friendly constructor that takes an Iterable of Vectors. KMeansModel(Vector[] clusterCenters)  Method Summary Methods  Modifier and Type Method and Description Vector[] clusterCenters()  double computeCost(RDD<Vector> data) Return the K-means cost (sum of squared distances of points to their nearest center) for this model on the given data. int k() Total number of clusters. static KMeansModel load(SparkContext sc, String path)  JavaRDD<Integer> predict(JavaRDD<Vector> points) Maps given points to their cluster indices. RDD<Object> predict(RDD<Vector> points) Maps given points to their cluster indices. int predict(Vector point) Returns the cluster index that a given point belongs to. void save(SparkContext sc, String path) Save this model to the given path. static String toPMML()  static void toPMML(java.io.OutputStream outputStream)  static void toPMML(SparkContext sc, String path)  static void toPMML(String localPath)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.pmml.PMMLExportable toPMML, toPMML, toPMML, toPMML, toPMML Constructor Detail KMeansModel public KMeansModel(Vector[] clusterCenters) KMeansModel public KMeansModel(Iterable<Vector> centers) A Java-friendly constructor that takes an Iterable of Vectors. Parameters:centers - (undocumented) Method Detail load public static KMeansModel load(SparkContext sc, String path) toPMML public static void toPMML(String localPath) toPMML public static void toPMML(SparkContext sc, String path) toPMML public static void toPMML(java.io.OutputStream outputStream) toPMML public static String toPMML() clusterCenters public Vector[] clusterCenters() k public int k() Total number of clusters. Returns:(undocumented) predict public int predict(Vector point) Returns the cluster index that a given point belongs to. Parameters:point - (undocumented) Returns:(undocumented) predict public RDD<Object> predict(RDD<Vector> points) Maps given points to their cluster indices. Parameters:points - (undocumented) Returns:(undocumented) predict public JavaRDD<Integer> predict(JavaRDD<Vector> points) Maps given points to their cluster indices. Parameters:points - (undocumented) Returns:(undocumented) computeCost public double computeCost(RDD<Vector> data) Return the K-means cost (sum of squared distances of points to their nearest center) for this model on the given data. Parameters:data - (undocumented) Returns:(undocumented) save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KMeansSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KMeansSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.clustering Class KMeansSummary Object org.apache.spark.ml.clustering.KMeansSummary All Implemented Interfaces: java.io.Serializable public class KMeansSummary extends Object implements scala.Serializable :: Experimental :: Summary of KMeans. param: predictions DataFrame produced by KMeansModel.transform() param: predictionCol Name for column of predicted clusters in predictions param: featuresCol Name for column of features in predictions param: k Number of clusters See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description Dataset<Row> cluster() Cluster centers of the transformed data. long[] clusterSizes() Size of (number of data points in) each cluster. String featuresCol()  int k()  String predictionCol()  Dataset<Row> predictions()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail predictions public Dataset<Row> predictions() predictionCol public String predictionCol() featuresCol public String featuresCol() k public int k() cluster public Dataset<Row> cluster() Cluster centers of the transformed data. Returns:(undocumented) clusterSizes public long[] clusterSizes() Size of (number of data points in) each cluster. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KafkaCluster.LeaderOffset$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KafkaCluster.LeaderOffset$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kafka Class KafkaCluster.LeaderOffset$ Object scala.runtime.AbstractFunction3<String,Object,Object,KafkaCluster.LeaderOffset> org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset$ All Implemented Interfaces: java.io.Serializable, scala.Function3<String,Object,Object,KafkaCluster.LeaderOffset> Enclosing class: KafkaCluster public static class KafkaCluster.LeaderOffset$ extends scala.runtime.AbstractFunction3<String,Object,Object,KafkaCluster.LeaderOffset> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static KafkaCluster.LeaderOffset$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description KafkaCluster.LeaderOffset$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction3 curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function3 apply Field Detail MODULE$ public static final KafkaCluster.LeaderOffset$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail KafkaCluster.LeaderOffset$ public KafkaCluster.LeaderOffset$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KafkaCluster.LeaderOffset (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KafkaCluster.LeaderOffset (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kafka Class KafkaCluster.LeaderOffset Object org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: KafkaCluster public static class KafkaCluster.LeaderOffset extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description KafkaCluster.LeaderOffset(String host, int port, long offset)  Method Summary Methods  Modifier and Type Method and Description String host()  long offset()  int port()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail KafkaCluster.LeaderOffset public KafkaCluster.LeaderOffset(String host, int port, long offset) Method Detail host public String host() port public int port() offset public long offset() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KafkaCluster.SimpleConsumerConfig$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KafkaCluster.SimpleConsumerConfig$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kafka Class KafkaCluster.SimpleConsumerConfig$ Object org.apache.spark.streaming.kafka.KafkaCluster.SimpleConsumerConfig$ Enclosing class: KafkaCluster public static class KafkaCluster.SimpleConsumerConfig$ extends Object Field Summary Fields  Modifier and Type Field and Description static KafkaCluster.SimpleConsumerConfig$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description KafkaCluster.SimpleConsumerConfig$()  Method Summary Methods  Modifier and Type Method and Description KafkaCluster.SimpleConsumerConfig apply(scala.collection.immutable.Map<String,String> kafkaParams) Make a consumer config without requiring group.id or zookeeper.connect, since communicating with brokers also needs common settings such as timeout Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final KafkaCluster.SimpleConsumerConfig$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail KafkaCluster.SimpleConsumerConfig$ public KafkaCluster.SimpleConsumerConfig$() Method Detail apply public KafkaCluster.SimpleConsumerConfig apply(scala.collection.immutable.Map<String,String> kafkaParams) Make a consumer config without requiring group.id or zookeeper.connect, since communicating with brokers also needs common settings such as timeout Parameters:kafkaParams - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KafkaCluster.SimpleConsumerConfig (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KafkaCluster.SimpleConsumerConfig (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kafka Class KafkaCluster.SimpleConsumerConfig Object kafka.utils.ZKConfig kafka.consumer.ConsumerConfig org.apache.spark.streaming.kafka.KafkaCluster.SimpleConsumerConfig Enclosing class: KafkaCluster public static class KafkaCluster.SimpleConsumerConfig extends kafka.consumer.ConsumerConfig High-level kafka consumers connect to ZK. ConsumerConfig assumes this use case. Simple consumers connect directly to brokers, but need many of the same configs. This subclass won't warn about missing ZK params, or presence of broker params. Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<String,Object>[] seedBrokers()  Methods inherited from class kafka.consumer.ConsumerConfig AutoCommit, autoCommitEnable, AutoCommitInterval, autoCommitIntervalMs, autoOffsetReset, AutoOffsetReset, clientId, consumerId, consumerTimeoutMs, ConsumerTimeoutMs, debug, debug, debug, DefaultClientId, DefaultFetcherBackoffMs, DefaultPartitionAssignmentStrategy, dualCommitEnabled, error, error, error, excludeInternalTopics, ExcludeInternalTopics, fatal, fatal, fatal, fetchMessageMaxBytes, fetchMinBytes, FetchSize, fetchWaitMaxMs, groupId, info, info, info, kafka$utils$Logging$_setter_$loggerName_$eq, logger, loggerName, logIdent_$eq, logIdent, MaxFetchSize, MaxFetchWaitMs, MaxQueuedChunks, MaxRebalanceRetries, MinFetchBytes, MirrorConsumerNumThreads, MirrorConsumerNumThreadsProp, MirrorTopicsBlacklist, MirrorTopicsBlacklistProp, MirrorTopicsWhitelist, MirrorTopicsWhitelistProp, numConsumerFetchers, NumConsumerFetchers, offsetsChannelBackoffMs, OffsetsChannelBackoffMs, offsetsChannelSocketTimeoutMs, OffsetsChannelSocketTimeoutMs, offsetsCommitMaxRetries, OffsetsCommitMaxRetries, offsetsStorage, OffsetsStorage, partitionAssignmentStrategy, props, queuedMaxMessages, rebalanceBackoffMs, rebalanceMaxRetries, refreshLeaderBackoffMs, RefreshMetadataBackoffMs, SocketBufferSize, socketReceiveBufferBytes, SocketTimeout, socketTimeoutMs, swallow, swallowDebug, swallowError, swallowInfo, swallowTrace, swallowWarn, trace, trace, trace, validate, validateAutoOffsetReset, validateChars, validateClientId, validateGroupId, validateOffsetsStorage, warn, warn, warn Methods inherited from class kafka.utils.ZKConfig zkConnect, zkConnectionTimeoutMs, zkSessionTimeoutMs, zkSyncTimeMs Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail seedBrokers public scala.Tuple2<String,Object>[] seedBrokers() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KafkaCluster (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KafkaCluster (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kafka Class KafkaCluster Object org.apache.spark.streaming.kafka.KafkaCluster All Implemented Interfaces: java.io.Serializable public class KafkaCluster extends Object implements scala.Serializable :: DeveloperApi :: Convenience methods for interacting with a Kafka cluster. See A Guide To The Kafka Protocol for more details on individual api calls. param: kafkaParams Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s), NOT zookeeper servers, specified in host1:port1,host2:port2 form See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  KafkaCluster.LeaderOffset  static class  KafkaCluster.LeaderOffset$  static class  KafkaCluster.SimpleConsumerConfig High-level kafka consumers connect to ZK. static class  KafkaCluster.SimpleConsumerConfig$  Constructor Summary Constructors  Constructor and Description KafkaCluster(scala.collection.immutable.Map<String,String> kafkaParams)  Method Summary Methods  Modifier and Type Method and Description static <T> T checkErrors(scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,T> result) If the result is right, return it, otherwise throw SparkException KafkaCluster.SimpleConsumerConfig config()  kafka.consumer.SimpleConsumer connect(String host, int port)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,kafka.consumer.SimpleConsumer> connectLeader(String topic, int partition)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.Tuple2<String,Object>> findLeader(String topic, int partition)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,scala.Tuple2<String,Object>>> findLeaders(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,kafka.common.OffsetMetadataAndError>> getConsumerOffsetMetadata(String groupId, scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions) Requires Kafka >= 0.8.1.1. scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,kafka.common.OffsetMetadataAndError>> getConsumerOffsetMetadata(String groupId, scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions, short consumerApiVersion)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> getConsumerOffsets(String groupId, scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions) Requires Kafka >= 0.8.1.1. scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> getConsumerOffsets(String groupId, scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions, short consumerApiVersion)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,KafkaCluster.LeaderOffset>> getEarliestLeaderOffsets(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,KafkaCluster.LeaderOffset>> getLatestLeaderOffsets(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,KafkaCluster.LeaderOffset>> getLeaderOffsets(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions, long before)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,scala.collection.Seq<KafkaCluster.LeaderOffset>>> getLeaderOffsets(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions, long before, int maxNumOffsets)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Set<kafka.api.TopicMetadata>> getPartitionMetadata(scala.collection.immutable.Set<String> topics)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Set<kafka.common.TopicAndPartition>> getPartitions(scala.collection.immutable.Set<String> topics)  scala.collection.immutable.Map<String,String> kafkaParams()  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> setConsumerOffsetMetadata(String groupId, scala.collection.immutable.Map<kafka.common.TopicAndPartition,kafka.common.OffsetAndMetadata> metadata) Requires Kafka >= 0.8.1.1. scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> setConsumerOffsetMetadata(String groupId, scala.collection.immutable.Map<kafka.common.TopicAndPartition,kafka.common.OffsetAndMetadata> metadata, short consumerApiVersion)  scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> setConsumerOffsets(String groupId, scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object> offsets) Requires Kafka >= 0.8.1.1. scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> setConsumerOffsets(String groupId, scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object> offsets, short consumerApiVersion)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail KafkaCluster public KafkaCluster(scala.collection.immutable.Map<String,String> kafkaParams) Method Detail checkErrors public static <T> T checkErrors(scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,T> result) If the result is right, return it, otherwise throw SparkException kafkaParams public scala.collection.immutable.Map<String,String> kafkaParams() config public KafkaCluster.SimpleConsumerConfig config() connect public kafka.consumer.SimpleConsumer connect(String host, int port) connectLeader public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,kafka.consumer.SimpleConsumer> connectLeader(String topic, int partition) findLeader public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.Tuple2<String,Object>> findLeader(String topic, int partition) findLeaders public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,scala.Tuple2<String,Object>>> findLeaders(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions) getPartitions public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Set<kafka.common.TopicAndPartition>> getPartitions(scala.collection.immutable.Set<String> topics) getPartitionMetadata public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Set<kafka.api.TopicMetadata>> getPartitionMetadata(scala.collection.immutable.Set<String> topics) getLatestLeaderOffsets public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,KafkaCluster.LeaderOffset>> getLatestLeaderOffsets(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions) getEarliestLeaderOffsets public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,KafkaCluster.LeaderOffset>> getEarliestLeaderOffsets(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions) getLeaderOffsets public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,KafkaCluster.LeaderOffset>> getLeaderOffsets(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions, long before) getLeaderOffsets public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,scala.collection.Seq<KafkaCluster.LeaderOffset>>> getLeaderOffsets(scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions, long before, int maxNumOffsets) getConsumerOffsets public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> getConsumerOffsets(String groupId, scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions) Requires Kafka >= 0.8.1.1. Defaults to the original ZooKeeper backed api version. getConsumerOffsets public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> getConsumerOffsets(String groupId, scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions, short consumerApiVersion) getConsumerOffsetMetadata public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,kafka.common.OffsetMetadataAndError>> getConsumerOffsetMetadata(String groupId, scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions) Requires Kafka >= 0.8.1.1. Defaults to the original ZooKeeper backed api version. getConsumerOffsetMetadata public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,kafka.common.OffsetMetadataAndError>> getConsumerOffsetMetadata(String groupId, scala.collection.immutable.Set<kafka.common.TopicAndPartition> topicAndPartitions, short consumerApiVersion) setConsumerOffsets public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> setConsumerOffsets(String groupId, scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object> offsets) Requires Kafka >= 0.8.1.1. Defaults to the original ZooKeeper backed api version. setConsumerOffsets public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> setConsumerOffsets(String groupId, scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object> offsets, short consumerApiVersion) setConsumerOffsetMetadata public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> setConsumerOffsetMetadata(String groupId, scala.collection.immutable.Map<kafka.common.TopicAndPartition,kafka.common.OffsetAndMetadata> metadata) Requires Kafka >= 0.8.1.1. Defaults to the original ZooKeeper backed api version. setConsumerOffsetMetadata public scala.util.Either<scala.collection.mutable.ArrayBuffer<Throwable>,scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object>> setConsumerOffsetMetadata(String groupId, scala.collection.immutable.Map<kafka.common.TopicAndPartition,kafka.common.OffsetAndMetadata> metadata, short consumerApiVersion) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KafkaUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KafkaUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kafka Class KafkaUtils Object org.apache.spark.streaming.kafka.KafkaUtils public class KafkaUtils extends Object Constructor Summary Constructors  Constructor and Description KafkaUtils()  Method Summary Methods  Modifier and Type Method and Description static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>,R> JavaInputDStream<R> createDirectStream(JavaStreamingContext jssc, Class<K> keyClass, Class<V> valueClass, Class<KD> keyDecoderClass, Class<VD> valueDecoderClass, Class<R> recordClass, java.util.Map<String,String> kafkaParams, java.util.Map<kafka.common.TopicAndPartition,Long> fromOffsets, Function<kafka.message.MessageAndMetadata<K,V>,R> messageHandler) Create an input stream that directly pulls messages from Kafka Brokers without using any receiver. static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>> JavaPairInputDStream<K,V> createDirectStream(JavaStreamingContext jssc, Class<K> keyClass, Class<V> valueClass, Class<KD> keyDecoderClass, Class<VD> valueDecoderClass, java.util.Map<String,String> kafkaParams, java.util.Set<String> topics) Create an input stream that directly pulls messages from Kafka Brokers without using any receiver. static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>,R> InputDStream<R> createDirectStream(StreamingContext ssc, scala.collection.immutable.Map<String,String> kafkaParams, scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object> fromOffsets, scala.Function1<kafka.message.MessageAndMetadata<K,V>,R> messageHandler, scala.reflect.ClassTag<K> evidence$14, scala.reflect.ClassTag<V> evidence$15, scala.reflect.ClassTag<KD> evidence$16, scala.reflect.ClassTag<VD> evidence$17, scala.reflect.ClassTag<R> evidence$18) Create an input stream that directly pulls messages from Kafka Brokers without using any receiver. static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>> InputDStream<scala.Tuple2<K,V>> createDirectStream(StreamingContext ssc, scala.collection.immutable.Map<String,String> kafkaParams, scala.collection.immutable.Set<String> topics, scala.reflect.ClassTag<K> evidence$19, scala.reflect.ClassTag<V> evidence$20, scala.reflect.ClassTag<KD> evidence$21, scala.reflect.ClassTag<VD> evidence$22) Create an input stream that directly pulls messages from Kafka Brokers without using any receiver. static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>,R> JavaRDD<R> createRDD(JavaSparkContext jsc, Class<K> keyClass, Class<V> valueClass, Class<KD> keyDecoderClass, Class<VD> valueDecoderClass, Class<R> recordClass, java.util.Map<String,String> kafkaParams, OffsetRange[] offsetRanges, java.util.Map<kafka.common.TopicAndPartition,Broker> leaders, Function<kafka.message.MessageAndMetadata<K,V>,R> messageHandler) Create a RDD from Kafka using offset ranges for each topic and partition. static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>> JavaPairRDD<K,V> createRDD(JavaSparkContext jsc, Class<K> keyClass, Class<V> valueClass, Class<KD> keyDecoderClass, Class<VD> valueDecoderClass, java.util.Map<String,String> kafkaParams, OffsetRange[] offsetRanges) Create a RDD from Kafka using offset ranges for each topic and partition. static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>> RDD<scala.Tuple2<K,V>> createRDD(SparkContext sc, scala.collection.immutable.Map<String,String> kafkaParams, OffsetRange[] offsetRanges, scala.reflect.ClassTag<K> evidence$5, scala.reflect.ClassTag<V> evidence$6, scala.reflect.ClassTag<KD> evidence$7, scala.reflect.ClassTag<VD> evidence$8) Create a RDD from Kafka using offset ranges for each topic and partition. static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>,R> RDD<R> createRDD(SparkContext sc, scala.collection.immutable.Map<String,String> kafkaParams, OffsetRange[] offsetRanges, scala.collection.immutable.Map<kafka.common.TopicAndPartition,Broker> leaders, scala.Function1<kafka.message.MessageAndMetadata<K,V>,R> messageHandler, scala.reflect.ClassTag<K> evidence$9, scala.reflect.ClassTag<V> evidence$10, scala.reflect.ClassTag<KD> evidence$11, scala.reflect.ClassTag<VD> evidence$12, scala.reflect.ClassTag<R> evidence$13) Create a RDD from Kafka using offset ranges for each topic and partition. static <K,V,U extends kafka.serializer.Decoder<?>,T extends kafka.serializer.Decoder<?>> JavaPairReceiverInputDStream<K,V> createStream(JavaStreamingContext jssc, Class<K> keyTypeClass, Class<V> valueTypeClass, Class<U> keyDecoderClass, Class<T> valueDecoderClass, java.util.Map<String,String> kafkaParams, java.util.Map<String,Integer> topics, StorageLevel storageLevel) Create an input stream that pulls messages from Kafka Brokers. static JavaPairReceiverInputDStream<String,String> createStream(JavaStreamingContext jssc, String zkQuorum, String groupId, java.util.Map<String,Integer> topics) Create an input stream that pulls messages from Kafka Brokers. static JavaPairReceiverInputDStream<String,String> createStream(JavaStreamingContext jssc, String zkQuorum, String groupId, java.util.Map<String,Integer> topics, StorageLevel storageLevel) Create an input stream that pulls messages from Kafka Brokers. static <K,V,U extends kafka.serializer.Decoder<?>,T extends kafka.serializer.Decoder<?>> ReceiverInputDStream<scala.Tuple2<K,V>> createStream(StreamingContext ssc, scala.collection.immutable.Map<String,String> kafkaParams, scala.collection.immutable.Map<String,Object> topics, StorageLevel storageLevel, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2, scala.reflect.ClassTag<U> evidence$3, scala.reflect.ClassTag<T> evidence$4) Create an input stream that pulls messages from Kafka Brokers. static ReceiverInputDStream<scala.Tuple2<String,String>> createStream(StreamingContext ssc, String zkQuorum, String groupId, scala.collection.immutable.Map<String,Object> topics, StorageLevel storageLevel) Create an input stream that pulls messages from Kafka Brokers. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail KafkaUtils public KafkaUtils() Method Detail createStream public static ReceiverInputDStream<scala.Tuple2<String,String>> createStream(StreamingContext ssc, String zkQuorum, String groupId, scala.collection.immutable.Map<String,Object> topics, StorageLevel storageLevel) Create an input stream that pulls messages from Kafka Brokers. Parameters:ssc - StreamingContext objectzkQuorum - Zookeeper quorum (hostname:port,hostname:port,..)groupId - The group id for this consumertopics - Map of (topic_name -> numPartitions) to consume. Each partition is consumed in its own threadstorageLevel - Storage level to use for storing the received objects (default: StorageLevel.MEMORY_AND_DISK_SER_2) Returns:DStream of (Kafka message key, Kafka message value) createStream public static <K,V,U extends kafka.serializer.Decoder<?>,T extends kafka.serializer.Decoder<?>> ReceiverInputDStream<scala.Tuple2<K,V>> createStream(StreamingContext ssc, scala.collection.immutable.Map<String,String> kafkaParams, scala.collection.immutable.Map<String,Object> topics, StorageLevel storageLevel, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2, scala.reflect.ClassTag<U> evidence$3, scala.reflect.ClassTag<T> evidence$4) Create an input stream that pulls messages from Kafka Brokers. Parameters:ssc - StreamingContext objectkafkaParams - Map of kafka configuration parameters, see http://kafka.apache.org/08/configuration.htmltopics - Map of (topic_name -> numPartitions) to consume. Each partition is consumed in its own thread.storageLevel - Storage level to use for storing the received objectsevidence$1 - (undocumented)evidence$2 - (undocumented)evidence$3 - (undocumented)evidence$4 - (undocumented) Returns:DStream of (Kafka message key, Kafka message value) createStream public static JavaPairReceiverInputDStream<String,String> createStream(JavaStreamingContext jssc, String zkQuorum, String groupId, java.util.Map<String,Integer> topics) Create an input stream that pulls messages from Kafka Brokers. Storage level of the data will be the default StorageLevel.MEMORY_AND_DISK_SER_2. Parameters:jssc - JavaStreamingContext objectzkQuorum - Zookeeper quorum (hostname:port,hostname:port,..)groupId - The group id for this consumertopics - Map of (topic_name -> numPartitions) to consume. Each partition is consumed in its own thread Returns:DStream of (Kafka message key, Kafka message value) createStream public static JavaPairReceiverInputDStream<String,String> createStream(JavaStreamingContext jssc, String zkQuorum, String groupId, java.util.Map<String,Integer> topics, StorageLevel storageLevel) Create an input stream that pulls messages from Kafka Brokers. Parameters:jssc - JavaStreamingContext objectzkQuorum - Zookeeper quorum (hostname:port,hostname:port,..).groupId - The group id for this consumer.topics - Map of (topic_name -> numPartitions) to consume. Each partition is consumed in its own thread.storageLevel - RDD storage level. Returns:DStream of (Kafka message key, Kafka message value) createStream public static <K,V,U extends kafka.serializer.Decoder<?>,T extends kafka.serializer.Decoder<?>> JavaPairReceiverInputDStream<K,V> createStream(JavaStreamingContext jssc, Class<K> keyTypeClass, Class<V> valueTypeClass, Class<U> keyDecoderClass, Class<T> valueDecoderClass, java.util.Map<String,String> kafkaParams, java.util.Map<String,Integer> topics, StorageLevel storageLevel) Create an input stream that pulls messages from Kafka Brokers. Parameters:jssc - JavaStreamingContext objectkeyTypeClass - Key type of DStreamvalueTypeClass - value type of DstreamkeyDecoderClass - Type of kafka key decodervalueDecoderClass - Type of kafka value decoderkafkaParams - Map of kafka configuration parameters, see http://kafka.apache.org/08/configuration.htmltopics - Map of (topic_name -> numPartitions) to consume. Each partition is consumed in its own threadstorageLevel - RDD storage level. Returns:DStream of (Kafka message key, Kafka message value) createRDD public static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>> RDD<scala.Tuple2<K,V>> createRDD(SparkContext sc, scala.collection.immutable.Map<String,String> kafkaParams, OffsetRange[] offsetRanges, scala.reflect.ClassTag<K> evidence$5, scala.reflect.ClassTag<V> evidence$6, scala.reflect.ClassTag<KD> evidence$7, scala.reflect.ClassTag<VD> evidence$8) Create a RDD from Kafka using offset ranges for each topic and partition. Parameters:sc - SparkContext objectkafkaParams - Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s) (NOT zookeeper servers) specified in host1:port1,host2:port2 form.offsetRanges - Each OffsetRange in the batch corresponds to a range of offsets for a given Kafka topic/partitionevidence$5 - (undocumented)evidence$6 - (undocumented)evidence$7 - (undocumented)evidence$8 - (undocumented) Returns:RDD of (Kafka message key, Kafka message value) createRDD public static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>,R> RDD<R> createRDD(SparkContext sc, scala.collection.immutable.Map<String,String> kafkaParams, OffsetRange[] offsetRanges, scala.collection.immutable.Map<kafka.common.TopicAndPartition,Broker> leaders, scala.Function1<kafka.message.MessageAndMetadata<K,V>,R> messageHandler, scala.reflect.ClassTag<K> evidence$9, scala.reflect.ClassTag<V> evidence$10, scala.reflect.ClassTag<KD> evidence$11, scala.reflect.ClassTag<VD> evidence$12, scala.reflect.ClassTag<R> evidence$13) Create a RDD from Kafka using offset ranges for each topic and partition. This allows you specify the Kafka leader to connect to (to optimize fetching) and access the message as well as the metadata. Parameters:sc - SparkContext objectkafkaParams - Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s) (NOT zookeeper servers) specified in host1:port1,host2:port2 form.offsetRanges - Each OffsetRange in the batch corresponds to a range of offsets for a given Kafka topic/partitionleaders - Kafka brokers for each TopicAndPartition in offsetRanges. May be an empty map, in which case leaders will be looked up on the driver.messageHandler - Function for translating each message and metadata into the desired typeevidence$9 - (undocumented)evidence$10 - (undocumented)evidence$11 - (undocumented)evidence$12 - (undocumented)evidence$13 - (undocumented) Returns:RDD of R createRDD public static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>> JavaPairRDD<K,V> createRDD(JavaSparkContext jsc, Class<K> keyClass, Class<V> valueClass, Class<KD> keyDecoderClass, Class<VD> valueDecoderClass, java.util.Map<String,String> kafkaParams, OffsetRange[] offsetRanges) Create a RDD from Kafka using offset ranges for each topic and partition. Parameters:jsc - JavaSparkContext objectkafkaParams - Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s) (NOT zookeeper servers) specified in host1:port1,host2:port2 form.offsetRanges - Each OffsetRange in the batch corresponds to a range of offsets for a given Kafka topic/partitionkeyClass - type of Kafka message keyvalueClass - type of Kafka message valuekeyDecoderClass - type of Kafka message key decodervalueDecoderClass - type of Kafka message value decoder Returns:RDD of (Kafka message key, Kafka message value) createRDD public static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>,R> JavaRDD<R> createRDD(JavaSparkContext jsc, Class<K> keyClass, Class<V> valueClass, Class<KD> keyDecoderClass, Class<VD> valueDecoderClass, Class<R> recordClass, java.util.Map<String,String> kafkaParams, OffsetRange[] offsetRanges, java.util.Map<kafka.common.TopicAndPartition,Broker> leaders, Function<kafka.message.MessageAndMetadata<K,V>,R> messageHandler) Create a RDD from Kafka using offset ranges for each topic and partition. This allows you specify the Kafka leader to connect to (to optimize fetching) and access the message as well as the metadata. Parameters:jsc - JavaSparkContext objectkafkaParams - Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s) (NOT zookeeper servers) specified in host1:port1,host2:port2 form.offsetRanges - Each OffsetRange in the batch corresponds to a range of offsets for a given Kafka topic/partitionleaders - Kafka brokers for each TopicAndPartition in offsetRanges. May be an empty map, in which case leaders will be looked up on the driver.messageHandler - Function for translating each message and metadata into the desired typekeyClass - (undocumented)valueClass - (undocumented)keyDecoderClass - (undocumented)valueDecoderClass - (undocumented)recordClass - (undocumented) Returns:RDD of R createDirectStream public static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>,R> InputDStream<R> createDirectStream(StreamingContext ssc, scala.collection.immutable.Map<String,String> kafkaParams, scala.collection.immutable.Map<kafka.common.TopicAndPartition,Object> fromOffsets, scala.Function1<kafka.message.MessageAndMetadata<K,V>,R> messageHandler, scala.reflect.ClassTag<K> evidence$14, scala.reflect.ClassTag<V> evidence$15, scala.reflect.ClassTag<KD> evidence$16, scala.reflect.ClassTag<VD> evidence$17, scala.reflect.ClassTag<R> evidence$18) Create an input stream that directly pulls messages from Kafka Brokers without using any receiver. This stream can guarantee that each message from Kafka is included in transformations exactly once (see points below). Points to note: - No receivers: This stream does not use any receiver. It directly queries Kafka - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked by the stream itself. For interoperability with Kafka monitoring tools that depend on Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application. You can access the offsets used in each batch from the generated RDDs (see HasOffsetRanges). - Failure Recovery: To recover from driver failures, you have to enable checkpointing in the StreamingContext. The information on consumed offset can be recovered from the checkpoint. See the programming guide for details (constraints, etc.). - End-to-end semantics: This stream ensures that every records is effectively received and transformed exactly once, but gives no guarantees on whether the transformed data are outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure that the output operation is idempotent, or use transactions to output records atomically. See the programming guide for more details. Parameters:ssc - StreamingContext objectkafkaParams - Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s) (NOT zookeeper servers) specified in host1:port1,host2:port2 form.fromOffsets - Per-topic/partition Kafka offsets defining the (inclusive) starting point of the streammessageHandler - Function for translating each message and metadata into the desired typeevidence$14 - (undocumented)evidence$15 - (undocumented)evidence$16 - (undocumented)evidence$17 - (undocumented)evidence$18 - (undocumented) Returns:DStream of R createDirectStream public static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>> InputDStream<scala.Tuple2<K,V>> createDirectStream(StreamingContext ssc, scala.collection.immutable.Map<String,String> kafkaParams, scala.collection.immutable.Set<String> topics, scala.reflect.ClassTag<K> evidence$19, scala.reflect.ClassTag<V> evidence$20, scala.reflect.ClassTag<KD> evidence$21, scala.reflect.ClassTag<VD> evidence$22) Create an input stream that directly pulls messages from Kafka Brokers without using any receiver. This stream can guarantee that each message from Kafka is included in transformations exactly once (see points below). Points to note: - No receivers: This stream does not use any receiver. It directly queries Kafka - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked by the stream itself. For interoperability with Kafka monitoring tools that depend on Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application. You can access the offsets used in each batch from the generated RDDs (see HasOffsetRanges). - Failure Recovery: To recover from driver failures, you have to enable checkpointing in the StreamingContext. The information on consumed offset can be recovered from the checkpoint. See the programming guide for details (constraints, etc.). - End-to-end semantics: This stream ensures that every records is effectively received and transformed exactly once, but gives no guarantees on whether the transformed data are outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure that the output operation is idempotent, or use transactions to output records atomically. See the programming guide for more details. Parameters:ssc - StreamingContext objectkafkaParams - Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s) (NOT zookeeper servers), specified in host1:port1,host2:port2 form. If not starting from a checkpoint, "auto.offset.reset" may be set to "largest" or "smallest" to determine where the stream starts (defaults to "largest")topics - Names of the topics to consumeevidence$19 - (undocumented)evidence$20 - (undocumented)evidence$21 - (undocumented)evidence$22 - (undocumented) Returns:DStream of (Kafka message key, Kafka message value) createDirectStream public static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>,R> JavaInputDStream<R> createDirectStream(JavaStreamingContext jssc, Class<K> keyClass, Class<V> valueClass, Class<KD> keyDecoderClass, Class<VD> valueDecoderClass, Class<R> recordClass, java.util.Map<String,String> kafkaParams, java.util.Map<kafka.common.TopicAndPartition,Long> fromOffsets, Function<kafka.message.MessageAndMetadata<K,V>,R> messageHandler) Create an input stream that directly pulls messages from Kafka Brokers without using any receiver. This stream can guarantee that each message from Kafka is included in transformations exactly once (see points below). Points to note: - No receivers: This stream does not use any receiver. It directly queries Kafka - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked by the stream itself. For interoperability with Kafka monitoring tools that depend on Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application. You can access the offsets used in each batch from the generated RDDs (see HasOffsetRanges). - Failure Recovery: To recover from driver failures, you have to enable checkpointing in the StreamingContext. The information on consumed offset can be recovered from the checkpoint. See the programming guide for details (constraints, etc.). - End-to-end semantics: This stream ensures that every records is effectively received and transformed exactly once, but gives no guarantees on whether the transformed data are outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure that the output operation is idempotent, or use transactions to output records atomically. See the programming guide for more details. Parameters:jssc - JavaStreamingContext objectkeyClass - Class of the keys in the Kafka recordsvalueClass - Class of the values in the Kafka recordskeyDecoderClass - Class of the key decodervalueDecoderClass - Class of the value decoderrecordClass - Class of the records in DStreamkafkaParams - Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s) (NOT zookeeper servers), specified in host1:port1,host2:port2 form.fromOffsets - Per-topic/partition Kafka offsets defining the (inclusive) starting point of the streammessageHandler - Function for translating each message and metadata into the desired type Returns:DStream of R createDirectStream public static <K,V,KD extends kafka.serializer.Decoder<K>,VD extends kafka.serializer.Decoder<V>> JavaPairInputDStream<K,V> createDirectStream(JavaStreamingContext jssc, Class<K> keyClass, Class<V> valueClass, Class<KD> keyDecoderClass, Class<VD> valueDecoderClass, java.util.Map<String,String> kafkaParams, java.util.Set<String> topics) Create an input stream that directly pulls messages from Kafka Brokers without using any receiver. This stream can guarantee that each message from Kafka is included in transformations exactly once (see points below). Points to note: - No receivers: This stream does not use any receiver. It directly queries Kafka - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked by the stream itself. For interoperability with Kafka monitoring tools that depend on Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application. You can access the offsets used in each batch from the generated RDDs (see HasOffsetRanges). - Failure Recovery: To recover from driver failures, you have to enable checkpointing in the StreamingContext. The information on consumed offset can be recovered from the checkpoint. See the programming guide for details (constraints, etc.). - End-to-end semantics: This stream ensures that every records is effectively received and transformed exactly once, but gives no guarantees on whether the transformed data are outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure that the output operation is idempotent, or use transactions to output records atomically. See the programming guide for more details. Parameters:jssc - JavaStreamingContext objectkeyClass - Class of the keys in the Kafka recordsvalueClass - Class of the values in the Kafka recordskeyDecoderClass - Class of the key decodervalueDecoderClass - Class type of the value decoderkafkaParams - Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s) (NOT zookeeper servers), specified in host1:port1,host2:port2 form. If not starting from a checkpoint, "auto.offset.reset" may be set to "largest" or "smallest" to determine where the stream starts (defaults to "largest")topics - Names of the topics to consume Returns:DStream of (Kafka message key, Kafka message value) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KernelDensity (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KernelDensity (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat Class KernelDensity Object org.apache.spark.mllib.stat.KernelDensity All Implemented Interfaces: java.io.Serializable public class KernelDensity extends Object implements scala.Serializable Kernel density estimation. Given a sample from a population, estimate its probability density function at each of the given evaluation points using kernels. Only Gaussian kernel is supported. Scala example: val sample = sc.parallelize(Seq(0.0, 1.0, 4.0, 4.0)) val kd = new KernelDensity() .setSample(sample) .setBandwidth(3.0) val densities = kd.estimate(Array(-1.0, 2.0, 5.0)) See Also:Serialized Form Constructor Summary Constructors  Constructor and Description KernelDensity()  Method Summary Methods  Modifier and Type Method and Description double[] estimate(double[] points) Estimates probability density function at the given array of points. static double normPdf(double mean, double standardDeviation, double logStandardDeviationPlusHalfLog2Pi, double x) Evaluates the PDF of a normal distribution. KernelDensity setBandwidth(double bandwidth) Sets the bandwidth (standard deviation) of the Gaussian kernel (default: 1.0). KernelDensity setSample(JavaRDD<Double> sample) Sets the sample to use for density estimation (for Java users). KernelDensity setSample(RDD<Object> sample) Sets the sample to use for density estimation. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail KernelDensity public KernelDensity() Method Detail normPdf public static double normPdf(double mean, double standardDeviation, double logStandardDeviationPlusHalfLog2Pi, double x) Evaluates the PDF of a normal distribution. setBandwidth public KernelDensity setBandwidth(double bandwidth) Sets the bandwidth (standard deviation) of the Gaussian kernel (default: 1.0). Parameters:bandwidth - (undocumented) Returns:(undocumented) setSample public KernelDensity setSample(RDD<Object> sample) Sets the sample to use for density estimation. Parameters:sample - (undocumented) Returns:(undocumented) setSample public KernelDensity setSample(JavaRDD<Double> sample) Sets the sample to use for density estimation (for Java users). Parameters:sample - (undocumented) Returns:(undocumented) estimate public double[] estimate(double[] points) Estimates probability density function at the given array of points. Parameters:points - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KeyValueGroupedDataset (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KeyValueGroupedDataset (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class KeyValueGroupedDataset<K,V> Object org.apache.spark.sql.KeyValueGroupedDataset<K,V> All Implemented Interfaces: java.io.Serializable public class KeyValueGroupedDataset<K,V> extends Object implements scala.Serializable :: Experimental :: A Dataset has been logically grouped by a user specified grouping key. Users should not construct a KeyValueGroupedDataset directly, but should instead call groupByKey on an existing Dataset. Since: 2.0.0 See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description <U1> Dataset<scala.Tuple2<K,U1>> agg(TypedColumn<V,U1> col1) Computes the given aggregation, returning a Dataset of tuples for each unique key and the result of computing this aggregation over all elements in the group. <U1,U2> Dataset<scala.Tuple3<K,U1,U2>> agg(TypedColumn<V,U1> col1, TypedColumn<V,U2> col2) Computes the given aggregations, returning a Dataset of tuples for each unique key and the result of computing these aggregations over all elements in the group. <U1,U2,U3> Dataset<scala.Tuple4<K,U1,U2,U3>> agg(TypedColumn<V,U1> col1, TypedColumn<V,U2> col2, TypedColumn<V,U3> col3) Computes the given aggregations, returning a Dataset of tuples for each unique key and the result of computing these aggregations over all elements in the group. <U1,U2,U3,U4> Dataset<scala.Tuple5<K,U1,U2,U3,U4>> agg(TypedColumn<V,U1> col1, TypedColumn<V,U2> col2, TypedColumn<V,U3> col3, TypedColumn<V,U4> col4) Computes the given aggregations, returning a Dataset of tuples for each unique key and the result of computing these aggregations over all elements in the group. <U,R> Dataset<R> cogroup(KeyValueGroupedDataset<K,U> other, CoGroupFunction<K,V,U,R> f, Encoder<R> encoder) Applies the given function to each cogrouped data. <U,R> Dataset<R> cogroup(KeyValueGroupedDataset<K,U> other, scala.Function3<K,scala.collection.Iterator<V>,scala.collection.Iterator<U>,scala.collection.TraversableOnce<R>> f, Encoder<R> evidence$4) Applies the given function to each cogrouped data. Dataset<scala.Tuple2<K,Object>> count() Returns a Dataset that contains a tuple with each key and the number of items present for that key. <U> Dataset<U> flatMapGroups(FlatMapGroupsFunction<K,V,U> f, Encoder<U> encoder) Applies the given function to each group of data. <U> Dataset<U> flatMapGroups(scala.Function2<K,scala.collection.Iterator<V>,scala.collection.TraversableOnce<U>> f, Encoder<U> evidence$2) Applies the given function to each group of data. <L> KeyValueGroupedDataset<L,V> keyAs(Encoder<L> evidence$1) Returns a new KeyValueGroupedDataset where the type of the key has been mapped to the specified type. Dataset<K> keys() Returns a Dataset that contains each unique key. <U> Dataset<U> mapGroups(scala.Function2<K,scala.collection.Iterator<V>,U> f, Encoder<U> evidence$3) Applies the given function to each group of data. <U> Dataset<U> mapGroups(MapGroupsFunction<K,V,U> f, Encoder<U> encoder) Applies the given function to each group of data. org.apache.spark.sql.execution.QueryExecution queryExecution()  Dataset<scala.Tuple2<K,V>> reduceGroups(scala.Function2<V,V,V> f) Reduces the elements of each group of data using the specified binary function. Dataset<scala.Tuple2<K,V>> reduceGroups(ReduceFunction<V> f) Reduces the elements of each group of data using the specified binary function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail queryExecution public org.apache.spark.sql.execution.QueryExecution queryExecution() keyAs public <L> KeyValueGroupedDataset<L,V> keyAs(Encoder<L> evidence$1) Returns a new KeyValueGroupedDataset where the type of the key has been mapped to the specified type. The mapping of key columns to the type follows the same rules as as on Dataset. Parameters:evidence$1 - (undocumented) Returns:(undocumented)Since: 1.6.0 keys public Dataset<K> keys() Returns a Dataset that contains each unique key. This is equivalent to doing mapping over the Dataset to extract the keys and then running a distinct operation on those. Returns:(undocumented)Since: 1.6.0 flatMapGroups public <U> Dataset<U> flatMapGroups(scala.Function2<K,scala.collection.Iterator<V>,scala.collection.TraversableOnce<U>> f, Encoder<U> evidence$2) Applies the given function to each group of data. For each unique group, the function will be passed the group key and an iterator that contains all of the elements in the group. The function can return an iterator containing elements of an arbitrary type which will be returned as a new Dataset. This function does not support partial aggregation, and as a result requires shuffling all the data in the Dataset. If an application intends to perform an aggregation over each key, it is best to use the reduce function or an Aggregator. Internally, the implementation will spill to disk if any given group is too large to fit into memory. However, users must take care to avoid materializing the whole iterator for a group (for example, by calling toList) unless they are sure that this is possible given the memory constraints of their cluster. Parameters:f - (undocumented)evidence$2 - (undocumented) Returns:(undocumented)Since: 1.6.0 flatMapGroups public <U> Dataset<U> flatMapGroups(FlatMapGroupsFunction<K,V,U> f, Encoder<U> encoder) Applies the given function to each group of data. For each unique group, the function will be passed the group key and an iterator that contains all of the elements in the group. The function can return an iterator containing elements of an arbitrary type which will be returned as a new Dataset. This function does not support partial aggregation, and as a result requires shuffling all the data in the Dataset. If an application intends to perform an aggregation over each key, it is best to use the reduce function or an Aggregator. Internally, the implementation will spill to disk if any given group is too large to fit into memory. However, users must take care to avoid materializing the whole iterator for a group (for example, by calling toList) unless they are sure that this is possible given the memory constraints of their cluster. Parameters:f - (undocumented)encoder - (undocumented) Returns:(undocumented)Since: 1.6.0 mapGroups public <U> Dataset<U> mapGroups(scala.Function2<K,scala.collection.Iterator<V>,U> f, Encoder<U> evidence$3) Applies the given function to each group of data. For each unique group, the function will be passed the group key and an iterator that contains all of the elements in the group. The function can return an element of arbitrary type which will be returned as a new Dataset. This function does not support partial aggregation, and as a result requires shuffling all the data in the Dataset. If an application intends to perform an aggregation over each key, it is best to use the reduce function or an Aggregator. Internally, the implementation will spill to disk if any given group is too large to fit into memory. However, users must take care to avoid materializing the whole iterator for a group (for example, by calling toList) unless they are sure that this is possible given the memory constraints of their cluster. Parameters:f - (undocumented)evidence$3 - (undocumented) Returns:(undocumented)Since: 1.6.0 mapGroups public <U> Dataset<U> mapGroups(MapGroupsFunction<K,V,U> f, Encoder<U> encoder) Applies the given function to each group of data. For each unique group, the function will be passed the group key and an iterator that contains all of the elements in the group. The function can return an element of arbitrary type which will be returned as a new Dataset. This function does not support partial aggregation, and as a result requires shuffling all the data in the Dataset. If an application intends to perform an aggregation over each key, it is best to use the reduce function or an Aggregator. Internally, the implementation will spill to disk if any given group is too large to fit into memory. However, users must take care to avoid materializing the whole iterator for a group (for example, by calling toList) unless they are sure that this is possible given the memory constraints of their cluster. Parameters:f - (undocumented)encoder - (undocumented) Returns:(undocumented)Since: 1.6.0 reduceGroups public Dataset<scala.Tuple2<K,V>> reduceGroups(scala.Function2<V,V,V> f) Reduces the elements of each group of data using the specified binary function. The given function must be commutative and associative or the result may be non-deterministic. Parameters:f - (undocumented) Returns:(undocumented)Since: 1.6.0 reduceGroups public Dataset<scala.Tuple2<K,V>> reduceGroups(ReduceFunction<V> f) Reduces the elements of each group of data using the specified binary function. The given function must be commutative and associative or the result may be non-deterministic. Parameters:f - (undocumented) Returns:(undocumented)Since: 1.6.0 agg public <U1> Dataset<scala.Tuple2<K,U1>> agg(TypedColumn<V,U1> col1) Computes the given aggregation, returning a Dataset of tuples for each unique key and the result of computing this aggregation over all elements in the group. Parameters:col1 - (undocumented) Returns:(undocumented)Since: 1.6.0 agg public <U1,U2> Dataset<scala.Tuple3<K,U1,U2>> agg(TypedColumn<V,U1> col1, TypedColumn<V,U2> col2) Computes the given aggregations, returning a Dataset of tuples for each unique key and the result of computing these aggregations over all elements in the group. Parameters:col1 - (undocumented)col2 - (undocumented) Returns:(undocumented)Since: 1.6.0 agg public <U1,U2,U3> Dataset<scala.Tuple4<K,U1,U2,U3>> agg(TypedColumn<V,U1> col1, TypedColumn<V,U2> col2, TypedColumn<V,U3> col3) Computes the given aggregations, returning a Dataset of tuples for each unique key and the result of computing these aggregations over all elements in the group. Parameters:col1 - (undocumented)col2 - (undocumented)col3 - (undocumented) Returns:(undocumented)Since: 1.6.0 agg public <U1,U2,U3,U4> Dataset<scala.Tuple5<K,U1,U2,U3,U4>> agg(TypedColumn<V,U1> col1, TypedColumn<V,U2> col2, TypedColumn<V,U3> col3, TypedColumn<V,U4> col4) Computes the given aggregations, returning a Dataset of tuples for each unique key and the result of computing these aggregations over all elements in the group. Parameters:col1 - (undocumented)col2 - (undocumented)col3 - (undocumented)col4 - (undocumented) Returns:(undocumented)Since: 1.6.0 count public Dataset<scala.Tuple2<K,Object>> count() Returns a Dataset that contains a tuple with each key and the number of items present for that key. Returns:(undocumented)Since: 1.6.0 cogroup public <U,R> Dataset<R> cogroup(KeyValueGroupedDataset<K,U> other, scala.Function3<K,scala.collection.Iterator<V>,scala.collection.Iterator<U>,scala.collection.TraversableOnce<R>> f, Encoder<R> evidence$4) Applies the given function to each cogrouped data. For each unique group, the function will be passed the grouping key and 2 iterators containing all elements in the group from Dataset this and other. The function can return an iterator containing elements of an arbitrary type which will be returned as a new Dataset. Parameters:other - (undocumented)f - (undocumented)evidence$4 - (undocumented) Returns:(undocumented)Since: 1.6.0 cogroup public <U,R> Dataset<R> cogroup(KeyValueGroupedDataset<K,U> other, CoGroupFunction<K,V,U,R> f, Encoder<R> encoder) Applies the given function to each cogrouped data. For each unique group, the function will be passed the grouping key and 2 iterators containing all elements in the group from Dataset this and other. The function can return an iterator containing elements of an arbitrary type which will be returned as a new Dataset. Parameters:other - (undocumented)f - (undocumented)encoder - (undocumented) Returns:(undocumented)Since: 1.6.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KillTask (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KillTask (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.local Class KillTask Object org.apache.spark.scheduler.local.KillTask All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class KillTask extends Object implements scala.Product, scala.Serializable See Also:Serialized Form Constructor Summary Constructors  Constructor and Description KillTask(long taskId, boolean interruptThread)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  boolean interruptThread()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  long taskId()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail KillTask public KillTask(long taskId, boolean interruptThread) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() taskId public long taskId() interruptThread public boolean interruptThread() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KinesisUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KinesisUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kinesis Class KinesisUtils Object org.apache.spark.streaming.kinesis.KinesisUtils public class KinesisUtils extends Object Constructor Summary Constructors  Constructor and Description KinesisUtils()  Method Summary Methods  Modifier and Type Method and Description static JavaReceiverInputDStream<byte[]> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel) Create an input stream that pulls messages from a Kinesis stream. static <T> JavaReceiverInputDStream<T> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, Function<com.amazonaws.services.kinesis.model.Record,T> messageHandler, Class<T> recordClass) Create an input stream that pulls messages from a Kinesis stream. static <T> JavaReceiverInputDStream<T> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, Function<com.amazonaws.services.kinesis.model.Record,T> messageHandler, Class<T> recordClass, String awsAccessKeyId, String awsSecretKey) Create an input stream that pulls messages from a Kinesis stream. static JavaReceiverInputDStream<byte[]> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, String awsAccessKeyId, String awsSecretKey) Create an input stream that pulls messages from a Kinesis stream. static ReceiverInputDStream<byte[]> createStream(StreamingContext ssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel) Create an input stream that pulls messages from a Kinesis stream. static <T> ReceiverInputDStream<T> createStream(StreamingContext ssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, scala.Function1<com.amazonaws.services.kinesis.model.Record,T> messageHandler, scala.reflect.ClassTag<T> evidence$1) Create an input stream that pulls messages from a Kinesis stream. static <T> ReceiverInputDStream<T> createStream(StreamingContext ssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, scala.Function1<com.amazonaws.services.kinesis.model.Record,T> messageHandler, String awsAccessKeyId, String awsSecretKey, scala.reflect.ClassTag<T> evidence$2) Create an input stream that pulls messages from a Kinesis stream. static ReceiverInputDStream<byte[]> createStream(StreamingContext ssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, String awsAccessKeyId, String awsSecretKey) Create an input stream that pulls messages from a Kinesis stream. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail KinesisUtils public KinesisUtils() Method Detail createStream public static <T> ReceiverInputDStream<T> createStream(StreamingContext ssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, scala.Function1<com.amazonaws.services.kinesis.model.Record,T> messageHandler, scala.reflect.ClassTag<T> evidence$1) Create an input stream that pulls messages from a Kinesis stream. This uses the Kinesis Client Library (KCL) to pull messages from Kinesis. Note: The AWS credentials will be discovered using the DefaultAWSCredentialsProviderChain on the workers. See AWS documentation to understand how DefaultAWSCredentialsProviderChain gets the AWS credentials. Parameters:ssc - StreamingContext objectkinesisAppName - Kinesis application name used by the Kinesis Client Library (KCL) to update DynamoDBstreamName - Kinesis stream nameendpointUrl - Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)regionName - Name of region used by the Kinesis Client Library (KCL) to update DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)initialPositionInStream - In the absence of Kinesis checkpoint info, this is the worker's initial starting position in the stream. The values are either the beginning of the stream per Kinesis' limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or the tip of the stream (InitialPositionInStream.LATEST).checkpointInterval - Checkpoint interval for Kinesis checkpointing. See the Kinesis Spark Streaming documentation for more details on the different types of checkpoints.storageLevel - Storage level to use for storing the received objects. StorageLevel.MEMORY_AND_DISK_2 is recommended.messageHandler - A custom message handler that can generate a generic output from a Kinesis Record, which contains both message data, and metadata.evidence$1 - (undocumented) Returns:(undocumented) createStream public static <T> ReceiverInputDStream<T> createStream(StreamingContext ssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, scala.Function1<com.amazonaws.services.kinesis.model.Record,T> messageHandler, String awsAccessKeyId, String awsSecretKey, scala.reflect.ClassTag<T> evidence$2) Create an input stream that pulls messages from a Kinesis stream. This uses the Kinesis Client Library (KCL) to pull messages from Kinesis. Note: The given AWS credentials will get saved in DStream checkpoints if checkpointing is enabled. Make sure that your checkpoint directory is secure. Parameters:ssc - StreamingContext objectkinesisAppName - Kinesis application name used by the Kinesis Client Library (KCL) to update DynamoDBstreamName - Kinesis stream nameendpointUrl - Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)regionName - Name of region used by the Kinesis Client Library (KCL) to update DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)initialPositionInStream - In the absence of Kinesis checkpoint info, this is the worker's initial starting position in the stream. The values are either the beginning of the stream per Kinesis' limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or the tip of the stream (InitialPositionInStream.LATEST).checkpointInterval - Checkpoint interval for Kinesis checkpointing. See the Kinesis Spark Streaming documentation for more details on the different types of checkpoints.storageLevel - Storage level to use for storing the received objects. StorageLevel.MEMORY_AND_DISK_2 is recommended.messageHandler - A custom message handler that can generate a generic output from a Kinesis Record, which contains both message data, and metadata.awsAccessKeyId - AWS AccessKeyId (if null, will use DefaultAWSCredentialsProviderChain)awsSecretKey - AWS SecretKey (if null, will use DefaultAWSCredentialsProviderChain)evidence$2 - (undocumented) Returns:(undocumented) createStream public static ReceiverInputDStream<byte[]> createStream(StreamingContext ssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel) Create an input stream that pulls messages from a Kinesis stream. This uses the Kinesis Client Library (KCL) to pull messages from Kinesis. Note: The AWS credentials will be discovered using the DefaultAWSCredentialsProviderChain on the workers. See AWS documentation to understand how DefaultAWSCredentialsProviderChain gets the AWS credentials. Parameters:ssc - StreamingContext objectkinesisAppName - Kinesis application name used by the Kinesis Client Library (KCL) to update DynamoDBstreamName - Kinesis stream nameendpointUrl - Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)regionName - Name of region used by the Kinesis Client Library (KCL) to update DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)initialPositionInStream - In the absence of Kinesis checkpoint info, this is the worker's initial starting position in the stream. The values are either the beginning of the stream per Kinesis' limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or the tip of the stream (InitialPositionInStream.LATEST).checkpointInterval - Checkpoint interval for Kinesis checkpointing. See the Kinesis Spark Streaming documentation for more details on the different types of checkpoints.storageLevel - Storage level to use for storing the received objects. StorageLevel.MEMORY_AND_DISK_2 is recommended. Returns:(undocumented) createStream public static ReceiverInputDStream<byte[]> createStream(StreamingContext ssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, String awsAccessKeyId, String awsSecretKey) Create an input stream that pulls messages from a Kinesis stream. This uses the Kinesis Client Library (KCL) to pull messages from Kinesis. Note: The given AWS credentials will get saved in DStream checkpoints if checkpointing is enabled. Make sure that your checkpoint directory is secure. Parameters:ssc - StreamingContext objectkinesisAppName - Kinesis application name used by the Kinesis Client Library (KCL) to update DynamoDBstreamName - Kinesis stream nameendpointUrl - Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)regionName - Name of region used by the Kinesis Client Library (KCL) to update DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)initialPositionInStream - In the absence of Kinesis checkpoint info, this is the worker's initial starting position in the stream. The values are either the beginning of the stream per Kinesis' limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or the tip of the stream (InitialPositionInStream.LATEST).checkpointInterval - Checkpoint interval for Kinesis checkpointing. See the Kinesis Spark Streaming documentation for more details on the different types of checkpoints.storageLevel - Storage level to use for storing the received objects. StorageLevel.MEMORY_AND_DISK_2 is recommended.awsAccessKeyId - AWS AccessKeyId (if null, will use DefaultAWSCredentialsProviderChain)awsSecretKey - AWS SecretKey (if null, will use DefaultAWSCredentialsProviderChain) Returns:(undocumented) createStream public static <T> JavaReceiverInputDStream<T> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, Function<com.amazonaws.services.kinesis.model.Record,T> messageHandler, Class<T> recordClass) Create an input stream that pulls messages from a Kinesis stream. This uses the Kinesis Client Library (KCL) to pull messages from Kinesis. Note: The AWS credentials will be discovered using the DefaultAWSCredentialsProviderChain on the workers. See AWS documentation to understand how DefaultAWSCredentialsProviderChain gets the AWS credentials. Parameters:jssc - Java StreamingContext objectkinesisAppName - Kinesis application name used by the Kinesis Client Library (KCL) to update DynamoDBstreamName - Kinesis stream nameendpointUrl - Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)regionName - Name of region used by the Kinesis Client Library (KCL) to update DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)initialPositionInStream - In the absence of Kinesis checkpoint info, this is the worker's initial starting position in the stream. The values are either the beginning of the stream per Kinesis' limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or the tip of the stream (InitialPositionInStream.LATEST).checkpointInterval - Checkpoint interval for Kinesis checkpointing. See the Kinesis Spark Streaming documentation for more details on the different types of checkpoints.storageLevel - Storage level to use for storing the received objects. StorageLevel.MEMORY_AND_DISK_2 is recommended.messageHandler - A custom message handler that can generate a generic output from a Kinesis Record, which contains both message data, and metadata.recordClass - Class of the records in DStream Returns:(undocumented) createStream public static <T> JavaReceiverInputDStream<T> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, Function<com.amazonaws.services.kinesis.model.Record,T> messageHandler, Class<T> recordClass, String awsAccessKeyId, String awsSecretKey) Create an input stream that pulls messages from a Kinesis stream. This uses the Kinesis Client Library (KCL) to pull messages from Kinesis. Note: The given AWS credentials will get saved in DStream checkpoints if checkpointing is enabled. Make sure that your checkpoint directory is secure. Parameters:jssc - Java StreamingContext objectkinesisAppName - Kinesis application name used by the Kinesis Client Library (KCL) to update DynamoDBstreamName - Kinesis stream nameendpointUrl - Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)regionName - Name of region used by the Kinesis Client Library (KCL) to update DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)initialPositionInStream - In the absence of Kinesis checkpoint info, this is the worker's initial starting position in the stream. The values are either the beginning of the stream per Kinesis' limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or the tip of the stream (InitialPositionInStream.LATEST).checkpointInterval - Checkpoint interval for Kinesis checkpointing. See the Kinesis Spark Streaming documentation for more details on the different types of checkpoints.storageLevel - Storage level to use for storing the received objects. StorageLevel.MEMORY_AND_DISK_2 is recommended.messageHandler - A custom message handler that can generate a generic output from a Kinesis Record, which contains both message data, and metadata.recordClass - Class of the records in DStreamawsAccessKeyId - AWS AccessKeyId (if null, will use DefaultAWSCredentialsProviderChain)awsSecretKey - AWS SecretKey (if null, will use DefaultAWSCredentialsProviderChain) Returns:(undocumented) createStream public static JavaReceiverInputDStream<byte[]> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel) Create an input stream that pulls messages from a Kinesis stream. This uses the Kinesis Client Library (KCL) to pull messages from Kinesis. Note: The AWS credentials will be discovered using the DefaultAWSCredentialsProviderChain on the workers. See AWS documentation to understand how DefaultAWSCredentialsProviderChain gets the AWS credentials. Parameters:jssc - Java StreamingContext objectkinesisAppName - Kinesis application name used by the Kinesis Client Library (KCL) to update DynamoDBstreamName - Kinesis stream nameendpointUrl - Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)regionName - Name of region used by the Kinesis Client Library (KCL) to update DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)initialPositionInStream - In the absence of Kinesis checkpoint info, this is the worker's initial starting position in the stream. The values are either the beginning of the stream per Kinesis' limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or the tip of the stream (InitialPositionInStream.LATEST).checkpointInterval - Checkpoint interval for Kinesis checkpointing. See the Kinesis Spark Streaming documentation for more details on the different types of checkpoints.storageLevel - Storage level to use for storing the received objects. StorageLevel.MEMORY_AND_DISK_2 is recommended. Returns:(undocumented) createStream public static JavaReceiverInputDStream<byte[]> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, String awsAccessKeyId, String awsSecretKey) Create an input stream that pulls messages from a Kinesis stream. This uses the Kinesis Client Library (KCL) to pull messages from Kinesis. Note: The given AWS credentials will get saved in DStream checkpoints if checkpointing is enabled. Make sure that your checkpoint directory is secure. Parameters:jssc - Java StreamingContext objectkinesisAppName - Kinesis application name used by the Kinesis Client Library (KCL) to update DynamoDBstreamName - Kinesis stream nameendpointUrl - Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)regionName - Name of region used by the Kinesis Client Library (KCL) to update DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)initialPositionInStream - In the absence of Kinesis checkpoint info, this is the worker's initial starting position in the stream. The values are either the beginning of the stream per Kinesis' limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or the tip of the stream (InitialPositionInStream.LATEST).checkpointInterval - Checkpoint interval for Kinesis checkpointing. See the Kinesis Spark Streaming documentation for more details on the different types of checkpoints.storageLevel - Storage level to use for storing the received objects. StorageLevel.MEMORY_AND_DISK_2 is recommended.awsAccessKeyId - AWS AccessKeyId (if null, will use DefaultAWSCredentialsProviderChain)awsSecretKey - AWS SecretKey (if null, will use DefaultAWSCredentialsProviderChain) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KinesisUtilsPythonHelper (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KinesisUtilsPythonHelper (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kinesis Class KinesisUtilsPythonHelper Object org.apache.spark.streaming.kinesis.KinesisUtilsPythonHelper public class KinesisUtilsPythonHelper extends Object This is a helper class that wraps the methods in KinesisUtils into more Python-friendly class and function so that it can be easily instantiated and called from Python's KinesisUtils. Constructor Summary Constructors  Constructor and Description KinesisUtilsPythonHelper()  Method Summary Methods  Modifier and Type Method and Description JavaReceiverInputDStream<byte[]> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, int initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, String awsAccessKeyId, String awsSecretKey)  com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream getInitialPositionInStream(int initialPositionInStream)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail KinesisUtilsPythonHelper public KinesisUtilsPythonHelper() Method Detail getInitialPositionInStream public com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream getInitialPositionInStream(int initialPositionInStream) createStream public JavaReceiverInputDStream<byte[]> createStream(JavaStreamingContext jssc, String kinesisAppName, String streamName, String endpointUrl, String regionName, int initialPositionInStream, Duration checkpointInterval, StorageLevel storageLevel, String awsAccessKeyId, String awsSecretKey) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KolmogorovSmirnovTest.NullHypothesis$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KolmogorovSmirnovTest.NullHypothesis$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.test Class KolmogorovSmirnovTest.NullHypothesis$ Object scala.Enumeration org.apache.spark.mllib.stat.test.KolmogorovSmirnovTest.NullHypothesis$ All Implemented Interfaces: java.io.Serializable Enclosing class: KolmogorovSmirnovTest public static class KolmogorovSmirnovTest.NullHypothesis$ extends scala.Enumeration See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from class scala.Enumeration scala.Enumeration.Val, scala.Enumeration.Value, scala.Enumeration.ValueOrdering$, scala.Enumeration.ValueSet, scala.Enumeration.ValueSet$ Field Summary Fields  Modifier and Type Field and Description static KolmogorovSmirnovTest.NullHypothesis$ MODULE$ Static reference to the singleton instance of this Scala object. Fields inherited from class scala.Enumeration serialVersionUID Constructor Summary Constructors  Constructor and Description KolmogorovSmirnovTest.NullHypothesis$()  Method Summary Methods  Modifier and Type Method and Description scala.Enumeration.Value OneSampleTwoSided()  Methods inherited from class scala.Enumeration apply, maxId, nextId_$eq, nextId, nextName_$eq, nextName, readResolve, scala$Enumeration$$bottomId_$eq, scala$Enumeration$$bottomId, scala$Enumeration$$isValDef$1, scala$Enumeration$$nameOf, scala$Enumeration$$nextNameOrNull, scala$Enumeration$$nmap, scala$Enumeration$$populateNameMap, scala$Enumeration$$topId_$eq, scala$Enumeration$$topId, scala$Enumeration$$vmap, scala$Enumeration$$vsetDefined_$eq, toString, Value, Value, Value, Value, ValueOrdering, values, ValueSet, withName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Field Detail MODULE$ public static final KolmogorovSmirnovTest.NullHypothesis$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail KolmogorovSmirnovTest.NullHypothesis$ public KolmogorovSmirnovTest.NullHypothesis$() Method Detail OneSampleTwoSided public scala.Enumeration.Value OneSampleTwoSided() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KolmogorovSmirnovTest (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KolmogorovSmirnovTest (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.test Class KolmogorovSmirnovTest Object org.apache.spark.mllib.stat.test.KolmogorovSmirnovTest public class KolmogorovSmirnovTest extends Object Conduct the two-sided Kolmogorov Smirnov (KS) test for data sampled from a continuous distribution. By comparing the largest difference between the empirical cumulative distribution of the sample data and the theoretical distribution we can provide a test for the the null hypothesis that the sample data comes from that theoretical distribution. For more information on KS Test: See Also: Implementation note: We seek to implement the KS test with a minimal number of distributed passes. We sort the RDD, and then perform the following operations on a per-partition basis: calculate an empirical cumulative distribution value for each observation, and a theoretical cumulative distribution value. We know the latter to be correct, while the former will be off by a constant (how large the constant is depends on how many values precede it in other partitions). However, given that this constant simply shifts the empirical CDF upwards, but doesn't change its shape, and furthermore, that constant is the same within a given partition, we can pick 2 values in each partition that can potentially resolve to the largest global distance. Namely, we pick the minimum distance and the maximum distance. Additionally, we keep track of how many elements are in each partition. Once these three values have been returned for every partition, we can collect and operate locally. Locally, we can now adjust each distance by the appropriate constant (the cumulative sum of number of elements in the prior partitions divided by the data set size). Finally, we take the maximum absolute value, and this is the statistic. Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  KolmogorovSmirnovTest.NullHypothesis$  Constructor Summary Constructors  Constructor and Description KolmogorovSmirnovTest()  Method Summary Methods  Modifier and Type Method and Description static KolmogorovSmirnovTestResult testOneSample(RDD<Object> data, scala.Function1<Object,Object> cdf)  static KolmogorovSmirnovTestResult testOneSample(RDD<Object> data, org.apache.commons.math3.distribution.RealDistribution distObj)  static KolmogorovSmirnovTestResult testOneSample(RDD<Object> data, String distName, double... params) A convenience function that allows running the KS test for 1 set of sample data against a named distribution static KolmogorovSmirnovTestResult testOneSample(RDD<Object> data, String distName, scala.collection.Seq<Object> params)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail KolmogorovSmirnovTest public KolmogorovSmirnovTest() Method Detail testOneSample public static KolmogorovSmirnovTestResult testOneSample(RDD<Object> data, String distName, double... params) A convenience function that allows running the KS test for 1 set of sample data against a named distribution Parameters:data - the sample data that we wish to evaluatedistName - the name of the theoretical distributionparams - Variable length parameter for distribution's parameters Returns:KolmogorovSmirnovTestResult summarizing the test results (p-value, statistic, and null hypothesis) testOneSample public static KolmogorovSmirnovTestResult testOneSample(RDD<Object> data, scala.Function1<Object,Object> cdf) testOneSample public static KolmogorovSmirnovTestResult testOneSample(RDD<Object> data, org.apache.commons.math3.distribution.RealDistribution distObj) testOneSample public static KolmogorovSmirnovTestResult testOneSample(RDD<Object> data, String distName, scala.collection.Seq<Object> params) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KolmogorovSmirnovTestResult (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KolmogorovSmirnovTestResult (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.test Class KolmogorovSmirnovTestResult Object org.apache.spark.mllib.stat.test.KolmogorovSmirnovTestResult All Implemented Interfaces: TestResult<Object> public class KolmogorovSmirnovTestResult extends Object implements TestResult<Object> Object containing the test results for the Kolmogorov-Smirnov test. Method Summary Methods  Modifier and Type Method and Description int degreesOfFreedom() Returns the degree(s) of freedom of the hypothesis test. String nullHypothesis() Null hypothesis of the test. double pValue() The probability of obtaining a test statistic result at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. double statistic() Test statistic. String toString() String explaining the hypothesis test result. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Method Detail pValue public double pValue() Description copied from interface: TestResult The probability of obtaining a test statistic result at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. Specified by: pValue in interface TestResult<Object> Returns:(undocumented) statistic public double statistic() Description copied from interface: TestResult Test statistic. Specified by: statistic in interface TestResult<Object> Returns:(undocumented) nullHypothesis public String nullHypothesis() Description copied from interface: TestResult Null hypothesis of the test. Specified by: nullHypothesis in interface TestResult<Object> Returns:(undocumented) degreesOfFreedom public int degreesOfFreedom() Description copied from interface: TestResult Returns the degree(s) of freedom of the hypothesis test. Return type should be Number(e.g. Int, Double) or tuples of Numbers for toString compatibility. Specified by: degreesOfFreedom in interface TestResult<Object> Returns:(undocumented) toString public String toString() Description copied from interface: TestResult String explaining the hypothesis test result. Specific classes implementing this trait should override this method to output test-specific information. Specified by: toString in interface TestResult<Object> Overrides: toString in class Object Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KryoRegistrator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KryoRegistrator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.serializer Interface KryoRegistrator public interface KryoRegistrator Interface implemented by clients to register their classes with Kryo when using Kryo serialization. Method Summary Methods  Modifier and Type Method and Description void registerClasses(com.esotericsoftware.kryo.Kryo kryo)  Method Detail registerClasses void registerClasses(com.esotericsoftware.kryo.Kryo kryo) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method KryoSerializer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="KryoSerializer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.serializer Class KryoSerializer Object org.apache.spark.serializer.Serializer org.apache.spark.serializer.KryoSerializer All Implemented Interfaces: java.io.Serializable public class KryoSerializer extends Serializer implements java.io.Serializable A Spark serializer that uses the Kryo serialization library. Note that this serializer is not guaranteed to be wire-compatible across different versions of Spark. It is intended to be used to serialize/de-serialize data within a single Spark application. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description KryoSerializer(SparkConf conf)  Method Summary Methods  Modifier and Type Method and Description int maxBufferSizeMb()  SerializerInstance newInstance() Creates a new SerializerInstance. com.esotericsoftware.kryo.Kryo newKryo()  com.esotericsoftware.kryo.io.Output newKryoOutput()  static Serializer setDefaultClassLoader(ClassLoader classLoader)  Methods inherited from class org.apache.spark.serializer.Serializer setDefaultClassLoader Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail KryoSerializer public KryoSerializer(SparkConf conf) Method Detail setDefaultClassLoader public static Serializer setDefaultClassLoader(ClassLoader classLoader) maxBufferSizeMb public int maxBufferSizeMb() newKryoOutput public com.esotericsoftware.kryo.io.Output newKryoOutput() newKryo public com.esotericsoftware.kryo.Kryo newKryo() newInstance public SerializerInstance newInstance() Description copied from class: Serializer Creates a new SerializerInstance. Specified by: newInstance in class Serializer Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method L1Updater (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="L1Updater (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Class L1Updater Object org.apache.spark.mllib.optimization.Updater org.apache.spark.mllib.optimization.L1Updater All Implemented Interfaces: java.io.Serializable public class L1Updater extends Updater :: DeveloperApi :: Updater for L1 regularized problems. R(w) = ||w||_1 Uses a step-size decreasing with the square root of the number of iterations. Instead of subgradient of the regularizer, the proximal operator for the L1 regularization is applied after the gradient step. This is known to result in better sparsity of the intermediate solution. The corresponding proximal operator for the L1 norm is the soft-thresholding function. That is, each weight component is shrunk towards 0 by shrinkageVal. If w > shrinkageVal, set weight component to w-shrinkageVal. If w < -shrinkageVal, set weight component to w+shrinkageVal. If -shrinkageVal < w < shrinkageVal, set weight component to 0. Equivalently, set weight component to signum(w) * max(0.0, abs(w) - shrinkageVal) See Also:Serialized Form Constructor Summary Constructors  Constructor and Description L1Updater()  Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<Vector,Object> compute(Vector weightsOld, Vector gradient, double stepSize, int iter, double regParam) Compute an updated value for weights given the gradient, stepSize, iteration number and regularization parameter. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail L1Updater public L1Updater() Method Detail compute public scala.Tuple2<Vector,Object> compute(Vector weightsOld, Vector gradient, double stepSize, int iter, double regParam) Description copied from class: Updater Compute an updated value for weights given the gradient, stepSize, iteration number and regularization parameter. Also returns the regularization value regParam * R(w) computed using the *updated* weights. Specified by: compute in class Updater Parameters:weightsOld - - Column matrix of size dx1 where d is the number of features.gradient - - Column matrix of size dx1 where d is the number of features.stepSize - - step size across iterationsiter - - Iteration numberregParam - - Regularization parameter Returns:A tuple of 2 elements. The first element is a column matrix containing updated weights, and the second element is the regularization value computed using updated weights. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LBFGS (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LBFGS (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Class LBFGS Object org.apache.spark.mllib.optimization.LBFGS All Implemented Interfaces: java.io.Serializable, Optimizer public class LBFGS extends Object implements Optimizer :: DeveloperApi :: Class used to solve an optimization problem using Limited-memory BFGS. Reference: http://en.wikipedia.org/wiki/Limited-memory_BFGS param: gradient Gradient function to be used. param: updater Updater to be used to update weights after every iteration. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LBFGS(Gradient gradient, Updater updater)  Method Summary Methods  Modifier and Type Method and Description Vector optimize(RDD<scala.Tuple2<Object,Vector>> data, Vector initialWeights) Solve the provided convex optimization problem. static scala.Tuple2<Vector,double[]> runLBFGS(RDD<scala.Tuple2<Object,Vector>> data, Gradient gradient, Updater updater, int numCorrections, double convergenceTol, int maxNumIterations, double regParam, Vector initialWeights) Run Limited-memory BFGS (L-BFGS) in parallel. LBFGS setConvergenceTol(double tolerance) Set the convergence tolerance of iterations for L-BFGS. LBFGS setGradient(Gradient gradient) Set the gradient function (of the loss function of one single data example) to be used for L-BFGS. LBFGS setNumCorrections(int corrections) Set the number of corrections used in the LBFGS update. LBFGS setNumIterations(int iters) Set the maximal number of iterations for L-BFGS. LBFGS setRegParam(double regParam) Set the regularization parameter. LBFGS setUpdater(Updater updater) Set the updater function to actually perform a gradient step in a given direction. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LBFGS public LBFGS(Gradient gradient, Updater updater) Method Detail runLBFGS public static scala.Tuple2<Vector,double[]> runLBFGS(RDD<scala.Tuple2<Object,Vector>> data, Gradient gradient, Updater updater, int numCorrections, double convergenceTol, int maxNumIterations, double regParam, Vector initialWeights) Run Limited-memory BFGS (L-BFGS) in parallel. Averaging the subgradients over different partitions is performed using one standard spark map-reduce in each iteration. Parameters:data - - Input data for L-BFGS. RDD of the set of data examples, each of the form (label, [feature values]).gradient - - Gradient object (used to compute the gradient of the loss function of one single data example)updater - - Updater function to actually perform a gradient step in a given direction.numCorrections - - The number of corrections used in the L-BFGS update.convergenceTol - - The convergence tolerance of iterations for L-BFGS which is must be nonnegative. Lower values are less tolerant and therefore generally cause more iterations to be run.maxNumIterations - - Maximal number of iterations that L-BFGS can be run.regParam - - Regularization parameter initialWeights - (undocumented) Returns:A tuple containing two elements. The first element is a column matrix containing weights for every feature, and the second element is an array containing the loss computed for every iteration. setNumCorrections public LBFGS setNumCorrections(int corrections) Set the number of corrections used in the LBFGS update. Default 10. Values of numCorrections less than 3 are not recommended; large values of numCorrections will result in excessive computing time. 3 < numCorrections < 10 is recommended. Restriction: numCorrections > 0 Parameters:corrections - (undocumented) Returns:(undocumented) setConvergenceTol public LBFGS setConvergenceTol(double tolerance) Set the convergence tolerance of iterations for L-BFGS. Default 1E-6. Smaller value will lead to higher accuracy with the cost of more iterations. This value must be nonnegative. Lower convergence values are less tolerant and therefore generally cause more iterations to be run. Parameters:tolerance - (undocumented) Returns:(undocumented) setNumIterations public LBFGS setNumIterations(int iters) Set the maximal number of iterations for L-BFGS. Default 100. Parameters:iters - (undocumented) Returns:(undocumented) setRegParam public LBFGS setRegParam(double regParam) Set the regularization parameter. Default 0.0. Parameters:regParam - (undocumented) Returns:(undocumented) setGradient public LBFGS setGradient(Gradient gradient) Set the gradient function (of the loss function of one single data example) to be used for L-BFGS. Parameters:gradient - (undocumented) Returns:(undocumented) setUpdater public LBFGS setUpdater(Updater updater) Set the updater function to actually perform a gradient step in a given direction. The updater is responsible to perform the update from the regularization term as well, and therefore determines what kind or regularization is used, if any. Parameters:updater - (undocumented) Returns:(undocumented) optimize public Vector optimize(RDD<scala.Tuple2<Object,Vector>> data, Vector initialWeights) Description copied from interface: Optimizer Solve the provided convex optimization problem. Specified by: optimize in interface Optimizer Parameters:data - (undocumented)initialWeights - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LDA (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LDA (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class LDA Object org.apache.spark.mllib.clustering.LDA public class LDA extends Object Latent Dirichlet Allocation (LDA), a topic model designed for text documents. Terminology: - "word" = "term": an element of the vocabulary - "token": instance of a term appearing in a document - "topic": multinomial distribution over words representing some concept References: - Original LDA paper (journal version): Blei, Ng, and Jordan. "Latent Dirichlet Allocation." JMLR, 2003. See Also:http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation Latent Dirichlet allocation (Wikipedia)} Constructor Summary Constructors  Constructor and Description LDA() Constructs a LDA instance with default parameters. Method Summary Methods  Modifier and Type Method and Description double getAlpha() Alias for getDocConcentration Vector getAsymmetricAlpha() Alias for getAsymmetricDocConcentration Vector getAsymmetricDocConcentration() Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). double getBeta() Alias for getTopicConcentration int getCheckpointInterval() Period (in iterations) between checkpoints. double getDocConcentration() Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). int getK() Number of topics to infer, i.e., the number of soft cluster centers. int getMaxIterations() Maximum number of iterations allowed. LDAOptimizer getOptimizer() :: DeveloperApi :: long getSeed() Random seed for cluster initialization. double getTopicConcentration() Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. LDAModel run(JavaPairRDD<Long,Vector> documents) Java-friendly version of run() LDAModel run(RDD<scala.Tuple2<Object,Vector>> documents) Learn an LDA model using the given dataset. LDA setAlpha(double alpha) Alias for setDocConcentration() LDA setAlpha(Vector alpha) Alias for setDocConcentration() LDA setBeta(double beta) Alias for setTopicConcentration() LDA setCheckpointInterval(int checkpointInterval) Parameter for set checkpoint interval (>= 1) or disable checkpoint (-1). LDA setDocConcentration(double docConcentration) Replicates a Double docConcentration to create a symmetric prior. LDA setDocConcentration(Vector docConcentration) Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). LDA setK(int k) Set the number of topics to infer, i.e., the number of soft cluster centers. LDA setMaxIterations(int maxIterations) Set the maximum number of iterations allowed. LDA setOptimizer(LDAOptimizer optimizer) :: DeveloperApi :: LDA setOptimizer(String optimizerName) Set the LDAOptimizer used to perform the actual calculation by algorithm name. LDA setSeed(long seed) Set the random seed for cluster initialization. LDA setTopicConcentration(double topicConcentration) Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LDA public LDA() Constructs a LDA instance with default parameters. Method Detail getK public int getK() Number of topics to infer, i.e., the number of soft cluster centers. Returns:(undocumented) setK public LDA setK(int k) Set the number of topics to infer, i.e., the number of soft cluster centers. (default = 10) Parameters:k - (undocumented) Returns:(undocumented) getAsymmetricDocConcentration public Vector getAsymmetricDocConcentration() Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). This is the parameter to a Dirichlet distribution. Returns:(undocumented) getDocConcentration public double getDocConcentration() Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). This method assumes the Dirichlet distribution is symmetric and can be described by a single Double parameter. It should fail if docConcentration is asymmetric. Returns:(undocumented) setDocConcentration public LDA setDocConcentration(Vector docConcentration) Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). This is the parameter to a Dirichlet distribution, where larger values mean more smoothing (more regularization). If set to a singleton vector Vector(-1), then docConcentration is set automatically. If set to singleton vector Vector(t) where t != -1, then t is replicated to a vector of length k during LDAOptimizer.initialize(). Otherwise, the docConcentration vector must be length k. (default = Vector(-1) = automatic) Optimizer-specific parameter settings: - EM - Currently only supports symmetric distributions, so all values in the vector should be the same. - Values should be > 1.0 - default = uniformly (50 / k) + 1, where 50/k is common in LDA libraries and +1 follows from Asuncion et al. (2009), who recommend a +1 adjustment for EM. - Online - Values should be >= 0 - default = uniformly (1.0 / k), following the implementation from https://github.com/Blei-Lab/onlineldavb. Parameters:docConcentration - (undocumented) Returns:(undocumented) setDocConcentration public LDA setDocConcentration(double docConcentration) Replicates a Double docConcentration to create a symmetric prior. Parameters:docConcentration - (undocumented) Returns:(undocumented) getAsymmetricAlpha public Vector getAsymmetricAlpha() Alias for getAsymmetricDocConcentration Returns:(undocumented) getAlpha public double getAlpha() Alias for getDocConcentration Returns:(undocumented) setAlpha public LDA setAlpha(Vector alpha) Alias for setDocConcentration() Parameters:alpha - (undocumented) Returns:(undocumented) setAlpha public LDA setAlpha(double alpha) Alias for setDocConcentration() Parameters:alpha - (undocumented) Returns:(undocumented) getTopicConcentration public double getTopicConcentration() Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. This is the parameter to a symmetric Dirichlet distribution. Note: The topics' distributions over terms are called "beta" in the original LDA paper by Blei et al., but are called "phi" in many later papers such as Asuncion et al., 2009. Returns:(undocumented) setTopicConcentration public LDA setTopicConcentration(double topicConcentration) Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. This is the parameter to a symmetric Dirichlet distribution. Note: The topics' distributions over terms are called "beta" in the original LDA paper by Blei et al., but are called "phi" in many later papers such as Asuncion et al., 2009. If set to -1, then topicConcentration is set automatically. (default = -1 = automatic) Optimizer-specific parameter settings: - EM - Value should be > 1.0 - default = 0.1 + 1, where 0.1 gives a small amount of smoothing and +1 follows Asuncion et al. (2009), who recommend a +1 adjustment for EM. - Online - Value should be >= 0 - default = (1.0 / k), following the implementation from https://github.com/Blei-Lab/onlineldavb. Parameters:topicConcentration - (undocumented) Returns:(undocumented) getBeta public double getBeta() Alias for getTopicConcentration Returns:(undocumented) setBeta public LDA setBeta(double beta) Alias for setTopicConcentration() Parameters:beta - (undocumented) Returns:(undocumented) getMaxIterations public int getMaxIterations() Maximum number of iterations allowed. Returns:(undocumented) setMaxIterations public LDA setMaxIterations(int maxIterations) Set the maximum number of iterations allowed. (default = 20) Parameters:maxIterations - (undocumented) Returns:(undocumented) getSeed public long getSeed() Random seed for cluster initialization. Returns:(undocumented) setSeed public LDA setSeed(long seed) Set the random seed for cluster initialization. Parameters:seed - (undocumented) Returns:(undocumented) getCheckpointInterval public int getCheckpointInterval() Period (in iterations) between checkpoints. Returns:(undocumented) setCheckpointInterval public LDA setCheckpointInterval(int checkpointInterval) Parameter for set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Checkpointing helps with recovery (when nodes fail). It also helps with eliminating temporary shuffle files on disk, which can be important when LDA is run for many iterations. If the checkpoint directory is not set in SparkContext, this setting is ignored. (default = 10) Parameters:checkpointInterval - (undocumented) Returns:(undocumented)See Also:SparkContext.setCheckpointDir(java.lang.String) getOptimizer public LDAOptimizer getOptimizer() :: DeveloperApi :: LDAOptimizer used to perform the actual calculation Returns:(undocumented) setOptimizer public LDA setOptimizer(LDAOptimizer optimizer) :: DeveloperApi :: LDAOptimizer used to perform the actual calculation (default = EMLDAOptimizer) Parameters:optimizer - (undocumented) Returns:(undocumented) setOptimizer public LDA setOptimizer(String optimizerName) Set the LDAOptimizer used to perform the actual calculation by algorithm name. Currently "em", "online" are supported. Parameters:optimizerName - (undocumented) Returns:(undocumented) run public LDAModel run(RDD<scala.Tuple2<Object,Vector>> documents) Learn an LDA model using the given dataset. Parameters:documents - RDD of documents, which are term (word) count vectors paired with IDs. The term count vectors are "bags of words" with a fixed-size vocabulary (where the vocabulary size is the length of the vector). Document IDs must be unique and >= 0. Returns:Inferred LDA model run public LDAModel run(JavaPairRDD<Long,Vector> documents) Java-friendly version of run() Parameters:documents - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LDAModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LDAModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class LDAModel Object org.apache.spark.mllib.clustering.LDAModel All Implemented Interfaces: Saveable Direct Known Subclasses: DistributedLDAModel, LocalLDAModel public abstract class LDAModel extends Object implements Saveable Latent Dirichlet Allocation (LDA) model. This abstraction permits for different underlying representations, including local and distributed data structures. Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<int[],double[]>[] describeTopics() Return the topics described by weighted terms. abstract scala.Tuple2<int[],double[]>[] describeTopics(int maxTermsPerTopic) Return the topics described by weighted terms. abstract Vector docConcentration() Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). abstract int k() Number of topics abstract double topicConcentration() Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. abstract Matrix topicsMatrix() Inferred topics, where each topic is represented by a distribution over terms. abstract int vocabSize() Vocabulary size (number of terms or terms in the vocabulary) Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.util.Saveable formatVersion, save Method Detail k public abstract int k() Number of topics vocabSize public abstract int vocabSize() Vocabulary size (number of terms or terms in the vocabulary) docConcentration public abstract Vector docConcentration() Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). This is the parameter to a Dirichlet distribution. Returns:(undocumented) topicConcentration public abstract double topicConcentration() Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. This is the parameter to a symmetric Dirichlet distribution. Note: The topics' distributions over terms are called "beta" in the original LDA paper by Blei et al., but are called "phi" in many later papers such as Asuncion et al., 2009. Returns:(undocumented) topicsMatrix public abstract Matrix topicsMatrix() Inferred topics, where each topic is represented by a distribution over terms. This is a matrix of size vocabSize x k, where each column is a topic. No guarantees are given about the ordering of the topics. Returns:(undocumented) describeTopics public abstract scala.Tuple2<int[],double[]>[] describeTopics(int maxTermsPerTopic) Return the topics described by weighted terms. Parameters:maxTermsPerTopic - Maximum number of terms to collect for each topic. Returns:Array over topics. Each topic is represented as a pair of matching arrays: (term indices, term weights in topic). Each topic's terms are sorted in order of decreasing weight. describeTopics public scala.Tuple2<int[],double[]>[] describeTopics() Return the topics described by weighted terms. WARNING: If vocabSize and k are large, this can return a large object! Returns:Array over topics. Each topic is represented as a pair of matching arrays: (term indices, term weights in topic). Each topic's terms are sorted in order of decreasing weight. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LDAOptimizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LDAOptimizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Interface LDAOptimizer All Known Implementing Classes: EMLDAOptimizer, OnlineLDAOptimizer public interface LDAOptimizer :: DeveloperApi :: An LDAOptimizer specifies which optimization/learning/inference algorithm to use, and it can hold optimizer-specific parameters for users to set. Method Summary Methods  Modifier and Type Method and Description LDAModel getLDAModel(double[] iterationTimes)  LDAOptimizer initialize(RDD<scala.Tuple2<Object,Vector>> docs, LDA lda) Initializer for the optimizer. LDAOptimizer next()  Method Detail initialize LDAOptimizer initialize(RDD<scala.Tuple2<Object,Vector>> docs, LDA lda) Initializer for the optimizer. LDA passes the common parameters to the optimizer and the internal structure can be initialized properly. Parameters:docs - (undocumented)lda - (undocumented) Returns:(undocumented) next LDAOptimizer next() getLDAModel LDAModel getLDAModel(double[] iterationTimes) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LDAUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LDAUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class LDAUtils Object org.apache.spark.mllib.clustering.LDAUtils public class LDAUtils extends Object Utility methods for LDA. Constructor Summary Constructors  Constructor and Description LDAUtils()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LDAUtils public LDAUtils() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LZ4BlockInputStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LZ4BlockInputStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.io Class LZ4BlockInputStream Object java.io.InputStream java.io.FilterInputStream org.apache.spark.io.LZ4BlockInputStream All Implemented Interfaces: java.io.Closeable, AutoCloseable public final class LZ4BlockInputStream extends java.io.FilterInputStream InputStream implementation to decode data written with LZ4BlockOutputStream. This class is not thread-safe and does not support mark(int)/reset(). See Also:This is based on net.jpountz.lz4.LZ4BlockInputStream changes: https://github.com/davies/lz4-java/commit/cc1fa940ac57cc66a0b937300f805d37e2bf8411 TODO: merge this into upstream Constructor Summary Constructors  Constructor and Description LZ4BlockInputStream(java.io.InputStream in) Create a new instance which uses the fastest LZ4FastDecompressor available. LZ4BlockInputStream(java.io.InputStream in, net.jpountz.lz4.LZ4FastDecompressor decompressor) Create a new instance using XXHash32 for checksuming. LZ4BlockInputStream(java.io.InputStream in, net.jpountz.lz4.LZ4FastDecompressor decompressor, java.util.zip.Checksum checksum) Create a new InputStream. Method Summary Methods  Modifier and Type Method and Description int available()  void mark(int readlimit)  boolean markSupported()  int read()  int read(byte[] b)  int read(byte[] b, int off, int len)  void reset()  long skip(long n)  String toString()  Methods inherited from class java.io.FilterInputStream close Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail LZ4BlockInputStream public LZ4BlockInputStream(java.io.InputStream in, net.jpountz.lz4.LZ4FastDecompressor decompressor, java.util.zip.Checksum checksum) Create a new InputStream. Parameters:in - the InputStream to polldecompressor - the decompressor instance to usechecksum - the Checksum instance to use, must be equivalent to the instance which has been used to write the stream LZ4BlockInputStream public LZ4BlockInputStream(java.io.InputStream in, net.jpountz.lz4.LZ4FastDecompressor decompressor) Create a new instance using XXHash32 for checksuming. See Also:LZ4BlockInputStream(InputStream, LZ4FastDecompressor, Checksum), StreamingXXHash32.asChecksum() LZ4BlockInputStream public LZ4BlockInputStream(java.io.InputStream in) Create a new instance which uses the fastest LZ4FastDecompressor available. See Also:LZ4Factory.fastestInstance(), LZ4BlockInputStream(InputStream, LZ4FastDecompressor) Method Detail available public int available() throws java.io.IOException Overrides: available in class java.io.FilterInputStream Throws: java.io.IOException read public int read() throws java.io.IOException Overrides: read in class java.io.FilterInputStream Throws: java.io.IOException read public int read(byte[] b, int off, int len) throws java.io.IOException Overrides: read in class java.io.FilterInputStream Throws: java.io.IOException read public int read(byte[] b) throws java.io.IOException Overrides: read in class java.io.FilterInputStream Throws: java.io.IOException skip public long skip(long n) throws java.io.IOException Overrides: skip in class java.io.FilterInputStream Throws: java.io.IOException markSupported public boolean markSupported() Overrides: markSupported in class java.io.FilterInputStream mark public void mark(int readlimit) Overrides: mark in class java.io.FilterInputStream reset public void reset() throws java.io.IOException Overrides: reset in class java.io.FilterInputStream Throws: java.io.IOException toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LZ4CompressionCodec (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LZ4CompressionCodec (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.io Class LZ4CompressionCodec Object org.apache.spark.io.LZ4CompressionCodec All Implemented Interfaces: CompressionCodec public class LZ4CompressionCodec extends Object implements CompressionCodec :: DeveloperApi :: LZ4 implementation of CompressionCodec. Block size can be configured by spark.io.compression.lz4.blockSize. Note: The wire protocol for this codec is not guaranteed to be compatible across versions of Spark. This is intended for use as an internal compression utility within a single Spark application. Constructor Summary Constructors  Constructor and Description LZ4CompressionCodec(SparkConf conf)  Method Summary Methods  Modifier and Type Method and Description java.io.InputStream compressedInputStream(java.io.InputStream s)  java.io.OutputStream compressedOutputStream(java.io.OutputStream s)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LZ4CompressionCodec public LZ4CompressionCodec(SparkConf conf) Method Detail compressedOutputStream public java.io.OutputStream compressedOutputStream(java.io.OutputStream s) Specified by: compressedOutputStream in interface CompressionCodec compressedInputStream public java.io.InputStream compressedInputStream(java.io.InputStream s) Specified by: compressedInputStream in interface CompressionCodec Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LZFCompressionCodec (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LZFCompressionCodec (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.io Class LZFCompressionCodec Object org.apache.spark.io.LZFCompressionCodec All Implemented Interfaces: CompressionCodec public class LZFCompressionCodec extends Object implements CompressionCodec :: DeveloperApi :: LZF implementation of CompressionCodec. Note: The wire protocol for this codec is not guaranteed to be compatible across versions of Spark. This is intended for use as an internal compression utility within a single Spark application. Constructor Summary Constructors  Constructor and Description LZFCompressionCodec(SparkConf conf)  Method Summary Methods  Modifier and Type Method and Description java.io.InputStream compressedInputStream(java.io.InputStream s)  java.io.OutputStream compressedOutputStream(java.io.OutputStream s)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LZFCompressionCodec public LZFCompressionCodec(SparkConf conf) Method Detail compressedOutputStream public java.io.OutputStream compressedOutputStream(java.io.OutputStream s) Specified by: compressedOutputStream in interface CompressionCodec compressedInputStream public java.io.InputStream compressedInputStream(java.io.InputStream s) Specified by: compressedInputStream in interface CompressionCodec Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LabelConverter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LabelConverter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class LabelConverter Object org.apache.spark.ml.classification.LabelConverter public class LabelConverter extends Object Label to vector converter. Constructor Summary Constructors  Constructor and Description LabelConverter()  Method Summary Methods  Modifier and Type Method and Description static double decodeLabel(Vector output) Converts a vector to a label. static scala.Tuple2<Vector,Vector> encodeLabeledPoint(LabeledPoint labeledPoint, int labelCount) Encodes a label as a vector. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LabelConverter public LabelConverter() Method Detail encodeLabeledPoint public static scala.Tuple2<Vector,Vector> encodeLabeledPoint(LabeledPoint labeledPoint, int labelCount) Encodes a label as a vector. Returns a vector of given length with zeroes at all positions and value 1.0 at the position that corresponds to the label. Parameters:labeledPoint - labeled pointlabelCount - total number of labels Returns:pair of features and vector encoding of a label decodeLabel public static double decodeLabel(Vector output) Converts a vector to a label. Returns the position of the maximal element of a vector. Parameters:output - label encoded with a vector Returns:label Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LabelPropagation (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LabelPropagation (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx.lib Class LabelPropagation Object org.apache.spark.graphx.lib.LabelPropagation public class LabelPropagation extends Object Label Propagation algorithm. Constructor Summary Constructors  Constructor and Description LabelPropagation()  Method Summary Methods  Modifier and Type Method and Description static <VD,ED> Graph<Object,ED> run(Graph<VD,ED> graph, int maxSteps, scala.reflect.ClassTag<ED> evidence$1) Run static Label Propagation for detecting communities in networks. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LabelPropagation public LabelPropagation() Method Detail run public static <VD,ED> Graph<Object,ED> run(Graph<VD,ED> graph, int maxSteps, scala.reflect.ClassTag<ED> evidence$1) Run static Label Propagation for detecting communities in networks. Each node in the network is initially assigned to its own community. At every superstep, nodes send their community affiliation to all neighbors and update their state to the mode community affiliation of incoming messages. LPA is a standard community detection algorithm for graphs. It is very inexpensive computationally, although (1) convergence is not guaranteed and (2) one can end up with trivial solutions (all nodes are identified into a single community). Parameters:graph - the graph for which to compute the community affiliationmaxSteps - the number of supersteps of LPA to be performed. Because this is a static implementation, the algorithm will run for exactly this many supersteps. evidence$1 - (undocumented) Returns:a graph with vertex attributes containing the label of community affiliation Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LabeledPoint (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LabeledPoint (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression Class LabeledPoint Object org.apache.spark.mllib.regression.LabeledPoint All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class LabeledPoint extends Object implements scala.Product, scala.Serializable Class that represents the features and labels of a data point. param: label Label for this data point. param: features List of features for this data point. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LabeledPoint(double label, Vector features)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  Vector features()  double label()  static LabeledPoint parse(String s) Parses a string resulted from LabeledPoint#toString into an LabeledPoint. abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail LabeledPoint public LabeledPoint(double label, Vector features) Method Detail parse public static LabeledPoint parse(String s) Parses a string resulted from LabeledPoint#toString into an LabeledPoint. Parameters:s - (undocumented) Returns:(undocumented) canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() label public double label() features public Vector features() toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LassoModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LassoModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression Class LassoModel Object org.apache.spark.mllib.regression.GeneralizedLinearModel org.apache.spark.mllib.regression.LassoModel All Implemented Interfaces: java.io.Serializable, PMMLExportable, RegressionModel, Saveable public class LassoModel extends GeneralizedLinearModel implements RegressionModel, scala.Serializable, Saveable, PMMLExportable Regression model trained using Lasso. param: weights Weights computed for every feature. param: intercept Intercept computed for this model. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LassoModel(Vector weights, double intercept)  Method Summary Methods  Modifier and Type Method and Description double intercept()  static LassoModel load(SparkContext sc, String path)  static JavaRDD<Double> predict(JavaRDD<Vector> testData)  static RDD<Object> predict(RDD<Vector> testData)  static double predict(Vector testData)  void save(SparkContext sc, String path) Save this model to the given path. static String toPMML()  static void toPMML(java.io.OutputStream outputStream)  static void toPMML(SparkContext sc, String path)  static void toPMML(String localPath)  static String toString()  Vector weights()  Methods inherited from class org.apache.spark.mllib.regression.GeneralizedLinearModel predict, predict, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.regression.RegressionModel predict, predict, predict Methods inherited from interface org.apache.spark.mllib.pmml.PMMLExportable toPMML, toPMML, toPMML, toPMML, toPMML Constructor Detail LassoModel public LassoModel(Vector weights, double intercept) Method Detail load public static LassoModel load(SparkContext sc, String path) predict public static RDD<Object> predict(RDD<Vector> testData) predict public static double predict(Vector testData) toString public static String toString() predict public static JavaRDD<Double> predict(JavaRDD<Vector> testData) toPMML public static void toPMML(String localPath) toPMML public static void toPMML(SparkContext sc, String path) toPMML public static void toPMML(java.io.OutputStream outputStream) toPMML public static String toPMML() weights public Vector weights() Overrides: weights in class GeneralizedLinearModel intercept public double intercept() Overrides: intercept in class GeneralizedLinearModel save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LassoWithSGD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LassoWithSGD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression Class LassoWithSGD Object org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm<LassoModel> org.apache.spark.mllib.regression.LassoWithSGD All Implemented Interfaces: java.io.Serializable public class LassoWithSGD extends GeneralizedLinearAlgorithm<LassoModel> implements scala.Serializable Train a regression model with L1-regularization using Stochastic Gradient Descent. This solves the l1-regularized least squares regression formulation f(weights) = 1/2n ||A weights-y||^2^ + regParam ||weights||_1 Here the data matrix has n rows, and the input RDD holds the set of rows of A, each with its corresponding right hand side label y. See also the documentation for the precise formulation. Constructor Summary Constructors  Constructor and Description LassoWithSGD() Deprecated.  Use ml.regression.LinearRegression with elasticNetParam = 1.0. Note the default regParam is 0.01 for LassoWithSGD, but is 0.0 for LinearRegression. Since 2.0.0. Method Summary Methods  Modifier and Type Method and Description static int getNumFeatures()  static boolean isAddIntercept()  GradientDescent optimizer() The optimizer to solve the problem. static M run(RDD<LabeledPoint> input)  static M run(RDD<LabeledPoint> input, Vector initialWeights)  static GeneralizedLinearAlgorithm<M> setIntercept(boolean addIntercept)  static GeneralizedLinearAlgorithm<M> setValidateData(boolean validateData)  static LassoModel train(RDD<LabeledPoint> input, int numIterations) Train a Lasso model given an RDD of (label, features) pairs. static LassoModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double regParam) Train a Lasso model given an RDD of (label, features) pairs. static LassoModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double regParam, double miniBatchFraction) Train a Lasso model given an RDD of (label, features) pairs. static LassoModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double regParam, double miniBatchFraction, Vector initialWeights) Train a Lasso model given an RDD of (label, features) pairs. Methods inherited from class org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm getNumFeatures, isAddIntercept, run, run, setIntercept, setValidateData Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LassoWithSGD public LassoWithSGD() Deprecated. Use ml.regression.LinearRegression with elasticNetParam = 1.0. Note the default regParam is 0.01 for LassoWithSGD, but is 0.0 for LinearRegression. Since 2.0.0. Construct a Lasso object with default parameters: {stepSize: 1.0, numIterations: 100, regParam: 0.01, miniBatchFraction: 1.0}. Method Detail train public static LassoModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double regParam, double miniBatchFraction, Vector initialWeights) Train a Lasso model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using the specified step size. Each iteration uses miniBatchFraction fraction of the data to calculate a stochastic gradient. The weights used in gradient descent are initialized using the initial weights provided. Parameters:input - RDD of (label, array of features) pairs. Each pair describes a row of the data matrix A as well as the corresponding right hand side label ynumIterations - Number of iterations of gradient descent to run.stepSize - Step size scaling to be used for the iterations of gradient descent.regParam - Regularization parameter.miniBatchFraction - Fraction of data to be used per iteration.initialWeights - Initial set of weights to be used. Array should be equal in size to the number of features in the data. Returns:(undocumented) train public static LassoModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double regParam, double miniBatchFraction) Train a Lasso model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using the specified step size. Each iteration uses miniBatchFraction fraction of the data to calculate a stochastic gradient. Parameters:input - RDD of (label, array of features) pairs. Each pair describes a row of the data matrix A as well as the corresponding right hand side label ynumIterations - Number of iterations of gradient descent to run.stepSize - Step size to be used for each iteration of gradient descent.regParam - Regularization parameter.miniBatchFraction - Fraction of data to be used per iteration. Returns:(undocumented) train public static LassoModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double regParam) Train a Lasso model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using the specified step size. We use the entire data set to update the true gradient in each iteration. Parameters:input - RDD of (label, array of features) pairs. Each pair describes a row of the data matrix A as well as the corresponding right hand side label ystepSize - Step size to be used for each iteration of Gradient Descent.regParam - Regularization parameter.numIterations - Number of iterations of gradient descent to run. Returns:a LassoModel which has the weights and offset from training. train public static LassoModel train(RDD<LabeledPoint> input, int numIterations) Train a Lasso model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using a step size of 1.0. We use the entire data set to compute the true gradient in each iteration. Parameters:input - RDD of (label, array of features) pairs. Each pair describes a row of the data matrix A as well as the corresponding right hand side label ynumIterations - Number of iterations of gradient descent to run. Returns:a LassoModel which has the weights and offset from training. getNumFeatures public static int getNumFeatures() isAddIntercept public static boolean isAddIntercept() setIntercept public static GeneralizedLinearAlgorithm<M> setIntercept(boolean addIntercept) setValidateData public static GeneralizedLinearAlgorithm<M> setValidateData(boolean validateData) run public static M run(RDD<LabeledPoint> input) run public static M run(RDD<LabeledPoint> input, Vector initialWeights) optimizer public GradientDescent optimizer() Description copied from class: GeneralizedLinearAlgorithm The optimizer to solve the problem. Specified by: optimizer in class GeneralizedLinearAlgorithm<LassoModel> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LeafNode (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LeafNode (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tree Class LeafNode Object org.apache.spark.ml.tree.Node org.apache.spark.ml.tree.LeafNode All Implemented Interfaces: java.io.Serializable public class LeafNode extends Node Decision tree leaf node. param: prediction Prediction this node makes param: impurity Impurity measure at this node (for training data) See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description double impurity() Impurity measure at this node (for training data) double prediction() Prediction a leaf node makes, or which an internal node would make if it were a leaf node String toString()  Methods inherited from class org.apache.spark.ml.tree.Node fromOld Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Method Detail prediction public double prediction() Description copied from class: Node Prediction a leaf node makes, or which an internal node would make if it were a leaf node Specified by: prediction in class Node impurity public double impurity() Description copied from class: Node Impurity measure at this node (for training data) Specified by: impurity in class Node toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LeastSquaresAggregator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LeastSquaresAggregator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class LeastSquaresAggregator Object org.apache.spark.ml.regression.LeastSquaresAggregator All Implemented Interfaces: java.io.Serializable public class LeastSquaresAggregator extends Object implements scala.Serializable LeastSquaresAggregator computes the gradient and loss for a Least-squared loss function, as used in linear regression for samples in sparse or dense vector in an online fashion. Two LeastSquaresAggregator can be merged together to have a summary of loss and gradient of the corresponding joint dataset. For improving the convergence rate during the optimization process, and also preventing against features with very large variances exerting an overly large influence during model training, package like R's GLMNET performs the scaling to unit variance and removing the mean to reduce the condition number, and then trains the model in scaled space but returns the coefficients in the original scale. See page 9 in http://cran.r-project.org/web/packages/glmnet/glmnet.pdf However, we don't want to apply the StandardScaler on the training dataset, and then cache the standardized dataset since it will create a lot of overhead. As a result, we perform the scaling implicitly when we compute the objective function. The following is the mathematical derivation. Note that we don't deal with intercept by adding bias here, because the intercept can be computed using closed form after the coefficients are converged. See this discussion for detail. http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet When training with intercept enabled, The objective function in the scaled space is given by L = 1/2n ||\sum_i w_i(x_i - \bar{x_i}) / \hat{x_i} - (y - \bar{y}) / \hat{y}||^2, where \bar{x_i} is the mean of x_i, \hat{x_i} is the standard deviation of x_i, \bar{y} is the mean of label, and \hat{y} is the standard deviation of label. If we fitting the intercept disabled (that is forced through 0.0), we can use the same equation except we set \bar{y} and \bar{x_i} to 0 instead of the respective means. This can be rewritten as L = 1/2n ||\sum_i (w_i/\hat{x_i})x_i - \sum_i (w_i/\hat{x_i})\bar{x_i} - y / \hat{y} + \bar{y} / \hat{y}||^2 = 1/2n ||\sum_i w_i^\prime x_i - y / \hat{y} + offset||^2 = 1/2n diff^2 where w_i^\prime^ is the effective coefficients defined by w_i/\hat{x_i}, offset is - \sum_i (w_i/\hat{x_i})\bar{x_i} + \bar{y} / \hat{y}. , and diff is \sum_i w_i^\prime x_i - y / \hat{y} + offset Note that the effective coefficients and offset don't depend on training dataset, so they can be precomputed. Now, the first derivative of the objective function in scaled space is \frac{\partial L}{\partial w_i} = diff/N (x_i - \bar{x_i}) / \hat{x_i} However, ($x_i - \bar{x_i}$) will densify the computation, so it's not an ideal formula when the training dataset is sparse format. This can be addressed by adding the dense \bar{x_i} / \hat{x_i} terms in the end by keeping the sum of diff. The first derivative of total objective function from all the samples is \frac{\partial L}{\partial w_i} = 1/N \sum_j diff_j (x_{ij} - \bar{x_i}) / \hat{x_i} = 1/N ((\sum_j diff_j x_{ij} / \hat{x_i}) - diffSum \bar{x_i} / \hat{x_i}) = 1/N ((\sum_j diff_j x_{ij} / \hat{x_i}) + correction_i) , where correction_i = - diffSum \bar{x_i} / \hat{x_i} A simple math can show that diffSum is actually zero, so we don't even need to add the correction terms in the end. From the definition of diff, diffSum = \sum_j (\sum_i w_i(x_{ij} - \bar{x_i}) / \hat{x_i} - (y_j - \bar{y}) / \hat{y}) = N * (\sum_i w_i(\bar{x_i} - \bar{x_i}) / \hat{x_i} - (\bar{y} - \bar{y}) / \hat{y}) = 0 As a result, the first derivative of the total objective function only depends on the training dataset, which can be easily computed in distributed fashion, and is sparse format friendly. \frac{\partial L}{\partial w_i} = 1/N ((\sum_j diff_j x_{ij} / \hat{x_i}) , param: coefficients The coefficients corresponding to the features. param: labelStd The standard deviation value of the label. param: labelMean The mean value of the label. param: fitIntercept Whether to fit an intercept term. param: featuresStd The standard deviation values of the features. param: featuresMean The mean values of the features. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LeastSquaresAggregator(Vector coefficients, double labelStd, double labelMean, boolean fitIntercept, double[] featuresStd, double[] featuresMean)  Method Summary Methods  Modifier and Type Method and Description LeastSquaresAggregator add(org.apache.spark.ml.feature.Instance instance) Add a new training instance to this LeastSquaresAggregator, and update the loss and gradient of the objective function. long count()  Vector gradient()  double loss()  LeastSquaresAggregator merge(LeastSquaresAggregator other) Merge another LeastSquaresAggregator, and update the loss and gradient of the objective function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LeastSquaresAggregator public LeastSquaresAggregator(Vector coefficients, double labelStd, double labelMean, boolean fitIntercept, double[] featuresStd, double[] featuresMean) Method Detail add public LeastSquaresAggregator add(org.apache.spark.ml.feature.Instance instance) Add a new training instance to this LeastSquaresAggregator, and update the loss and gradient of the objective function. Parameters:instance - The instance of data point to be added. Returns:This LeastSquaresAggregator object. merge public LeastSquaresAggregator merge(LeastSquaresAggregator other) Merge another LeastSquaresAggregator, and update the loss and gradient of the objective function. (Note that it's in place merging; as a result, this object will be modified.) Parameters:other - The other LeastSquaresAggregator to be merged. Returns:This LeastSquaresAggregator object. count public long count() loss public double loss() gradient public Vector gradient() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LeastSquaresCostFun (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LeastSquaresCostFun (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class LeastSquaresCostFun Object org.apache.spark.ml.regression.LeastSquaresCostFun All Implemented Interfaces: breeze.optimize.DiffFunction<breeze.linalg.DenseVector<Object>>, breeze.optimize.StochasticDiffFunction<breeze.linalg.DenseVector<Object>>, scala.Function1<breeze.linalg.DenseVector<Object>,Object> public class LeastSquaresCostFun extends Object implements breeze.optimize.DiffFunction<breeze.linalg.DenseVector<Object>> LeastSquaresCostFun implements Breeze's DiffFunction[T] for Least Squares cost. It returns the loss and gradient with L2 regularization at a particular point (coefficients). It's used in Breeze's convex optimization routines. Constructor Summary Constructors  Constructor and Description LeastSquaresCostFun(RDD<org.apache.spark.ml.feature.Instance> instances, double labelStd, double labelMean, boolean fitIntercept, boolean standardization, double[] featuresStd, double[] featuresMean, double effectiveL2regParam)  Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<Object,breeze.linalg.DenseVector<Object>> calculate(breeze.linalg.DenseVector<Object> coefficients)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface breeze.optimize.DiffFunction cached, throughLens Methods inherited from interface breeze.optimize.StochasticDiffFunction apply, gradientAt, valueAt Methods inherited from interface scala.Function1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Constructor Detail LeastSquaresCostFun public LeastSquaresCostFun(RDD<org.apache.spark.ml.feature.Instance> instances, double labelStd, double labelMean, boolean fitIntercept, boolean standardization, double[] featuresStd, double[] featuresMean, double effectiveL2regParam) Method Detail calculate public scala.Tuple2<Object,breeze.linalg.DenseVector<Object>> calculate(breeze.linalg.DenseVector<Object> coefficients) Specified by: calculate in interface breeze.optimize.StochasticDiffFunction<breeze.linalg.DenseVector<Object>> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LeastSquaresGradient (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LeastSquaresGradient (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Class LeastSquaresGradient Object org.apache.spark.mllib.optimization.Gradient org.apache.spark.mllib.optimization.LeastSquaresGradient All Implemented Interfaces: java.io.Serializable public class LeastSquaresGradient extends Gradient :: DeveloperApi :: Compute gradient and loss for a Least-squared loss function, as used in linear regression. This is correct for the averaged least squares loss function (mean squared error) L = 1/2n ||A weights-y||^2 See also the documentation for the precise formulation. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LeastSquaresGradient()  Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<Vector,Object> compute(Vector data, double label, Vector weights) Compute the gradient and loss given the features of a single data point. double compute(Vector data, double label, Vector weights, Vector cumGradient) Compute the gradient and loss given the features of a single data point, add the gradient to a provided vector to avoid creating new objects, and return loss. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LeastSquaresGradient public LeastSquaresGradient() Method Detail compute public scala.Tuple2<Vector,Object> compute(Vector data, double label, Vector weights) Description copied from class: Gradient Compute the gradient and loss given the features of a single data point. Overrides: compute in class Gradient Parameters:data - features for one data pointlabel - label for this data pointweights - weights/coefficients corresponding to features Returns:(gradient: Vector, loss: Double) compute public double compute(Vector data, double label, Vector weights, Vector cumGradient) Description copied from class: Gradient Compute the gradient and loss given the features of a single data point, add the gradient to a provided vector to avoid creating new objects, and return loss. Specified by: compute in class Gradient Parameters:data - features for one data pointlabel - label for this data pointweights - weights/coefficients corresponding to featurescumGradient - the computed gradient will be added to this vector Returns:loss Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LegacyAccumulatorWrapper (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LegacyAccumulatorWrapper (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class LegacyAccumulatorWrapper<R,T> Object org.apache.spark.util.AccumulatorV2<T,R> org.apache.spark.util.LegacyAccumulatorWrapper<R,T> All Implemented Interfaces: java.io.Serializable public class LegacyAccumulatorWrapper<R,T> extends AccumulatorV2<T,R> See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LegacyAccumulatorWrapper(R initialValue, AccumulableParam<R,T> param)  Method Summary Methods  Modifier and Type Method and Description void add(T v) Takes the inputs and accumulates. LegacyAccumulatorWrapper<R,T> copy() Creates a new copy of this accumulator. boolean isZero() Returns if this accumulator is zero value or not. void merge(AccumulatorV2<T,R> other) Merges another same-type accumulator into this one and update its state, i.e. void reset() Resets this accumulator, which is zero value. R value() Defines the current value of this accumulator Methods inherited from class org.apache.spark.util.AccumulatorV2 copyAndReset, id, isRegistered, name, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail LegacyAccumulatorWrapper public LegacyAccumulatorWrapper(R initialValue, AccumulableParam<R,T> param) Method Detail isZero public boolean isZero() Description copied from class: AccumulatorV2 Returns if this accumulator is zero value or not. e.g. for a counter accumulator, 0 is zero value; for a list accumulator, Nil is zero value. Specified by: isZero in class AccumulatorV2<T,R> Returns:(undocumented) copy public LegacyAccumulatorWrapper<R,T> copy() Description copied from class: AccumulatorV2 Creates a new copy of this accumulator. Specified by: copy in class AccumulatorV2<T,R> Returns:(undocumented) reset public void reset() Description copied from class: AccumulatorV2 Resets this accumulator, which is zero value. i.e. call isZero must return true. Specified by: reset in class AccumulatorV2<T,R> add public void add(T v) Description copied from class: AccumulatorV2 Takes the inputs and accumulates. Specified by: add in class AccumulatorV2<T,R> Parameters:v - (undocumented) merge public void merge(AccumulatorV2<T,R> other) Description copied from class: AccumulatorV2 Merges another same-type accumulator into this one and update its state, i.e. this should be merge-in-place. Specified by: merge in class AccumulatorV2<T,R> Parameters:other - (undocumented) value public R value() Description copied from class: AccumulatorV2 Defines the current value of this accumulator Specified by: value in class AccumulatorV2<T,R> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LessThan (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LessThan (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class LessThan Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.LessThan All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class LessThan extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff the attribute evaluates to a value less than value. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LessThan(String attribute, Object value)  Method Summary Methods  Modifier and Type Method and Description String attribute()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Object value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail LessThan public LessThan(String attribute, Object value) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() attribute public String attribute() value public Object value() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LessThanOrEqual (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LessThanOrEqual (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class LessThanOrEqual Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.LessThanOrEqual All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class LessThanOrEqual extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff the attribute evaluates to a value less than or equal to value. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LessThanOrEqual(String attribute, Object value)  Method Summary Methods  Modifier and Type Method and Description String attribute()  abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Object value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail LessThanOrEqual public LessThanOrEqual(String attribute, Object value) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() attribute public String attribute() value public Object value() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LibSVMDataSource (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LibSVMDataSource (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.source.libsvm Class LibSVMDataSource Object org.apache.spark.ml.source.libsvm.LibSVMDataSource public class LibSVMDataSource extends Object libsvm package implements Spark SQL data source API for loading LIBSVM data as DataFrame. The loaded DataFrame has two columns: label containing labels stored as doubles and features containing feature vectors stored as Vectors. To use LIBSVM data source, you need to set "libsvm" as the format in DataFrameReader and optionally specify options, for example: // Scala val df = spark.read.format("libsvm") .option("numFeatures", "780") .load("data/mllib/sample_libsvm_data.txt") // Java Dataset<Row> df = spark.read().format("libsvm") .option("numFeatures, "780") .load("data/mllib/sample_libsvm_data.txt"); LIBSVM data source supports the following options: - "numFeatures": number of features. If unspecified or nonpositive, the number of features will be determined automatically at the cost of one additional pass. This is also useful when the dataset is already split into multiple files and you want to load them separately, because some features may not present in certain files, which leads to inconsistent feature dimensions. - "vectorType": feature vector type, "sparse" (default) or "dense". Note that this class is public for documentation purpose. Please don't use this class directly. Rather, use the data source API as illustrated above. See Also: Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LinearDataGenerator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LinearDataGenerator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.util Class LinearDataGenerator Object org.apache.spark.mllib.util.LinearDataGenerator public class LinearDataGenerator extends Object :: DeveloperApi :: Generate sample data used for Linear Data. This class generates uniformly random values for every feature and adds Gaussian noise with mean eps to the response variable Y. Constructor Summary Constructors  Constructor and Description LinearDataGenerator()  Method Summary Methods  Modifier and Type Method and Description static scala.collection.Seq<LabeledPoint> generateLinearInput(double intercept, double[] weights, double[] xMean, double[] xVariance, int nPoints, int seed, double eps)  static scala.collection.Seq<LabeledPoint> generateLinearInput(double intercept, double[] weights, double[] xMean, double[] xVariance, int nPoints, int seed, double eps, double sparsity)  static scala.collection.Seq<LabeledPoint> generateLinearInput(double intercept, double[] weights, int nPoints, int seed, double eps) For compatibility, the generated data without specifying the mean and variance will have zero mean and variance of (1.0/3.0) since the original output range is [-1, 1] with uniform distribution, and the variance of uniform distribution is (b - a)^2^ / 12 which will be (1.0/3.0) static java.util.List<LabeledPoint> generateLinearInputAsList(double intercept, double[] weights, int nPoints, int seed, double eps) Return a Java List of synthetic data randomly generated according to a multi collinear model. static RDD<LabeledPoint> generateLinearRDD(SparkContext sc, int nexamples, int nfeatures, double eps, int nparts, double intercept) Generate an RDD containing sample data for Linear Regression models - including Ridge, Lasso, and unregularized variants. static void main(String[] args)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LinearDataGenerator public LinearDataGenerator() Method Detail generateLinearInputAsList public static java.util.List<LabeledPoint> generateLinearInputAsList(double intercept, double[] weights, int nPoints, int seed, double eps) Return a Java List of synthetic data randomly generated according to a multi collinear model. Parameters:intercept - Data interceptweights - Weights to be applied.nPoints - Number of points in sample.seed - Random seedeps - (undocumented) Returns:Java List of input. generateLinearInput public static scala.collection.Seq<LabeledPoint> generateLinearInput(double intercept, double[] weights, int nPoints, int seed, double eps) For compatibility, the generated data without specifying the mean and variance will have zero mean and variance of (1.0/3.0) since the original output range is [-1, 1] with uniform distribution, and the variance of uniform distribution is (b - a)^2^ / 12 which will be (1.0/3.0) Parameters:intercept - Data interceptweights - Weights to be applied.nPoints - Number of points in sample.seed - Random seedeps - Epsilon scaling factor. Returns:Seq of input. generateLinearInput public static scala.collection.Seq<LabeledPoint> generateLinearInput(double intercept, double[] weights, double[] xMean, double[] xVariance, int nPoints, int seed, double eps) Parameters:intercept - Data interceptweights - Weights to be applied.xMean - the mean of the generated features. Lots of time, if the features are not properly standardized, the algorithm with poor implementation will have difficulty to converge.xVariance - the variance of the generated features.nPoints - Number of points in sample.seed - Random seedeps - Epsilon scaling factor. Returns:Seq of input. generateLinearInput public static scala.collection.Seq<LabeledPoint> generateLinearInput(double intercept, double[] weights, double[] xMean, double[] xVariance, int nPoints, int seed, double eps, double sparsity) Parameters:intercept - Data interceptweights - Weights to be applied.xMean - the mean of the generated features. Lots of time, if the features are not properly standardized, the algorithm with poor implementation will have difficulty to converge.xVariance - the variance of the generated features.nPoints - Number of points in sample.seed - Random seedeps - Epsilon scaling factor.sparsity - The ratio of zero elements. If it is 0.0, LabeledPoints with DenseVector is returned. Returns:Seq of input. generateLinearRDD public static RDD<LabeledPoint> generateLinearRDD(SparkContext sc, int nexamples, int nfeatures, double eps, int nparts, double intercept) Generate an RDD containing sample data for Linear Regression models - including Ridge, Lasso, and unregularized variants. Parameters:sc - SparkContext to be used for generating the RDD.nexamples - Number of examples that will be contained in the RDD.nfeatures - Number of features to generate for each example.eps - Epsilon factor by which examples are scaled.nparts - Number of partitions in the RDD. Default value is 2. intercept - (undocumented) Returns:RDD of LabeledPoint containing sample data. main public static void main(String[] args) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LinearRegression (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LinearRegression (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class LinearRegression Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<FeaturesType,Learner,M> org.apache.spark.ml.regression.LinearRegression All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class LinearRegression extends Predictor<FeaturesType,Learner,M> implements DefaultParamsWritable Linear regression. The learning objective is to minimize the squared error, with regularization. The specific squared error loss function used is: L = 1/2n ||A coefficients - y||^2^ This supports multiple types of regularization: - none (a.k.a. ordinary least squares) - L2 (ridge regression) - L1 (Lasso) - L2 + L1 (elastic net) See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LinearRegression()  LinearRegression(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  LinearRegression copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static DoubleParam elasticNetParam()  static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static M fit(Dataset<?> dataset)  static M fit(Dataset<?> dataset, ParamMap paramMap)  static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static BooleanParam fitIntercept()  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static double getElasticNetParam()  static String getFeaturesCol()  String getFeaturesCol()  static boolean getFitIntercept()  static String getLabelCol()  String getLabelCol()  static int getMaxIter()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static double getRegParam()  static String getSolver()  static boolean getStandardization()  static double getTol()  static String getWeightCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static LinearRegression load(String path)  static IntParam maxIter()  static Param<?>[] params()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static DoubleParam regParam()  static void save(String path)  static <T> Params set(Param<T> param, T value)  LinearRegression setElasticNetParam(double value) Set the ElasticNet mixing parameter. static Learner setFeaturesCol(String value)  LinearRegression setFitIntercept(boolean value) Set if we should fit the intercept Default is true. static Learner setLabelCol(String value)  LinearRegression setMaxIter(int value) Set the maximum number of iterations. static Learner setPredictionCol(String value)  LinearRegression setRegParam(double value) Set the regularization parameter. LinearRegression setSolver(String value) Set the solver algorithm used for optimization. LinearRegression setStandardization(boolean value) Whether to standardize the training features before fitting the model. LinearRegression setTol(double value) Set the convergence tolerance of iterations. LinearRegression setWeightCol(String value) Whether to over-/under-sample training instances according to the given weights in weightCol. static Param<String> solver()  static BooleanParam standardization()  static DoubleParam tol()  static String toString()  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  static Param<String> weightCol()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Predictor fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail LinearRegression public LinearRegression(String uid) LinearRegression public LinearRegression() Method Detail load public static LinearRegression load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) fit public static M fit(Dataset<?> dataset, ParamMap paramMap) fit public static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setLabelCol public static Learner setLabelCol(String value) setFeaturesCol public static Learner setFeaturesCol(String value) setPredictionCol public static Learner setPredictionCol(String value) fit public static M fit(Dataset<?> dataset) transformSchema public static StructType transformSchema(StructType schema) regParam public static final DoubleParam regParam() getRegParam public static final double getRegParam() elasticNetParam public static final DoubleParam elasticNetParam() getElasticNetParam public static final double getElasticNetParam() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() tol public static final DoubleParam tol() getTol public static final double getTol() fitIntercept public static final BooleanParam fitIntercept() getFitIntercept public static final boolean getFitIntercept() standardization public static final BooleanParam standardization() getStandardization public static final boolean getStandardization() weightCol public static final Param<String> weightCol() getWeightCol public static final String getWeightCol() solver public static final Param<String> solver() getSolver public static final String getSolver() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setRegParam public LinearRegression setRegParam(double value) Set the regularization parameter. Default is 0.0. Parameters:value - (undocumented) Returns:(undocumented) setFitIntercept public LinearRegression setFitIntercept(boolean value) Set if we should fit the intercept Default is true. Parameters:value - (undocumented) Returns:(undocumented) setStandardization public LinearRegression setStandardization(boolean value) Whether to standardize the training features before fitting the model. The coefficients of models will be always returned on the original scale, so it will be transparent for users. Note that with/without standardization, the models should be always converged to the same solution when no regularization is applied. In R's GLMNET package, the default behavior is true as well. Default is true. Parameters:value - (undocumented) Returns:(undocumented) setElasticNetParam public LinearRegression setElasticNetParam(double value) Set the ElasticNet mixing parameter. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. For 0 < alpha < 1, the penalty is a combination of L1 and L2. Default is 0.0 which is an L2 penalty. Parameters:value - (undocumented) Returns:(undocumented) setMaxIter public LinearRegression setMaxIter(int value) Set the maximum number of iterations. Default is 100. Parameters:value - (undocumented) Returns:(undocumented) setTol public LinearRegression setTol(double value) Set the convergence tolerance of iterations. Smaller value will lead to higher accuracy with the cost of more iterations. Default is 1E-6. Parameters:value - (undocumented) Returns:(undocumented) setWeightCol public LinearRegression setWeightCol(String value) Whether to over-/under-sample training instances according to the given weights in weightCol. If not set or empty, all instances are treated equally (weight 1.0). Default is not set, so all instances have weight one. Parameters:value - (undocumented) Returns:(undocumented) setSolver public LinearRegression setSolver(String value) Set the solver algorithm used for optimization. In case of linear regression, this can be "l-bfgs", "normal" and "auto". "l-bfgs" denotes Limited-memory BFGS which is a limited-memory quasi-Newton optimization method. "normal" denotes using Normal Equation as an analytical solution to the linear regression problem. The default value is "auto" which means that the solver algorithm is selected automatically. Parameters:value - (undocumented) Returns:(undocumented) copy public LinearRegression copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Predictor<Vector,LinearRegression,LinearRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LinearRegressionModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LinearRegressionModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression Class LinearRegressionModel Object org.apache.spark.mllib.regression.GeneralizedLinearModel org.apache.spark.mllib.regression.LinearRegressionModel All Implemented Interfaces: java.io.Serializable, PMMLExportable, RegressionModel, Saveable public class LinearRegressionModel extends GeneralizedLinearModel implements RegressionModel, scala.Serializable, Saveable, PMMLExportable Regression model trained using LinearRegression. param: weights Weights computed for every feature. param: intercept Intercept computed for this model. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LinearRegressionModel(Vector weights, double intercept)  Method Summary Methods  Modifier and Type Method and Description double intercept()  static LinearRegressionModel load(SparkContext sc, String path)  static JavaRDD<Double> predict(JavaRDD<Vector> testData)  static RDD<Object> predict(RDD<Vector> testData)  static double predict(Vector testData)  void save(SparkContext sc, String path) Save this model to the given path. static String toPMML()  static void toPMML(java.io.OutputStream outputStream)  static void toPMML(SparkContext sc, String path)  static void toPMML(String localPath)  static String toString()  Vector weights()  Methods inherited from class org.apache.spark.mllib.regression.GeneralizedLinearModel predict, predict, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.regression.RegressionModel predict, predict, predict Methods inherited from interface org.apache.spark.mllib.pmml.PMMLExportable toPMML, toPMML, toPMML, toPMML, toPMML Constructor Detail LinearRegressionModel public LinearRegressionModel(Vector weights, double intercept) Method Detail load public static LinearRegressionModel load(SparkContext sc, String path) predict public static RDD<Object> predict(RDD<Vector> testData) predict public static double predict(Vector testData) toString public static String toString() predict public static JavaRDD<Double> predict(JavaRDD<Vector> testData) toPMML public static void toPMML(String localPath) toPMML public static void toPMML(SparkContext sc, String path) toPMML public static void toPMML(java.io.OutputStream outputStream) toPMML public static String toPMML() weights public Vector weights() Overrides: weights in class GeneralizedLinearModel intercept public double intercept() Overrides: intercept in class GeneralizedLinearModel save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LinearRegressionSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LinearRegressionSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class LinearRegressionSummary Object org.apache.spark.ml.regression.LinearRegressionSummary All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: LinearRegressionTrainingSummary public class LinearRegressionSummary extends Object implements scala.Serializable :: Experimental :: Linear regression results evaluated on a dataset. param: predictions predictions output by the model's transform method. param: predictionCol Field in "predictions" which gives the predicted value of the label at each instance. param: labelCol Field in "predictions" which gives the true label of each instance. param: featuresCol Field in "predictions" which gives the features of each instance as a vector. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description double[] coefficientStandardErrors() Standard error of estimated coefficients and intercept. double[] devianceResiduals() The weighted residuals, the usual residuals rescaled by the square root of the instance weights. double explainedVariance() Returns the explained variance regression score. String featuresCol()  String labelCol()  double meanAbsoluteError() Returns the mean absolute error, which is a risk function corresponding to the expected value of the absolute error loss or l1-norm loss. double meanSquaredError() Returns the mean squared error, which is a risk function corresponding to the expected value of the squared error loss or quadratic loss. LinearRegressionModel model() Deprecated.  The model field is deprecated and will be removed in 2.1.0. Since 2.0.0. long numInstances() Number of instances in DataFrame predictions String predictionCol()  Dataset<Row> predictions()  double[] pValues() Two-sided p-value of estimated coefficients and intercept. double r2() Returns R^2^, the coefficient of determination. Dataset<Row> residuals() Residuals (label - predicted value) double rootMeanSquaredError() Returns the root mean squared error, which is defined as the square root of the mean squared error. double[] tValues() T-statistic of estimated coefficients and intercept. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail predictions public Dataset<Row> predictions() predictionCol public String predictionCol() labelCol public String labelCol() featuresCol public String featuresCol() model public LinearRegressionModel model() Deprecated. The model field is deprecated and will be removed in 2.1.0. Since 2.0.0. explainedVariance public double explainedVariance() Returns the explained variance regression score. explainedVariance = 1 - variance(y - \hat{y}) / variance(y) Reference: http://en.wikipedia.org/wiki/Explained_variation Note: This ignores instance weights (setting all to 1.0) from LinearRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) meanAbsoluteError public double meanAbsoluteError() Returns the mean absolute error, which is a risk function corresponding to the expected value of the absolute error loss or l1-norm loss. Note: This ignores instance weights (setting all to 1.0) from LinearRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) meanSquaredError public double meanSquaredError() Returns the mean squared error, which is a risk function corresponding to the expected value of the squared error loss or quadratic loss. Note: This ignores instance weights (setting all to 1.0) from LinearRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) rootMeanSquaredError public double rootMeanSquaredError() Returns the root mean squared error, which is defined as the square root of the mean squared error. Note: This ignores instance weights (setting all to 1.0) from LinearRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) r2 public double r2() Returns R^2^, the coefficient of determination. Reference: http://en.wikipedia.org/wiki/Coefficient_of_determination Note: This ignores instance weights (setting all to 1.0) from LinearRegression.weightCol. This will change in later Spark versions. Returns:(undocumented) residuals public Dataset<Row> residuals() Residuals (label - predicted value) numInstances public long numInstances() Number of instances in DataFrame predictions devianceResiduals public double[] devianceResiduals() The weighted residuals, the usual residuals rescaled by the square root of the instance weights. Returns:(undocumented) coefficientStandardErrors public double[] coefficientStandardErrors() Standard error of estimated coefficients and intercept. This value is only available when using the "normal" solver. If LinearRegression.fitIntercept is set to true, then the last element returned corresponds to the intercept. Returns:(undocumented)See Also:LinearRegression.solver tValues public double[] tValues() T-statistic of estimated coefficients and intercept. This value is only available when using the "normal" solver. If LinearRegression.fitIntercept is set to true, then the last element returned corresponds to the intercept. Returns:(undocumented)See Also:LinearRegression.solver pValues public double[] pValues() Two-sided p-value of estimated coefficients and intercept. This value is only available when using the "normal" solver. If LinearRegression.fitIntercept is set to true, then the last element returned corresponds to the intercept. Returns:(undocumented)See Also:LinearRegression.solver Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LinearRegressionTrainingSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LinearRegressionTrainingSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.regression Class LinearRegressionTrainingSummary Object org.apache.spark.ml.regression.LinearRegressionSummary org.apache.spark.ml.regression.LinearRegressionTrainingSummary All Implemented Interfaces: java.io.Serializable public class LinearRegressionTrainingSummary extends LinearRegressionSummary :: Experimental :: Linear regression training results. Currently, the training summary ignores the training weights except for the objective trace. param: predictions predictions output by the model's transform method. param: objectiveHistory objective function (scaled loss + regularization) at each iteration. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description double[] objectiveHistory()  int totalIterations() Number of training iterations until termination Methods inherited from class org.apache.spark.ml.regression.LinearRegressionSummary coefficientStandardErrors, devianceResiduals, explainedVariance, featuresCol, labelCol, meanAbsoluteError, meanSquaredError, model, numInstances, predictionCol, predictions, pValues, r2, residuals, rootMeanSquaredError, tValues Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail objectiveHistory public double[] objectiveHistory() totalIterations public int totalIterations() Number of training iterations until termination This value is only available when using the "l-bfgs" solver. Returns:(undocumented)See Also:LinearRegression.solver Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LinearRegressionWithSGD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LinearRegressionWithSGD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.regression Class LinearRegressionWithSGD Object org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm<LinearRegressionModel> org.apache.spark.mllib.regression.LinearRegressionWithSGD All Implemented Interfaces: java.io.Serializable public class LinearRegressionWithSGD extends GeneralizedLinearAlgorithm<LinearRegressionModel> implements scala.Serializable Train a linear regression model with no regularization using Stochastic Gradient Descent. This solves the least squares regression formulation f(weights) = 1/n ||A weights-y||^2^ (which is the mean squared error). Here the data matrix has n rows, and the input RDD holds the set of rows of A, each with its corresponding right hand side label y. See also the documentation for the precise formulation. Constructor Summary Constructors  Constructor and Description LinearRegressionWithSGD() Deprecated.  Use ml.regression.LinearRegression or LBFGS. Since 2.0.0. Method Summary Methods  Modifier and Type Method and Description static int getNumFeatures()  static boolean isAddIntercept()  GradientDescent optimizer() The optimizer to solve the problem. static M run(RDD<LabeledPoint> input)  static M run(RDD<LabeledPoint> input, Vector initialWeights)  static GeneralizedLinearAlgorithm<M> setIntercept(boolean addIntercept)  static GeneralizedLinearAlgorithm<M> setValidateData(boolean validateData)  static LinearRegressionModel train(RDD<LabeledPoint> input, int numIterations) Train a LinearRegression model given an RDD of (label, features) pairs. static LinearRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize) Train a LinearRegression model given an RDD of (label, features) pairs. static LinearRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double miniBatchFraction) Train a LinearRegression model given an RDD of (label, features) pairs. static LinearRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double miniBatchFraction, Vector initialWeights) Train a Linear Regression model given an RDD of (label, features) pairs. Methods inherited from class org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm getNumFeatures, isAddIntercept, run, run, setIntercept, setValidateData Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LinearRegressionWithSGD public LinearRegressionWithSGD() Deprecated. Use ml.regression.LinearRegression or LBFGS. Since 2.0.0. Construct a LinearRegression object with default parameters: {stepSize: 1.0, numIterations: 100, miniBatchFraction: 1.0}. Method Detail train public static LinearRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double miniBatchFraction, Vector initialWeights) Train a Linear Regression model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using the specified step size. Each iteration uses miniBatchFraction fraction of the data to calculate a stochastic gradient. The weights used in gradient descent are initialized using the initial weights provided. Parameters:input - RDD of (label, array of features) pairs. Each pair describes a row of the data matrix A as well as the corresponding right hand side label ynumIterations - Number of iterations of gradient descent to run.stepSize - Step size to be used for each iteration of gradient descent.miniBatchFraction - Fraction of data to be used per iteration.initialWeights - Initial set of weights to be used. Array should be equal in size to the number of features in the data. Returns:(undocumented) train public static LinearRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double miniBatchFraction) Train a LinearRegression model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using the specified step size. Each iteration uses miniBatchFraction fraction of the data to calculate a stochastic gradient. Parameters:input - RDD of (label, array of features) pairs. Each pair describes a row of the data matrix A as well as the corresponding right hand side label ynumIterations - Number of iterations of gradient descent to run.stepSize - Step size to be used for each iteration of gradient descent.miniBatchFraction - Fraction of data to be used per iteration. Returns:(undocumented) train public static LinearRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize) Train a LinearRegression model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using the specified step size. We use the entire data set to compute the true gradient in each iteration. Parameters:input - RDD of (label, array of features) pairs. Each pair describes a row of the data matrix A as well as the corresponding right hand side label ystepSize - Step size to be used for each iteration of Gradient Descent.numIterations - Number of iterations of gradient descent to run. Returns:a LinearRegressionModel which has the weights and offset from training. train public static LinearRegressionModel train(RDD<LabeledPoint> input, int numIterations) Train a LinearRegression model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using a step size of 1.0. We use the entire data set to compute the true gradient in each iteration. Parameters:input - RDD of (label, array of features) pairs. Each pair describes a row of the data matrix A as well as the corresponding right hand side label ynumIterations - Number of iterations of gradient descent to run. Returns:a LinearRegressionModel which has the weights and offset from training. getNumFeatures public static int getNumFeatures() isAddIntercept public static boolean isAddIntercept() setIntercept public static GeneralizedLinearAlgorithm<M> setIntercept(boolean addIntercept) setValidateData public static GeneralizedLinearAlgorithm<M> setValidateData(boolean validateData) run public static M run(RDD<LabeledPoint> input) run public static M run(RDD<LabeledPoint> input, Vector initialWeights) optimizer public GradientDescent optimizer() Description copied from class: GeneralizedLinearAlgorithm The optimizer to solve the problem. Specified by: optimizer in class GeneralizedLinearAlgorithm<LinearRegressionModel> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Loader (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Loader (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.util Interface Loader<M extends Saveable> public interface Loader<M extends Saveable> :: DeveloperApi :: Trait for classes which can load models and transformers from files. This should be inherited by an object paired with the model class. Method Summary Methods  Modifier and Type Method and Description M load(SparkContext sc, String path) Load a model from the given path. Method Detail load M load(SparkContext sc, String path) Load a model from the given path. The model should have been saved by Saveable.save. Parameters:sc - Spark context used for loading model files.path - Path specifying the directory to which the model was saved. Returns:Model instance Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LocalKMeans (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LocalKMeans (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class LocalKMeans Object org.apache.spark.mllib.clustering.LocalKMeans public class LocalKMeans extends Object An utility object to run K-means locally. This is private to the ML package because it's used in the initialization of KMeans but not meant to be publicly exposed. Constructor Summary Constructors  Constructor and Description LocalKMeans()  Method Summary Methods  Modifier and Type Method and Description static org.apache.spark.mllib.clustering.VectorWithNorm[] kMeansPlusPlus(int seed, org.apache.spark.mllib.clustering.VectorWithNorm[] points, double[] weights, int k, int maxIterations) Run K-means++ on the weighted point set points. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LocalKMeans public LocalKMeans() Method Detail kMeansPlusPlus public static org.apache.spark.mllib.clustering.VectorWithNorm[] kMeansPlusPlus(int seed, org.apache.spark.mllib.clustering.VectorWithNorm[] points, double[] weights, int k, int maxIterations) Run K-means++ on the weighted point set points. This first does the K-means++ initialization procedure and then rounds of Lloyd's algorithm. Parameters:seed - (undocumented)points - (undocumented)weights - (undocumented)k - (undocumented)maxIterations - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LocalLDAModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LocalLDAModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class LocalLDAModel Object org.apache.spark.mllib.clustering.LDAModel org.apache.spark.mllib.clustering.LocalLDAModel All Implemented Interfaces: java.io.Serializable, Saveable public class LocalLDAModel extends LDAModel implements scala.Serializable Local LDA model. This model stores only the inferred topics. param: topics Inferred topics (vocabSize x k matrix). See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<int[],double[]>[] describeTopics(int maxTermsPerTopic) Return the topics described by weighted terms. Vector docConcentration() Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). int k() Number of topics static LocalLDAModel load(SparkContext sc, String path)  double logLikelihood(JavaPairRDD<Long,Vector> documents) Java-friendly version of logLikelihood double logLikelihood(RDD<scala.Tuple2<Object,Vector>> documents) Calculates a lower bound on the log likelihood of the entire corpus. double logPerplexity(JavaPairRDD<Long,Vector> documents) Java-friendly version of logPerplexity double logPerplexity(RDD<scala.Tuple2<Object,Vector>> documents) Calculate an upper bound bound on perplexity. void save(SparkContext sc, String path) Save this model to the given path. double topicConcentration() Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. Vector topicDistribution(Vector document) Predicts the topic mixture distribution for a document (often called "theta" in the literature). JavaPairRDD<Long,Vector> topicDistributions(JavaPairRDD<Long,Vector> documents) Java-friendly version of topicDistributions RDD<scala.Tuple2<Object,Vector>> topicDistributions(RDD<scala.Tuple2<Object,Vector>> documents) Predicts the topic mixture distribution for each document (often called "theta" in the literature). Matrix topics()  Matrix topicsMatrix() Inferred topics, where each topic is represented by a distribution over terms. int vocabSize() Vocabulary size (number of terms or terms in the vocabulary) Methods inherited from class org.apache.spark.mllib.clustering.LDAModel describeTopics Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail load public static LocalLDAModel load(SparkContext sc, String path) topics public Matrix topics() docConcentration public Vector docConcentration() Description copied from class: LDAModel Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). This is the parameter to a Dirichlet distribution. Specified by: docConcentration in class LDAModel Returns:(undocumented) topicConcentration public double topicConcentration() Description copied from class: LDAModel Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms. This is the parameter to a symmetric Dirichlet distribution. Note: The topics' distributions over terms are called "beta" in the original LDA paper by Blei et al., but are called "phi" in many later papers such as Asuncion et al., 2009. Specified by: topicConcentration in class LDAModel Returns:(undocumented) k public int k() Description copied from class: LDAModel Number of topics Specified by: k in class LDAModel vocabSize public int vocabSize() Description copied from class: LDAModel Vocabulary size (number of terms or terms in the vocabulary) Specified by: vocabSize in class LDAModel topicsMatrix public Matrix topicsMatrix() Description copied from class: LDAModel Inferred topics, where each topic is represented by a distribution over terms. This is a matrix of size vocabSize x k, where each column is a topic. No guarantees are given about the ordering of the topics. Specified by: topicsMatrix in class LDAModel Returns:(undocumented) describeTopics public scala.Tuple2<int[],double[]>[] describeTopics(int maxTermsPerTopic) Description copied from class: LDAModel Return the topics described by weighted terms. Specified by: describeTopics in class LDAModel Parameters:maxTermsPerTopic - Maximum number of terms to collect for each topic. Returns:Array over topics. Each topic is represented as a pair of matching arrays: (term indices, term weights in topic). Each topic's terms are sorted in order of decreasing weight. save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. logLikelihood public double logLikelihood(RDD<scala.Tuple2<Object,Vector>> documents) Calculates a lower bound on the log likelihood of the entire corpus. See Equation (16) in original Online LDA paper. Parameters:documents - test corpus to use for calculating log likelihood Returns:variational lower bound on the log likelihood of the entire corpus logLikelihood public double logLikelihood(JavaPairRDD<Long,Vector> documents) Java-friendly version of logLikelihood Parameters:documents - (undocumented) Returns:(undocumented) logPerplexity public double logPerplexity(RDD<scala.Tuple2<Object,Vector>> documents) Calculate an upper bound bound on perplexity. (Lower is better.) See Equation (16) in original Online LDA paper. Parameters:documents - test corpus to use for calculating perplexity Returns:Variational upper bound on log perplexity per token. logPerplexity public double logPerplexity(JavaPairRDD<Long,Vector> documents) Java-friendly version of logPerplexity topicDistributions public RDD<scala.Tuple2<Object,Vector>> topicDistributions(RDD<scala.Tuple2<Object,Vector>> documents) Predicts the topic mixture distribution for each document (often called "theta" in the literature). Returns a vector of zeros for an empty document. This uses a variational approximation following Hoffman et al. (2010), where the approximate distribution is called "gamma." Technically, this method returns this approximation "gamma" for each document. Parameters:documents - documents to predict topic mixture distributions for Returns:An RDD of (document ID, topic mixture distribution for document) topicDistribution public Vector topicDistribution(Vector document) Predicts the topic mixture distribution for a document (often called "theta" in the literature). Returns a vector of zeros for an empty document. Note this means to allow quick query for single document. For batch documents, please refer to topicDistributions() to avoid overhead. Parameters:document - document to predict topic mixture distributions for Returns:topic mixture distribution for the document topicDistributions public JavaPairRDD<Long,Vector> topicDistributions(JavaPairRDD<Long,Vector> documents) Java-friendly version of topicDistributions Parameters:documents - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogLoss (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogLoss (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.loss Class LogLoss Object org.apache.spark.mllib.tree.loss.LogLoss public class LogLoss extends Object :: DeveloperApi :: Class for log loss calculation (for classification). This uses twice the binomial negative log likelihood, called "deviance" in Friedman (1999). The log loss is defined as: 2 log(1 + exp(-2 y F(x))) where y is a label in {-1, 1} and F(x) is the model prediction for features x. Constructor Summary Constructors  Constructor and Description LogLoss()  Method Summary Methods  Modifier and Type Method and Description static double gradient(double prediction, double label) Method to calculate the loss gradients for the gradient boosting calculation for binary classification The gradient with respect to F(x) is: - 4 y / (1 + exp(2 y F(x))) Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LogLoss public LogLoss() Method Detail gradient public static double gradient(double prediction, double label) Method to calculate the loss gradients for the gradient boosting calculation for binary classification The gradient with respect to F(x) is: - 4 y / (1 + exp(2 y F(x))) Parameters:prediction - Predicted label.label - True label. Returns:Loss gradient Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogNormalGenerator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogNormalGenerator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.random Class LogNormalGenerator Object org.apache.spark.mllib.random.LogNormalGenerator All Implemented Interfaces: java.io.Serializable, RandomDataGenerator<Object>, Pseudorandom public class LogNormalGenerator extends Object implements RandomDataGenerator<Object> :: DeveloperApi :: Generates i.i.d. samples from the log normal distribution with the given mean and standard deviation. param: mean mean for the log normal distribution. param: std standard deviation for the log normal distribution See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LogNormalGenerator(double mean, double std)  Method Summary Methods  Modifier and Type Method and Description LogNormalGenerator copy() Returns a copy of the RandomDataGenerator with a new instance of the rng object used in the class when applicable for non-locking concurrent usage. double mean()  double nextValue() Returns an i.i.d. void setSeed(long seed) Set random seed. double std()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LogNormalGenerator public LogNormalGenerator(double mean, double std) Method Detail mean public double mean() std public double std() nextValue public double nextValue() Description copied from interface: RandomDataGenerator Returns an i.i.d. sample as a generic type from an underlying distribution. Specified by: nextValue in interface RandomDataGenerator<Object> Returns:(undocumented) setSeed public void setSeed(long seed) Description copied from interface: Pseudorandom Set random seed. Specified by: setSeed in interface Pseudorandom copy public LogNormalGenerator copy() Description copied from interface: RandomDataGenerator Returns a copy of the RandomDataGenerator with a new instance of the rng object used in the class when applicable for non-locking concurrent usage. Specified by: copy in interface RandomDataGenerator<Object> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticAggregator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticAggregator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class LogisticAggregator Object org.apache.spark.ml.classification.LogisticAggregator All Implemented Interfaces: java.io.Serializable public class LogisticAggregator extends Object implements scala.Serializable LogisticAggregator computes the gradient and loss for binary logistic loss function, as used in binary classification for instances in sparse or dense vector in an online fashion. Note that multinomial logistic loss is not supported yet! Two LogisticAggregator can be merged together to have a summary of loss and gradient of the corresponding joint dataset. param: numClasses the number of possible outcomes for k classes classification problem in Multinomial Logistic Regression. param: fitIntercept Whether to fit an intercept term. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LogisticAggregator(int numFeatures, int numClasses, boolean fitIntercept)  Method Summary Methods  Modifier and Type Method and Description LogisticAggregator add(org.apache.spark.ml.feature.Instance instance, Vector coefficients, double[] featuresStd) Add a new training instance to this LogisticAggregator, and update the loss and gradient of the objective function. Vector gradient()  double loss()  LogisticAggregator merge(LogisticAggregator other) Merge another LogisticAggregator, and update the loss and gradient of the objective function. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LogisticAggregator public LogisticAggregator(int numFeatures, int numClasses, boolean fitIntercept) Method Detail add public LogisticAggregator add(org.apache.spark.ml.feature.Instance instance, Vector coefficients, double[] featuresStd) Add a new training instance to this LogisticAggregator, and update the loss and gradient of the objective function. Parameters:instance - The instance of data point to be added.coefficients - The coefficients corresponding to the features.featuresStd - The standard deviation values of the features. Returns:This LogisticAggregator object. merge public LogisticAggregator merge(LogisticAggregator other) Merge another LogisticAggregator, and update the loss and gradient of the objective function. (Note that it's in place merging; as a result, this object will be modified.) Parameters:other - The other LogisticAggregator to be merged. Returns:This LogisticAggregator object. loss public double loss() gradient public Vector gradient() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticCostFun (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticCostFun (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class LogisticCostFun Object org.apache.spark.ml.classification.LogisticCostFun All Implemented Interfaces: breeze.optimize.DiffFunction<breeze.linalg.DenseVector<Object>>, breeze.optimize.StochasticDiffFunction<breeze.linalg.DenseVector<Object>>, scala.Function1<breeze.linalg.DenseVector<Object>,Object> public class LogisticCostFun extends Object implements breeze.optimize.DiffFunction<breeze.linalg.DenseVector<Object>> LogisticCostFun implements Breeze's DiffFunction[T] for a multinomial logistic loss function, as used in multi-class classification (it is also used in binary logistic regression). It returns the loss and gradient with L2 regularization at a particular point (coefficients). It's used in Breeze's convex optimization routines. Constructor Summary Constructors  Constructor and Description LogisticCostFun(RDD<org.apache.spark.ml.feature.Instance> instances, int numClasses, boolean fitIntercept, boolean standardization, double[] featuresStd, double[] featuresMean, double regParamL2)  Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<Object,breeze.linalg.DenseVector<Object>> calculate(breeze.linalg.DenseVector<Object> coefficients)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface breeze.optimize.DiffFunction cached, throughLens Methods inherited from interface breeze.optimize.StochasticDiffFunction apply, gradientAt, valueAt Methods inherited from interface scala.Function1 andThen, apply$mcDD$sp, apply$mcDF$sp, apply$mcDI$sp, apply$mcDJ$sp, apply$mcFD$sp, apply$mcFF$sp, apply$mcFI$sp, apply$mcFJ$sp, apply$mcID$sp, apply$mcIF$sp, apply$mcII$sp, apply$mcIJ$sp, apply$mcJD$sp, apply$mcJF$sp, apply$mcJI$sp, apply$mcJJ$sp, apply$mcVD$sp, apply$mcVF$sp, apply$mcVI$sp, apply$mcVJ$sp, apply$mcZD$sp, apply$mcZF$sp, apply$mcZI$sp, apply$mcZJ$sp, compose, toString Constructor Detail LogisticCostFun public LogisticCostFun(RDD<org.apache.spark.ml.feature.Instance> instances, int numClasses, boolean fitIntercept, boolean standardization, double[] featuresStd, double[] featuresMean, double regParamL2) Method Detail calculate public scala.Tuple2<Object,breeze.linalg.DenseVector<Object>> calculate(breeze.linalg.DenseVector<Object> coefficients) Specified by: calculate in interface breeze.optimize.StochasticDiffFunction<breeze.linalg.DenseVector<Object>> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticGradient (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticGradient (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Class LogisticGradient Object org.apache.spark.mllib.optimization.Gradient org.apache.spark.mllib.optimization.LogisticGradient All Implemented Interfaces: java.io.Serializable public class LogisticGradient extends Gradient :: DeveloperApi :: Compute gradient and loss for a multinomial logistic loss function, as used in multi-class classification (it is also used in binary logistic regression). In The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, which can be downloaded from http://statweb.stanford.edu/~tibs/ElemStatLearn/ , Eq. (4.17) on page 119 gives the formula of multinomial logistic regression model. A simple calculation shows that P(y=0|x, w) = 1 / (1 + \sum_i^{K-1} \exp(x w_i)) P(y=1|x, w) = exp(x w_1) / (1 + \sum_i^{K-1} \exp(x w_i)) ... P(y=K-1|x, w) = exp(x w_{K-1}) / (1 + \sum_i^{K-1} \exp(x w_i)) for K classes multiclass classification problem. The model weights w = (w_1, w_2, ..., w_{K-1})^T becomes a matrix which has dimension of (K-1) * (N+1) if the intercepts are added. If the intercepts are not added, the dimension will be (K-1) * N. As a result, the loss of objective function for a single instance of data can be written as l(w, x) = -log P(y|x, w) = -\alpha(y) log P(y=0|x, w) - (1-\alpha(y)) log P(y|x, w) = log(1 + \sum_i^{K-1}\exp(x w_i)) - (1-\alpha(y)) x w_{y-1} = log(1 + \sum_i^{K-1}\exp(margins_i)) - (1-\alpha(y)) margins_{y-1} where \alpha(i) = 1 if i != 0, and \alpha(i) = 0 if i == 0, margins_i = x w_i. For optimization, we have to calculate the first derivative of the loss function, and a simple calculation shows that \frac{\partial l(w, x)}{\partial w_{ij}} = (\exp(x w_i) / (1 + \sum_k^{K-1} \exp(x w_k)) - (1-\alpha(y)\delta_{y, i+1})) * x_j = multiplier_i * x_j where \delta_{i, j} = 1 if i == j, \delta_{i, j} = 0 if i != j, and multiplier = \exp(margins_i) / (1 + \sum_k^{K-1} \exp(margins_i)) - (1-\alpha(y)\delta_{y, i+1}) If any of margins is larger than 709.78, the numerical computation of multiplier and loss function will be suffered from arithmetic overflow. This issue occurs when there are outliers in data which are far away from hyperplane, and this will cause the failing of training once infinity / infinity is introduced. Note that this is only a concern when max(margins) > 0. Fortunately, when max(margins) = maxMargin > 0, the loss function and the multiplier can be easily rewritten into the following equivalent numerically stable formula. l(w, x) = log(1 + \sum_i^{K-1}\exp(margins_i)) - (1-\alpha(y)) margins_{y-1} = log(\exp(-maxMargin) + \sum_i^{K-1}\exp(margins_i - maxMargin)) + maxMargin - (1-\alpha(y)) margins_{y-1} = log(1 + sum) + maxMargin - (1-\alpha(y)) margins_{y-1} where sum = \exp(-maxMargin) + \sum_i^{K-1}\exp(margins_i - maxMargin) - 1. Note that each term, (margins_i - maxMargin) in \exp is smaller than zero; as a result, overflow will not happen with this formula. For multiplier, similar trick can be applied as the following, multiplier = \exp(margins_i) / (1 + \sum_k^{K-1} \exp(margins_i)) - (1-\alpha(y)\delta_{y, i+1}) = \exp(margins_i - maxMargin) / (1 + sum) - (1-\alpha(y)\delta_{y, i+1}) where each term in \exp is also smaller than zero, so overflow is not a concern. For the detailed mathematical derivation, see the reference at http://www.slideshare.net/dbtsai/2014-0620-mlor-36132297 param: numClasses the number of possible outcomes for k classes classification problem in Multinomial Logistic Regression. By default, it is binary logistic regression so numClasses will be set to 2. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LogisticGradient()  LogisticGradient(int numClasses)  Method Summary Methods  Modifier and Type Method and Description double compute(Vector data, double label, Vector weights, Vector cumGradient) Compute the gradient and loss given the features of a single data point, add the gradient to a provided vector to avoid creating new objects, and return loss. Methods inherited from class org.apache.spark.mllib.optimization.Gradient compute Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LogisticGradient public LogisticGradient(int numClasses) LogisticGradient public LogisticGradient() Method Detail compute public double compute(Vector data, double label, Vector weights, Vector cumGradient) Description copied from class: Gradient Compute the gradient and loss given the features of a single data point, add the gradient to a provided vector to avoid creating new objects, and return loss. Specified by: compute in class Gradient Parameters:data - features for one data pointlabel - label for this data pointweights - weights/coefficients corresponding to featurescumGradient - the computed gradient will be added to this vector Returns:loss Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticRegression (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticRegression (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class LogisticRegression Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<FeaturesType,E,M> org.apache.spark.ml.classification.Classifier<FeaturesType,E,M> org.apache.spark.ml.classification.ProbabilisticClassifier<Vector,LogisticRegression,LogisticRegressionModel> org.apache.spark.ml.classification.LogisticRegression All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class LogisticRegression extends ProbabilisticClassifier<Vector,LogisticRegression,LogisticRegressionModel> implements DefaultParamsWritable Logistic regression. Currently, this class only supports binary classification. It will support multiclass in the future. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LogisticRegression()  LogisticRegression(String uid)  Method Summary Methods  Modifier and Type Method and Description void checkThresholdConsistency() If threshold and thresholds are both set, ensures they are consistent. static Params clear(Param<?> param)  LogisticRegression copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static DoubleParam elasticNetParam()  static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static M fit(Dataset<?> dataset)  static M fit(Dataset<?> dataset, ParamMap paramMap)  static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static BooleanParam fitIntercept()  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static double getElasticNetParam()  static String getFeaturesCol()  String getFeaturesCol()  static boolean getFitIntercept()  static String getLabelCol()  String getLabelCol()  static int getMaxIter()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static String getProbabilityCol()  static String getRawPredictionCol()  String getRawPredictionCol()  static double getRegParam()  static boolean getStandardization()  double getThreshold() Get threshold for binary classification. double[] getThresholds() Get thresholds for binary or multiclass classification. static double getTol()  static String getWeightCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static LogisticRegression load(String path)  static IntParam maxIter()  static Param<?>[] params()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static Param<String> probabilityCol()  static Param<String> rawPredictionCol()  Param<String> rawPredictionCol() Param for raw prediction (a.k.a. static DoubleParam regParam()  static void save(String path)  static <T> Params set(Param<T> param, T value)  LogisticRegression setElasticNetParam(double value) Set the ElasticNet mixing parameter. static Learner setFeaturesCol(String value)  LogisticRegression setFitIntercept(boolean value) Whether to fit an intercept term. static Learner setLabelCol(String value)  LogisticRegression setMaxIter(int value) Set the maximum number of iterations. static Learner setPredictionCol(String value)  static E setProbabilityCol(String value)  static E setRawPredictionCol(String value)  LogisticRegression setRegParam(double value) Set the regularization parameter. LogisticRegression setStandardization(boolean value) Whether to standardize the training features before fitting the model. LogisticRegression setThreshold(double value) Set threshold in binary classification, in range [0, 1]. LogisticRegression setThresholds(double[] value) Set thresholds in multiclass (or binary) classification to adjust the probability of predicting each class. LogisticRegression setTol(double value) Set the convergence tolerance of iterations. LogisticRegression setWeightCol(String value) Whether to over-/under-sample training instances according to the given weights in weightCol. static BooleanParam standardization()  static DoubleParam threshold()  static DoubleArrayParam thresholds()  static DoubleParam tol()  static String toString()  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  void validateParams() Validates parameter values stored internally. static Param<String> weightCol()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.classification.ProbabilisticClassifier setProbabilityCol Methods inherited from class org.apache.spark.ml.classification.Classifier setRawPredictionCol Methods inherited from class org.apache.spark.ml.Predictor fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Constructor Detail LogisticRegression public LogisticRegression(String uid) LogisticRegression public LogisticRegression() Method Detail load public static LogisticRegression load(String path) toString public static String toString() params public static Param<?>[] params() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) fit public static M fit(Dataset<?> dataset, ParamMap paramMap) fit public static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setLabelCol public static Learner setLabelCol(String value) setFeaturesCol public static Learner setFeaturesCol(String value) setPredictionCol public static Learner setPredictionCol(String value) fit public static M fit(Dataset<?> dataset) transformSchema public static StructType transformSchema(StructType schema) rawPredictionCol public static final Param<String> rawPredictionCol() getRawPredictionCol public static final String getRawPredictionCol() setRawPredictionCol public static E setRawPredictionCol(String value) probabilityCol public static final Param<String> probabilityCol() getProbabilityCol public static final String getProbabilityCol() thresholds public static final DoubleArrayParam thresholds() setProbabilityCol public static E setProbabilityCol(String value) regParam public static final DoubleParam regParam() getRegParam public static final double getRegParam() elasticNetParam public static final DoubleParam elasticNetParam() getElasticNetParam public static final double getElasticNetParam() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() fitIntercept public static final BooleanParam fitIntercept() getFitIntercept public static final boolean getFitIntercept() tol public static final DoubleParam tol() getTol public static final double getTol() standardization public static final BooleanParam standardization() getStandardization public static final boolean getStandardization() weightCol public static final Param<String> weightCol() getWeightCol public static final String getWeightCol() threshold public static final DoubleParam threshold() validateParams public static void validateParams() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setRegParam public LogisticRegression setRegParam(double value) Set the regularization parameter. Default is 0.0. Parameters:value - (undocumented) Returns:(undocumented) setElasticNetParam public LogisticRegression setElasticNetParam(double value) Set the ElasticNet mixing parameter. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. For 0 < alpha < 1, the penalty is a combination of L1 and L2. Default is 0.0 which is an L2 penalty. Parameters:value - (undocumented) Returns:(undocumented) setMaxIter public LogisticRegression setMaxIter(int value) Set the maximum number of iterations. Default is 100. Parameters:value - (undocumented) Returns:(undocumented) setTol public LogisticRegression setTol(double value) Set the convergence tolerance of iterations. Smaller value will lead to higher accuracy with the cost of more iterations. Default is 1E-6. Parameters:value - (undocumented) Returns:(undocumented) setFitIntercept public LogisticRegression setFitIntercept(boolean value) Whether to fit an intercept term. Default is true. Parameters:value - (undocumented) Returns:(undocumented) setStandardization public LogisticRegression setStandardization(boolean value) Whether to standardize the training features before fitting the model. The coefficients of models will be always returned on the original scale, so it will be transparent for users. Note that with/without standardization, the models should be always converged to the same solution when no regularization is applied. In R's GLMNET package, the default behavior is true as well. Default is true. Parameters:value - (undocumented) Returns:(undocumented) setThreshold public LogisticRegression setThreshold(double value) Set threshold in binary classification, in range [0, 1]. If the estimated probability of class label 1 is > threshold, then predict 1, else 0. A high threshold encourages the model to predict 0 more often; a low threshold encourages the model to predict 1 more often. Note: Calling this with threshold p is equivalent to calling setThresholds(Array(1-p, p)). When setThreshold() is called, any user-set value for thresholds will be cleared. If both threshold and thresholds are set in a ParamMap, then they must be equivalent. Default is 0.5. Parameters:value - (undocumented) Returns:(undocumented) getThreshold public double getThreshold() Get threshold for binary classification. If thresholds is set with length 2 (i.e., binary classification), this returns the equivalent threshold: 1 / (1 + thresholds(0) / thresholds(1)). Otherwise, returns {@link threshold} if set, or its default value if unset. @group getParam @throws IllegalArgumentException if {@link thresholds} is set to an array of length other than 2. Returns:(undocumented) setWeightCol public LogisticRegression setWeightCol(String value) Whether to over-/under-sample training instances according to the given weights in weightCol. If not set or empty String, all instances are treated equally (weight 1.0). Default is not set, so all instances have weight one. Parameters:value - (undocumented) Returns:(undocumented) setThresholds public LogisticRegression setThresholds(double[] value) Set thresholds in multiclass (or binary) classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values >= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold. Note: When setThresholds() is called, any user-set value for threshold will be cleared. If both threshold and thresholds are set in a ParamMap, then they must be equivalent. Overrides: setThresholds in class ProbabilisticClassifier<Vector,LogisticRegression,LogisticRegressionModel> Parameters:value - (undocumented) Returns:(undocumented) getThresholds public double[] getThresholds() Get thresholds for binary or multiclass classification. If thresholds is set, return its value. Otherwise, if threshold is set, return the equivalent thresholds for binary classification: (1-threshold, threshold). If neither are set, throw an exception. Returns:(undocumented) copy public LogisticRegression copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Predictor<Vector,LogisticRegression,LogisticRegressionModel> Parameters:extra - (undocumented) Returns:(undocumented) checkThresholdConsistency public void checkThresholdConsistency() If threshold and thresholds are both set, ensures they are consistent. Throws: IllegalArgumentException - if threshold and thresholds are not equivalent validateParams public void validateParams() Description copied from interface: Params Validates parameter values stored internally. Raise an exception if any parameter value is invalid. This only needs to check for interactions between parameters. Parameter value checks which do not depend on other parameters are handled by Param.validate(). This method does not handle input/output column parameters; those are checked during schema validation. validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) rawPredictionCol public Param<String> rawPredictionCol() Param for raw prediction (a.k.a. confidence) column name. Returns:(undocumented) getRawPredictionCol public String getRawPredictionCol() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticRegressionDataGenerator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticRegressionDataGenerator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.util Class LogisticRegressionDataGenerator Object org.apache.spark.mllib.util.LogisticRegressionDataGenerator public class LogisticRegressionDataGenerator extends Object :: DeveloperApi :: Generate test data for LogisticRegression. This class chooses positive labels with probability probOne and scales features for positive examples by eps. Constructor Summary Constructors  Constructor and Description LogisticRegressionDataGenerator()  Method Summary Methods  Modifier and Type Method and Description static RDD<LabeledPoint> generateLogisticRDD(SparkContext sc, int nexamples, int nfeatures, double eps, int nparts, double probOne) Generate an RDD containing test data for LogisticRegression. static void main(String[] args)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LogisticRegressionDataGenerator public LogisticRegressionDataGenerator() Method Detail generateLogisticRDD public static RDD<LabeledPoint> generateLogisticRDD(SparkContext sc, int nexamples, int nfeatures, double eps, int nparts, double probOne) Generate an RDD containing test data for LogisticRegression. Parameters:sc - SparkContext to use for creating the RDD.nexamples - Number of examples that will be contained in the RDD.nfeatures - Number of features to generate for each example.eps - Epsilon factor by which positive examples are scaled.nparts - Number of partitions of the generated RDD. Default value is 2.probOne - Probability that a label is 1 (and not 0). Default value is 0.5. Returns:(undocumented) main public static void main(String[] args) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticRegressionModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticRegressionModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification Class LogisticRegressionModel Object org.apache.spark.mllib.regression.GeneralizedLinearModel org.apache.spark.mllib.classification.LogisticRegressionModel All Implemented Interfaces: java.io.Serializable, ClassificationModel, PMMLExportable, Saveable public class LogisticRegressionModel extends GeneralizedLinearModel implements ClassificationModel, scala.Serializable, Saveable, PMMLExportable Classification model trained using Multinomial/Binary Logistic Regression. param: weights Weights computed for every feature. param: intercept Intercept computed for this model. (Only used in Binary Logistic Regression. In Multinomial Logistic Regression, the intercepts will not be a single value, so the intercepts will be part of the weights.) param: numFeatures the dimension of the features. param: numClasses the number of possible outcomes for k classes classification problem in Multinomial Logistic Regression. By default, it is binary logistic regression so numClasses will be set to 2. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LogisticRegressionModel(Vector weights, double intercept) Constructs a LogisticRegressionModel with weights and intercept for binary classification. LogisticRegressionModel(Vector weights, double intercept, int numFeatures, int numClasses)  Method Summary Methods  Modifier and Type Method and Description LogisticRegressionModel clearThreshold() Clears the threshold so that predict will output raw prediction scores. scala.Option<Object> getThreshold() Returns the threshold (if any) used for converting raw prediction scores into 0/1 predictions. double intercept()  static LogisticRegressionModel load(SparkContext sc, String path)  int numClasses()  int numFeatures()  static JavaRDD<Double> predict(JavaRDD<Vector> testData)  static RDD<Object> predict(RDD<Vector> testData)  static double predict(Vector testData)  void save(SparkContext sc, String path) Save this model to the given path. LogisticRegressionModel setThreshold(double threshold) Sets the threshold that separates positive predictions from negative predictions in Binary Logistic Regression. static String toPMML()  static void toPMML(java.io.OutputStream outputStream)  static void toPMML(SparkContext sc, String path)  static void toPMML(String localPath)  String toString() Print a summary of the model. Vector weights()  Methods inherited from class org.apache.spark.mllib.regression.GeneralizedLinearModel predict, predict Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.classification.ClassificationModel predict, predict, predict Methods inherited from interface org.apache.spark.mllib.pmml.PMMLExportable toPMML, toPMML, toPMML, toPMML, toPMML Constructor Detail LogisticRegressionModel public LogisticRegressionModel(Vector weights, double intercept, int numFeatures, int numClasses) LogisticRegressionModel public LogisticRegressionModel(Vector weights, double intercept) Constructs a LogisticRegressionModel with weights and intercept for binary classification. Parameters:weights - (undocumented)intercept - (undocumented) Method Detail load public static LogisticRegressionModel load(SparkContext sc, String path) predict public static RDD<Object> predict(RDD<Vector> testData) predict public static double predict(Vector testData) predict public static JavaRDD<Double> predict(JavaRDD<Vector> testData) toPMML public static void toPMML(String localPath) toPMML public static void toPMML(SparkContext sc, String path) toPMML public static void toPMML(java.io.OutputStream outputStream) toPMML public static String toPMML() weights public Vector weights() Overrides: weights in class GeneralizedLinearModel intercept public double intercept() Overrides: intercept in class GeneralizedLinearModel numFeatures public int numFeatures() numClasses public int numClasses() setThreshold public LogisticRegressionModel setThreshold(double threshold) Sets the threshold that separates positive predictions from negative predictions in Binary Logistic Regression. An example with prediction score greater than or equal to this threshold is identified as a positive, and negative otherwise. The default value is 0.5. It is only used for binary classification. Parameters:threshold - (undocumented) Returns:(undocumented) getThreshold public scala.Option<Object> getThreshold() Returns the threshold (if any) used for converting raw prediction scores into 0/1 predictions. It is only used for binary classification. Returns:(undocumented) clearThreshold public LogisticRegressionModel clearThreshold() Clears the threshold so that predict will output raw prediction scores. It is only used for binary classification. Returns:(undocumented) save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. toString public String toString() Description copied from class: GeneralizedLinearModel Print a summary of the model. Overrides: toString in class GeneralizedLinearModel Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticRegressionSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticRegressionSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Interface LogisticRegressionSummary All Superinterfaces: java.io.Serializable All Known Subinterfaces: LogisticRegressionTrainingSummary All Known Implementing Classes: BinaryLogisticRegressionSummary, BinaryLogisticRegressionTrainingSummary public interface LogisticRegressionSummary extends scala.Serializable Abstraction for Logistic Regression Results for a given model. Method Summary Methods  Modifier and Type Method and Description String featuresCol() Field in "predictions" which gives the features of each instance as a vector. String labelCol() Field in "predictions" which gives the true label of each instance (if available). Dataset<Row> predictions() Dataframe output by the model's `transform` method. String probabilityCol() Field in "predictions" which gives the probability of each class as a vector. Method Detail predictions Dataset<Row> predictions() Dataframe output by the model's `transform` method. probabilityCol String probabilityCol() Field in "predictions" which gives the probability of each class as a vector. labelCol String labelCol() Field in "predictions" which gives the true label of each instance (if available). featuresCol String featuresCol() Field in "predictions" which gives the features of each instance as a vector. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticRegressionTrainingSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticRegressionTrainingSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Interface LogisticRegressionTrainingSummary All Superinterfaces: LogisticRegressionSummary, java.io.Serializable All Known Implementing Classes: BinaryLogisticRegressionTrainingSummary public interface LogisticRegressionTrainingSummary extends LogisticRegressionSummary Abstraction for multinomial Logistic Regression Training results. Currently, the training summary ignores the training weights except for the objective trace. Method Summary Methods  Modifier and Type Method and Description double[] objectiveHistory() objective function (scaled loss + regularization) at each iteration. int totalIterations() Number of training iterations until termination Methods inherited from interface org.apache.spark.ml.classification.LogisticRegressionSummary featuresCol, labelCol, predictions, probabilityCol Method Detail objectiveHistory double[] objectiveHistory() objective function (scaled loss + regularization) at each iteration. totalIterations int totalIterations() Number of training iterations until termination Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticRegressionWithLBFGS (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticRegressionWithLBFGS (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification Class LogisticRegressionWithLBFGS Object org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm<LogisticRegressionModel> org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS All Implemented Interfaces: java.io.Serializable public class LogisticRegressionWithLBFGS extends GeneralizedLinearAlgorithm<LogisticRegressionModel> implements scala.Serializable Train a classification model for Multinomial/Binary Logistic Regression using Limited-memory BFGS. Standard feature scaling and L2 regularization are used by default. NOTE: Labels used in Logistic Regression should be {0, 1, ..., k - 1} for k classes multi-label classification problem. Earlier implementations of LogisticRegressionWithLBFGS applies a regularization penalty to all elements including the intercept. If this is called with one of standard updaters (L1Updater, or SquaredL2Updater) this is translated into a call to ml.LogisticRegression, otherwise this will use the existing mllib GeneralizedLinearAlgorithm trainer, resulting in a regularization penalty to the intercept. Constructor Summary Constructors  Constructor and Description LogisticRegressionWithLBFGS()  Method Summary Methods  Modifier and Type Method and Description LBFGS optimizer() The optimizer to solve the problem. LogisticRegressionModel run(RDD<LabeledPoint> input) Run Logistic Regression with the configured parameters on an input RDD of LabeledPoint entries. LogisticRegressionModel run(RDD<LabeledPoint> input, Vector initialWeights) Run Logistic Regression with the configured parameters on an input RDD of LabeledPoint entries starting from the initial weights provided. LogisticRegressionWithLBFGS setNumClasses(int numClasses) Set the number of possible outcomes for k classes classification problem in Multinomial Logistic Regression. Methods inherited from class org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm getNumFeatures, isAddIntercept, setIntercept, setValidateData Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LogisticRegressionWithLBFGS public LogisticRegressionWithLBFGS() Method Detail optimizer public LBFGS optimizer() Description copied from class: GeneralizedLinearAlgorithm The optimizer to solve the problem. Specified by: optimizer in class GeneralizedLinearAlgorithm<LogisticRegressionModel> Returns:(undocumented) setNumClasses public LogisticRegressionWithLBFGS setNumClasses(int numClasses) Set the number of possible outcomes for k classes classification problem in Multinomial Logistic Regression. By default, it is binary logistic regression so k will be set to 2. Parameters:numClasses - (undocumented) Returns:(undocumented) run public LogisticRegressionModel run(RDD<LabeledPoint> input) Run Logistic Regression with the configured parameters on an input RDD of LabeledPoint entries. If a known updater is used calls the ml implementation, to avoid applying a regularization penalty to the intercept, otherwise defaults to the mllib implementation. If more than two classes or feature scaling is disabled, always uses mllib implementation. If using ml implementation, uses ml code to generate initial weights. Overrides: run in class GeneralizedLinearAlgorithm<LogisticRegressionModel> Parameters:input - (undocumented) Returns:(undocumented) run public LogisticRegressionModel run(RDD<LabeledPoint> input, Vector initialWeights) Run Logistic Regression with the configured parameters on an input RDD of LabeledPoint entries starting from the initial weights provided. If a known updater is used calls the ml implementation, to avoid applying a regularization penalty to the intercept, otherwise defaults to the mllib implementation. If more than two classes or feature scaling is disabled, always uses mllib implementation. Uses user provided weights. In the ml LogisticRegression implementation, the number of corrections used in the LBFGS update can not be configured. So optimizer.setNumCorrections() will have no effect if we fall into that route. Overrides: run in class GeneralizedLinearAlgorithm<LogisticRegressionModel> Parameters:input - (undocumented)initialWeights - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LogisticRegressionWithSGD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LogisticRegressionWithSGD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification Class LogisticRegressionWithSGD Object org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm<LogisticRegressionModel> org.apache.spark.mllib.classification.LogisticRegressionWithSGD All Implemented Interfaces: java.io.Serializable public class LogisticRegressionWithSGD extends GeneralizedLinearAlgorithm<LogisticRegressionModel> implements scala.Serializable Train a classification model for Binary Logistic Regression using Stochastic Gradient Descent. By default L2 regularization is used, which can be changed via LogisticRegressionWithSGD.optimizer. NOTE: Labels used in Logistic Regression should be {0, 1, ..., k - 1} for k classes multi-label classification problem. Using LogisticRegressionWithLBFGS is recommended over this. Constructor Summary Constructors  Constructor and Description LogisticRegressionWithSGD() Deprecated.  Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS. Since 2.0.0. Method Summary Methods  Modifier and Type Method and Description static int getNumFeatures()  static boolean isAddIntercept()  GradientDescent optimizer() The optimizer to solve the problem. static M run(RDD<LabeledPoint> input)  static M run(RDD<LabeledPoint> input, Vector initialWeights)  static GeneralizedLinearAlgorithm<M> setIntercept(boolean addIntercept)  static GeneralizedLinearAlgorithm<M> setValidateData(boolean validateData)  static LogisticRegressionModel train(RDD<LabeledPoint> input, int numIterations) Train a logistic regression model given an RDD of (label, features) pairs. static LogisticRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize) Train a logistic regression model given an RDD of (label, features) pairs. static LogisticRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double miniBatchFraction) Train a logistic regression model given an RDD of (label, features) pairs. static LogisticRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double miniBatchFraction, Vector initialWeights) Train a logistic regression model given an RDD of (label, features) pairs. Methods inherited from class org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm getNumFeatures, isAddIntercept, run, run, setIntercept, setValidateData Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LogisticRegressionWithSGD public LogisticRegressionWithSGD() Deprecated. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS. Since 2.0.0. Construct a LogisticRegression object with default parameters: {stepSize: 1.0, numIterations: 100, regParm: 0.01, miniBatchFraction: 1.0}. Method Detail train public static LogisticRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double miniBatchFraction, Vector initialWeights) Train a logistic regression model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using the specified step size. Each iteration uses miniBatchFraction fraction of the data to calculate the gradient. The weights used in gradient descent are initialized using the initial weights provided. NOTE: Labels used in Logistic Regression should be {0, 1} Parameters:input - RDD of (label, array of features) pairs.numIterations - Number of iterations of gradient descent to run.stepSize - Step size to be used for each iteration of gradient descent.miniBatchFraction - Fraction of data to be used per iteration.initialWeights - Initial set of weights to be used. Array should be equal in size to the number of features in the data. Returns:(undocumented) train public static LogisticRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize, double miniBatchFraction) Train a logistic regression model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using the specified step size. Each iteration uses miniBatchFraction fraction of the data to calculate the gradient. NOTE: Labels used in Logistic Regression should be {0, 1} Parameters:input - RDD of (label, array of features) pairs.numIterations - Number of iterations of gradient descent to run.stepSize - Step size to be used for each iteration of gradient descent. miniBatchFraction - Fraction of data to be used per iteration. Returns:(undocumented) train public static LogisticRegressionModel train(RDD<LabeledPoint> input, int numIterations, double stepSize) Train a logistic regression model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using the specified step size. We use the entire data set to update the gradient in each iteration. NOTE: Labels used in Logistic Regression should be {0, 1} Parameters:input - RDD of (label, array of features) pairs.stepSize - Step size to be used for each iteration of Gradient Descent. numIterations - Number of iterations of gradient descent to run. Returns:a LogisticRegressionModel which has the weights and offset from training. train public static LogisticRegressionModel train(RDD<LabeledPoint> input, int numIterations) Train a logistic regression model given an RDD of (label, features) pairs. We run a fixed number of iterations of gradient descent using a step size of 1.0. We use the entire data set to update the gradient in each iteration. NOTE: Labels used in Logistic Regression should be {0, 1} Parameters:input - RDD of (label, array of features) pairs.numIterations - Number of iterations of gradient descent to run. Returns:a LogisticRegressionModel which has the weights and offset from training. getNumFeatures public static int getNumFeatures() isAddIntercept public static boolean isAddIntercept() setIntercept public static GeneralizedLinearAlgorithm<M> setIntercept(boolean addIntercept) setValidateData public static GeneralizedLinearAlgorithm<M> setValidateData(boolean validateData) run public static M run(RDD<LabeledPoint> input) run public static M run(RDD<LabeledPoint> input, Vector initialWeights) optimizer public GradientDescent optimizer() Description copied from class: GeneralizedLinearAlgorithm The optimizer to solve the problem. Specified by: optimizer in class GeneralizedLinearAlgorithm<LogisticRegressionModel> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LongAccumulator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LongAccumulator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class LongAccumulator Object org.apache.spark.util.AccumulatorV2<Long,Long> org.apache.spark.util.LongAccumulator All Implemented Interfaces: java.io.Serializable public class LongAccumulator extends AccumulatorV2<Long,Long> An accumulator for computing sum, count, and averages for 64-bit integers. Since: 2.0.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LongAccumulator()  Method Summary Methods  Modifier and Type Method and Description void add(long v) Adds v to the accumulator, i.e. void add(Long v) Adds v to the accumulator, i.e. double avg() Returns the average of elements added to the accumulator. LongAccumulator copy() Creates a new copy of this accumulator. long count() Returns the number of elements added to the accumulator. boolean isZero() Adds v to the accumulator, i.e. void merge(AccumulatorV2<Long,Long> other) Merges another same-type accumulator into this one and update its state, i.e. void reset() Resets this accumulator, which is zero value. long sum() Returns the sum of elements added to the accumulator. Long value() Defines the current value of this accumulator Methods inherited from class org.apache.spark.util.AccumulatorV2 copyAndReset, id, isRegistered, name, toString Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail LongAccumulator public LongAccumulator() Method Detail isZero public boolean isZero() Adds v to the accumulator, i.e. increment sum by v and count by 1. Specified by: isZero in class AccumulatorV2<Long,Long> Returns:(undocumented)Since: 2.0.0 copy public LongAccumulator copy() Description copied from class: AccumulatorV2 Creates a new copy of this accumulator. Specified by: copy in class AccumulatorV2<Long,Long> Returns:(undocumented) reset public void reset() Description copied from class: AccumulatorV2 Resets this accumulator, which is zero value. i.e. call isZero must return true. Specified by: reset in class AccumulatorV2<Long,Long> add public void add(Long v) Adds v to the accumulator, i.e. increment sum by v and count by 1. Specified by: add in class AccumulatorV2<Long,Long> Parameters:v - (undocumented)Since: 2.0.0 add public void add(long v) Adds v to the accumulator, i.e. increment sum by v and count by 1. Parameters:v - (undocumented)Since: 2.0.0 count public long count() Returns the number of elements added to the accumulator. Returns:(undocumented)Since: 2.0.0 sum public long sum() Returns the sum of elements added to the accumulator. Returns:(undocumented)Since: 2.0.0 avg public double avg() Returns the average of elements added to the accumulator. Returns:(undocumented)Since: 2.0.0 merge public void merge(AccumulatorV2<Long,Long> other) Description copied from class: AccumulatorV2 Merges another same-type accumulator into this one and update its state, i.e. this should be merge-in-place. Specified by: merge in class AccumulatorV2<Long,Long> Parameters:other - (undocumented) value public Long value() Description copied from class: AccumulatorV2 Defines the current value of this accumulator Specified by: value in class AccumulatorV2<Long,Long> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LongParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LongParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class LongParam Object org.apache.spark.ml.param.Param<Object> org.apache.spark.ml.param.LongParam All Implemented Interfaces: java.io.Serializable public class LongParam extends Param<Object> :: DeveloperApi :: Specialized version of Param[Long] for Java. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description LongParam(Identifiable parent, String name, String doc)  LongParam(Identifiable parent, String name, String doc, scala.Function1<Object,Object> isValid)  LongParam(String parent, String name, String doc)  LongParam(String parent, String name, String doc, scala.Function1<Object,Object> isValid)  Method Summary Methods  Modifier and Type Method and Description long jsonDecode(String json)  String jsonEncode(long value)  ParamPair<Object> w(long value) Creates a param pair with the given value (for Java). Methods inherited from class org.apache.spark.ml.param.Param doc, equals, hashCode, isValid, jsonEncode, name, parent, toString, w Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail LongParam public LongParam(String parent, String name, String doc, scala.Function1<Object,Object> isValid) LongParam public LongParam(String parent, String name, String doc) LongParam public LongParam(Identifiable parent, String name, String doc, scala.Function1<Object,Object> isValid) LongParam public LongParam(Identifiable parent, String name, String doc) Method Detail w public ParamPair<Object> w(long value) Creates a param pair with the given value (for Java). jsonEncode public String jsonEncode(long value) jsonDecode public long jsonDecode(String json) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LongType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LongType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class LongType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.NumericType org.apache.spark.sql.types.LongType public class LongType extends NumericType :: DeveloperApi :: The data type representing Long values. Please use the singleton DataTypes.LongType. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of the LongType is 8 bytes. static String json()  static String prettyJson()  String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() The default size of a value of the LongType is 8 bytes. Returns:(undocumented) simpleString public String simpleString() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Loss (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Loss (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.loss Interface Loss All Superinterfaces: java.io.Serializable public interface Loss extends scala.Serializable :: DeveloperApi :: Trait for adding "pluggable" loss functions for the gradient boosting algorithm. Method Summary Methods  Modifier and Type Method and Description double computeError(double prediction, double label) Method to calculate loss when the predictions are already known. double computeError(org.apache.spark.mllib.tree.model.TreeEnsembleModel model, RDD<LabeledPoint> data) Method to calculate error of the base learner for the gradient boosting calculation. double gradient(double prediction, double label) Method to calculate the gradients for the gradient boosting calculation. Method Detail gradient double gradient(double prediction, double label) Method to calculate the gradients for the gradient boosting calculation. Parameters:prediction - Predicted featurelabel - true label. Returns:Loss gradient. computeError double computeError(org.apache.spark.mllib.tree.model.TreeEnsembleModel model, RDD<LabeledPoint> data) Method to calculate error of the base learner for the gradient boosting calculation. Note: This method is not used by the gradient boosting algorithm but is useful for debugging purposes. Parameters:model - Model of the weak learner.data - Training dataset: RDD of LabeledPoint. Returns:Measure of model error on data computeError double computeError(double prediction, double label) Method to calculate loss when the predictions are already known. Note: This method is used in the method evaluateEachIteration to avoid recomputing the predicted values from previously fit trees. Parameters:prediction - Predicted label.label - True label. Returns:Measure of model error on datapoint. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method LossReasonPending (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="LossReasonPending (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler Class LossReasonPending Object org.apache.spark.scheduler.LossReasonPending public class LossReasonPending extends Object A loss reason that means we don't yet know why the executor exited. This is used by the task scheduler to remove state associated with the executor, but not yet fail any tasks that were running in the executor before the real loss reason is known. Constructor Summary Constructors  Constructor and Description LossReasonPending()  Method Summary Methods  Modifier and Type Method and Description static String message()  static String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail LossReasonPending public LossReasonPending() Method Detail message public static String message() toString public static String toString() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Losses (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Losses (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.loss Class Losses Object org.apache.spark.mllib.tree.loss.Losses public class Losses extends Object Constructor Summary Constructors  Constructor and Description Losses()  Method Summary Methods  Modifier and Type Method and Description static Loss fromString(String name)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Losses public Losses() Method Detail fromString public static Loss fromString(String name) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MFDataGenerator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MFDataGenerator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.util Class MFDataGenerator Object org.apache.spark.mllib.util.MFDataGenerator public class MFDataGenerator extends Object :: DeveloperApi :: Generate RDD(s) containing data for Matrix Factorization. This method samples training entries according to the oversampling factor 'trainSampFact', which is a multiplicative factor of the number of degrees of freedom of the matrix: rank*(m+n-rank). It optionally samples entries for a testing matrix using 'testSampFact', the percentage of the number of training entries to use for testing. This method takes the following inputs: sparkMaster (String) The master URL. outputPath (String) Directory to save output. m (Int) Number of rows in data matrix. n (Int) Number of columns in data matrix. rank (Int) Underlying rank of data matrix. trainSampFact (Double) Oversampling factor. noise (Boolean) Whether to add gaussian noise to training data. sigma (Double) Standard deviation of added gaussian noise. test (Boolean) Whether to create testing RDD. testSampFact (Double) Percentage of training data to use as test data. Constructor Summary Constructors  Constructor and Description MFDataGenerator()  Method Summary Methods  Modifier and Type Method and Description static void main(String[] args)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MFDataGenerator public MFDataGenerator() Method Detail main public static void main(String[] args) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MLPairRDDFunctions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MLPairRDDFunctions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.rdd Class MLPairRDDFunctions<K,V> Object org.apache.spark.mllib.rdd.MLPairRDDFunctions<K,V> All Implemented Interfaces: java.io.Serializable public class MLPairRDDFunctions<K,V> extends Object implements scala.Serializable :: DeveloperApi :: Machine learning specific Pair RDD functions. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MLPairRDDFunctions(RDD<scala.Tuple2<K,V>> self, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2)  Method Summary Methods  Modifier and Type Method and Description static <K,V> MLPairRDDFunctions<K,V> fromPairRDD(RDD<scala.Tuple2<K,V>> rdd, scala.reflect.ClassTag<K> evidence$3, scala.reflect.ClassTag<V> evidence$4) Implicit conversion from a pair RDD to MLPairRDDFunctions. RDD<scala.Tuple2<K,Object>> topByKey(int num, scala.math.Ordering<V> ord) Returns the top k (largest) elements for each key from this RDD as defined by the specified implicit Ordering[T]. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MLPairRDDFunctions public MLPairRDDFunctions(RDD<scala.Tuple2<K,V>> self, scala.reflect.ClassTag<K> evidence$1, scala.reflect.ClassTag<V> evidence$2) Method Detail fromPairRDD public static <K,V> MLPairRDDFunctions<K,V> fromPairRDD(RDD<scala.Tuple2<K,V>> rdd, scala.reflect.ClassTag<K> evidence$3, scala.reflect.ClassTag<V> evidence$4) Implicit conversion from a pair RDD to MLPairRDDFunctions. topByKey public RDD<scala.Tuple2<K,Object>> topByKey(int num, scala.math.Ordering<V> ord) Returns the top k (largest) elements for each key from this RDD as defined by the specified implicit Ordering[T]. If the number of elements for a certain key is less than k, all of them will be returned. Parameters:num - k, the number of top elements to returnord - the implicit ordering for T Returns:an RDD that contains the top k values for each key Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MLReadable (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MLReadable (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.util Interface MLReadable<T> All Known Subinterfaces: DefaultParamsReadable<T> public interface MLReadable<T> :: Experimental :: Trait for objects that provide MLReader. Method Summary Methods  Modifier and Type Method and Description T load(String path) Reads an ML instance from the input path, a shortcut of read.load(path). MLReader<T> read() Returns an MLReader instance for this class. Method Detail read MLReader<T> read() Returns an MLReader instance for this class. Returns:(undocumented) load T load(String path) Reads an ML instance from the input path, a shortcut of read.load(path). Note: Implementing classes should override this to be Java-friendly. Parameters:path - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MLReader (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MLReader (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.util Class MLReader<T> Object org.apache.spark.ml.util.MLReader<T> public abstract class MLReader<T> extends Object :: Experimental :: Abstract class for utility classes that can load ML instances. Constructor Summary Constructors  Constructor and Description MLReader()  Method Summary Methods  Modifier and Type Method and Description MLReader<T> context(SQLContext sqlContext) Sets the Spark SQLContext to use for saving/loading. abstract T load(String path) Loads the ML component from the input path. scala.Option<SparkSession> optionSparkSession()  SparkContext sc() Returns the underlying SparkContext. MLReader<T> session(SparkSession sparkSession) Sets the Spark Session to use for saving/loading. SparkSession sparkSession() Returns the user-specified Spark Session or the default. SQLContext sqlContext() Returns the user-specified SQL context or the default. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MLReader public MLReader() Method Detail load public abstract T load(String path) Loads the ML component from the input path. Parameters:path - (undocumented) Returns:(undocumented) session public MLReader<T> session(SparkSession sparkSession) Sets the Spark Session to use for saving/loading. Parameters:sparkSession - (undocumented) Returns:(undocumented) context public MLReader<T> context(SQLContext sqlContext) Sets the Spark SQLContext to use for saving/loading. Parameters:sqlContext - (undocumented) Returns:(undocumented) optionSparkSession public scala.Option<SparkSession> optionSparkSession() sparkSession public SparkSession sparkSession() Returns the user-specified Spark Session or the default. Returns:(undocumented) sqlContext public SQLContext sqlContext() Returns the user-specified SQL context or the default. Returns:(undocumented) sc public SparkContext sc() Returns the underlying SparkContext. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MLUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MLUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.util Class MLUtils Object org.apache.spark.mllib.util.MLUtils public class MLUtils extends Object Helper methods to load, save and pre-process data used in ML Lib. Constructor Summary Constructors  Constructor and Description MLUtils()  Method Summary Methods  Modifier and Type Method and Description static Vector appendBias(Vector vector) Returns a new vector with 1.0 (bias) appended to the input vector. static Dataset<Row> convertMatrixColumnsFromML(Dataset<?> dataset, scala.collection.Seq<String> cols) Converts matrix columns in an input Dataset to the Matrix type from the new Matrix type under the spark.ml package. static Dataset<Row> convertMatrixColumnsFromML(Dataset<?> dataset, String... cols) Converts matrix columns in an input Dataset to the Matrix type from the new Matrix type under the spark.ml package. static Dataset<Row> convertMatrixColumnsToML(Dataset<?> dataset, scala.collection.Seq<String> cols) Converts Matrix columns in an input Dataset from the Matrix type to the new Matrix type under the spark.ml package. static Dataset<Row> convertMatrixColumnsToML(Dataset<?> dataset, String... cols) Converts Matrix columns in an input Dataset from the Matrix type to the new Matrix type under the spark.ml package. static Dataset<Row> convertVectorColumnsFromML(Dataset<?> dataset, scala.collection.Seq<String> cols) Converts vector columns in an input Dataset to the Vector type from the new Vector type under the spark.ml package. static Dataset<Row> convertVectorColumnsFromML(Dataset<?> dataset, String... cols) Converts vector columns in an input Dataset to the Vector type from the new Vector type under the spark.ml package. static Dataset<Row> convertVectorColumnsToML(Dataset<?> dataset, scala.collection.Seq<String> cols) Converts vector columns in an input Dataset from the Vector type to the new Vector type under the spark.ml package. static Dataset<Row> convertVectorColumnsToML(Dataset<?> dataset, String... cols) Converts vector columns in an input Dataset from the Vector type to the new Vector type under the spark.ml package. static <T> scala.Tuple2<RDD<T>,RDD<T>>[] kFold(RDD<T> rdd, int numFolds, int seed, scala.reflect.ClassTag<T> evidence$1) Return a k element array of pairs of RDDs with the first element of each pair containing the training data, a complement of the validation data and the second element, the validation data, containing a unique 1/kth of the data. static <T> scala.Tuple2<RDD<T>,RDD<T>>[] kFold(RDD<T> rdd, int numFolds, long seed, scala.reflect.ClassTag<T> evidence$2) Version of kFold() taking a Long seed. static RDD<LabeledPoint> loadLabeledPoints(SparkContext sc, String dir) Loads labeled points saved using RDD[LabeledPoint].saveAsTextFile with the default number of partitions. static RDD<LabeledPoint> loadLabeledPoints(SparkContext sc, String path, int minPartitions) Loads labeled points saved using RDD[LabeledPoint].saveAsTextFile. static RDD<LabeledPoint> loadLibSVMFile(SparkContext sc, String path) Loads binary labeled data in the LIBSVM format into an RDD[LabeledPoint], with number of features determined automatically and the default number of partitions. static RDD<LabeledPoint> loadLibSVMFile(SparkContext sc, String path, int numFeatures) Loads labeled data in the LIBSVM format into an RDD[LabeledPoint], with the default number of partitions. static RDD<LabeledPoint> loadLibSVMFile(SparkContext sc, String path, int numFeatures, int minPartitions) Loads labeled data in the LIBSVM format into an RDD[LabeledPoint]. static RDD<Vector> loadVectors(SparkContext sc, String path) Loads vectors saved using RDD[Vector].saveAsTextFile with the default number of partitions. static RDD<Vector> loadVectors(SparkContext sc, String path, int minPartitions) Loads vectors saved using RDD[Vector].saveAsTextFile. static void saveAsLibSVMFile(RDD<LabeledPoint> data, String dir) Save labeled data in LIBSVM format. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MLUtils public MLUtils() Method Detail convertVectorColumnsToML public static Dataset<Row> convertVectorColumnsToML(Dataset<?> dataset, String... cols) Converts vector columns in an input Dataset from the Vector type to the new Vector type under the spark.ml package. Parameters:dataset - input datasetcols - a list of vector columns to be converted. New vector columns will be ignored. If unspecified, all old vector columns will be converted except nested ones. Returns:the input DataFrame with old vector columns converted to the new vector type convertVectorColumnsFromML public static Dataset<Row> convertVectorColumnsFromML(Dataset<?> dataset, String... cols) Converts vector columns in an input Dataset to the Vector type from the new Vector type under the spark.ml package. Parameters:dataset - input datasetcols - a list of vector columns to be converted. Old vector columns will be ignored. If unspecified, all new vector columns will be converted except nested ones. Returns:the input DataFrame with new vector columns converted to the old vector type convertMatrixColumnsToML public static Dataset<Row> convertMatrixColumnsToML(Dataset<?> dataset, String... cols) Converts Matrix columns in an input Dataset from the Matrix type to the new Matrix type under the spark.ml package. Parameters:dataset - input datasetcols - a list of matrix columns to be converted. New matrix columns will be ignored. If unspecified, all old matrix columns will be converted except nested ones. Returns:the input DataFrame with old matrix columns converted to the new matrix type convertMatrixColumnsFromML public static Dataset<Row> convertMatrixColumnsFromML(Dataset<?> dataset, String... cols) Converts matrix columns in an input Dataset to the Matrix type from the new Matrix type under the spark.ml package. Parameters:dataset - input datasetcols - a list of matrix columns to be converted. Old matrix columns will be ignored. If unspecified, all new matrix columns will be converted except nested ones. Returns:the input DataFrame with new matrix columns converted to the old matrix type loadLibSVMFile public static RDD<LabeledPoint> loadLibSVMFile(SparkContext sc, String path, int numFeatures, int minPartitions) Loads labeled data in the LIBSVM format into an RDD[LabeledPoint]. The LIBSVM format is a text-based format used by LIBSVM and LIBLINEAR. Each line represents a labeled sparse feature vector using the following format: label index1:value1 index2:value2 ... where the indices are one-based and in ascending order. This method parses each line into a {@link org.apache.spark.mllib.regression.LabeledPoint}, where the feature indices are converted to zero-based. @param sc Spark context @param path file or directory path in any Hadoop-supported file system URI @param numFeatures number of features, which will be determined from the input data if a nonpositive value is given. This is useful when the dataset is already split into multiple files and you want to load them separately, because some features may not present in certain files, which leads to inconsistent feature dimensions. @param minPartitions min number of partitions @return labeled data stored as an RDD[LabeledPoint] Parameters:sc - (undocumented)path - (undocumented)numFeatures - (undocumented)minPartitions - (undocumented) Returns:(undocumented) loadLibSVMFile public static RDD<LabeledPoint> loadLibSVMFile(SparkContext sc, String path, int numFeatures) Loads labeled data in the LIBSVM format into an RDD[LabeledPoint], with the default number of partitions. Parameters:sc - (undocumented)path - (undocumented)numFeatures - (undocumented) Returns:(undocumented) loadLibSVMFile public static RDD<LabeledPoint> loadLibSVMFile(SparkContext sc, String path) Loads binary labeled data in the LIBSVM format into an RDD[LabeledPoint], with number of features determined automatically and the default number of partitions. Parameters:sc - (undocumented)path - (undocumented) Returns:(undocumented) saveAsLibSVMFile public static void saveAsLibSVMFile(RDD<LabeledPoint> data, String dir) Save labeled data in LIBSVM format. Parameters:data - an RDD of LabeledPoint to be saveddir - directory to save the dataSee Also:loadLibSVMFile(org.apache.spark.SparkContext, java.lang.String, int, int) loadVectors public static RDD<Vector> loadVectors(SparkContext sc, String path, int minPartitions) Loads vectors saved using RDD[Vector].saveAsTextFile. Parameters:sc - Spark contextpath - file or directory path in any Hadoop-supported file system URIminPartitions - min number of partitions Returns:vectors stored as an RDD[Vector] loadVectors public static RDD<Vector> loadVectors(SparkContext sc, String path) Loads vectors saved using RDD[Vector].saveAsTextFile with the default number of partitions. Parameters:sc - (undocumented)path - (undocumented) Returns:(undocumented) loadLabeledPoints public static RDD<LabeledPoint> loadLabeledPoints(SparkContext sc, String path, int minPartitions) Loads labeled points saved using RDD[LabeledPoint].saveAsTextFile. Parameters:sc - Spark contextpath - file or directory path in any Hadoop-supported file system URIminPartitions - min number of partitions Returns:labeled points stored as an RDD[LabeledPoint] loadLabeledPoints public static RDD<LabeledPoint> loadLabeledPoints(SparkContext sc, String dir) Loads labeled points saved using RDD[LabeledPoint].saveAsTextFile with the default number of partitions. Parameters:sc - (undocumented)dir - (undocumented) Returns:(undocumented) kFold public static <T> scala.Tuple2<RDD<T>,RDD<T>>[] kFold(RDD<T> rdd, int numFolds, int seed, scala.reflect.ClassTag<T> evidence$1) Return a k element array of pairs of RDDs with the first element of each pair containing the training data, a complement of the validation data and the second element, the validation data, containing a unique 1/kth of the data. Where k=numFolds. Parameters:rdd - (undocumented)numFolds - (undocumented)seed - (undocumented)evidence$1 - (undocumented) Returns:(undocumented) kFold public static <T> scala.Tuple2<RDD<T>,RDD<T>>[] kFold(RDD<T> rdd, int numFolds, long seed, scala.reflect.ClassTag<T> evidence$2) Version of kFold() taking a Long seed. Parameters:rdd - (undocumented)numFolds - (undocumented)seed - (undocumented)evidence$2 - (undocumented) Returns:(undocumented) appendBias public static Vector appendBias(Vector vector) Returns a new vector with 1.0 (bias) appended to the input vector. Parameters:vector - (undocumented) Returns:(undocumented) convertVectorColumnsToML public static Dataset<Row> convertVectorColumnsToML(Dataset<?> dataset, scala.collection.Seq<String> cols) Converts vector columns in an input Dataset from the Vector type to the new Vector type under the spark.ml package. Parameters:dataset - input datasetcols - a list of vector columns to be converted. New vector columns will be ignored. If unspecified, all old vector columns will be converted except nested ones. Returns:the input DataFrame with old vector columns converted to the new vector type convertVectorColumnsFromML public static Dataset<Row> convertVectorColumnsFromML(Dataset<?> dataset, scala.collection.Seq<String> cols) Converts vector columns in an input Dataset to the Vector type from the new Vector type under the spark.ml package. Parameters:dataset - input datasetcols - a list of vector columns to be converted. Old vector columns will be ignored. If unspecified, all new vector columns will be converted except nested ones. Returns:the input DataFrame with new vector columns converted to the old vector type convertMatrixColumnsToML public static Dataset<Row> convertMatrixColumnsToML(Dataset<?> dataset, scala.collection.Seq<String> cols) Converts Matrix columns in an input Dataset from the Matrix type to the new Matrix type under the spark.ml package. Parameters:dataset - input datasetcols - a list of matrix columns to be converted. New matrix columns will be ignored. If unspecified, all old matrix columns will be converted except nested ones. Returns:the input DataFrame with old matrix columns converted to the new matrix type convertMatrixColumnsFromML public static Dataset<Row> convertMatrixColumnsFromML(Dataset<?> dataset, scala.collection.Seq<String> cols) Converts matrix columns in an input Dataset to the Matrix type from the new Matrix type under the spark.ml package. Parameters:dataset - input datasetcols - a list of matrix columns to be converted. Old matrix columns will be ignored. If unspecified, all new matrix columns will be converted except nested ones. Returns:the input DataFrame with new matrix columns converted to the old matrix type Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MLWritable (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MLWritable (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.util Interface MLWritable All Known Subinterfaces: DefaultParamsWritable All Known Implementing Classes: AFTSurvivalRegression, AFTSurvivalRegressionModel, ALS, ALSModel, Binarizer, BinaryClassificationEvaluator, BisectingKMeans, BisectingKMeansModel, Bucketizer, ChiSqSelector, ChiSqSelectorModel, ColumnPruner, CountVectorizer, CountVectorizerModel, CrossValidator, CrossValidatorModel, DCT, DecisionTreeClassificationModel, DecisionTreeClassifier, DecisionTreeRegressionModel, DecisionTreeRegressor, DistributedLDAModel, ElementwiseProduct, GaussianMixture, GaussianMixtureModel, GBTClassificationModel, GBTClassifier, GBTRegressionModel, GBTRegressor, GeneralizedLinearRegression, GeneralizedLinearRegressionModel, HashingTF, IDF, IDFModel, IndexToString, Interaction, IsotonicRegression, IsotonicRegressionModel, KMeans, KMeansModel, LDA, LDAModel, LinearRegression, LinearRegressionModel, LocalLDAModel, LogisticRegression, LogisticRegressionModel, MaxAbsScaler, MaxAbsScalerModel, MinMaxScaler, MinMaxScalerModel, MulticlassClassificationEvaluator, MultilayerPerceptronClassificationModel, MultilayerPerceptronClassifier, NaiveBayes, NaiveBayesModel, NGram, Normalizer, OneHotEncoder, OneVsRest, OneVsRestModel, PCA, PCAModel, Pipeline, PipelineModel, PolynomialExpansion, QuantileDiscretizer, RandomForestClassificationModel, RandomForestClassifier, RandomForestRegressionModel, RandomForestRegressor, RegexTokenizer, RegressionEvaluator, RFormula, RFormulaModel, SQLTransformer, StandardScaler, StandardScalerModel, StopWordsRemover, StringIndexer, StringIndexerModel, Tokenizer, TrainValidationSplit, TrainValidationSplitModel, VectorAssembler, VectorAttributeRewriter, VectorIndexer, VectorIndexerModel, VectorSlicer, Word2Vec, Word2VecModel public interface MLWritable :: Experimental :: Trait for classes that provide MLWriter. Method Summary Methods  Modifier and Type Method and Description void save(String path) Saves this ML instance to the input path, a shortcut of write.save(path). MLWriter write() Returns an MLWriter instance for this ML instance. Method Detail write MLWriter write() Returns an MLWriter instance for this ML instance. Returns:(undocumented) save void save(String path) throws java.io.IOException Saves this ML instance to the input path, a shortcut of write.save(path). Parameters:path - (undocumented) Throws: java.io.IOException Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MLWriter (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MLWriter (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.util Class MLWriter Object org.apache.spark.ml.util.MLWriter public abstract class MLWriter extends Object :: Experimental :: Abstract class for utility classes that can save ML instances. Constructor Summary Constructors  Constructor and Description MLWriter()  Method Summary Methods  Modifier and Type Method and Description MLWriter context(SQLContext sqlContext) Sets the Spark SQLContext to use for saving/loading. scala.Option<SparkSession> optionSparkSession()  MLWriter overwrite() Overwrites if the output path already exists. void save(String path) Saves the ML instances to the input path. SparkContext sc() Returns the underlying SparkContext. MLWriter session(SparkSession sparkSession) Sets the Spark Session to use for saving/loading. SparkSession sparkSession() Returns the user-specified Spark Session or the default. SQLContext sqlContext() Returns the user-specified SQL context or the default. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MLWriter public MLWriter() Method Detail save public void save(String path) throws java.io.IOException Saves the ML instances to the input path. Parameters:path - (undocumented) Throws: java.io.IOException overwrite public MLWriter overwrite() Overwrites if the output path already exists. Returns:(undocumented) session public MLWriter session(SparkSession sparkSession) Sets the Spark Session to use for saving/loading. Parameters:sparkSession - (undocumented) Returns:(undocumented) context public MLWriter context(SQLContext sqlContext) Sets the Spark SQLContext to use for saving/loading. Parameters:sqlContext - (undocumented) Returns:(undocumented) optionSparkSession public scala.Option<SparkSession> optionSparkSession() sparkSession public SparkSession sparkSession() Returns the user-specified Spark Session or the default. Returns:(undocumented) sqlContext public SQLContext sqlContext() Returns the user-specified SQL context or the default. Returns:(undocumented) sc public SparkContext sc() Returns the underlying SparkContext. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MapFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MapFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface MapFunction<T,U> All Superinterfaces: java.io.Serializable public interface MapFunction<T,U> extends java.io.Serializable Base interface for a map function used in Dataset's map function. Method Summary Methods  Modifier and Type Method and Description U call(T value)  Method Detail call U call(T value) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MapGroupsFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MapGroupsFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface MapGroupsFunction<K,V,R> All Superinterfaces: java.io.Serializable public interface MapGroupsFunction<K,V,R> extends java.io.Serializable Base interface for a map function used in GroupedDataset's mapGroup function. Method Summary Methods  Modifier and Type Method and Description R call(K key, java.util.Iterator<V> values)  Method Detail call R call(K key, java.util.Iterator<V> values) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MapPartitionsFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MapPartitionsFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface MapPartitionsFunction<T,U> All Superinterfaces: java.io.Serializable public interface MapPartitionsFunction<T,U> extends java.io.Serializable Base interface for function used in Dataset's mapPartitions. Method Summary Methods  Modifier and Type Method and Description java.util.Iterator<U> call(java.util.Iterator<T> input)  Method Detail call java.util.Iterator<U> call(java.util.Iterator<T> input) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MapType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MapType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class MapType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.MapType All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class MapType extends DataType implements scala.Product, scala.Serializable :: DeveloperApi :: The data type for Maps. Keys in a map are not allowed to have null values. Please use DataTypes.createMapType() to create a specific instance. param: keyType The data type of map keys. param: valueType The data type of map values. param: valueContainsNull Indicates if map values have null values. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MapType() No-arg constructor for kryo. MapType(DataType keyType, DataType valueType, boolean valueContainsNull)  Method Summary Methods  Modifier and Type Method and Description static MapType apply(DataType keyType, DataType valueType) Construct a MapType object with the given key type and value type. abstract static boolean canEqual(Object that)  String catalogString() String representation for the type saved in external catalogs. int defaultSize() The default size of a value of the MapType is 100 * (the default size of the key type + the default size of the value type). abstract static boolean equals(Object that)  static String json()  DataType keyType()  static String prettyJson()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  String simpleString() Readable string representation for the type. String sql()  static String typeName()  boolean valueContainsNull()  DataType valueType()  Methods inherited from class org.apache.spark.sql.types.DataType fromJson, json, prettyJson, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail MapType public MapType(DataType keyType, DataType valueType, boolean valueContainsNull) MapType public MapType() No-arg constructor for kryo. Method Detail apply public static MapType apply(DataType keyType, DataType valueType) Construct a MapType object with the given key type and value type. The valueContainsNull is true. Parameters:keyType - (undocumented)valueType - (undocumented) Returns:(undocumented) typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() keyType public DataType keyType() valueType public DataType valueType() valueContainsNull public boolean valueContainsNull() defaultSize public int defaultSize() The default size of a value of the MapType is 100 * (the default size of the key type + the default size of the value type). (We assume that there are 100 elements). Specified by: defaultSize in class DataType Returns:(undocumented) simpleString public String simpleString() Description copied from class: DataType Readable string representation for the type. Overrides: simpleString in class DataType catalogString public String catalogString() Description copied from class: DataType String representation for the type saved in external catalogs. Overrides: catalogString in class DataType sql public String sql() Overrides: sql in class DataType Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MapWithStateDStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MapWithStateDStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.dstream Class MapWithStateDStream<KeyType,ValueType,StateType,MappedType> Object org.apache.spark.streaming.dstream.DStream<MappedType> org.apache.spark.streaming.dstream.MapWithStateDStream<KeyType,ValueType,StateType,MappedType> All Implemented Interfaces: java.io.Serializable public abstract class MapWithStateDStream<KeyType,ValueType,StateType,MappedType> extends DStream<MappedType> :: Experimental :: DStream representing the stream of data generated by mapWithState operation on a pair DStream. Additionally, it also gives access to the stream of state snapshots, that is, the state data of all keys after a batch has updated them. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MapWithStateDStream(StreamingContext ssc, scala.reflect.ClassTag<MappedType> evidence$1)  Method Summary Methods  Modifier and Type Method and Description abstract DStream<scala.Tuple2<KeyType,StateType>> stateSnapshots() Return a pair DStream where each RDD is the snapshot of the state of all the keys. Methods inherited from class org.apache.spark.streaming.dstream.DStream cache, checkpoint, compute, context, count, countByValue, countByValueAndWindow, countByWindow, dependencies, filter, flatMap, foreachRDD, foreachRDD, glom, map, mapPartitions, persist, persist, print, print, reduce, reduceByWindow, reduceByWindow, repartition, saveAsObjectFiles, saveAsTextFiles, slice, slice, slideDuration, toPairDStreamFunctions, transform, transform, transformWith, transformWith, union, window, window Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MapWithStateDStream public MapWithStateDStream(StreamingContext ssc, scala.reflect.ClassTag<MappedType> evidence$1) Method Detail stateSnapshots public abstract DStream<scala.Tuple2<KeyType,StateType>> stateSnapshots() Return a pair DStream where each RDD is the snapshot of the state of all the keys. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Matrices (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Matrices (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg Class Matrices Object org.apache.spark.mllib.linalg.Matrices public class Matrices extends Object Factory methods for Matrix. Constructor Summary Constructors  Constructor and Description Matrices()  Method Summary Methods  Modifier and Type Method and Description static Matrix dense(int numRows, int numCols, double[] values) Creates a column-major dense matrix. static Matrix diag(Vector vector) Generate a diagonal matrix in Matrix format from the supplied values. static Matrix eye(int n) Generate a dense Identity Matrix in Matrix format. static Matrix fromML(Matrix m) Convert new linalg type to spark.mllib type. static Matrix horzcat(Matrix[] matrices) Horizontally concatenate a sequence of matrices. static Matrix ones(int numRows, int numCols) Generate a DenseMatrix consisting of ones. static Matrix rand(int numRows, int numCols, java.util.Random rng) Generate a DenseMatrix consisting of i.i.d. uniform random numbers. static Matrix randn(int numRows, int numCols, java.util.Random rng) Generate a DenseMatrix consisting of i.i.d. gaussian random numbers. static Matrix sparse(int numRows, int numCols, int[] colPtrs, int[] rowIndices, double[] values) Creates a column-major sparse matrix in Compressed Sparse Column (CSC) format. static Matrix speye(int n) Generate a sparse Identity Matrix in Matrix format. static Matrix sprand(int numRows, int numCols, double density, java.util.Random rng) Generate a SparseMatrix consisting of i.i.d. gaussian random numbers. static Matrix sprandn(int numRows, int numCols, double density, java.util.Random rng) Generate a SparseMatrix consisting of i.i.d. gaussian random numbers. static Matrix vertcat(Matrix[] matrices) Vertically concatenate a sequence of matrices. static Matrix zeros(int numRows, int numCols) Generate a Matrix consisting of zeros. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Matrices public Matrices() Method Detail dense public static Matrix dense(int numRows, int numCols, double[] values) Creates a column-major dense matrix. Parameters:numRows - number of rowsnumCols - number of columnsvalues - matrix entries in column major Returns:(undocumented) sparse public static Matrix sparse(int numRows, int numCols, int[] colPtrs, int[] rowIndices, double[] values) Creates a column-major sparse matrix in Compressed Sparse Column (CSC) format. Parameters:numRows - number of rowsnumCols - number of columnscolPtrs - the index corresponding to the start of a new columnrowIndices - the row index of the entryvalues - non-zero matrix entries in column major Returns:(undocumented) zeros public static Matrix zeros(int numRows, int numCols) Generate a Matrix consisting of zeros. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrix Returns:Matrix with size numRows x numCols and values of zeros ones public static Matrix ones(int numRows, int numCols) Generate a DenseMatrix consisting of ones. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrix Returns:Matrix with size numRows x numCols and values of ones eye public static Matrix eye(int n) Generate a dense Identity Matrix in Matrix format. Parameters:n - number of rows and columns of the matrix Returns:Matrix with size n x n and values of ones on the diagonal speye public static Matrix speye(int n) Generate a sparse Identity Matrix in Matrix format. Parameters:n - number of rows and columns of the matrix Returns:Matrix with size n x n and values of ones on the diagonal rand public static Matrix rand(int numRows, int numCols, java.util.Random rng) Generate a DenseMatrix consisting of i.i.d. uniform random numbers. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrixrng - a random number generator Returns:Matrix with size numRows x numCols and values in U(0, 1) sprand public static Matrix sprand(int numRows, int numCols, double density, java.util.Random rng) Generate a SparseMatrix consisting of i.i.d. gaussian random numbers. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrixdensity - the desired density for the matrixrng - a random number generator Returns:Matrix with size numRows x numCols and values in U(0, 1) randn public static Matrix randn(int numRows, int numCols, java.util.Random rng) Generate a DenseMatrix consisting of i.i.d. gaussian random numbers. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrixrng - a random number generator Returns:Matrix with size numRows x numCols and values in N(0, 1) sprandn public static Matrix sprandn(int numRows, int numCols, double density, java.util.Random rng) Generate a SparseMatrix consisting of i.i.d. gaussian random numbers. Parameters:numRows - number of rows of the matrixnumCols - number of columns of the matrixdensity - the desired density for the matrixrng - a random number generator Returns:Matrix with size numRows x numCols and values in N(0, 1) diag public static Matrix diag(Vector vector) Generate a diagonal matrix in Matrix format from the supplied values. Parameters:vector - a Vector that will form the values on the diagonal of the matrix Returns:Square Matrix with size values.length x values.length and values on the diagonal horzcat public static Matrix horzcat(Matrix[] matrices) Horizontally concatenate a sequence of matrices. The returned matrix will be in the format the matrices are supplied in. Supplying a mix of dense and sparse matrices will result in a sparse matrix. If the Array is empty, an empty DenseMatrix will be returned. Parameters:matrices - array of matrices Returns:a single Matrix composed of the matrices that were horizontally concatenated vertcat public static Matrix vertcat(Matrix[] matrices) Vertically concatenate a sequence of matrices. The returned matrix will be in the format the matrices are supplied in. Supplying a mix of dense and sparse matrices will result in a sparse matrix. If the Array is empty, an empty DenseMatrix will be returned. Parameters:matrices - array of matrices Returns:a single Matrix composed of the matrices that were vertically concatenated fromML public static Matrix fromML(Matrix m) Convert new linalg type to spark.mllib type. Light copy; only copies references Parameters:m - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Matrix (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Matrix (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg Interface Matrix All Superinterfaces: java.io.Serializable All Known Implementing Classes: DenseMatrix, SparseMatrix public interface Matrix extends scala.Serializable Trait for a local matrix. Method Summary Methods  Modifier and Type Method and Description double apply(int i, int j) Gets the (i, j)-th element. breeze.linalg.Matrix<Object> asBreeze() Converts to a breeze matrix. Matrix asML() Convert this matrix to the new mllib-local representation. scala.collection.Iterator<Vector> colIter() Returns an iterator of column vectors. Matrix copy() Get a deep copy of the matrix. void foreachActive(scala.Function3<Object,Object,Object,scala.runtime.BoxedUnit> f) Applies a function f to all the active elements of dense and sparse matrix. int index(int i, int j) Return the index for the (i, j)-th element in the backing array. boolean isTransposed() Flag that keeps track whether the matrix is transposed or not. Matrix map(scala.Function1<Object,Object> f) Map the values of this matrix using a function. DenseMatrix multiply(DenseMatrix y) Convenience method for `Matrix`-`DenseMatrix` multiplication. DenseVector multiply(DenseVector y) Convenience method for `Matrix`-`DenseVector` multiplication. DenseVector multiply(Vector y) Convenience method for `Matrix`-`Vector` multiplication. int numActives() Find the number of values stored explicitly. int numCols() Number of columns. int numNonzeros() Find the number of non-zero active values. int numRows() Number of rows. scala.collection.Iterator<Vector> rowIter() Returns an iterator of row vectors. double[] toArray() Converts to a dense array in column major. String toString() A human readable representation of the matrix String toString(int maxLines, int maxLineWidth) A human readable representation of the matrix with maximum lines and width Matrix transpose() Transpose the Matrix. Matrix update(scala.Function1<Object,Object> f) Update all the values of this matrix using the function f. void update(int i, int j, double v) Update element at (i, j) Method Detail numRows int numRows() Number of rows. numCols int numCols() Number of columns. isTransposed boolean isTransposed() Flag that keeps track whether the matrix is transposed or not. False by default. toArray double[] toArray() Converts to a dense array in column major. colIter scala.collection.Iterator<Vector> colIter() Returns an iterator of column vectors. This operation could be expensive, depending on the underlying storage. Returns:(undocumented) rowIter scala.collection.Iterator<Vector> rowIter() Returns an iterator of row vectors. This operation could be expensive, depending on the underlying storage. Returns:(undocumented) asBreeze breeze.linalg.Matrix<Object> asBreeze() Converts to a breeze matrix. apply double apply(int i, int j) Gets the (i, j)-th element. index int index(int i, int j) Return the index for the (i, j)-th element in the backing array. update void update(int i, int j, double v) Update element at (i, j) copy Matrix copy() Get a deep copy of the matrix. transpose Matrix transpose() Transpose the Matrix. Returns a new `Matrix` instance sharing the same underlying data. multiply DenseMatrix multiply(DenseMatrix y) Convenience method for `Matrix`-`DenseMatrix` multiplication. multiply DenseVector multiply(DenseVector y) Convenience method for `Matrix`-`DenseVector` multiplication. For binary compatibility. multiply DenseVector multiply(Vector y) Convenience method for `Matrix`-`Vector` multiplication. toString String toString() A human readable representation of the matrix Overrides: toString in class Object toString String toString(int maxLines, int maxLineWidth) A human readable representation of the matrix with maximum lines and width map Matrix map(scala.Function1<Object,Object> f) Map the values of this matrix using a function. Generates a new matrix. Performs the function on only the backing array. For example, an operation such as addition or subtraction will only be performed on the non-zero values in a SparseMatrix. Parameters:f - (undocumented) Returns:(undocumented) update Matrix update(scala.Function1<Object,Object> f) Update all the values of this matrix using the function f. Performed in-place on the backing array. For example, an operation such as addition or subtraction will only be performed on the non-zero values in a SparseMatrix. Parameters:f - (undocumented) Returns:(undocumented) foreachActive void foreachActive(scala.Function3<Object,Object,Object,scala.runtime.BoxedUnit> f) Applies a function f to all the active elements of dense and sparse matrix. The ordering of the elements are not defined. Parameters:f - the function takes three parameters where the first two parameters are the row and column indices respectively with the type Int, and the final parameter is the corresponding value in the matrix with type Double. numNonzeros int numNonzeros() Find the number of non-zero active values. Returns:(undocumented) numActives int numActives() Find the number of values stored explicitly. These values can be zero as well. Returns:(undocumented) asML Matrix asML() Convert this matrix to the new mllib-local representation. This does NOT copy the data; it copies references. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MatrixEntry (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MatrixEntry (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg.distributed Class MatrixEntry Object org.apache.spark.mllib.linalg.distributed.MatrixEntry All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class MatrixEntry extends Object implements scala.Product, scala.Serializable Represents an entry in a distributed matrix. param: i row index param: j column index param: value value of the entry See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MatrixEntry(long i, long j, double value)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  long i()  long j()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  double value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail MatrixEntry public MatrixEntry(long i, long j, double value) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() i public long i() j public long j() value public double value() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MatrixFactorizationModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MatrixFactorizationModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.recommendation Class MatrixFactorizationModel.SaveLoadV1_0$ Object org.apache.spark.mllib.recommendation.MatrixFactorizationModel.SaveLoadV1_0$ Enclosing class: MatrixFactorizationModel public static class MatrixFactorizationModel.SaveLoadV1_0$ extends Object Field Summary Fields  Modifier and Type Field and Description static MatrixFactorizationModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description MatrixFactorizationModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description MatrixFactorizationModel load(SparkContext sc, String path)  void save(MatrixFactorizationModel model, String path) Saves a MatrixFactorizationModel, where user features are saved under data/users and product features are saved under data/products. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final MatrixFactorizationModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail MatrixFactorizationModel.SaveLoadV1_0$ public MatrixFactorizationModel.SaveLoadV1_0$() Method Detail save public void save(MatrixFactorizationModel model, String path) Saves a MatrixFactorizationModel, where user features are saved under data/users and product features are saved under data/products. Parameters:model - (undocumented)path - (undocumented) load public MatrixFactorizationModel load(SparkContext sc, String path) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MatrixFactorizationModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MatrixFactorizationModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.recommendation Class MatrixFactorizationModel Object org.apache.spark.mllib.recommendation.MatrixFactorizationModel All Implemented Interfaces: java.io.Serializable, Saveable public class MatrixFactorizationModel extends Object implements Saveable, scala.Serializable Model representing the result of matrix factorization. Note: If you create the model directly using constructor, please be aware that fast prediction requires cached user/product features and their associated partitioners. param: rank Rank for the features in this model. param: userFeatures RDD of tuples where each tuple represents the userId and the features computed for this user. param: productFeatures RDD of tuples where each tuple represents the productId and the features computed for this product. See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  MatrixFactorizationModel.SaveLoadV1_0$  Constructor Summary Constructors  Constructor and Description MatrixFactorizationModel(int rank, RDD<scala.Tuple2<Object,double[]>> userFeatures, RDD<scala.Tuple2<Object,double[]>> productFeatures)  Method Summary Methods  Modifier and Type Method and Description static MatrixFactorizationModel load(SparkContext sc, String path) Load a model from the given path. double predict(int user, int product) Predict the rating of one user for one product. JavaRDD<Rating> predict(JavaPairRDD<Integer,Integer> usersProducts) Java-friendly version of MatrixFactorizationModel.predict. RDD<Rating> predict(RDD<scala.Tuple2<Object,Object>> usersProducts) Predict the rating of many users for many products. RDD<scala.Tuple2<Object,double[]>> productFeatures()  int rank()  Rating[] recommendProducts(int user, int num) Recommends products to a user. RDD<scala.Tuple2<Object,Rating[]>> recommendProductsForUsers(int num) Recommends top products for all users. Rating[] recommendUsers(int product, int num) Recommends users to a product. RDD<scala.Tuple2<Object,Rating[]>> recommendUsersForProducts(int num) Recommends top users for all products. void save(SparkContext sc, String path) Save this model to the given path. RDD<scala.Tuple2<Object,double[]>> userFeatures()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MatrixFactorizationModel public MatrixFactorizationModel(int rank, RDD<scala.Tuple2<Object,double[]>> userFeatures, RDD<scala.Tuple2<Object,double[]>> productFeatures) Method Detail load public static MatrixFactorizationModel load(SparkContext sc, String path) Load a model from the given path. The model should have been saved by Saveable.save. Parameters:sc - Spark context used for loading model files.path - Path specifying the directory to which the model was saved. Returns:Model instance rank public int rank() userFeatures public RDD<scala.Tuple2<Object,double[]>> userFeatures() productFeatures public RDD<scala.Tuple2<Object,double[]>> productFeatures() predict public double predict(int user, int product) Predict the rating of one user for one product. predict public RDD<Rating> predict(RDD<scala.Tuple2<Object,Object>> usersProducts) Predict the rating of many users for many products. The output RDD has an element per each element in the input RDD (including all duplicates) unless a user or product is missing in the training set. Parameters:usersProducts - RDD of (user, product) pairs. Returns:RDD of Ratings. predict public JavaRDD<Rating> predict(JavaPairRDD<Integer,Integer> usersProducts) Java-friendly version of MatrixFactorizationModel.predict. Parameters:usersProducts - (undocumented) Returns:(undocumented) recommendProducts public Rating[] recommendProducts(int user, int num) Recommends products to a user. Parameters:user - the user to recommend products tonum - how many products to return. The number returned may be less than this. Returns:Rating objects, each of which contains the given user ID, a product ID, and a "score" in the rating field. Each represents one recommended product, and they are sorted by score, decreasing. The first returned is the one predicted to be most strongly recommended to the user. The score is an opaque value that indicates how strongly recommended the product is. recommendUsers public Rating[] recommendUsers(int product, int num) Recommends users to a product. That is, this returns users who are most likely to be interested in a product. Parameters:product - the product to recommend users tonum - how many users to return. The number returned may be less than this. Returns:Rating objects, each of which contains a user ID, the given product ID, and a "score" in the rating field. Each represents one recommended user, and they are sorted by score, decreasing. The first returned is the one predicted to be most strongly recommended to the product. The score is an opaque value that indicates how strongly recommended the user is. save public void save(SparkContext sc, String path) Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. recommendProductsForUsers public RDD<scala.Tuple2<Object,Rating[]>> recommendProductsForUsers(int num) Recommends top products for all users. Parameters:num - how many products to return for every user. Returns:[(Int, Array[Rating])] objects, where every tuple contains a userID and an array of rating objects which contains the same userId, recommended productID and a "score" in the rating field. Semantics of score is same as recommendProducts API recommendUsersForProducts public RDD<scala.Tuple2<Object,Rating[]>> recommendUsersForProducts(int num) Recommends top users for all products. Parameters:num - how many users to return for every product. Returns:[(Int, Array[Rating])] objects, where every tuple contains a productID and an array of rating objects which contains the recommended userId, same productID and a "score" in the rating field. Semantics of score is same as recommendUsers API Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MatrixImplicits (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MatrixImplicits (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg Class MatrixImplicits Object org.apache.spark.mllib.linalg.MatrixImplicits public class MatrixImplicits extends Object Implicit methods available in Scala for converting Matrix to Matrix and vice versa. Constructor Summary Constructors  Constructor and Description MatrixImplicits()  Method Summary Methods  Modifier and Type Method and Description static DenseMatrix mlDenseMatrixToMLlibDenseMatrix(DenseMatrix m)  static DenseMatrix mllibDenseMatrixToMLDenseMatrix(DenseMatrix m)  static Matrix mllibMatrixToMLMatrix(Matrix m)  static SparseMatrix mllibSparseMatrixToMLSparseMatrix(SparseMatrix m)  static Matrix mlMatrixToMLlibMatrix(Matrix m)  static SparseMatrix mlSparseMatrixToMLlibSparseMatrix(SparseMatrix m)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MatrixImplicits public MatrixImplicits() Method Detail mllibMatrixToMLMatrix public static Matrix mllibMatrixToMLMatrix(Matrix m) mllibDenseMatrixToMLDenseMatrix public static DenseMatrix mllibDenseMatrixToMLDenseMatrix(DenseMatrix m) mllibSparseMatrixToMLSparseMatrix public static SparseMatrix mllibSparseMatrixToMLSparseMatrix(SparseMatrix m) mlMatrixToMLlibMatrix public static Matrix mlMatrixToMLlibMatrix(Matrix m) mlDenseMatrixToMLlibDenseMatrix public static DenseMatrix mlDenseMatrixToMLlibDenseMatrix(DenseMatrix m) mlSparseMatrixToMLlibSparseMatrix public static SparseMatrix mlSparseMatrixToMLlibSparseMatrix(SparseMatrix m) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MaxAbsScaler (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MaxAbsScaler (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class MaxAbsScaler Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<MaxAbsScalerModel> org.apache.spark.ml.feature.MaxAbsScaler All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class MaxAbsScaler extends Estimator<MaxAbsScalerModel> implements DefaultParamsWritable :: Experimental :: Rescale each feature individually to range [-1, 1] by dividing through the largest maximum absolute value in each feature. It does not shift/center the data, and thus does not destroy any sparsity. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MaxAbsScaler()  MaxAbsScaler(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  MaxAbsScaler copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  MaxAbsScalerModel fit(Dataset<?> dataset) Fits a model to the input data. static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static MaxAbsScaler load(String path)  static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  MaxAbsScaler setInputCol(String value)  MaxAbsScaler setOutputCol(String value)  static String toString()  StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Constructor Detail MaxAbsScaler public MaxAbsScaler(String uid) MaxAbsScaler public MaxAbsScaler() Method Detail load public static MaxAbsScaler load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setInputCol public MaxAbsScaler setInputCol(String value) setOutputCol public MaxAbsScaler setOutputCol(String value) fit public MaxAbsScalerModel fit(Dataset<?> dataset) Description copied from class: Estimator Fits a model to the input data. Specified by: fit in class Estimator<MaxAbsScalerModel> Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public MaxAbsScaler copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Estimator<MaxAbsScalerModel> Parameters:extra - (undocumented) Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MaxAbsScalerModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MaxAbsScalerModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class MaxAbsScalerModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<MaxAbsScalerModel> org.apache.spark.ml.feature.MaxAbsScalerModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class MaxAbsScalerModel extends Model<MaxAbsScalerModel> implements MLWritable :: Experimental :: Model fitted by MaxAbsScaler. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  MaxAbsScalerModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static MaxAbsScalerModel load(String path)  Vector maxAbs()  static Param<String> outputCol()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static MLReader<MaxAbsScalerModel> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  MaxAbsScalerModel setInputCol(String value)  MaxAbsScalerModel setOutputCol(String value)  static M setParent(Estimator<M> parent)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.MLWritable save Method Detail read public static MLReader<MaxAbsScalerModel> read() load public static MaxAbsScalerModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) maxAbs public Vector maxAbs() setInputCol public MaxAbsScalerModel setInputCol(String value) setOutputCol public MaxAbsScalerModel setOutputCol(String value) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public MaxAbsScalerModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<MaxAbsScalerModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MemoryEntry (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MemoryEntry (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.storage.memory Interface MemoryEntry<T> All Known Implementing Classes: DeserializedMemoryEntry, SerializedMemoryEntry public interface MemoryEntry<T> Method Summary Methods  Modifier and Type Method and Description scala.reflect.ClassTag<T> classTag()  org.apache.spark.memory.MemoryMode memoryMode()  long size()  Method Detail size long size() memoryMode org.apache.spark.memory.MemoryMode memoryMode() classTag scala.reflect.ClassTag<T> classTag() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MemoryParam (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MemoryParam (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class MemoryParam Object org.apache.spark.util.MemoryParam public class MemoryParam extends Object An extractor object for parsing JVM memory strings, such as "10g", into an Int representing the number of megabytes. Supports the same formats as Utils.memoryStringToMb. Constructor Summary Constructors  Constructor and Description MemoryParam()  Method Summary Methods  Modifier and Type Method and Description static scala.Option<Object> unapply(String str)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MemoryParam public MemoryParam() Method Detail unapply public static scala.Option<Object> unapply(String str) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MesosSchedulerBackendUtil (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MesosSchedulerBackendUtil (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler.cluster.mesos Class MesosSchedulerBackendUtil Object org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackendUtil public class MesosSchedulerBackendUtil extends Object A collection of utility functions which can be used by both the MesosSchedulerBackend and the MesosFineGrainedSchedulerBackend. Constructor Summary Constructors  Constructor and Description MesosSchedulerBackendUtil()  Method Summary Methods  Modifier and Type Method and Description static void addDockerInfo(org.apache.mesos.Protos.ContainerInfo.Builder container, String image, scala.Option<scala.collection.immutable.List<org.apache.mesos.Protos.Volume>> volumes, scala.Option<org.apache.mesos.Protos.ContainerInfo.DockerInfo.Network> network, scala.Option<scala.collection.immutable.List<org.apache.mesos.Protos.ContainerInfo.DockerInfo.PortMapping>> portmaps) Construct a DockerInfo structure and insert it into a ContainerInfo static scala.collection.immutable.List<org.apache.mesos.Protos.ContainerInfo.DockerInfo.PortMapping> parsePortMappingsSpec(String portmaps) Parse a comma-delimited list of port mapping specs, each of which takes the form host_port:container_port[:udp|:tcp] static scala.collection.immutable.List<org.apache.mesos.Protos.Volume> parseVolumesSpec(String volumes) Parse a comma-delimited list of volume specs, each of which takes the form [host-dir:]container-dir[:rw|:ro]. static void setupContainerBuilderDockerInfo(String imageName, SparkConf conf, org.apache.mesos.Protos.ContainerInfo.Builder builder) Setup a docker containerizer Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MesosSchedulerBackendUtil public MesosSchedulerBackendUtil() Method Detail parseVolumesSpec public static scala.collection.immutable.List<org.apache.mesos.Protos.Volume> parseVolumesSpec(String volumes) Parse a comma-delimited list of volume specs, each of which takes the form [host-dir:]container-dir[:rw|:ro]. Parameters:volumes - (undocumented) Returns:(undocumented) parsePortMappingsSpec public static scala.collection.immutable.List<org.apache.mesos.Protos.ContainerInfo.DockerInfo.PortMapping> parsePortMappingsSpec(String portmaps) Parse a comma-delimited list of port mapping specs, each of which takes the form host_port:container_port[:udp|:tcp] Note: the docker form is [ip:]host_port:container_port, but the DockerInfo message has no field for 'ip', and instead has a 'protocol' field. Docker itself only appears to support TCP, so this alternative form anticipates the expansion of the docker form to allow for a protocol and leaves open the chance for mesos to begin to accept an 'ip' field Parameters:portmaps - (undocumented) Returns:(undocumented) addDockerInfo public static void addDockerInfo(org.apache.mesos.Protos.ContainerInfo.Builder container, String image, scala.Option<scala.collection.immutable.List<org.apache.mesos.Protos.Volume>> volumes, scala.Option<org.apache.mesos.Protos.ContainerInfo.DockerInfo.Network> network, scala.Option<scala.collection.immutable.List<org.apache.mesos.Protos.ContainerInfo.DockerInfo.PortMapping>> portmaps) Construct a DockerInfo structure and insert it into a ContainerInfo Parameters:container - (undocumented)image - (undocumented)volumes - (undocumented)network - (undocumented)portmaps - (undocumented) setupContainerBuilderDockerInfo public static void setupContainerBuilderDockerInfo(String imageName, SparkConf conf, org.apache.mesos.Protos.ContainerInfo.Builder builder) Setup a docker containerizer Parameters:imageName - (undocumented)conf - (undocumented)builder - (undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MetaAlgorithmReadWrite (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MetaAlgorithmReadWrite (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.util Class MetaAlgorithmReadWrite Object org.apache.spark.ml.util.MetaAlgorithmReadWrite public class MetaAlgorithmReadWrite extends Object Default Meta-Algorithm read and write implementation. Constructor Summary Constructors  Constructor and Description MetaAlgorithmReadWrite()  Method Summary Methods  Modifier and Type Method and Description static scala.collection.immutable.Map<String,Params> getUidMap(Params instance) Examine the given estimator (which may be a compound estimator) and extract a mapping from UIDs to corresponding Params instances. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MetaAlgorithmReadWrite public MetaAlgorithmReadWrite() Method Detail getUidMap public static scala.collection.immutable.Map<String,Params> getUidMap(Params instance) Examine the given estimator (which may be a compound estimator) and extract a mapping from UIDs to corresponding Params instances. Parameters:instance - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Metadata (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Metadata (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class Metadata Object org.apache.spark.sql.types.Metadata All Implemented Interfaces: java.io.Serializable public class Metadata extends Object implements scala.Serializable :: DeveloperApi :: Metadata is a wrapper over Map[String, Any] that limits the value type to simple ones: Boolean, Long, Double, String, Metadata, Array[Boolean], Array[Long], Array[Double], Array[String], and Array[Metadata]. JSON is used for serialization. The default constructor is private. User should use either MetadataBuilder or Metadata.fromJson() to create Metadata instances. param: map an immutable map that stores the data See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description boolean contains(String key) Tests whether this Metadata contains a binding for a key. static Metadata empty() Returns an empty Metadata. boolean equals(Object obj)  static Metadata fromJson(String json) Creates a Metadata instance from JSON. boolean getBoolean(String key) Gets a Boolean. boolean[] getBooleanArray(String key) Gets a Boolean array. double getDouble(String key) Gets a Double. double[] getDoubleArray(String key) Gets a Double array. long getLong(String key) Gets a Long. long[] getLongArray(String key) Gets a Long array. Metadata getMetadata(String key) Gets a Metadata. Metadata[] getMetadataArray(String key) Gets a Metadata array. String getString(String key) Gets a String. String[] getStringArray(String key) Gets a String array. int hashCode()  String json() Converts to its JSON representation. String toString()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Method Detail empty public static Metadata empty() Returns an empty Metadata. fromJson public static Metadata fromJson(String json) Creates a Metadata instance from JSON. contains public boolean contains(String key) Tests whether this Metadata contains a binding for a key. getLong public long getLong(String key) Gets a Long. getDouble public double getDouble(String key) Gets a Double. getBoolean public boolean getBoolean(String key) Gets a Boolean. getString public String getString(String key) Gets a String. getMetadata public Metadata getMetadata(String key) Gets a Metadata. getLongArray public long[] getLongArray(String key) Gets a Long array. getDoubleArray public double[] getDoubleArray(String key) Gets a Double array. getBooleanArray public boolean[] getBooleanArray(String key) Gets a Boolean array. getStringArray public String[] getStringArray(String key) Gets a String array. getMetadataArray public Metadata[] getMetadataArray(String key) Gets a Metadata array. json public String json() Converts to its JSON representation. toString public String toString() Overrides: toString in class Object equals public boolean equals(Object obj) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MetadataBuilder (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MetadataBuilder (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class MetadataBuilder Object org.apache.spark.sql.types.MetadataBuilder public class MetadataBuilder extends Object :: DeveloperApi :: Builder for Metadata. If there is a key collision, the latter will overwrite the former. Constructor Summary Constructors  Constructor and Description MetadataBuilder()  Method Summary Methods  Modifier and Type Method and Description Metadata build() Builds the Metadata instance. MetadataBuilder putBoolean(String key, boolean value) Puts a Boolean. MetadataBuilder putBooleanArray(String key, boolean[] value) Puts a Boolean array. MetadataBuilder putDouble(String key, double value) Puts a Double. MetadataBuilder putDoubleArray(String key, double[] value) Puts a Double array. MetadataBuilder putLong(String key, long value) Puts a Long. MetadataBuilder putLongArray(String key, long[] value) Puts a Long array. MetadataBuilder putMetadata(String key, Metadata value) Puts a Metadata. MetadataBuilder putMetadataArray(String key, Metadata[] value) Puts a Metadata array. MetadataBuilder putNull(String key) Puts a null. MetadataBuilder putString(String key, String value) Puts a String. MetadataBuilder putStringArray(String key, String[] value) Puts a String array. MetadataBuilder remove(String key)  MetadataBuilder withMetadata(Metadata metadata) Include the content of an existing Metadata instance. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MetadataBuilder public MetadataBuilder() Method Detail withMetadata public MetadataBuilder withMetadata(Metadata metadata) Include the content of an existing Metadata instance. putNull public MetadataBuilder putNull(String key) Puts a null. putLong public MetadataBuilder putLong(String key, long value) Puts a Long. putDouble public MetadataBuilder putDouble(String key, double value) Puts a Double. putBoolean public MetadataBuilder putBoolean(String key, boolean value) Puts a Boolean. putString public MetadataBuilder putString(String key, String value) Puts a String. putMetadata public MetadataBuilder putMetadata(String key, Metadata value) Puts a Metadata. putLongArray public MetadataBuilder putLongArray(String key, long[] value) Puts a Long array. putDoubleArray public MetadataBuilder putDoubleArray(String key, double[] value) Puts a Double array. putBooleanArray public MetadataBuilder putBooleanArray(String key, boolean[] value) Puts a Boolean array. putStringArray public MetadataBuilder putStringArray(String key, String[] value) Puts a String array. putMetadataArray public MetadataBuilder putMetadataArray(String key, Metadata[] value) Puts a Metadata array. build public Metadata build() Builds the Metadata instance. remove public MetadataBuilder remove(String key) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MetadataUtils (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MetadataUtils (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.util Class MetadataUtils Object org.apache.spark.ml.util.MetadataUtils public class MetadataUtils extends Object Helper utilities for algorithms using ML metadata Constructor Summary Constructors  Constructor and Description MetadataUtils()  Method Summary Methods  Modifier and Type Method and Description static scala.collection.immutable.Map<Object,Object> getCategoricalFeatures(StructField featuresSchema) Examine a schema to identify categorical (Binary and Nominal) features. static int[] getFeatureIndicesFromNames(StructField col, String[] names) Takes a Vector column and a list of feature names, and returns the corresponding list of feature indices in the column, in order. static scala.Option<Object> getNumClasses(StructField labelSchema) Examine a schema to identify the number of classes in a label column. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MetadataUtils public MetadataUtils() Method Detail getNumClasses public static scala.Option<Object> getNumClasses(StructField labelSchema) Examine a schema to identify the number of classes in a label column. Returns None if the number of labels is not specified, or if the label column is continuous. Parameters:labelSchema - (undocumented) Returns:(undocumented) getCategoricalFeatures public static scala.collection.immutable.Map<Object,Object> getCategoricalFeatures(StructField featuresSchema) Examine a schema to identify categorical (Binary and Nominal) features. Parameters:featuresSchema - Schema of the features column. If a feature does not have metadata, it is assumed to be continuous. If a feature is Nominal, then it must have the number of values specified. Returns:Map: feature index --> number of categories. The map's set of keys will be the set of categorical feature indices. getFeatureIndicesFromNames public static int[] getFeatureIndicesFromNames(StructField col, String[] names) Takes a Vector column and a list of feature names, and returns the corresponding list of feature indices in the column, in order. Parameters:col - Vector column which must have feature names specified via attributesnames - List of feature names Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MethodIdentifier (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MethodIdentifier (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class MethodIdentifier<T> Object org.apache.spark.util.MethodIdentifier<T> All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class MethodIdentifier<T> extends Object implements scala.Product, scala.Serializable Helper class to identify a method. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MethodIdentifier(Class<T> cls, String name, String desc)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  Class<T> cls()  String desc()  abstract static boolean equals(Object that)  String name()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail MethodIdentifier public MethodIdentifier(Class<T> cls, String name, String desc) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() cls public Class<T> cls() name public String name() desc public String desc() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Milliseconds (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Milliseconds (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming Class Milliseconds Object org.apache.spark.streaming.Milliseconds public class Milliseconds extends Object Helper object that creates instance of Duration representing a given number of milliseconds. Constructor Summary Constructors  Constructor and Description Milliseconds()  Method Summary Methods  Modifier and Type Method and Description static Duration apply(long milliseconds)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Milliseconds public Milliseconds() Method Detail apply public static Duration apply(long milliseconds) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MinMaxScaler (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MinMaxScaler (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class MinMaxScaler Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<MinMaxScalerModel> org.apache.spark.ml.feature.MinMaxScaler All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class MinMaxScaler extends Estimator<MinMaxScalerModel> implements DefaultParamsWritable Rescale each feature individually to a common range [min, max] linearly using column summary statistics, which is also known as min-max normalization or Rescaling. The rescaled value for feature E is calculated as, Rescaled(e_i) = \frac{e_i - E_{min}}{E_{max} - E_{min}} * (max - min) + min For the case E_{max} == E_{min}, Rescaled(e_i) = 0.5 * (max + min). Note that since zero values will probably be transformed to non-zero values, output of the transformer will be DenseVector even for sparse input. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MinMaxScaler()  MinMaxScaler(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  MinMaxScaler copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  MinMaxScalerModel fit(Dataset<?> dataset) Fits a model to the input data. static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  static double getMax()  double getMax()  static double getMin()  double getMin()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static MinMaxScaler load(String path)  static DoubleParam max()  DoubleParam max() upper bound after transformation, shared by all features Default: 1.0 static DoubleParam min()  DoubleParam min() lower bound after transformation, shared by all features Default: 0.0 static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  MinMaxScaler setInputCol(String value)  MinMaxScaler setMax(double value)  MinMaxScaler setMin(double value)  MinMaxScaler setOutputCol(String value)  static String toString()  StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Constructor Detail MinMaxScaler public MinMaxScaler(String uid) MinMaxScaler public MinMaxScaler() Method Detail load public static MinMaxScaler load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() min public static DoubleParam min() getMin public static double getMin() max public static DoubleParam max() getMax public static double getMax() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setInputCol public MinMaxScaler setInputCol(String value) setOutputCol public MinMaxScaler setOutputCol(String value) setMin public MinMaxScaler setMin(double value) setMax public MinMaxScaler setMax(double value) fit public MinMaxScalerModel fit(Dataset<?> dataset) Description copied from class: Estimator Fits a model to the input data. Specified by: fit in class Estimator<MinMaxScalerModel> Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public MinMaxScaler copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Estimator<MinMaxScalerModel> Parameters:extra - (undocumented) Returns:(undocumented) min public DoubleParam min() lower bound after transformation, shared by all features Default: 0.0 Returns:(undocumented) getMin public double getMin() max public DoubleParam max() upper bound after transformation, shared by all features Default: 1.0 Returns:(undocumented) getMax public double getMax() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MinMaxScalerModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MinMaxScalerModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class MinMaxScalerModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<MinMaxScalerModel> org.apache.spark.ml.feature.MinMaxScalerModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class MinMaxScalerModel extends Model<MinMaxScalerModel> implements MLWritable Model fitted by MinMaxScaler. param: originalMin min value for each original column during fitting param: originalMax max value for each original column during fitting TODO: The transformer does not yet set the metadata in the output column (SPARK-8529). See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  MinMaxScalerModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  static double getMax()  double getMax()  static double getMin()  double getMin()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static MinMaxScalerModel load(String path)  static DoubleParam max()  DoubleParam max() upper bound after transformation, shared by all features Default: 1.0 static DoubleParam min()  DoubleParam min() lower bound after transformation, shared by all features Default: 0.0 Vector originalMax()  Vector originalMin()  static Param<String> outputCol()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static MLReader<MinMaxScalerModel> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  MinMaxScalerModel setInputCol(String value)  MinMaxScalerModel setMax(double value)  MinMaxScalerModel setMin(double value)  MinMaxScalerModel setOutputCol(String value)  static M setParent(Estimator<M> parent)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.MLWritable save Method Detail read public static MLReader<MinMaxScalerModel> read() load public static MinMaxScalerModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() min public static DoubleParam min() getMin public static double getMin() max public static DoubleParam max() getMax public static double getMax() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) originalMin public Vector originalMin() originalMax public Vector originalMax() setInputCol public MinMaxScalerModel setInputCol(String value) setOutputCol public MinMaxScalerModel setOutputCol(String value) setMin public MinMaxScalerModel setMin(double value) setMax public MinMaxScalerModel setMax(double value) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public MinMaxScalerModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<MinMaxScalerModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) min public DoubleParam min() lower bound after transformation, shared by all features Default: 0.0 Returns:(undocumented) getMin public double getMin() max public DoubleParam max() upper bound after transformation, shared by all features Default: 1.0 Returns:(undocumented) getMax public double getMax() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema) Validates and transforms the input schema. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Minutes (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Minutes (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming Class Minutes Object org.apache.spark.streaming.Minutes public class Minutes extends Object Helper object that creates instance of Duration representing a given number of minutes. Constructor Summary Constructors  Constructor and Description Minutes()  Method Summary Methods  Modifier and Type Method and Description static Duration apply(long minutes)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Minutes public Minutes() Method Detail apply public static Duration apply(long minutes) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Model (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Model (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml Class Model<M extends Model<M>> Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<M> All Implemented Interfaces: java.io.Serializable, Params, Identifiable Direct Known Subclasses: AFTSurvivalRegressionModel, ALSModel, BisectingKMeansModel, Bucketizer, ChiSqSelectorModel, CountVectorizerModel, CrossValidatorModel, GaussianMixtureModel, IDFModel, IsotonicRegressionModel, KMeansModel, LDAModel, MaxAbsScalerModel, MinMaxScalerModel, OneVsRestModel, PCAModel, PipelineModel, PredictionModel, RFormulaModel, StandardScalerModel, StringIndexerModel, TrainValidationSplitModel, VectorIndexerModel, Word2VecModel public abstract class Model<M extends Model<M>> extends Transformer :: DeveloperApi :: A fitted model, i.e., a Transformer produced by an Estimator. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Model()  Method Summary Methods  Modifier and Type Method and Description abstract M copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. boolean hasParent() Indicates whether this Model has a corresponding parent. Estimator<M> parent() The parent estimator that produced this model. M setParent(Estimator<M> parent) Sets the parent of this model (Java API). Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform, transform Methods inherited from class org.apache.spark.ml.PipelineStage transformSchema Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail Model public Model() Method Detail parent public Estimator<M> parent() The parent estimator that produced this model. Note: For ensembles' component Models, this value can be null. Returns:(undocumented) setParent public M setParent(Estimator<M> parent) Sets the parent of this model (Java API). Parameters:parent - (undocumented) Returns:(undocumented) hasParent public boolean hasParent() Indicates whether this Model has a corresponding parent. copy public abstract M copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Transformer Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MsSqlServerDialect (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MsSqlServerDialect (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class MsSqlServerDialect Object org.apache.spark.sql.jdbc.MsSqlServerDialect public class MsSqlServerDialect extends Object Constructor Summary Constructors  Constructor and Description MsSqlServerDialect()  Method Summary Methods  Modifier and Type Method and Description static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties)  static boolean canHandle(String url)  static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md)  static scala.Option<JdbcType> getJDBCType(DataType dt)  static String getTableExistsQuery(String table)  static String quoteIdentifier(String colName)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MsSqlServerDialect public MsSqlServerDialect() Method Detail canHandle public static boolean canHandle(String url) getCatalystType public static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) getJDBCType public static scala.Option<JdbcType> getJDBCType(DataType dt) quoteIdentifier public static String quoteIdentifier(String colName) getTableExistsQuery public static String getTableExistsQuery(String table) beforeFetch public static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MulticlassClassificationEvaluator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MulticlassClassificationEvaluator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.evaluation Class MulticlassClassificationEvaluator Object org.apache.spark.ml.evaluation.Evaluator org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class MulticlassClassificationEvaluator extends Evaluator implements DefaultParamsWritable :: Experimental :: Evaluator for multiclass classification, which expects two input columns: prediction and label. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MulticlassClassificationEvaluator()  MulticlassClassificationEvaluator(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  MulticlassClassificationEvaluator copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. double evaluate(Dataset<?> dataset) Evaluates model output and returns a scalar metric. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getLabelCol()  String getMetricName()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean isDefined(Param<?> param)  boolean isLargerBetter() Indicates whether the metric returned by evaluate should be maximized (true, default) or minimized (false). static boolean isSet(Param<?> param)  static Param<String> labelCol()  static MulticlassClassificationEvaluator load(String path)  Param<String> metricName() param for metric name in evaluation (supports "f1" (default), "weightedPrecision", "weightedRecall", "accuracy") static Param<?>[] params()  static Param<String> predictionCol()  static void save(String path)  static <T> Params set(Param<T> param, T value)  MulticlassClassificationEvaluator setLabelCol(String value)  MulticlassClassificationEvaluator setMetricName(String value)  MulticlassClassificationEvaluator setPredictionCol(String value)  static String toString()  String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.evaluation.Evaluator evaluate Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail MulticlassClassificationEvaluator public MulticlassClassificationEvaluator(String uid) MulticlassClassificationEvaluator public MulticlassClassificationEvaluator() Method Detail load public static MulticlassClassificationEvaluator load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) metricName public Param<String> metricName() param for metric name in evaluation (supports "f1" (default), "weightedPrecision", "weightedRecall", "accuracy") Returns:(undocumented) getMetricName public String getMetricName() setMetricName public MulticlassClassificationEvaluator setMetricName(String value) setPredictionCol public MulticlassClassificationEvaluator setPredictionCol(String value) setLabelCol public MulticlassClassificationEvaluator setLabelCol(String value) evaluate public double evaluate(Dataset<?> dataset) Description copied from class: Evaluator Evaluates model output and returns a scalar metric. The value of isLargerBetter specifies whether larger values are better. Specified by: evaluate in class Evaluator Parameters:dataset - a dataset that contains labels/observations and predictions. Returns:metric isLargerBetter public boolean isLargerBetter() Description copied from class: Evaluator Indicates whether the metric returned by evaluate should be maximized (true, default) or minimized (false). A given evaluator may support multiple metrics which may be maximized or minimized. Overrides: isLargerBetter in class Evaluator Returns:(undocumented) copy public MulticlassClassificationEvaluator copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Evaluator Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MulticlassMetrics (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MulticlassMetrics (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.evaluation Class MulticlassMetrics Object org.apache.spark.mllib.evaluation.MulticlassMetrics public class MulticlassMetrics extends Object Evaluator for multiclass classification. param: predictionAndLabels an RDD of (prediction, label) pairs. Constructor Summary Constructors  Constructor and Description MulticlassMetrics(RDD<scala.Tuple2<Object,Object>> predictionAndLabels)  Method Summary Methods  Modifier and Type Method and Description double accuracy() Returns accuracy (equals to the total number of correctly classified instances out of the total number of instances.) Matrix confusionMatrix() Returns confusion matrix: predicted classes are in columns, they are ordered by class label ascending, as in "labels" double falsePositiveRate(double label) Returns false positive rate for a given label (category) double fMeasure() Deprecated.  Use accuracy. Since 2.0.0. double fMeasure(double label) Returns f1-measure for a given label (category) double fMeasure(double label, double beta) Returns f-measure for a given label (category) double[] labels() Returns the sequence of labels in ascending order double precision() Deprecated.  Use accuracy. Since 2.0.0. double precision(double label) Returns precision for a given label (category) double recall() Deprecated.  Use accuracy. Since 2.0.0. double recall(double label) Returns recall for a given label (category) double truePositiveRate(double label) Returns true positive rate for a given label (category) double weightedFalsePositiveRate() Returns weighted false positive rate double weightedFMeasure() Returns weighted averaged f1-measure double weightedFMeasure(double beta) Returns weighted averaged f-measure double weightedPrecision() Returns weighted averaged precision double weightedRecall() Returns weighted averaged recall (equals to precision, recall and f-measure) double weightedTruePositiveRate() Returns weighted true positive rate (equals to precision, recall and f-measure) Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MulticlassMetrics public MulticlassMetrics(RDD<scala.Tuple2<Object,Object>> predictionAndLabels) Method Detail confusionMatrix public Matrix confusionMatrix() Returns confusion matrix: predicted classes are in columns, they are ordered by class label ascending, as in "labels" Returns:(undocumented) truePositiveRate public double truePositiveRate(double label) Returns true positive rate for a given label (category) Parameters:label - the label. Returns:(undocumented) falsePositiveRate public double falsePositiveRate(double label) Returns false positive rate for a given label (category) Parameters:label - the label. Returns:(undocumented) precision public double precision(double label) Returns precision for a given label (category) Parameters:label - the label. Returns:(undocumented) recall public double recall(double label) Returns recall for a given label (category) Parameters:label - the label. Returns:(undocumented) fMeasure public double fMeasure(double label, double beta) Returns f-measure for a given label (category) Parameters:label - the label.beta - the beta parameter. Returns:(undocumented) fMeasure public double fMeasure(double label) Returns f1-measure for a given label (category) Parameters:label - the label. Returns:(undocumented) precision public double precision() Deprecated. Use accuracy. Since 2.0.0. Returns precision Returns:(undocumented) recall public double recall() Deprecated. Use accuracy. Since 2.0.0. Returns recall (equals to precision for multiclass classifier because sum of all false positives is equal to sum of all false negatives) Returns:(undocumented) fMeasure public double fMeasure() Deprecated. Use accuracy. Since 2.0.0. Returns f-measure (equals to precision and recall because precision equals recall) Returns:(undocumented) accuracy public double accuracy() Returns accuracy (equals to the total number of correctly classified instances out of the total number of instances.) Returns:(undocumented) weightedTruePositiveRate public double weightedTruePositiveRate() Returns weighted true positive rate (equals to precision, recall and f-measure) Returns:(undocumented) weightedFalsePositiveRate public double weightedFalsePositiveRate() Returns weighted false positive rate Returns:(undocumented) weightedRecall public double weightedRecall() Returns weighted averaged recall (equals to precision, recall and f-measure) Returns:(undocumented) weightedPrecision public double weightedPrecision() Returns weighted averaged precision Returns:(undocumented) weightedFMeasure public double weightedFMeasure(double beta) Returns weighted averaged f-measure Parameters:beta - the beta parameter. Returns:(undocumented) weightedFMeasure public double weightedFMeasure() Returns weighted averaged f1-measure Returns:(undocumented) labels public double[] labels() Returns the sequence of labels in ascending order Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MultilabelMetrics (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MultilabelMetrics (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.evaluation Class MultilabelMetrics Object org.apache.spark.mllib.evaluation.MultilabelMetrics public class MultilabelMetrics extends Object Evaluator for multilabel classification. param: predictionAndLabels an RDD of (predictions, labels) pairs, both are non-null Arrays, each with unique elements. Constructor Summary Constructors  Constructor and Description MultilabelMetrics(RDD<scala.Tuple2<double[],double[]>> predictionAndLabels)  Method Summary Methods  Modifier and Type Method and Description double accuracy() Returns accuracy double f1Measure() Returns document-based f1-measure averaged by the number of documents double f1Measure(double label) Returns f1-measure for a given label (category) double hammingLoss() Returns Hamming-loss double[] labels() Returns the sequence of labels in ascending order double microF1Measure() Returns micro-averaged label-based f1-measure (equals to micro-averaged document-based f1-measure) double microPrecision() Returns micro-averaged label-based precision (equals to micro-averaged document-based precision) double microRecall() Returns micro-averaged label-based recall (equals to micro-averaged document-based recall) double precision() Returns document-based precision averaged by the number of documents double precision(double label) Returns precision for a given label (category) double recall() Returns document-based recall averaged by the number of documents double recall(double label) Returns recall for a given label (category) double subsetAccuracy() Returns subset accuracy (for equal sets of labels) Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MultilabelMetrics public MultilabelMetrics(RDD<scala.Tuple2<double[],double[]>> predictionAndLabels) Method Detail subsetAccuracy public double subsetAccuracy() Returns subset accuracy (for equal sets of labels) Returns:(undocumented) accuracy public double accuracy() Returns accuracy Returns:(undocumented) hammingLoss public double hammingLoss() Returns Hamming-loss Returns:(undocumented) precision public double precision() Returns document-based precision averaged by the number of documents Returns:(undocumented) recall public double recall() Returns document-based recall averaged by the number of documents Returns:(undocumented) f1Measure public double f1Measure() Returns document-based f1-measure averaged by the number of documents Returns:(undocumented) precision public double precision(double label) Returns precision for a given label (category) Parameters:label - the label. Returns:(undocumented) recall public double recall(double label) Returns recall for a given label (category) Parameters:label - the label. Returns:(undocumented) f1Measure public double f1Measure(double label) Returns f1-measure for a given label (category) Parameters:label - the label. Returns:(undocumented) microPrecision public double microPrecision() Returns micro-averaged label-based precision (equals to micro-averaged document-based precision) Returns:(undocumented) microRecall public double microRecall() Returns micro-averaged label-based recall (equals to micro-averaged document-based recall) Returns:(undocumented) microF1Measure public double microF1Measure() Returns micro-averaged label-based f1-measure (equals to micro-averaged document-based f1-measure) Returns:(undocumented) labels public double[] labels() Returns the sequence of labels in ascending order Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MultilayerPerceptronClassificationModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MultilayerPerceptronClassificationModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class MultilayerPerceptronClassificationModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<M> org.apache.spark.ml.PredictionModel<Vector,MultilayerPerceptronClassificationModel> org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class MultilayerPerceptronClassificationModel extends PredictionModel<Vector,MultilayerPerceptronClassificationModel> implements scala.Serializable, MLWritable :: Experimental :: Classification model based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax. param: uid uid param: layers array of layer sizes including input and output layers param: weights vector of initial weights for the model that consists of the weights of layers See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  MultilayerPerceptronClassificationModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getLabelCol()  String getLabelCol()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. int[] layers()  static MultilayerPerceptronClassificationModel load(String path)  int numFeatures() Returns the number of features the model was trained on. static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static MLReader<MultilayerPerceptronClassificationModel> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  static M setFeaturesCol(String value)  static M setParent(Estimator<M> parent)  static M setPredictionCol(String value)  static String toString()  static Dataset<Row> transform(Dataset<?> dataset)  static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  Vector weights()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.PredictionModel setFeaturesCol, setPredictionCol, transform, transformSchema Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Method Detail read public static MLReader<MultilayerPerceptronClassificationModel> read() load public static MultilayerPerceptronClassificationModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setFeaturesCol public static M setFeaturesCol(String value) setPredictionCol public static M setPredictionCol(String value) transformSchema public static StructType transformSchema(StructType schema) transform public static Dataset<Row> transform(Dataset<?> dataset) save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) layers public int[] layers() weights public Vector weights() numFeatures public int numFeatures() Description copied from class: PredictionModel Returns the number of features the model was trained on. If unknown, returns -1 Overrides: numFeatures in class PredictionModel<Vector,MultilayerPerceptronClassificationModel> copy public MultilayerPerceptronClassificationModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<MultilayerPerceptronClassificationModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MultilayerPerceptronClassifier (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MultilayerPerceptronClassifier (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class MultilayerPerceptronClassifier Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<Vector,MultilayerPerceptronClassifier,MultilayerPerceptronClassificationModel> org.apache.spark.ml.classification.MultilayerPerceptronClassifier All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class MultilayerPerceptronClassifier extends Predictor<Vector,MultilayerPerceptronClassifier,MultilayerPerceptronClassificationModel> implements DefaultParamsWritable :: Experimental :: Classifier trainer based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax. Number of inputs has to be equal to the size of feature vectors. Number of outputs has to be equal to the total number of labels. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MultilayerPerceptronClassifier()  MultilayerPerceptronClassifier(String uid)  Method Summary Methods  Modifier and Type Method and Description static IntParam blockSize()  IntParam blockSize() Block size for stacking input data in matrices to speed up the computation. static Params clear(Param<?> param)  MultilayerPerceptronClassifier copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static M fit(Dataset<?> dataset)  static M fit(Dataset<?> dataset, ParamMap paramMap)  static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static <T> scala.Option<T> get(Param<T> param)  static int getBlockSize()  int getBlockSize()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static Vector getInitialWeights()  Vector getInitialWeights()  static String getLabelCol()  String getLabelCol()  static int[] getLayers()  int[] getLayers()  static int getMaxIter()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static long getSeed()  static String getSolver()  String getSolver()  static double getStepSize()  static double getTol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<Vector> initialWeights()  Param<Vector> initialWeights() The initial weights of the model. static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static IntArrayParam layers()  IntArrayParam layers() Layer sizes including input size and output size. static MultilayerPerceptronClassifier load(String path)  static IntParam maxIter()  static Param<?>[] params()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static void save(String path)  static LongParam seed()  static <T> Params set(Param<T> param, T value)  MultilayerPerceptronClassifier setBlockSize(int value) Sets the value of param blockSize. static Learner setFeaturesCol(String value)  MultilayerPerceptronClassifier setInitialWeights(Vector value) Sets the value of param initialWeights. static Learner setLabelCol(String value)  MultilayerPerceptronClassifier setLayers(int[] value) Sets the value of param layers. MultilayerPerceptronClassifier setMaxIter(int value) Set the maximum number of iterations. static Learner setPredictionCol(String value)  MultilayerPerceptronClassifier setSeed(long value) Set the seed for weights initialization if weights are not set MultilayerPerceptronClassifier setSolver(String value) Sets the value of param solver. MultilayerPerceptronClassifier setStepSize(double value) Sets the value of param stepSize (applicable only for solver "gd"). MultilayerPerceptronClassifier setTol(double value) Set the convergence tolerance of iterations. static Param<String> solver()  Param<String> solver() The solver algorithm for optimization. static DoubleParam stepSize()  static DoubleParam tol()  static String toString()  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Predictor fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail MultilayerPerceptronClassifier public MultilayerPerceptronClassifier(String uid) MultilayerPerceptronClassifier public MultilayerPerceptronClassifier() Method Detail load public static MultilayerPerceptronClassifier load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) fit public static M fit(Dataset<?> dataset, ParamMap paramMap) fit public static scala.collection.Seq<M> fit(Dataset<?> dataset, ParamMap[] paramMaps) fit public static M fit(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setLabelCol public static Learner setLabelCol(String value) setFeaturesCol public static Learner setFeaturesCol(String value) setPredictionCol public static Learner setPredictionCol(String value) fit public static M fit(Dataset<?> dataset) transformSchema public static StructType transformSchema(StructType schema) seed public static final LongParam seed() getSeed public static final long getSeed() maxIter public static final IntParam maxIter() getMaxIter public static final int getMaxIter() tol public static final DoubleParam tol() getTol public static final double getTol() stepSize public static final DoubleParam stepSize() getStepSize public static final double getStepSize() layers public static final IntArrayParam layers() getLayers public static final int[] getLayers() blockSize public static final IntParam blockSize() getBlockSize public static final int getBlockSize() solver public static final Param<String> solver() getSolver public static final String getSolver() initialWeights public static final Param<Vector> initialWeights() getInitialWeights public static final Vector getInitialWeights() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setLayers public MultilayerPerceptronClassifier setLayers(int[] value) Sets the value of param layers. Parameters:value - (undocumented) Returns:(undocumented) setBlockSize public MultilayerPerceptronClassifier setBlockSize(int value) Sets the value of param blockSize. Default is 128. Parameters:value - (undocumented) Returns:(undocumented) setSolver public MultilayerPerceptronClassifier setSolver(String value) Sets the value of param solver. Default is "l-bfgs". Parameters:value - (undocumented) Returns:(undocumented) setMaxIter public MultilayerPerceptronClassifier setMaxIter(int value) Set the maximum number of iterations. Default is 100. Parameters:value - (undocumented) Returns:(undocumented) setTol public MultilayerPerceptronClassifier setTol(double value) Set the convergence tolerance of iterations. Smaller value will lead to higher accuracy with the cost of more iterations. Default is 1E-4. Parameters:value - (undocumented) Returns:(undocumented) setSeed public MultilayerPerceptronClassifier setSeed(long value) Set the seed for weights initialization if weights are not set Parameters:value - (undocumented) Returns:(undocumented) setInitialWeights public MultilayerPerceptronClassifier setInitialWeights(Vector value) Sets the value of param initialWeights. Parameters:value - (undocumented) Returns:(undocumented) setStepSize public MultilayerPerceptronClassifier setStepSize(double value) Sets the value of param stepSize (applicable only for solver "gd"). Default is 0.03. Parameters:value - (undocumented) Returns:(undocumented) copy public MultilayerPerceptronClassifier copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Predictor<Vector,MultilayerPerceptronClassifier,MultilayerPerceptronClassificationModel> Parameters:extra - (undocumented) Returns:(undocumented) layers public IntArrayParam layers() Layer sizes including input size and output size. Returns:(undocumented) getLayers public int[] getLayers() blockSize public IntParam blockSize() Block size for stacking input data in matrices to speed up the computation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128 Returns:(undocumented) getBlockSize public int getBlockSize() solver public Param<String> solver() The solver algorithm for optimization. Supported options: "gd" (minibatch gradient descent) or "l-bfgs". Default: "l-bfgs" Returns:(undocumented) getSolver public String getSolver() initialWeights public Param<Vector> initialWeights() The initial weights of the model. Returns:(undocumented) getInitialWeights public Vector getInitialWeights() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MultivariateGaussian (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MultivariateGaussian (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.distribution Class MultivariateGaussian Object org.apache.spark.mllib.stat.distribution.MultivariateGaussian All Implemented Interfaces: java.io.Serializable public class MultivariateGaussian extends Object implements scala.Serializable :: DeveloperApi :: This class provides basic functionality for a Multivariate Gaussian (Normal) Distribution. In the event that the covariance matrix is singular, the density will be computed in a reduced dimensional subspace under which the distribution is supported. (see http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Degenerate_case) param: mu The mean vector of the distribution param: sigma The covariance matrix of the distribution See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MultivariateGaussian(Vector mu, Matrix sigma)  Method Summary Methods  Modifier and Type Method and Description double logpdf(Vector x) Returns the log-density of this multivariate Gaussian at given point, x Vector mu()  double pdf(Vector x) Returns density of this multivariate Gaussian at given point, x Matrix sigma()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MultivariateGaussian public MultivariateGaussian(Vector mu, Matrix sigma) Method Detail mu public Vector mu() sigma public Matrix sigma() pdf public double pdf(Vector x) Returns density of this multivariate Gaussian at given point, x Parameters:x - (undocumented) Returns:(undocumented) logpdf public double logpdf(Vector x) Returns the log-density of this multivariate Gaussian at given point, x Parameters:x - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MultivariateOnlineSummarizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MultivariateOnlineSummarizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat Class MultivariateOnlineSummarizer Object org.apache.spark.mllib.stat.MultivariateOnlineSummarizer All Implemented Interfaces: java.io.Serializable, MultivariateStatisticalSummary public class MultivariateOnlineSummarizer extends Object implements MultivariateStatisticalSummary, scala.Serializable :: DeveloperApi :: MultivariateOnlineSummarizer implements MultivariateStatisticalSummary to compute the mean, variance, minimum, maximum, counts, and nonzero counts for instances in sparse or dense vector format in an online fashion. Two MultivariateOnlineSummarizer can be merged together to have a statistical summary of the corresponding joint dataset. A numerically stable algorithm is implemented to compute the mean and variance of instances: Reference: variance-wiki Zero elements (including explicit zero values) are skipped when calling add(), to have time complexity O(nnz) instead of O(n) for each column. For weighted instances, the unbiased estimation of variance is defined by the reliability weights: https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Reliability_weights. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MultivariateOnlineSummarizer()  Method Summary Methods  Modifier and Type Method and Description MultivariateOnlineSummarizer add(Vector sample) Add a new sample to this summarizer, and update the statistical summary. long count() Sample size. Vector max() Maximum value of each dimension. Vector mean() Sample mean of each dimension. MultivariateOnlineSummarizer merge(MultivariateOnlineSummarizer other) Merge another MultivariateOnlineSummarizer, and update the statistical summary. Vector min() Minimum value of each dimension. Vector normL1() L1 norm of each dimension. Vector normL2() L2 (Euclidian) norm of each dimension. Vector numNonzeros() Number of nonzero elements in each dimension. Vector variance() Unbiased estimate of sample variance of each dimension. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MultivariateOnlineSummarizer public MultivariateOnlineSummarizer() Method Detail add public MultivariateOnlineSummarizer add(Vector sample) Add a new sample to this summarizer, and update the statistical summary. Parameters:sample - The sample in dense/sparse vector format to be added into this summarizer. Returns:This MultivariateOnlineSummarizer object. merge public MultivariateOnlineSummarizer merge(MultivariateOnlineSummarizer other) Merge another MultivariateOnlineSummarizer, and update the statistical summary. (Note that it's in place merging; as a result, this object will be modified.) Parameters:other - The other MultivariateOnlineSummarizer to be merged. Returns:This MultivariateOnlineSummarizer object. mean public Vector mean() Sample mean of each dimension. Specified by: mean in interface MultivariateStatisticalSummary Returns:(undocumented) variance public Vector variance() Unbiased estimate of sample variance of each dimension. Specified by: variance in interface MultivariateStatisticalSummary Returns:(undocumented) count public long count() Sample size. Specified by: count in interface MultivariateStatisticalSummary Returns:(undocumented) numNonzeros public Vector numNonzeros() Number of nonzero elements in each dimension. Specified by: numNonzeros in interface MultivariateStatisticalSummary Returns:(undocumented) max public Vector max() Maximum value of each dimension. Specified by: max in interface MultivariateStatisticalSummary Returns:(undocumented) min public Vector min() Minimum value of each dimension. Specified by: min in interface MultivariateStatisticalSummary Returns:(undocumented) normL2 public Vector normL2() L2 (Euclidian) norm of each dimension. Specified by: normL2 in interface MultivariateStatisticalSummary Returns:(undocumented) normL1 public Vector normL1() L1 norm of each dimension. Specified by: normL1 in interface MultivariateStatisticalSummary Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MultivariateStatisticalSummary (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MultivariateStatisticalSummary (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat Interface MultivariateStatisticalSummary All Known Implementing Classes: MultivariateOnlineSummarizer public interface MultivariateStatisticalSummary Trait for multivariate statistical summary of a data matrix. Method Summary Methods  Modifier and Type Method and Description long count() Sample size. Vector max() Maximum value of each column. Vector mean() Sample mean vector. Vector min() Minimum value of each column. Vector normL1() L1 norm of each column Vector normL2() Euclidean magnitude of each column Vector numNonzeros() Number of nonzero elements (including explicitly presented zero values) in each column. Vector variance() Sample variance vector. Method Detail mean Vector mean() Sample mean vector. Returns:(undocumented) variance Vector variance() Sample variance vector. Should return a zero vector if the sample size is 1. Returns:(undocumented) count long count() Sample size. Returns:(undocumented) numNonzeros Vector numNonzeros() Number of nonzero elements (including explicitly presented zero values) in each column. Returns:(undocumented) max Vector max() Maximum value of each column. Returns:(undocumented) min Vector min() Minimum value of each column. Returns:(undocumented) normL2 Vector normL2() Euclidean magnitude of each column Returns:(undocumented) normL1 Vector normL1() L1 norm of each column Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MutableAggregationBuffer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MutableAggregationBuffer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.expressions Class MutableAggregationBuffer Object org.apache.spark.sql.expressions.MutableAggregationBuffer All Implemented Interfaces: java.io.Serializable, Row public abstract class MutableAggregationBuffer extends Object implements Row :: Experimental :: A Row representing a mutable aggregation buffer. This is not meant to be extended outside of Spark. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MutableAggregationBuffer()  Method Summary Methods  Modifier and Type Method and Description abstract void update(int i, Object value) Update the ith value of this buffer. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.sql.Row anyNull, apply, copy, equals, fieldIndex, get, getAnyValAs, getAs, getAs, getBoolean, getByte, getDate, getDecimal, getDouble, getFloat, getInt, getJavaMap, getList, getLong, getMap, getSeq, getShort, getString, getStruct, getTimestamp, getValuesMap, hashCode, isNullAt, length, mkString, mkString, mkString, schema, size, toSeq, toString Constructor Detail MutableAggregationBuffer public MutableAggregationBuffer() Method Detail update public abstract void update(int i, Object value) Update the ith value of this buffer. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MutablePair (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MutablePair (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util Class MutablePair<T1,T2> Object org.apache.spark.util.MutablePair<T1,T2> All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product, scala.Product2<T1,T2> public class MutablePair<T1,T2> extends Object implements scala.Product2<T1,T2>, scala.Product, scala.Serializable :: DeveloperApi :: A tuple of 2 elements. This can be used as an alternative to Scala's Tuple2 when we want to minimize object allocation. param: _1 Element 1 of this MutablePair param: _2 Element 2 of this MutablePair See Also:Serialized Form Constructor Summary Constructors  Constructor and Description MutablePair() No-arg constructor for serialization MutablePair(T1 _1, T2 _2)  Method Summary Methods  Modifier and Type Method and Description T1 _1()  T2 _2()  boolean canEqual(Object that)  abstract static boolean equals(Object that)  static int productArity()  static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  String toString()  MutablePair<T1,T2> update(T1 n1, T2 n2) Updates this pair with new values and returns itself Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Product2 _1$mcD$sp, _1$mcI$sp, _1$mcJ$sp, _2$mcD$sp, _2$mcI$sp, _2$mcJ$sp, productArity, productElement Methods inherited from interface scala.Product productIterator, productPrefix Methods inherited from interface scala.Equals equals Constructor Detail MutablePair public MutablePair(T1 _1, T2 _2) MutablePair public MutablePair() No-arg constructor for serialization Method Detail equals public abstract static boolean equals(Object that) productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() productArity public static int productArity() productElement public static Object productElement(int n) throws IndexOutOfBoundsException Throws: IndexOutOfBoundsException _1 public T1 _1() Specified by: _1 in interface scala.Product2<T1,T2> _2 public T2 _2() Specified by: _2 in interface scala.Product2<T1,T2> update public MutablePair<T1,T2> update(T1 n1, T2 n2) Updates this pair with new values and returns itself toString public String toString() Overrides: toString in class Object canEqual public boolean canEqual(Object that) Specified by: canEqual in interface scala.Equals Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method MySQLDialect (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="MySQLDialect (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class MySQLDialect Object org.apache.spark.sql.jdbc.MySQLDialect public class MySQLDialect extends Object Constructor Summary Constructors  Constructor and Description MySQLDialect()  Method Summary Methods  Modifier and Type Method and Description static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties)  abstract static boolean canEqual(Object that)  static boolean canHandle(String url)  abstract static boolean equals(Object that)  static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md)  static scala.Option<JdbcType> getJDBCType(DataType dt)  static String getTableExistsQuery(String table)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  static String quoteIdentifier(String colName)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail MySQLDialect public MySQLDialect() Method Detail canHandle public static boolean canHandle(String url) getCatalystType public static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) quoteIdentifier public static String quoteIdentifier(String colName) getTableExistsQuery public static String getTableExistsQuery(String table) getJDBCType public static scala.Option<JdbcType> getJDBCType(DataType dt) beforeFetch public static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties) canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NGram (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NGram (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class NGram Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.UnaryTransformer<scala.collection.Seq<String>,scala.collection.Seq<String>,NGram> org.apache.spark.ml.feature.NGram All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class NGram extends UnaryTransformer<scala.collection.Seq<String>,scala.collection.Seq<String>,NGram> implements DefaultParamsWritable A feature transformer that converts the input array of strings into an array of n-grams. Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words. When the input is empty, an empty array is returned. When the input array length is less than n (number of elements per n-gram), no n-grams are returned. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description NGram()  NGram(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  static T copy(ParamMap extra)  static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  int getN()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static NGram load(String path)  IntParam n() Minimum n-gram length, >= 1. static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  static T setInputCol(String value)  NGram setN(int value)  static T setOutputCol(String value)  static String toString()  static Dataset<Row> transform(Dataset<?> dataset)  static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.UnaryTransformer copy, setInputCol, setOutputCol, transform, transformSchema Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail NGram public NGram(String uid) NGram public NGram() Method Detail load public static NGram load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() setInputCol public static T setInputCol(String value) setOutputCol public static T setOutputCol(String value) transformSchema public static StructType transformSchema(StructType schema) transform public static Dataset<Row> transform(Dataset<?> dataset) copy public static T copy(ParamMap extra) save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) n public IntParam n() Minimum n-gram length, >= 1. Default: 2, bigram features Returns:(undocumented) setN public NGram setN(int value) getN public int getN() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NNLS.Workspace (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NNLS.Workspace (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Class NNLS.Workspace Object org.apache.spark.mllib.optimization.NNLS.Workspace Enclosing class: NNLS public static class NNLS.Workspace extends Object Constructor Summary Constructors  Constructor and Description NNLS.Workspace(int n)  Method Summary Methods  Modifier and Type Method and Description double[] dir()  double[] grad()  double[] lastDir()  int n()  double[] res()  double[] scratch()  void wipe()  double[] x()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail NNLS.Workspace public NNLS.Workspace(int n) Method Detail n public int n() scratch public double[] scratch() grad public double[] grad() x public double[] x() dir public double[] dir() lastDir public double[] lastDir() res public double[] res() wipe public void wipe() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NNLS (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NNLS (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Class NNLS Object org.apache.spark.mllib.optimization.NNLS public class NNLS extends Object Object used to solve nonnegative least squares problems using a modified projected gradient method. Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  NNLS.Workspace  Constructor Summary Constructors  Constructor and Description NNLS()  Method Summary Methods  Modifier and Type Method and Description static NNLS.Workspace createWorkspace(int n)  static double[] solve(double[] ata, double[] atb, NNLS.Workspace ws) Solve a least squares problem, possibly with nonnegativity constraints, by a modified projected gradient method. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail NNLS public NNLS() Method Detail createWorkspace public static NNLS.Workspace createWorkspace(int n) solve public static double[] solve(double[] ata, double[] atb, NNLS.Workspace ws) Solve a least squares problem, possibly with nonnegativity constraints, by a modified projected gradient method. That is, find x minimising ||Ax - b||_2 given A^T A and A^T b. We solve the problem min_x 1/2 x^T ata x^T - x^T atb subject to x >= 0 The method used is similar to one described by Polyak (B. T. Polyak, The conjugate gradient method in extremal problems, Zh. Vychisl. Mat. Mat. Fiz. 9(4)(1969), pp. 94-112) for bound- constrained nonlinear programming. Polyak unconditionally uses a conjugate gradient direction, however, while this method only uses a conjugate gradient direction if the last iteration did not cause a previously-inactive constraint to become active. Parameters:ata - (undocumented)atb - (undocumented)ws - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NaiveBayes (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NaiveBayes (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification Class NaiveBayes Object org.apache.spark.mllib.classification.NaiveBayes All Implemented Interfaces: java.io.Serializable public class NaiveBayes extends Object implements scala.Serializable Trains a Naive Bayes model given an RDD of (label, features) pairs. This is the Multinomial NB (http://tinyurl.com/lsdw6p) which can handle all kinds of discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. By making every vector a 0-1 vector, it can also be used as Bernoulli NB (http://tinyurl.com/p7c96j6). The input feature values must be nonnegative. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description NaiveBayes()  NaiveBayes(double lambda)  Method Summary Methods  Modifier and Type Method and Description double getLambda() Get the smoothing parameter. String getModelType() Get the model type. NaiveBayesModel run(RDD<LabeledPoint> data) Run the algorithm with the configured parameters on an input RDD of LabeledPoint entries. NaiveBayes setLambda(double lambda) Set the smoothing parameter. NaiveBayes setModelType(String modelType) Set the model type using a string (case-sensitive). static NaiveBayesModel train(RDD<LabeledPoint> input) Trains a Naive Bayes model given an RDD of (label, features) pairs. static NaiveBayesModel train(RDD<LabeledPoint> input, double lambda) Trains a Naive Bayes model given an RDD of (label, features) pairs. static NaiveBayesModel train(RDD<LabeledPoint> input, double lambda, String modelType) Trains a Naive Bayes model given an RDD of (label, features) pairs. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail NaiveBayes public NaiveBayes(double lambda) NaiveBayes public NaiveBayes() Method Detail train public static NaiveBayesModel train(RDD<LabeledPoint> input) Trains a Naive Bayes model given an RDD of (label, features) pairs. This is the default Multinomial NB (http://tinyurl.com/lsdw6p) which can handle all kinds of discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. This version of the method uses a default smoothing parameter of 1.0. Parameters:input - RDD of (label, array of features) pairs. Every vector should be a frequency vector or a count vector. Returns:(undocumented) train public static NaiveBayesModel train(RDD<LabeledPoint> input, double lambda) Trains a Naive Bayes model given an RDD of (label, features) pairs. This is the default Multinomial NB (http://tinyurl.com/lsdw6p) which can handle all kinds of discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. Parameters:input - RDD of (label, array of features) pairs. Every vector should be a frequency vector or a count vector.lambda - The smoothing parameter Returns:(undocumented) train public static NaiveBayesModel train(RDD<LabeledPoint> input, double lambda, String modelType) Trains a Naive Bayes model given an RDD of (label, features) pairs. The model type can be set to either Multinomial NB (http://tinyurl.com/lsdw6p) or Bernoulli NB (http://tinyurl.com/p7c96j6). The Multinomial NB can handle discrete count data and can be called by setting the model type to "multinomial". For example, it can be used with word counts or TF_IDF vectors of documents. The Bernoulli model fits presence or absence (0-1) counts. By making every vector a 0-1 vector and setting the model type to "bernoulli", the fits and predicts as Bernoulli NB. Parameters:input - RDD of (label, array of features) pairs. Every vector should be a frequency vector or a count vector.lambda - The smoothing parameter modelType - The type of NB model to fit from the enumeration NaiveBayesModels, can be multinomial or bernoulli Returns:(undocumented) setLambda public NaiveBayes setLambda(double lambda) Set the smoothing parameter. Default: 1.0. getLambda public double getLambda() Get the smoothing parameter. setModelType public NaiveBayes setModelType(String modelType) Set the model type using a string (case-sensitive). Supported options: "multinomial" (default) and "bernoulli". Parameters:modelType - (undocumented) Returns:(undocumented) getModelType public String getModelType() Get the model type. run public NaiveBayesModel run(RDD<LabeledPoint> data) Run the algorithm with the configured parameters on an input RDD of LabeledPoint entries. Parameters:data - RDD of LabeledPoint. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NaiveBayesModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NaiveBayesModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification Class NaiveBayesModel.SaveLoadV1_0$ Object org.apache.spark.mllib.classification.NaiveBayesModel.SaveLoadV1_0$ Enclosing class: NaiveBayesModel public static class NaiveBayesModel.SaveLoadV1_0$ extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description class  NaiveBayesModel.SaveLoadV1_0$.Data Model data for model import/export Field Summary Fields  Modifier and Type Field and Description static NaiveBayesModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description NaiveBayesModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description NaiveBayesModel load(SparkContext sc, String path)  void save(SparkContext sc, String path, org.apache.spark.mllib.classification.NaiveBayesModel.SaveLoadV1_0.Data data)  String thisClassName() Hard-code class name string in case it changes in the future String thisFormatVersion()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final NaiveBayesModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail NaiveBayesModel.SaveLoadV1_0$ public NaiveBayesModel.SaveLoadV1_0$() Method Detail thisFormatVersion public String thisFormatVersion() thisClassName public String thisClassName() Hard-code class name string in case it changes in the future save public void save(SparkContext sc, String path, org.apache.spark.mllib.classification.NaiveBayesModel.SaveLoadV1_0.Data data) load public NaiveBayesModel load(SparkContext sc, String path) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NaiveBayesModel.SaveLoadV2_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NaiveBayesModel.SaveLoadV2_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification Class NaiveBayesModel.SaveLoadV2_0$ Object org.apache.spark.mllib.classification.NaiveBayesModel.SaveLoadV2_0$ Enclosing class: NaiveBayesModel public static class NaiveBayesModel.SaveLoadV2_0$ extends Object Nested Class Summary Nested Classes  Modifier and Type Class and Description class  NaiveBayesModel.SaveLoadV2_0$.Data Model data for model import/export Field Summary Fields  Modifier and Type Field and Description static NaiveBayesModel.SaveLoadV2_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description NaiveBayesModel.SaveLoadV2_0$()  Method Summary Methods  Modifier and Type Method and Description NaiveBayesModel load(SparkContext sc, String path)  void save(SparkContext sc, String path, org.apache.spark.mllib.classification.NaiveBayesModel.SaveLoadV2_0.Data data)  String thisClassName() Hard-code class name string in case it changes in the future String thisFormatVersion()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final NaiveBayesModel.SaveLoadV2_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail NaiveBayesModel.SaveLoadV2_0$ public NaiveBayesModel.SaveLoadV2_0$() Method Detail thisFormatVersion public String thisFormatVersion() thisClassName public String thisClassName() Hard-code class name string in case it changes in the future save public void save(SparkContext sc, String path, org.apache.spark.mllib.classification.NaiveBayesModel.SaveLoadV2_0.Data data) load public NaiveBayesModel load(SparkContext sc, String path) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NaiveBayesModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NaiveBayesModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.classification Class NaiveBayesModel Object org.apache.spark.mllib.classification.NaiveBayesModel All Implemented Interfaces: java.io.Serializable, ClassificationModel, Saveable public class NaiveBayesModel extends Object implements ClassificationModel, scala.Serializable, Saveable Model for Naive Bayes Classifiers. param: labels list of labels param: pi log of class priors, whose dimension is C, number of labels param: theta log of class conditional probabilities, whose dimension is C-by-D, where D is number of features param: modelType The type of NB model to fit can be "multinomial" or "bernoulli" See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  NaiveBayesModel.SaveLoadV1_0$  static class  NaiveBayesModel.SaveLoadV2_0$  Method Summary Methods  Modifier and Type Method and Description double[] labels()  static NaiveBayesModel load(SparkContext sc, String path)  String modelType()  double[] pi()  RDD<Object> predict(RDD<Vector> testData) Predict values for the given data set using the model trained. double predict(Vector testData) Predict values for a single data point using the model trained. RDD<Vector> predictProbabilities(RDD<Vector> testData) Predict values for the given data set using the model trained. Vector predictProbabilities(Vector testData) Predict posterior class probabilities for a single data point using the model trained. void save(SparkContext sc, String path) Save this model to the given path. double[][] theta()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.classification.ClassificationModel predict Method Detail load public static NaiveBayesModel load(SparkContext sc, String path) labels public double[] labels() pi public double[] pi() theta public double[][] theta() modelType public String modelType() predict public RDD<Object> predict(RDD<Vector> testData) Description copied from interface: ClassificationModel Predict values for the given data set using the model trained. Specified by: predict in interface ClassificationModel Parameters:testData - RDD representing data points to be predicted Returns:an RDD[Double] where each entry contains the corresponding prediction predict public double predict(Vector testData) Description copied from interface: ClassificationModel Predict values for a single data point using the model trained. Specified by: predict in interface ClassificationModel Parameters:testData - array representing a single data point Returns:predicted category from the trained model predictProbabilities public RDD<Vector> predictProbabilities(RDD<Vector> testData) Predict values for the given data set using the model trained. Parameters:testData - RDD representing data points to be predicted Returns:an RDD[Vector] where each entry contains the predicted posterior class probabilities, in the same order as class labels predictProbabilities public Vector predictProbabilities(Vector testData) Predict posterior class probabilities for a single data point using the model trained. Parameters:testData - array representing a single data point Returns:predicted posterior class probabilities from the trained model, in the same order as class labels save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NarrowDependency (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NarrowDependency (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class NarrowDependency<T> Object org.apache.spark.Dependency<T> org.apache.spark.NarrowDependency<T> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: OneToOneDependency, RangeDependency public abstract class NarrowDependency<T> extends Dependency<T> :: DeveloperApi :: Base class for dependencies where each partition of the child RDD depends on a small number of partitions of the parent RDD. Narrow dependencies allow for pipelined execution. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description NarrowDependency(RDD<T> _rdd)  Method Summary Methods  Modifier and Type Method and Description abstract scala.collection.Seq<Object> getParents(int partitionId) Get the parent partitions for a child partition. RDD<T> rdd()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail NarrowDependency public NarrowDependency(RDD<T> _rdd) Method Detail getParents public abstract scala.collection.Seq<Object> getParents(int partitionId) Get the parent partitions for a child partition. Parameters:partitionId - a partition of the child RDD Returns:the partitions of the parent RDD that the child partition depends upon rdd public RDD<T> rdd() Specified by: rdd in class Dependency<T> Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$ Object org.apache.spark.rdd.NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$ All Implemented Interfaces: java.io.Serializable Enclosing class: NewHadoopRDD<K,V> public static class NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$ public NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NewHadoopRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NewHadoopRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class NewHadoopRDD<K,V> Object org.apache.spark.rdd.RDD<scala.Tuple2<K,V>> org.apache.spark.rdd.NewHadoopRDD<K,V> All Implemented Interfaces: java.io.Serializable public class NewHadoopRDD<K,V> extends RDD<scala.Tuple2<K,V>> :: DeveloperApi :: An RDD that provides core functionality for reading data stored in Hadoop (e.g., files in HDFS, sources in HBase, or S3), using the new MapReduce API (org.apache.hadoop.mapreduce). Note: Instantiating this class directly is not recommended, please use org.apache.spark.SparkContext.newAPIHadoopRDD() param: sc The SparkContext to associate the RDD with. param: inputFormatClass Storage format of the data to be read. param: keyClass Class of the key associated with the inputFormatClass. param: valueClass Class of the value associated with the inputFormatClass. See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  NewHadoopRDD.NewHadoopMapPartitionsWithSplitRDD$  Constructor Summary Constructors  Constructor and Description NewHadoopRDD(SparkContext sc, Class<? extends org.apache.hadoop.mapreduce.InputFormat<K,V>> inputFormatClass, Class<K> keyClass, Class<V> valueClass, org.apache.hadoop.conf.Configuration _conf)  Method Summary Methods  Modifier and Type Method and Description static RDD<T> $plus$plus(RDD<T> other)  static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29)  static RDD<T> cache()  static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5)  static void checkpoint()  static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord)  static boolean coalesce$default$2()  static scala.Option<PartitionCoalescer> coalesce$default$3()  static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer)  static Object collect()  static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28)  InterruptibleIterator<scala.Tuple2<K,V>> compute(Partition theSplit, TaskContext context) :: DeveloperApi :: Implemented by subclasses to compute a given partition. static Object CONFIGURATION_INSTANTIATION_LOCK() Configuration's constructor is not threadsafe (see SPARK-1097 and HADOOP-10456). static SparkContext context()  static long count()  static PartialResult<BoundedDouble> countApprox(long timeout, double confidence)  static double countApprox$default$2()  static long countApproxDistinct(double relativeSD)  static long countApproxDistinct(int p, int sp)  static double countApproxDistinct$default$1()  static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord)  static scala.math.Ordering<T> countByValue$default$1()  static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord)  static double countByValueApprox$default$2()  static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence)  static scala.collection.Seq<Dependency<?>> dependencies()  static RDD<T> distinct()  static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> distinct$default$2(int numPartitions)  static RDD<T> filter(scala.Function1<T,Object> f)  static T first()  static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4)  static T fold(T zeroValue, scala.Function2<T,T,T> op)  static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f)  static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f)  static scala.Option<String> getCheckpointFile()  org.apache.hadoop.conf.Configuration getConf()  static int getNumPartitions()  Partition[] getPartitions() Implemented by subclasses to return the set of partitions in this RDD. scala.collection.Seq<String> getPreferredLocations(Partition hsplit) Optionally overridden by subclasses to specify placement preferences. static StorageLevel getStorageLevel()  static RDD<Object> glom()  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord)  static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p)  static int id()  static RDD<T> intersection(RDD<T> other)  static RDD<T> intersection(RDD<T> other, int numPartitions)  static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner)  static boolean isCheckpointed()  static boolean isEmpty()  static scala.collection.Iterator<T> iterator(Partition split, TaskContext context)  static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f)  static RDD<T> localCheckpoint()  static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3)  static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6)  static <U> boolean mapPartitions$default$2()  static <U> boolean mapPartitionsInternal$default$2()  static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8)  static <U> boolean mapPartitionsWithIndex$default$2()  <U> RDD<U> mapPartitionsWithInputSplit(scala.Function2<org.apache.hadoop.mapreduce.InputSplit,scala.collection.Iterator<scala.Tuple2<K,V>>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$1) Maps over a partition, providing the InputSplit that was used as the base of the partition. static T max(scala.math.Ordering<T> ord)  static T min(scala.math.Ordering<T> ord)  static void name_$eq(String x$1)  static String name()  static scala.Option<Partitioner> partitioner()  static Partition[] partitions()  NewHadoopRDD<K,V> persist(StorageLevel storageLevel) Set this RDD's storage level to persist its values across operations after the first time it is computed. static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding)  static RDD<String> pipe(String command)  static RDD<String> pipe(String command, scala.collection.Map<String,String> env)  static scala.collection.Map<String,String> pipe$default$2()  static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3()  static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4()  static boolean pipe$default$5()  static int pipe$default$6()  static String pipe$default$7()  static scala.collection.Seq<String> preferredLocations(Partition split)  static RDD<T>[] randomSplit(double[] weights, long seed)  static long randomSplit$default$2()  static T reduce(scala.Function2<T,T,T> f)  static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> repartition$default$2(int numPartitions)  static RDD<T> sample(boolean withReplacement, double fraction, long seed)  static long sample$default$3()  static void saveAsObjectFile(String path)  static void saveAsTextFile(String path)  static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec)  static RDD<T> setName(String _name)  static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag)  static <K> boolean sortBy$default$2()  static <K> int sortBy$default$3()  static SparkContext sparkContext()  static RDD<T> subtract(RDD<T> other)  static RDD<T> subtract(RDD<T> other, int numPartitions)  static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p)  static Object take(int num)  static Object takeOrdered(int num, scala.math.Ordering<T> ord)  static Object takeSample(boolean withReplacement, int num, long seed)  static long takeSample$default$3()  static String toDebugString()  static JavaRDD<T> toJavaRDD()  static scala.collection.Iterator<T> toLocalIterator()  static Object top(int num, scala.math.Ordering<T> ord)  static String toString()  static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30)  static <U> int treeAggregate$default$4(U zeroValue)  static T treeReduce(scala.Function2<T,T,T> f, int depth)  static int treeReduce$default$2()  static RDD<T> union(RDD<T> other)  static RDD<T> unpersist(boolean blocking)  static boolean unpersist$default$1()  static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27)  static RDD<scala.Tuple2<T,Object>> zipWithIndex()  static RDD<scala.Tuple2<T,Object>> zipWithUniqueId()  Methods inherited from class org.apache.spark.rdd.RDD aggregate, cache, cartesian, checkpoint, coalesce, collect, collect, context, count, countApprox, countApproxDistinct, countApproxDistinct, countByValue, countByValueApprox, dependencies, distinct, distinct, doubleRDDToDoubleRDDFunctions, filter, first, flatMap, fold, foreach, foreachPartition, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupBy, id, intersection, intersection, intersection, isCheckpointed, isEmpty, iterator, keyBy, localCheckpoint, map, mapPartitions, mapPartitionsWithIndex, max, min, name, numericRDDToDoubleRDDFunctions, partitioner, partitions, persist, pipe, pipe, pipe, preferredLocations, randomSplit, rddToAsyncRDDActions, rddToOrderedRDDFunctions, rddToPairRDDFunctions, rddToSequenceFileRDDFunctions, reduce, repartition, sample, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortBy, sparkContext, subtract, subtract, subtract, take, takeOrdered, takeSample, toDebugString, toJavaRDD, toLocalIterator, top, toString, treeAggregate, treeReduce, union, unpersist, zip, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail NewHadoopRDD public NewHadoopRDD(SparkContext sc, Class<? extends org.apache.hadoop.mapreduce.InputFormat<K,V>> inputFormatClass, Class<K> keyClass, Class<V> valueClass, org.apache.hadoop.conf.Configuration _conf) Method Detail CONFIGURATION_INSTANTIATION_LOCK public static Object CONFIGURATION_INSTANTIATION_LOCK() Configuration's constructor is not threadsafe (see SPARK-1097 and HADOOP-10456). Therefore, we synchronize on this lock before calling new Configuration(). Returns:(undocumented) partitioner public static scala.Option<Partitioner> partitioner() sparkContext public static SparkContext sparkContext() id public static int id() name public static String name() name_$eq public static void name_$eq(String x$1) setName public static RDD<T> setName(String _name) cache public static RDD<T> cache() unpersist public static RDD<T> unpersist(boolean blocking) getStorageLevel public static StorageLevel getStorageLevel() dependencies public static final scala.collection.Seq<Dependency<?>> dependencies() partitions public static final Partition[] partitions() getNumPartitions public static final int getNumPartitions() preferredLocations public static final scala.collection.Seq<String> preferredLocations(Partition split) iterator public static final scala.collection.Iterator<T> iterator(Partition split, TaskContext context) map public static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3) flatMap public static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4) filter public static RDD<T> filter(scala.Function1<T,Object> f) distinct public static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord) distinct public static RDD<T> distinct() repartition public static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord) coalesce public static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord) sample public static RDD<T> sample(boolean withReplacement, double fraction, long seed) randomSplit public static RDD<T>[] randomSplit(double[] weights, long seed) takeSample public static Object takeSample(boolean withReplacement, int num, long seed) union public static RDD<T> union(RDD<T> other) $plus$plus public static RDD<T> $plus$plus(RDD<T> other) sortBy public static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag) intersection public static RDD<T> intersection(RDD<T> other) intersection public static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord) intersection public static RDD<T> intersection(RDD<T> other, int numPartitions) glom public static RDD<Object> glom() cartesian public static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord) pipe public static RDD<String> pipe(String command) pipe public static RDD<String> pipe(String command, scala.collection.Map<String,String> env) pipe public static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding) mapPartitions public static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6) mapPartitionsWithIndex public static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8) zip public static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27) foreach public static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f) foreachPartition public static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) collect public static Object collect() toLocalIterator public static scala.collection.Iterator<T> toLocalIterator() collect public static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28) subtract public static RDD<T> subtract(RDD<T> other) subtract public static RDD<T> subtract(RDD<T> other, int numPartitions) subtract public static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord) reduce public static T reduce(scala.Function2<T,T,T> f) treeReduce public static T treeReduce(scala.Function2<T,T,T> f, int depth) fold public static T fold(T zeroValue, scala.Function2<T,T,T> op) aggregate public static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29) treeAggregate public static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30) count public static long count() countApprox public static PartialResult<BoundedDouble> countApprox(long timeout, double confidence) countByValue public static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord) countByValueApprox public static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord) countApproxDistinct public static long countApproxDistinct(int p, int sp) countApproxDistinct public static long countApproxDistinct(double relativeSD) zipWithIndex public static RDD<scala.Tuple2<T,Object>> zipWithIndex() zipWithUniqueId public static RDD<scala.Tuple2<T,Object>> zipWithUniqueId() take public static Object take(int num) first public static T first() top public static Object top(int num, scala.math.Ordering<T> ord) takeOrdered public static Object takeOrdered(int num, scala.math.Ordering<T> ord) max public static T max(scala.math.Ordering<T> ord) min public static T min(scala.math.Ordering<T> ord) isEmpty public static boolean isEmpty() saveAsTextFile public static void saveAsTextFile(String path) saveAsTextFile public static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) saveAsObjectFile public static void saveAsObjectFile(String path) keyBy public static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f) checkpoint public static void checkpoint() localCheckpoint public static RDD<T> localCheckpoint() isCheckpointed public static boolean isCheckpointed() getCheckpointFile public static scala.Option<String> getCheckpointFile() context public static SparkContext context() toDebugString public static String toDebugString() toString public static String toString() toJavaRDD public static JavaRDD<T> toJavaRDD() sample$default$3 public static long sample$default$3() mapPartitionsWithIndex$default$2 public static <U> boolean mapPartitionsWithIndex$default$2() unpersist$default$1 public static boolean unpersist$default$1() distinct$default$2 public static scala.math.Ordering<T> distinct$default$2(int numPartitions) coalesce$default$2 public static boolean coalesce$default$2() coalesce$default$3 public static scala.Option<PartitionCoalescer> coalesce$default$3() coalesce$default$4 public static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer) repartition$default$2 public static scala.math.Ordering<T> repartition$default$2(int numPartitions) subtract$default$3 public static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p) intersection$default$3 public static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner) randomSplit$default$2 public static long randomSplit$default$2() sortBy$default$2 public static <K> boolean sortBy$default$2() sortBy$default$3 public static <K> int sortBy$default$3() mapPartitions$default$2 public static <U> boolean mapPartitions$default$2() groupBy$default$4 public static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p) pipe$default$2 public static scala.collection.Map<String,String> pipe$default$2() pipe$default$3 public static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3() pipe$default$4 public static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4() pipe$default$5 public static boolean pipe$default$5() pipe$default$6 public static int pipe$default$6() pipe$default$7 public static String pipe$default$7() treeReduce$default$2 public static int treeReduce$default$2() treeAggregate$default$4 public static <U> int treeAggregate$default$4(U zeroValue) countApprox$default$2 public static double countApprox$default$2() countByValue$default$1 public static scala.math.Ordering<T> countByValue$default$1() countByValueApprox$default$2 public static double countByValueApprox$default$2() countByValueApprox$default$3 public static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence) takeSample$default$3 public static long takeSample$default$3() countApproxDistinct$default$1 public static double countApproxDistinct$default$1() mapPartitionsInternal$default$2 public static <U> boolean mapPartitionsInternal$default$2() getConf public org.apache.hadoop.conf.Configuration getConf() getPartitions public Partition[] getPartitions() Description copied from class: RDD Implemented by subclasses to return the set of partitions in this RDD. This method will only be called once, so it is safe to implement a time-consuming computation in it. The partitions in this array must satisfy the following property: rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index } Returns:(undocumented) compute public InterruptibleIterator<scala.Tuple2<K,V>> compute(Partition theSplit, TaskContext context) Description copied from class: RDD :: DeveloperApi :: Implemented by subclasses to compute a given partition. Specified by: compute in class RDD<scala.Tuple2<K,V>> Parameters:theSplit - (undocumented)context - (undocumented) Returns:(undocumented) mapPartitionsWithInputSplit public <U> RDD<U> mapPartitionsWithInputSplit(scala.Function2<org.apache.hadoop.mapreduce.InputSplit,scala.collection.Iterator<scala.Tuple2<K,V>>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$1) Maps over a partition, providing the InputSplit that was used as the base of the partition. getPreferredLocations public scala.collection.Seq<String> getPreferredLocations(Partition hsplit) Description copied from class: RDD Optionally overridden by subclasses to specify placement preferences. Parameters:hsplit - (undocumented) Returns:(undocumented) persist public NewHadoopRDD<K,V> persist(StorageLevel storageLevel) Description copied from class: RDD Set this RDD's storage level to persist its values across operations after the first time it is computed. This can only be used to assign a new storage level if the RDD does not have a storage level set yet. Local checkpointing is an exception. Overrides: persist in class RDD<scala.Tuple2<K,V>> Parameters:storageLevel - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Node (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Node (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.model Class Node Object org.apache.spark.mllib.tree.model.Node All Implemented Interfaces: java.io.Serializable public class Node extends Object implements scala.Serializable :: DeveloperApi :: Node in a decision tree. About node indexing: Nodes are indexed from 1. Node 1 is the root; nodes 2, 3 are the left, right children. Node index 0 is not used. param: id integer node id, from 1 param: predict predicted value at the node param: impurity current node impurity param: isLeaf whether the node is a leaf param: split split to calculate left and right nodes param: leftNode left child param: rightNode right child param: stats information gain stats See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Node(int id, Predict predict, double impurity, boolean isLeaf, scala.Option<Split> split, scala.Option<Node> leftNode, scala.Option<Node> rightNode, scala.Option<InformationGainStats> stats)  Method Summary Methods  Modifier and Type Method and Description static Node apply(int nodeIndex, Predict predict, double impurity, boolean isLeaf) Construct a node with nodeIndex, predict, impurity and isLeaf parameters. static Node emptyNode(int nodeIndex) Return a node with the given node id (but nothing else set). static Node getNode(int nodeIndex, Node rootNode) Traces down from a root node to get the node with the given node index. int id()  double impurity()  static int indexToLevel(int nodeIndex) Return the level of a tree which the given node is in. boolean isLeaf()  static boolean isLeftChild(int nodeIndex) Returns true if this is a left child. static int leftChildIndex(int nodeIndex) Return the index of the left child of this node. scala.Option<Node> leftNode()  static int maxNodesInLevel(int level) Return the maximum number of nodes which can be in the given level of the tree. static int parentIndex(int nodeIndex) Get the parent index of the given node, or 0 if it is the root. Predict predict()  double predict(Vector features) predict value if node is not leaf static int rightChildIndex(int nodeIndex) Return the index of the right child of this node. scala.Option<Node> rightNode()  scala.Option<Split> split()  static int startIndexInLevel(int level) Return the index of the first node in the given level. scala.Option<InformationGainStats> stats()  String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail Node public Node(int id, Predict predict, double impurity, boolean isLeaf, scala.Option<Split> split, scala.Option<Node> leftNode, scala.Option<Node> rightNode, scala.Option<InformationGainStats> stats) Method Detail emptyNode public static Node emptyNode(int nodeIndex) Return a node with the given node id (but nothing else set). Parameters:nodeIndex - (undocumented) Returns:(undocumented) apply public static Node apply(int nodeIndex, Predict predict, double impurity, boolean isLeaf) Construct a node with nodeIndex, predict, impurity and isLeaf parameters. This is used in DecisionTree.findBestSplits to construct child nodes after finding the best splits for parent nodes. Other fields are set at next level. Parameters:nodeIndex - integer node id, from 1predict - predicted value at the nodeimpurity - current node impurityisLeaf - whether the node is a leaf Returns:new node instance leftChildIndex public static int leftChildIndex(int nodeIndex) Return the index of the left child of this node. Parameters:nodeIndex - (undocumented) Returns:(undocumented) rightChildIndex public static int rightChildIndex(int nodeIndex) Return the index of the right child of this node. Parameters:nodeIndex - (undocumented) Returns:(undocumented) parentIndex public static int parentIndex(int nodeIndex) Get the parent index of the given node, or 0 if it is the root. Parameters:nodeIndex - (undocumented) Returns:(undocumented) indexToLevel public static int indexToLevel(int nodeIndex) Return the level of a tree which the given node is in. Parameters:nodeIndex - (undocumented) Returns:(undocumented) isLeftChild public static boolean isLeftChild(int nodeIndex) Returns true if this is a left child. Note: Returns false for the root. Parameters:nodeIndex - (undocumented) Returns:(undocumented) maxNodesInLevel public static int maxNodesInLevel(int level) Return the maximum number of nodes which can be in the given level of the tree. Parameters:level - Level of tree (0 = root). Returns:(undocumented) startIndexInLevel public static int startIndexInLevel(int level) Return the index of the first node in the given level. Parameters:level - Level of tree (0 = root). Returns:(undocumented) getNode public static Node getNode(int nodeIndex, Node rootNode) Traces down from a root node to get the node with the given node index. This assumes the node exists. Parameters:nodeIndex - (undocumented)rootNode - (undocumented) Returns:(undocumented) id public int id() predict public Predict predict() impurity public double impurity() isLeaf public boolean isLeaf() split public scala.Option<Split> split() leftNode public scala.Option<Node> leftNode() rightNode public scala.Option<Node> rightNode() stats public scala.Option<InformationGainStats> stats() toString public String toString() Overrides: toString in class Object predict public double predict(Vector features) predict value if node is not leaf Parameters:features - feature value Returns:predicted value Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NominalAttribute (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NominalAttribute (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.attribute Class NominalAttribute Object org.apache.spark.ml.attribute.Attribute org.apache.spark.ml.attribute.NominalAttribute All Implemented Interfaces: java.io.Serializable public class NominalAttribute extends Attribute :: DeveloperApi :: A nominal attribute. param: name optional name param: index optional index param: isOrdinal whether this attribute is ordinal (optional) param: numValues optional number of values. At most one of numValues and values can be defined. param: values optional values. At most one of numValues and values can be defined. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description AttributeType attrType() Attribute type. static NominalAttribute defaultAttr() The default nominal attribute. boolean equals(Object other)  scala.Option<Object> getNumValues() Get the number of values, either from numValues or from values. String getValue(int index) Gets a value given its index. int hashCode()  boolean hasValue(String value) Tests whether this attribute contains a specific value. scala.Option<Object> index() Index of the attribute. int indexOf(String value) Index of a specific value. boolean isNominal() Tests whether this attribute is nominal, true for NominalAttribute and BinaryAttribute. boolean isNumeric() Tests whether this attribute is numeric, true for NumericAttribute and BinaryAttribute. scala.Option<Object> isOrdinal()  scala.Option<String> name() Name of the attribute. scala.Option<Object> numValues()  static Metadata toMetadata()  static Metadata toMetadata(Metadata existingMetadata)  static String toString()  static StructField toStructField()  static StructField toStructField(Metadata existingMetadata)  scala.Option<String[]> values()  NominalAttribute withIndex(int index) Copy with a new index. NominalAttribute withName(String name) Copy with a new name. NominalAttribute withNumValues(int numValues) Copy with a new `numValues` and empty `values`. NominalAttribute withoutIndex() Copy without the index. NominalAttribute withoutName() Copy without the name. NominalAttribute withoutNumValues() Copy without the `numValues`. NominalAttribute withoutValues() Copy without the values. NominalAttribute withValues(String[] values) Copy with new values and empty `numValues`. NominalAttribute withValues(String first, scala.collection.Seq<String> others) Copy with new values and empty `numValues`. NominalAttribute withValues(String first, String... others) Copy with new values and empty `numValues`. Methods inherited from class org.apache.spark.ml.attribute.Attribute toMetadata, toMetadata, toString, toStructField, toStructField Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Method Detail defaultAttr public static final NominalAttribute defaultAttr() The default nominal attribute. toMetadata public static Metadata toMetadata(Metadata existingMetadata) toMetadata public static Metadata toMetadata() toStructField public static StructField toStructField(Metadata existingMetadata) toStructField public static StructField toStructField() toString public static String toString() withValues public NominalAttribute withValues(String first, String... others) Copy with new values and empty `numValues`. name public scala.Option<String> name() Description copied from class: Attribute Name of the attribute. None if it is not set. Specified by: name in class Attribute index public scala.Option<Object> index() Description copied from class: Attribute Index of the attribute. None if it is not set. Specified by: index in class Attribute isOrdinal public scala.Option<Object> isOrdinal() numValues public scala.Option<Object> numValues() values public scala.Option<String[]> values() attrType public AttributeType attrType() Description copied from class: Attribute Attribute type. Specified by: attrType in class Attribute isNumeric public boolean isNumeric() Description copied from class: Attribute Tests whether this attribute is numeric, true for NumericAttribute and BinaryAttribute. Specified by: isNumeric in class Attribute Returns:(undocumented) isNominal public boolean isNominal() Description copied from class: Attribute Tests whether this attribute is nominal, true for NominalAttribute and BinaryAttribute. Specified by: isNominal in class Attribute Returns:(undocumented) indexOf public int indexOf(String value) Index of a specific value. hasValue public boolean hasValue(String value) Tests whether this attribute contains a specific value. getValue public String getValue(int index) Gets a value given its index. withName public NominalAttribute withName(String name) Description copied from class: Attribute Copy with a new name. Specified by: withName in class Attribute withoutName public NominalAttribute withoutName() Description copied from class: Attribute Copy without the name. Specified by: withoutName in class Attribute withIndex public NominalAttribute withIndex(int index) Description copied from class: Attribute Copy with a new index. Specified by: withIndex in class Attribute withoutIndex public NominalAttribute withoutIndex() Description copied from class: Attribute Copy without the index. Specified by: withoutIndex in class Attribute withValues public NominalAttribute withValues(String[] values) Copy with new values and empty `numValues`. withValues public NominalAttribute withValues(String first, scala.collection.Seq<String> others) Copy with new values and empty `numValues`. withoutValues public NominalAttribute withoutValues() Copy without the values. withNumValues public NominalAttribute withNumValues(int numValues) Copy with a new `numValues` and empty `values`. withoutNumValues public NominalAttribute withoutNumValues() Copy without the `numValues`. getNumValues public scala.Option<Object> getNumValues() Get the number of values, either from numValues or from values. Return None if unknown. Returns:(undocumented) equals public boolean equals(Object other) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NoopDialect (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NoopDialect (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class NoopDialect Object org.apache.spark.sql.jdbc.NoopDialect public class NoopDialect extends Object NOOP dialect object, always returning the neutral element. Constructor Summary Constructors  Constructor and Description NoopDialect()  Method Summary Methods  Modifier and Type Method and Description static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties)  static boolean canHandle(String url)  static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md)  static scala.Option<JdbcType> getJDBCType(DataType dt)  static String getTableExistsQuery(String table)  static String quoteIdentifier(String colName)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail NoopDialect public NoopDialect() Method Detail canHandle public static boolean canHandle(String url) getCatalystType public static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) getJDBCType public static scala.Option<JdbcType> getJDBCType(DataType dt) quoteIdentifier public static String quoteIdentifier(String colName) getTableExistsQuery public static String getTableExistsQuery(String table) beforeFetch public static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Normalizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Normalizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class Normalizer Object org.apache.spark.mllib.feature.Normalizer All Implemented Interfaces: java.io.Serializable, VectorTransformer public class Normalizer extends Object implements VectorTransformer Normalizes samples individually to unit L^p^ norm For any 1 &lt;= p &lt; Double.PositiveInfinity, normalizes samples using sum(abs(vector).^p^)^(1/p)^ as norm. For p = Double.PositiveInfinity, max(abs(vector)) will be used as norm for normalization. param: p Normalization in L^p^ space, p = 2 by default. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Normalizer()  Normalizer(double p)  Method Summary Methods  Modifier and Type Method and Description Vector transform(Vector vector) Applies unit length normalization on a vector. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.feature.VectorTransformer transform, transform Constructor Detail Normalizer public Normalizer(double p) Normalizer public Normalizer() Method Detail transform public Vector transform(Vector vector) Applies unit length normalization on a vector. Specified by: transform in interface VectorTransformer Parameters:vector - vector to be normalized. Returns:normalized vector. If the norm of the input is zero, it will return the input vector. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Not (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Not (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class Not Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.Not All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class Not extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff child is evaluated to false. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Not(Filter child)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  Filter child()  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail Not public Not(Filter child) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() child public Filter child() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NullType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NullType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class NullType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.NullType public class NullType extends DataType :: DeveloperApi :: The data type representing NULL values. Please use the singleton DataTypes.NullType. Method Summary Methods  Modifier and Type Method and Description static String catalogString()  int defaultSize() The default size of a value of this data type, used internally for size estimation. static String json()  static String prettyJson()  static String simpleString()  static String sql()  static String typeName()  Methods inherited from class org.apache.spark.sql.types.DataType catalogString, fromJson, json, prettyJson, simpleString, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() simpleString public static String simpleString() catalogString public static String catalogString() sql public static String sql() defaultSize public int defaultSize() Description copied from class: DataType The default size of a value of this data type, used internally for size estimation. Specified by: defaultSize in class DataType Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NumericAttribute (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NumericAttribute (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.attribute Class NumericAttribute Object org.apache.spark.ml.attribute.Attribute org.apache.spark.ml.attribute.NumericAttribute All Implemented Interfaces: java.io.Serializable public class NumericAttribute extends Attribute :: DeveloperApi :: A numeric attribute with optional summary statistics. param: name optional name param: index optional index param: min optional min value param: max optional max value param: std optional standard deviation param: sparsity optional sparsity (ratio of zeros) See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description AttributeType attrType() Attribute type. static NumericAttribute defaultAttr() The default numeric attribute. boolean equals(Object other)  int hashCode()  scala.Option<Object> index() Index of the attribute. boolean isNominal() Tests whether this attribute is nominal, true for NominalAttribute and BinaryAttribute. boolean isNumeric() Tests whether this attribute is numeric, true for NumericAttribute and BinaryAttribute. scala.Option<Object> max()  scala.Option<Object> min()  scala.Option<String> name() Name of the attribute. scala.Option<Object> sparsity()  scala.Option<Object> std()  static Metadata toMetadata()  static Metadata toMetadata(Metadata existingMetadata)  static String toString()  static StructField toStructField()  static StructField toStructField(Metadata existingMetadata)  NumericAttribute withIndex(int index) Copy with a new index. NumericAttribute withMax(double max) Copy with a new max value. NumericAttribute withMin(double min) Copy with a new min value. NumericAttribute withName(String name) Copy with a new name. NumericAttribute withoutIndex() Copy without the index. NumericAttribute withoutMax() Copy without the max value. NumericAttribute withoutMin() Copy without the min value. NumericAttribute withoutName() Copy without the name. NumericAttribute withoutSparsity() Copy without the sparsity. NumericAttribute withoutStd() Copy without the standard deviation. NumericAttribute withoutSummary() Copy without summary statistics. NumericAttribute withSparsity(double sparsity) Copy with a new sparsity. NumericAttribute withStd(double std) Copy with a new standard deviation. Methods inherited from class org.apache.spark.ml.attribute.Attribute toMetadata, toMetadata, toString, toStructField, toStructField Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Method Detail defaultAttr public static NumericAttribute defaultAttr() The default numeric attribute. toMetadata public static Metadata toMetadata(Metadata existingMetadata) toMetadata public static Metadata toMetadata() toStructField public static StructField toStructField(Metadata existingMetadata) toStructField public static StructField toStructField() toString public static String toString() name public scala.Option<String> name() Description copied from class: Attribute Name of the attribute. None if it is not set. Specified by: name in class Attribute index public scala.Option<Object> index() Description copied from class: Attribute Index of the attribute. None if it is not set. Specified by: index in class Attribute min public scala.Option<Object> min() max public scala.Option<Object> max() std public scala.Option<Object> std() sparsity public scala.Option<Object> sparsity() attrType public AttributeType attrType() Description copied from class: Attribute Attribute type. Specified by: attrType in class Attribute withName public NumericAttribute withName(String name) Description copied from class: Attribute Copy with a new name. Specified by: withName in class Attribute withoutName public NumericAttribute withoutName() Description copied from class: Attribute Copy without the name. Specified by: withoutName in class Attribute withIndex public NumericAttribute withIndex(int index) Description copied from class: Attribute Copy with a new index. Specified by: withIndex in class Attribute withoutIndex public NumericAttribute withoutIndex() Description copied from class: Attribute Copy without the index. Specified by: withoutIndex in class Attribute withMin public NumericAttribute withMin(double min) Copy with a new min value. withoutMin public NumericAttribute withoutMin() Copy without the min value. withMax public NumericAttribute withMax(double max) Copy with a new max value. withoutMax public NumericAttribute withoutMax() Copy without the max value. withStd public NumericAttribute withStd(double std) Copy with a new standard deviation. withoutStd public NumericAttribute withoutStd() Copy without the standard deviation. withSparsity public NumericAttribute withSparsity(double sparsity) Copy with a new sparsity. withoutSparsity public NumericAttribute withoutSparsity() Copy without the sparsity. withoutSummary public NumericAttribute withoutSummary() Copy without summary statistics. isNumeric public boolean isNumeric() Description copied from class: Attribute Tests whether this attribute is numeric, true for NumericAttribute and BinaryAttribute. Specified by: isNumeric in class Attribute Returns:(undocumented) isNominal public boolean isNominal() Description copied from class: Attribute Tests whether this attribute is nominal, true for NominalAttribute and BinaryAttribute. Specified by: isNominal in class Attribute Returns:(undocumented) equals public boolean equals(Object other) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NumericParser (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NumericParser (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.util Class NumericParser Object org.apache.spark.mllib.util.NumericParser public class NumericParser extends Object Simple parser for a numeric structure consisting of three types: - number: a double in Java's floating number format - array: an array of numbers stored as [v0,v1,...,vn] - tuple: a list of numbers, arrays, or tuples stored as (...) Constructor Summary Constructors  Constructor and Description NumericParser()  Method Summary Methods  Modifier and Type Method and Description static Object parse(String s) Parses a string into a Double, an Array[Double], or a Seq[Any]. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail NumericParser public NumericParser() Method Detail parse public static Object parse(String s) Parses a string into a Double, an Array[Double], or a Seq[Any]. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method NumericType (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="NumericType (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.types Class NumericType Object org.apache.spark.sql.types.DataType org.apache.spark.sql.types.NumericType Direct Known Subclasses: ByteType, DecimalType, DoubleType, FloatType, IntegerType, LongType, ShortType public abstract class NumericType extends DataType :: DeveloperApi :: Numeric data types. Constructor Summary Constructors  Constructor and Description NumericType()  Method Summary Methods  Modifier and Type Method and Description static String catalogString()  abstract static int defaultSize()  static String json()  static String prettyJson()  static String sql()  static String typeName()  static boolean unapply(org.apache.spark.sql.catalyst.expressions.Expression e) Enables matching against NumericType for expressions: Methods inherited from class org.apache.spark.sql.types.DataType catalogString, defaultSize, fromJson, json, prettyJson, simpleString, sql, typeName Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail NumericType public NumericType() Method Detail unapply public static boolean unapply(org.apache.spark.sql.catalyst.expressions.Expression e) Enables matching against NumericType for expressions: case Cast(child @ NumericType(), StringType) => ... Parameters:e - (undocumented) Returns:(undocumented) defaultSize public abstract static int defaultSize() typeName public static String typeName() json public static String json() prettyJson public static String prettyJson() catalogString public static String catalogString() sql public static String sql() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OffsetRange (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OffsetRange (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.kafka Class OffsetRange Object org.apache.spark.streaming.kafka.OffsetRange All Implemented Interfaces: java.io.Serializable public final class OffsetRange extends Object implements scala.Serializable Represents a range of offsets from a single Kafka TopicAndPartition. Instances of this class can be created with OffsetRange.create(). param: topic Kafka topic name param: partition Kafka partition id param: fromOffset Inclusive starting offset param: untilOffset Exclusive ending offset See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static OffsetRange apply(String topic, int partition, long fromOffset, long untilOffset)  static OffsetRange apply(kafka.common.TopicAndPartition topicAndPartition, long fromOffset, long untilOffset)  long count() Number of messages this OffsetRange refers to static OffsetRange create(String topic, int partition, long fromOffset, long untilOffset)  static OffsetRange create(kafka.common.TopicAndPartition topicAndPartition, long fromOffset, long untilOffset)  boolean equals(Object obj)  long fromOffset()  int hashCode()  int partition()  String topic()  kafka.common.TopicAndPartition topicAndPartition() Kafka TopicAndPartition object, for convenience String toString()  long untilOffset()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Method Detail create public static OffsetRange create(String topic, int partition, long fromOffset, long untilOffset) create public static OffsetRange create(kafka.common.TopicAndPartition topicAndPartition, long fromOffset, long untilOffset) apply public static OffsetRange apply(String topic, int partition, long fromOffset, long untilOffset) apply public static OffsetRange apply(kafka.common.TopicAndPartition topicAndPartition, long fromOffset, long untilOffset) topic public String topic() partition public int partition() fromOffset public long fromOffset() untilOffset public long untilOffset() topicAndPartition public kafka.common.TopicAndPartition topicAndPartition() Kafka TopicAndPartition object, for convenience count public long count() Number of messages this OffsetRange refers to equals public boolean equals(Object obj) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OnStart (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OnStart (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rpc.netty Class OnStart Object org.apache.spark.rpc.netty.OnStart public class OnStart extends Object Constructor Summary Constructors  Constructor and Description OnStart()  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OnStart public OnStart() Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OnStop (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OnStop (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rpc.netty Class OnStop Object org.apache.spark.rpc.netty.OnStop public class OnStop extends Object Constructor Summary Constructors  Constructor and Description OnStop()  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OnStop public OnStop() Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OneHotEncoder (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OneHotEncoder (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class OneHotEncoder Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.feature.OneHotEncoder All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class OneHotEncoder extends Transformer implements DefaultParamsWritable A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via OneHotEncoder!.dropLast because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0]. Note that this is different from scikit-learn's OneHotEncoder, which keeps all categories. The output vectors are sparse. See Also:StringIndexer} for converting categorical values into category indices, Serialized Form Constructor Summary Constructors  Constructor and Description OneHotEncoder()  OneHotEncoder(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  OneHotEncoder copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. BooleanParam dropLast() Whether to drop the last category in the encoded vector (default: true) static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  boolean getDropLast()  static String getInputCol()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static OneHotEncoder load(String path)  static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  OneHotEncoder setDropLast(boolean value)  OneHotEncoder setInputCol(String value)  OneHotEncoder setOutputCol(String value)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail OneHotEncoder public OneHotEncoder(String uid) OneHotEncoder public OneHotEncoder() Method Detail load public static OneHotEncoder load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) dropLast public final BooleanParam dropLast() Whether to drop the last category in the encoded vector (default: true) Returns:(undocumented) getDropLast public boolean getDropLast() setDropLast public OneHotEncoder setDropLast(boolean value) setInputCol public OneHotEncoder setInputCol(String value) setOutputCol public OneHotEncoder setOutputCol(String value) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) copy public OneHotEncoder copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Transformer Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OneToOneDependency (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OneToOneDependency (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class OneToOneDependency<T> Object org.apache.spark.Dependency<T> org.apache.spark.NarrowDependency<T> org.apache.spark.OneToOneDependency<T> All Implemented Interfaces: java.io.Serializable public class OneToOneDependency<T> extends NarrowDependency<T> :: DeveloperApi :: Represents a one-to-one dependency between partitions of the parent and child RDDs. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description OneToOneDependency(RDD<T> rdd)  Method Summary Methods  Modifier and Type Method and Description scala.collection.immutable.List<Object> getParents(int partitionId) Get the parent partitions for a child partition. Methods inherited from class org.apache.spark.NarrowDependency rdd Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OneToOneDependency public OneToOneDependency(RDD<T> rdd) Method Detail getParents public scala.collection.immutable.List<Object> getParents(int partitionId) Description copied from class: NarrowDependency Get the parent partitions for a child partition. Specified by: getParents in class NarrowDependency<T> Parameters:partitionId - a partition of the child RDD Returns:the partitions of the parent RDD that the child partition depends upon Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OneVsRest (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OneVsRest (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class OneVsRest Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<OneVsRestModel> org.apache.spark.ml.classification.OneVsRest All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public final class OneVsRest extends Estimator<OneVsRestModel> implements MLWritable Reduction of Multiclass Classification to Binary Classification. Performs reduction using one against all strategy. For a multiclass classification with k classes, train k models (one per class). Each example is scored against all k models and the model with highest score is picked to label the example. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description OneVsRest()  OneVsRest(String uid)  Method Summary Methods  Modifier and Type Method and Description static Param<Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>>> classifier()  Param<Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>>> classifier() param for the base binary classifier that we reduce multiclass classification into. static Params clear(Param<?> param)  OneVsRest copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  OneVsRestModel fit(Dataset<?> dataset) Fits a model to the input data. static <T> scala.Option<T> get(Param<T> param)  static Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>> getClassifier()  Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>> getClassifier()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  static String getLabelCol()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  static OneVsRest load(String path)  static Param<?>[] params()  static Param<String> predictionCol()  static MLReader<OneVsRest> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  OneVsRest setClassifier(Classifier<?,?,?> value)  OneVsRest setFeaturesCol(String value)  OneVsRest setLabelCol(String value)  OneVsRest setPredictionCol(String value)  static String toString()  StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail OneVsRest public OneVsRest(String uid) OneVsRest public OneVsRest() Method Detail read public static MLReader<OneVsRest> read() load public static OneVsRest load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() classifier public static Param<Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>>> classifier() getClassifier public static Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>> getClassifier() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setClassifier public OneVsRest setClassifier(Classifier<?,?,?> value) setLabelCol public OneVsRest setLabelCol(String value) setFeaturesCol public OneVsRest setFeaturesCol(String value) setPredictionCol public OneVsRest setPredictionCol(String value) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) fit public OneVsRestModel fit(Dataset<?> dataset) Description copied from class: Estimator Fits a model to the input data. Specified by: fit in class Estimator<OneVsRestModel> Parameters:dataset - (undocumented) Returns:(undocumented) copy public OneVsRest copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Estimator<OneVsRestModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) classifier public Param<Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>>> classifier() param for the base binary classifier that we reduce multiclass classification into. The base classifier input and output columns are ignored in favor of the ones specified in OneVsRest. Returns:(undocumented) getClassifier public Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>> getClassifier() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OneVsRestModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OneVsRestModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class OneVsRestModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<OneVsRestModel> org.apache.spark.ml.classification.OneVsRestModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public final class OneVsRestModel extends Model<OneVsRestModel> implements MLWritable Model produced by OneVsRest. This stores the models resulting from training k binary classifiers: one for each class. Each example is scored against all k models, and the model with the highest score is picked to label the example. param: labelMetadata Metadata of label column if it exists, or Nominal attribute representing the number of classes in training dataset otherwise. param: models The binary classification models for the reduction. The i-th model is produced by testing the i-th class (taking label 1) vs the rest (taking label 0). See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static Param<Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>>> classifier()  Param<Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>>> classifier() param for the base binary classifier that we reduce multiclass classification into. static Params clear(Param<?> param)  OneVsRestModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  static <T> scala.Option<T> get(Param<T> param)  static Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>> getClassifier()  Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>> getClassifier()  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  static String getLabelCol()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  static OneVsRestModel load(String path)  ClassificationModel[] models()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static Param<String> predictionCol()  static MLReader<OneVsRestModel> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  static M setParent(Estimator<M> parent)  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Method Detail read public static MLReader<OneVsRestModel> read() load public static OneVsRestModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() classifier public static Param<Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>>> classifier() getClassifier public static Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>> getClassifier() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) models public ClassificationModel[] models() transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) copy public OneVsRestModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<OneVsRestModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) classifier public Param<Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>>> classifier() param for the base binary classifier that we reduce multiclass classification into. The base classifier input and output columns are ignored in favor of the ones specified in OneVsRest. Returns:(undocumented) getClassifier public Classifier<?,? extends Classifier<Object,Classifier,ClassificationModel>,? extends ClassificationModel<Object,ClassificationModel>> getClassifier() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OnlineLDAOptimizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OnlineLDAOptimizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class OnlineLDAOptimizer Object org.apache.spark.mllib.clustering.OnlineLDAOptimizer All Implemented Interfaces: LDAOptimizer public final class OnlineLDAOptimizer extends Object implements LDAOptimizer :: DeveloperApi :: An online optimizer for LDA. The Optimizer implements the Online variational Bayes LDA algorithm, which processes a subset of the corpus on each iteration, and updates the term-topic distribution adaptively. Original Online LDA paper: Hoffman, Blei and Bach, "Online Learning for Latent Dirichlet Allocation." NIPS, 2010. Constructor Summary Constructors  Constructor and Description OnlineLDAOptimizer()  Method Summary Methods  Modifier and Type Method and Description double getKappa() Learning rate: exponential decay rate double getMiniBatchFraction() Mini-batch fraction, which sets the fraction of document sampled and used in each iteration boolean getOptimizeDocConcentration() Optimize docConcentration, indicates whether docConcentration (Dirichlet parameter for document-topic distribution) will be optimized during training. double getTau0() A (positive) learning parameter that downweights early iterations. OnlineLDAOptimizer setKappa(double kappa) Learning rate: exponential decay rate---should be between (0.5, 1.0] to guarantee asymptotic convergence. OnlineLDAOptimizer setMiniBatchFraction(double miniBatchFraction) Mini-batch fraction in (0, 1], which sets the fraction of document sampled and used in each iteration. OnlineLDAOptimizer setOptimizeDocConcentration(boolean optimizeDocConcentration) Sets whether to optimize docConcentration parameter during training. OnlineLDAOptimizer setTau0(double tau0) A (positive) learning parameter that downweights early iterations. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OnlineLDAOptimizer public OnlineLDAOptimizer() Method Detail getTau0 public double getTau0() A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. Returns:(undocumented) setTau0 public OnlineLDAOptimizer setTau0(double tau0) A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. Default: 1024, following the original Online LDA paper. Parameters:tau0 - (undocumented) Returns:(undocumented) getKappa public double getKappa() Learning rate: exponential decay rate Returns:(undocumented) setKappa public OnlineLDAOptimizer setKappa(double kappa) Learning rate: exponential decay rate---should be between (0.5, 1.0] to guarantee asymptotic convergence. Default: 0.51, based on the original Online LDA paper. Parameters:kappa - (undocumented) Returns:(undocumented) getMiniBatchFraction public double getMiniBatchFraction() Mini-batch fraction, which sets the fraction of document sampled and used in each iteration Returns:(undocumented) setMiniBatchFraction public OnlineLDAOptimizer setMiniBatchFraction(double miniBatchFraction) Mini-batch fraction in (0, 1], which sets the fraction of document sampled and used in each iteration. Note that this should be adjusted in synch with LDA.setMaxIterations() so the entire corpus is used. Specifically, set both so that maxIterations * miniBatchFraction >= 1. Default: 0.05, i.e., 5% of total documents. Parameters:miniBatchFraction - (undocumented) Returns:(undocumented) getOptimizeDocConcentration public boolean getOptimizeDocConcentration() Optimize docConcentration, indicates whether docConcentration (Dirichlet parameter for document-topic distribution) will be optimized during training. Returns:(undocumented) setOptimizeDocConcentration public OnlineLDAOptimizer setOptimizeDocConcentration(boolean optimizeDocConcentration) Sets whether to optimize docConcentration parameter during training. Default: false Parameters:optimizeDocConcentration - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Optimizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Optimizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.optimization Interface Optimizer All Superinterfaces: java.io.Serializable All Known Implementing Classes: GradientDescent, LBFGS public interface Optimizer extends scala.Serializable :: DeveloperApi :: Trait for optimization problem solvers. Method Summary Methods  Modifier and Type Method and Description Vector optimize(RDD<scala.Tuple2<Object,Vector>> data, Vector initialWeights) Solve the provided convex optimization problem. Method Detail optimize Vector optimize(RDD<scala.Tuple2<Object,Vector>> data, Vector initialWeights) Solve the provided convex optimization problem. Parameters:data - (undocumented)initialWeights - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Optional (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Optional (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java Class Optional<T> Object org.apache.spark.api.java.Optional<T> Type Parameters:T - type of value held inside All Implemented Interfaces: java.io.Serializable public final class Optional<T> extends Object implements java.io.Serializable Like java.util.Optional in Java 8, scala.Option in Scala, and com.google.common.base.Optional in Google Guava, this class represents a value of a given type that may or may not exist. It is used in methods that wish to optionally return a value, in preference to returning null. In fact, the class here is a reimplementation of the essential API of both java.util.Optional and com.google.common.base.Optional. From java.util.Optional, it implements: empty() of(Object) ofNullable(Object) get() orElse(Object) isPresent() From com.google.common.base.Optional it implements: absent() of(Object) fromNullable(Object) get() or(Object) orNull() isPresent() java.util.Optional itself is not used at this time because the project does not require Java 8. Using com.google.common.base.Optional has in the past caused serious library version conflicts with Guava that can't be resolved by shading. Hence this work-alike clone. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static <T> Optional<T> absent()  static <T> Optional<T> empty()  boolean equals(Object obj)  static <T> Optional<T> fromNullable(T value)  T get()  int hashCode()  boolean isPresent()  static <T> Optional<T> of(T value)  static <T> Optional<T> ofNullable(T value)  T or(T other)  T orElse(T other)  T orNull()  String toString()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Method Detail empty public static <T> Optional<T> empty() Returns:an empty Optional of public static <T> Optional<T> of(T value) Parameters:value - non-null value to wrap Returns:Optional wrapping this value Throws: NullPointerException - if value is null ofNullable public static <T> Optional<T> ofNullable(T value) Parameters:value - value to wrap, which may be null Returns:Optional wrapping this value, which may be empty get public T get() Returns:the value wrapped by this Optional Throws: NullPointerException - if this is empty (contains no value) orElse public T orElse(T other) Parameters:other - value to return if this is empty Returns:this Optional's value if present, or else the given value isPresent public boolean isPresent() Returns:true iff this Optional contains a value (non-empty) absent public static <T> Optional<T> absent() Returns:an empty Optional fromNullable public static <T> Optional<T> fromNullable(T value) Parameters:value - value to wrap, which may be null Returns:Optional wrapping this value, which may be empty or public T or(T other) Parameters:other - value to return if this is empty Returns:this Optional's value if present, or else the given value orNull public T orNull() Returns:this Optional's value if present, or else null equals public boolean equals(Object obj) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Or (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Or (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Class Or Object org.apache.spark.sql.sources.Filter org.apache.spark.sql.sources.Or All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class Or extends Filter implements scala.Product, scala.Serializable A filter that evaluates to true iff at least one of left or right evaluates to true. Since: 1.3.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Or(Filter left, Filter right)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  Filter left()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Filter right()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail Or public Or(Filter left, Filter right) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() left public Filter left() right public Filter right() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OracleDialect (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OracleDialect (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class OracleDialect Object org.apache.spark.sql.jdbc.OracleDialect public class OracleDialect extends Object Constructor Summary Constructors  Constructor and Description OracleDialect()  Method Summary Methods  Modifier and Type Method and Description static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties)  abstract static boolean canEqual(Object that)  static boolean canHandle(String url)  abstract static boolean equals(Object that)  static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md)  static scala.Option<JdbcType> getJDBCType(DataType dt)  static String getTableExistsQuery(String table)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  static String quoteIdentifier(String colName)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OracleDialect public OracleDialect() Method Detail canHandle public static boolean canHandle(String url) getCatalystType public static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) getJDBCType public static scala.Option<JdbcType> getJDBCType(DataType dt) quoteIdentifier public static String quoteIdentifier(String colName) getTableExistsQuery public static String getTableExistsQuery(String table) beforeFetch public static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties) canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OrcFileFormat (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OrcFileFormat (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive.orc Class OrcFileFormat Object org.apache.spark.sql.hive.orc.OrcFileFormat All Implemented Interfaces: java.io.Serializable, org.apache.spark.sql.execution.datasources.FileFormat, DataSourceRegister public class OrcFileFormat extends Object implements org.apache.spark.sql.execution.datasources.FileFormat, DataSourceRegister, scala.Serializable FileFormat for reading ORC files. If this is moved or renamed, please update DataSource's backwardCompatibilityMap. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description OrcFileFormat()  Method Summary Methods  Modifier and Type Method and Description scala.Function1<org.apache.spark.sql.execution.datasources.PartitionedFile,scala.collection.Iterator<org.apache.spark.sql.catalyst.InternalRow>> buildReader(SparkSession sparkSession, StructType dataSchema, StructType partitionSchema, StructType requiredSchema, scala.collection.Seq<Filter> filters, scala.collection.immutable.Map<String,String> options, org.apache.hadoop.conf.Configuration hadoopConf)  scala.Option<StructType> inferSchema(SparkSession sparkSession, scala.collection.immutable.Map<String,String> options, scala.collection.Seq<org.apache.hadoop.fs.FileStatus> files)  boolean isSplitable(SparkSession sparkSession, scala.collection.immutable.Map<String,String> options, org.apache.hadoop.fs.Path path)  org.apache.spark.sql.execution.datasources.OutputWriterFactory prepareWrite(SparkSession sparkSession, org.apache.hadoop.mapreduce.Job job, scala.collection.immutable.Map<String,String> options, StructType dataSchema)  String shortName() The string that represents the format that this data source provider uses. String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface org.apache.spark.sql.execution.datasources.FileFormat buildReaderWithPartitionValues, buildWriter, supportBatch Constructor Detail OrcFileFormat public OrcFileFormat() Method Detail shortName public String shortName() Description copied from interface: DataSourceRegister The string that represents the format that this data source provider uses. This is overridden by children to provide a nice alias for the data source. For example: override def shortName(): String = "parquet" Specified by: shortName in interface DataSourceRegister Returns:(undocumented) toString public String toString() Overrides: toString in class Object inferSchema public scala.Option<StructType> inferSchema(SparkSession sparkSession, scala.collection.immutable.Map<String,String> options, scala.collection.Seq<org.apache.hadoop.fs.FileStatus> files) Specified by: inferSchema in interface org.apache.spark.sql.execution.datasources.FileFormat prepareWrite public org.apache.spark.sql.execution.datasources.OutputWriterFactory prepareWrite(SparkSession sparkSession, org.apache.hadoop.mapreduce.Job job, scala.collection.immutable.Map<String,String> options, StructType dataSchema) Specified by: prepareWrite in interface org.apache.spark.sql.execution.datasources.FileFormat isSplitable public boolean isSplitable(SparkSession sparkSession, scala.collection.immutable.Map<String,String> options, org.apache.hadoop.fs.Path path) Specified by: isSplitable in interface org.apache.spark.sql.execution.datasources.FileFormat buildReader public scala.Function1<org.apache.spark.sql.execution.datasources.PartitionedFile,scala.collection.Iterator<org.apache.spark.sql.catalyst.InternalRow>> buildReader(SparkSession sparkSession, StructType dataSchema, StructType partitionSchema, StructType requiredSchema, scala.collection.Seq<Filter> filters, scala.collection.immutable.Map<String,String> options, org.apache.hadoop.conf.Configuration hadoopConf) Specified by: buildReader in interface org.apache.spark.sql.execution.datasources.FileFormat Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OrcFileOperator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OrcFileOperator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive.orc Class OrcFileOperator Object org.apache.spark.sql.hive.orc.OrcFileOperator public class OrcFileOperator extends Object Constructor Summary Constructors  Constructor and Description OrcFileOperator()  Method Summary Methods  Modifier and Type Method and Description static scala.Option<org.apache.hadoop.hive.ql.io.orc.Reader> getFileReader(String basePath, scala.Option<org.apache.hadoop.conf.Configuration> config) Retrieves an ORC file reader from a given path. static scala.Option<org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector> getObjectInspector(String path, scala.Option<org.apache.hadoop.conf.Configuration> conf)  static scala.collection.Seq<org.apache.hadoop.fs.Path> listOrcFiles(String pathStr, org.apache.hadoop.conf.Configuration conf)  static scala.Option<StructType> readSchema(scala.collection.Seq<String> paths, scala.Option<org.apache.hadoop.conf.Configuration> conf)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OrcFileOperator public OrcFileOperator() Method Detail getFileReader public static scala.Option<org.apache.hadoop.hive.ql.io.orc.Reader> getFileReader(String basePath, scala.Option<org.apache.hadoop.conf.Configuration> config) Retrieves an ORC file reader from a given path. The path can point to either a directory or a single ORC file. If it points to a directory, it picks any non-empty ORC file within that directory. The reader returned by this method is mainly used for two purposes: 1. Retrieving file metadata (schema and compression codecs, etc.) 2. Read the actual file content (in this case, the given path should point to the target file) Parameters:basePath - (undocumented)config - (undocumented) Returns:(undocumented) readSchema public static scala.Option<StructType> readSchema(scala.collection.Seq<String> paths, scala.Option<org.apache.hadoop.conf.Configuration> conf) getObjectInspector public static scala.Option<org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector> getObjectInspector(String path, scala.Option<org.apache.hadoop.conf.Configuration> conf) listOrcFiles public static scala.collection.Seq<org.apache.hadoop.fs.Path> listOrcFiles(String pathStr, org.apache.hadoop.conf.Configuration conf) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OrcFilters (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OrcFilters (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive.orc Class OrcFilters Object org.apache.spark.sql.hive.orc.OrcFilters public class OrcFilters extends Object Helper object for building ORC SearchArguments, which are used for ORC predicate push-down. Due to limitation of ORC SearchArgument builder, we had to end up with a pretty weird double- checking pattern when converting And/Or/Not filters. An ORC SearchArgument must be built in one pass using a single builder. For example, you can't build a = 1 and b = 2 first, and then combine them into a = 1 AND b = 2. This is quite different from the cases in Spark SQL or Parquet, where complex filters can be easily built using existing simpler ones. The annoying part is that, SearchArgument builder methods like startAnd(), startOr(), and startNot() mutate internal state of the builder instance. This forces us to translate all convertible filters with a single builder instance. However, before actually converting a filter, we've no idea whether it can be recognized by ORC or not. Thus, when an inconvertible filter is found, we may already end up with a builder whose internal state is inconsistent. For example, to convert an And filter with builder b, we call b.startAnd() first, and then try to convert its children. Say we convert left child successfully, but find that right child is inconvertible. Alas, b.startAnd() call can't be rolled back, and b is inconsistent now. The workaround employed here is that, for And/Or/Not, we first try to convert their children with brand new builders, and only do the actual conversion with the right builder instance when the children are proven to be convertible. P.S.: Hive seems to use SearchArgument together with ExprNodeGenericFuncDesc only. Usage of builder methods mentioned above can only be found in test code, where all tested filters are known to be convertible. Constructor Summary Constructors  Constructor and Description OrcFilters()  Method Summary Methods  Modifier and Type Method and Description static scala.Option<org.apache.hadoop.hive.ql.io.sarg.SearchArgument> createFilter(StructType schema, Filter[] filters)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OrcFilters public OrcFilters() Method Detail createFilter public static scala.Option<org.apache.hadoop.hive.ql.io.sarg.SearchArgument> createFilter(StructType schema, Filter[] filters) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OrcRelation (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OrcRelation (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.hive.orc Class OrcRelation Object org.apache.spark.sql.hive.orc.OrcRelation public class OrcRelation extends Object Constructor Summary Constructors  Constructor and Description OrcRelation()  Method Summary Methods  Modifier and Type Method and Description static scala.collection.immutable.Map<String,String> extensionsForCompressionCodecNames()  static DataType inspectorToDataType(org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector inspector)  static DataType javaClassToDataType(Class<?> clz)  static String ORC_COMPRESSION()  static void setRequiredColumns(org.apache.hadoop.conf.Configuration conf, StructType physicalSchema, StructType requestedSchema)  static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector toInspector(DataType dataType)  static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector toInspector(org.apache.spark.sql.catalyst.expressions.Expression expr)  static org.apache.spark.sql.hive.HiveInspectors.typeInfoConversions typeInfoConversions(DataType dt)  static Object unwrap(Object data, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector oi)  static scala.collection.Iterator<org.apache.spark.sql.catalyst.InternalRow> unwrapOrcStructs(org.apache.hadoop.conf.Configuration conf, StructType dataSchema, scala.Option<org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector> maybeStructOI, scala.collection.Iterator<org.apache.hadoop.io.Writable> iterator)  static scala.Function3<Object,org.apache.spark.sql.catalyst.expressions.MutableRow,Object,scala.runtime.BoxedUnit> unwrapperFor(org.apache.hadoop.hive.serde2.objectinspector.StructField field)  static Object[] wrap(org.apache.spark.sql.catalyst.InternalRow row, scala.collection.Seq<org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector> inspectors, Object[] cache, DataType[] dataTypes)  static Object wrap(Object a, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector oi, DataType dataType)  static Object[] wrap(scala.collection.Seq<Object> row, scala.collection.Seq<org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector> inspectors, Object[] cache, DataType[] dataTypes)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OrcRelation public OrcRelation() Method Detail ORC_COMPRESSION public static String ORC_COMPRESSION() extensionsForCompressionCodecNames public static scala.collection.immutable.Map<String,String> extensionsForCompressionCodecNames() unwrapOrcStructs public static scala.collection.Iterator<org.apache.spark.sql.catalyst.InternalRow> unwrapOrcStructs(org.apache.hadoop.conf.Configuration conf, StructType dataSchema, scala.Option<org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector> maybeStructOI, scala.collection.Iterator<org.apache.hadoop.io.Writable> iterator) setRequiredColumns public static void setRequiredColumns(org.apache.hadoop.conf.Configuration conf, StructType physicalSchema, StructType requestedSchema) javaClassToDataType public static DataType javaClassToDataType(Class<?> clz) unwrap public static Object unwrap(Object data, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector oi) unwrapperFor public static scala.Function3<Object,org.apache.spark.sql.catalyst.expressions.MutableRow,Object,scala.runtime.BoxedUnit> unwrapperFor(org.apache.hadoop.hive.serde2.objectinspector.StructField field) wrap public static Object wrap(Object a, org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector oi, DataType dataType) wrap public static Object[] wrap(org.apache.spark.sql.catalyst.InternalRow row, scala.collection.Seq<org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector> inspectors, Object[] cache, DataType[] dataTypes) wrap public static Object[] wrap(scala.collection.Seq<Object> row, scala.collection.Seq<org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector> inspectors, Object[] cache, DataType[] dataTypes) toInspector public static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector toInspector(DataType dataType) toInspector public static org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector toInspector(org.apache.spark.sql.catalyst.expressions.Expression expr) inspectorToDataType public static DataType inspectorToDataType(org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector inspector) typeInfoConversions public static org.apache.spark.sql.hive.HiveInspectors.typeInfoConversions typeInfoConversions(DataType dt) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OrderedRDDFunctions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OrderedRDDFunctions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class OrderedRDDFunctions<K,V,P extends scala.Product2<K,V>> Object org.apache.spark.rdd.OrderedRDDFunctions<K,V,P> All Implemented Interfaces: java.io.Serializable public class OrderedRDDFunctions<K,V,P extends scala.Product2<K,V>> extends Object implements scala.Serializable Extra functions available on RDDs of (key, value) pairs where the key is sortable through an implicit conversion. They will work with any key type K that has an implicit Ordering[K] in scope. Ordering objects already exist for all of the standard primitive types. Users can also define their own orderings for custom types, or to override the default ordering. The implicit ordering that is in the closest scope will be used. import org.apache.spark.SparkContext._ val rdd: RDD[(String, Int)] = ... implicit val caseInsensitiveOrdering = new Ordering[String] { override def compare(a: String, b: String) = a.toLowerCase.compare(b.toLowerCase) } // Sort by key, using the above case insensitive ordering. rdd.sortByKey() See Also:Serialized Form Constructor Summary Constructors  Constructor and Description OrderedRDDFunctions(RDD<P> self, scala.math.Ordering<K> evidence$1, scala.reflect.ClassTag<K> evidence$2, scala.reflect.ClassTag<V> evidence$3, scala.reflect.ClassTag<P> evidence$4)  Method Summary Methods  Modifier and Type Method and Description RDD<P> filterByRange(K lower, K upper) Returns an RDD containing only the elements in the inclusive range lower to upper. RDD<scala.Tuple2<K,V>> repartitionAndSortWithinPartitions(Partitioner partitioner) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. RDD<scala.Tuple2<K,V>> sortByKey(boolean ascending, int numPartitions) Sort the RDD by key, so that each partition contains a sorted range of the elements. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OrderedRDDFunctions public OrderedRDDFunctions(RDD<P> self, scala.math.Ordering<K> evidence$1, scala.reflect.ClassTag<K> evidence$2, scala.reflect.ClassTag<V> evidence$3, scala.reflect.ClassTag<P> evidence$4) Method Detail sortByKey public RDD<scala.Tuple2<K,V>> sortByKey(boolean ascending, int numPartitions) Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling collect or save on the resulting RDD will return or output an ordered list of records (in the save case, they will be written to multiple part-X files in the filesystem, in order of the keys). Parameters:ascending - (undocumented)numPartitions - (undocumented) Returns:(undocumented) repartitionAndSortWithinPartitions public RDD<scala.Tuple2<K,V>> repartitionAndSortWithinPartitions(Partitioner partitioner) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery. Parameters:partitioner - (undocumented) Returns:(undocumented) filterByRange public RDD<P> filterByRange(K lower, K upper) Returns an RDD containing only the elements in the inclusive range lower to upper. If the RDD has been partitioned using a RangePartitioner, then this operation can be performed efficiently by only scanning the partitions that might contain matching elements. Otherwise, a standard filter is applied to all partitions. Parameters:lower - (undocumented)upper - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OutputCommitCoordinationMessage (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OutputCommitCoordinationMessage (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.scheduler Interface OutputCommitCoordinationMessage All Superinterfaces: java.io.Serializable public interface OutputCommitCoordinationMessage extends scala.Serializable Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OutputMetricDistributions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OutputMetricDistributions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class OutputMetricDistributions Object org.apache.spark.status.api.v1.OutputMetricDistributions public class OutputMetricDistributions extends Object Method Summary Methods  Modifier and Type Method and Description scala.collection.IndexedSeq<Object> bytesWritten()  scala.collection.IndexedSeq<Object> recordsWritten()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail bytesWritten public scala.collection.IndexedSeq<Object> bytesWritten() recordsWritten public scala.collection.IndexedSeq<Object> recordsWritten() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OutputMetrics (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OutputMetrics (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.status.api.v1 Class OutputMetrics Object org.apache.spark.status.api.v1.OutputMetrics public class OutputMetrics extends Object Method Summary Methods  Modifier and Type Method and Description long bytesWritten()  long recordsWritten()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Method Detail bytesWritten public long bytesWritten() recordsWritten public long recordsWritten() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OutputMode (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OutputMode (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.streaming Class OutputMode Object org.apache.spark.sql.streaming.OutputMode Direct Known Subclasses: InternalOutputModes.Append$, InternalOutputModes.Complete$, InternalOutputModes.Update$ public class OutputMode extends Object :: Experimental :: OutputMode is used to what data will be written to a streaming sink when there is new data available in a streaming DataFrame/Dataset. Since: 2.0.0 Constructor Summary Constructors  Constructor and Description OutputMode()  Method Summary Methods  Modifier and Type Method and Description static OutputMode Append() OutputMode in which only the new rows in the streaming DataFrame/Dataset will be written to the sink. static OutputMode Complete() OutputMode in which all the rows in the streaming DataFrame/Dataset will be written to the sink every time these is some updates. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail OutputMode public OutputMode() Method Detail Append public static OutputMode Append() OutputMode in which only the new rows in the streaming DataFrame/Dataset will be written to the sink. This output mode can be only be used in queries that do not contain any aggregation. Since: 2.0.0 Complete public static OutputMode Complete() OutputMode in which all the rows in the streaming DataFrame/Dataset will be written to the sink every time these is some updates. This output mode can only be used in queries that contain aggregations. Since: 2.0.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method OutputOperationInfo (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="OutputOperationInfo (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.scheduler Class OutputOperationInfo Object org.apache.spark.streaming.scheduler.OutputOperationInfo All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class OutputOperationInfo extends Object implements scala.Product, scala.Serializable :: DeveloperApi :: Class having information on output operations. param: batchTime Time of the batch param: id Id of this output operation. Different output operations have different ids in a batch. param: name The name of this output operation. param: description The description of this output operation. param: startTime Clock time of when the output operation started processing param: endTime Clock time of when the output operation started processing param: failureReason Failure reason if this output operation fails See Also:Serialized Form Constructor Summary Constructors  Constructor and Description OutputOperationInfo(Time batchTime, int id, String name, String description, scala.Option<Object> startTime, scala.Option<Object> endTime, scala.Option<String> failureReason)  Method Summary Methods  Modifier and Type Method and Description Time batchTime()  abstract static boolean canEqual(Object that)  String description()  scala.Option<Object> duration() Return the duration of this output operation. scala.Option<Object> endTime()  abstract static boolean equals(Object that)  scala.Option<String> failureReason()  int id()  String name()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  scala.Option<Object> startTime()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail OutputOperationInfo public OutputOperationInfo(Time batchTime, int id, String name, String description, scala.Option<Object> startTime, scala.Option<Object> endTime, scala.Option<String> failureReason) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() batchTime public Time batchTime() id public int id() name public String name() description public String description() startTime public scala.Option<Object> startTime() endTime public scala.Option<Object> endTime() failureReason public scala.Option<String> failureReason() duration public scala.Option<Object> duration() Return the duration of this output operation. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PCA (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PCA (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class PCA Object org.apache.spark.mllib.feature.PCA public class PCA extends Object A feature transformer that projects vectors to a low-dimensional space using PCA. param: k number of principal components Constructor Summary Constructors  Constructor and Description PCA(int k)  Method Summary Methods  Modifier and Type Method and Description PCAModel fit(JavaRDD<Vector> sources) Java-friendly version of fit(). PCAModel fit(RDD<Vector> sources) Computes a PCAModel that contains the principal components of the input vectors. int k()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PCA public PCA(int k) Method Detail k public int k() fit public PCAModel fit(RDD<Vector> sources) Computes a PCAModel that contains the principal components of the input vectors. Parameters:sources - source vectors Returns:(undocumented) fit public PCAModel fit(JavaRDD<Vector> sources) Java-friendly version of fit(). Parameters:sources - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PCAModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PCAModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.feature Class PCAModel Object org.apache.spark.mllib.feature.PCAModel All Implemented Interfaces: java.io.Serializable, VectorTransformer public class PCAModel extends Object implements VectorTransformer Model fitted by PCA that can project vectors to a low-dimensional space using PCA. param: k number of principal components. param: pc a principal components Matrix. Each column is one principal component. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description DenseVector explainedVariance()  int k()  DenseMatrix pc()  Vector transform(Vector vector) Transform a vector by computed Principal Components. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.mllib.feature.VectorTransformer transform, transform Method Detail k public int k() pc public DenseMatrix pc() explainedVariance public DenseVector explainedVariance() transform public Vector transform(Vector vector) Transform a vector by computed Principal Components. Specified by: transform in interface VectorTransformer Parameters:vector - vector to be transformed. Vector must be the same length as the source vectors given to PCA.fit(). Returns:transformed vector. Vector will be of length k. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PMMLExportable (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PMMLExportable (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.pmml Interface PMMLExportable All Known Implementing Classes: KMeansModel, LassoModel, LinearRegressionModel, LogisticRegressionModel, RidgeRegressionModel, StreamingKMeansModel, SVMModel public interface PMMLExportable :: DeveloperApi :: Export model to the PMML format Predictive Model Markup Language (PMML) is an XML-based file format developed by the Data Mining Group (www.dmg.org). Method Summary Methods  Modifier and Type Method and Description String toPMML() Export the model to a String in PMML format void toPMML(java.io.OutputStream outputStream) Export the model to the OutputStream in PMML format void toPMML(SparkContext sc, String path) Export the model to a directory on a distributed file system in PMML format void toPMML(javax.xml.transform.stream.StreamResult streamResult) Export the model to the stream result in PMML format void toPMML(String localPath) Export the model to a local file in PMML format Method Detail toPMML void toPMML(javax.xml.transform.stream.StreamResult streamResult) Export the model to the stream result in PMML format Parameters:streamResult - (undocumented) toPMML void toPMML(String localPath) Export the model to a local file in PMML format Parameters:localPath - (undocumented) toPMML void toPMML(SparkContext sc, String path) Export the model to a directory on a distributed file system in PMML format Parameters:sc - (undocumented)path - (undocumented) toPMML void toPMML(java.io.OutputStream outputStream) Export the model to the OutputStream in PMML format Parameters:outputStream - (undocumented) toPMML String toPMML() Export the model to a String in PMML format Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PMMLModelExportFactory (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PMMLModelExportFactory (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.pmml.export Class PMMLModelExportFactory Object org.apache.spark.mllib.pmml.export.PMMLModelExportFactory public class PMMLModelExportFactory extends Object Constructor Summary Constructors  Constructor and Description PMMLModelExportFactory()  Method Summary Methods  Modifier and Type Method and Description static org.apache.spark.mllib.pmml.export.PMMLModelExport createPMMLModelExport(Object model) Factory object to help creating the necessary PMMLModelExport implementation taking as input the machine learning model (for example KMeansModel). Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PMMLModelExportFactory public PMMLModelExportFactory() Method Detail createPMMLModelExport public static org.apache.spark.mllib.pmml.export.PMMLModelExport createPMMLModelExport(Object model) Factory object to help creating the necessary PMMLModelExport implementation taking as input the machine learning model (for example KMeansModel). Parameters:model - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PageRank (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PageRank (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx.lib Class PageRank Object org.apache.spark.graphx.lib.PageRank public class PageRank extends Object PageRank algorithm implementation. There are two implementations of PageRank implemented. The first implementation uses the standalone Graph interface and runs PageRank for a fixed number of iterations: var PR = Array.fill(n)( 1.0 ) val oldPR = Array.fill(n)( 1.0 ) for( iter <- 0 until numIter ) { swap(oldPR, PR) for( i <- 0 until n ) { PR[i] = alpha + (1 - alpha) * inNbrs[i].map(j => oldPR[j] / outDeg[j]).sum } } The second implementation uses the Pregel interface and runs PageRank until convergence: var PR = Array.fill(n)( 1.0 ) val oldPR = Array.fill(n)( 0.0 ) while( max(abs(PR - oldPr)) > tol ) { swap(oldPR, PR) for( i <- 0 until n if abs(PR[i] - oldPR[i]) > tol ) { PR[i] = alpha + (1 - \alpha) * inNbrs[i].map(j => oldPR[j] / outDeg[j]).sum } } alpha is the random reset probability (typically 0.15), inNbrs[i] is the set of neighbors which link to i and outDeg[j] is the out degree of vertex j. Note that this is not the "normalized" PageRank and as a consequence pages that have no inlinks will have a PageRank of alpha. Constructor Summary Constructors  Constructor and Description PageRank()  Method Summary Methods  Modifier and Type Method and Description static <VD,ED> Graph<Object,Object> run(Graph<VD,ED> graph, int numIter, double resetProb, scala.reflect.ClassTag<VD> evidence$1, scala.reflect.ClassTag<ED> evidence$2) Run PageRank for a fixed number of iterations returning a graph with vertex attributes containing the PageRank and edge attributes the normalized edge weight. static <VD,ED> Graph<Object,Object> runUntilConvergence(Graph<VD,ED> graph, double tol, double resetProb, scala.reflect.ClassTag<VD> evidence$5, scala.reflect.ClassTag<ED> evidence$6) Run a dynamic version of PageRank returning a graph with vertex attributes containing the PageRank and edge attributes containing the normalized edge weight. static <VD,ED> Graph<Object,Object> runUntilConvergenceWithOptions(Graph<VD,ED> graph, double tol, double resetProb, scala.Option<Object> srcId, scala.reflect.ClassTag<VD> evidence$7, scala.reflect.ClassTag<ED> evidence$8) Run a dynamic version of PageRank returning a graph with vertex attributes containing the PageRank and edge attributes containing the normalized edge weight. static <VD,ED> Graph<Object,Object> runWithOptions(Graph<VD,ED> graph, int numIter, double resetProb, scala.Option<Object> srcId, scala.reflect.ClassTag<VD> evidence$3, scala.reflect.ClassTag<ED> evidence$4) Run PageRank for a fixed number of iterations returning a graph with vertex attributes containing the PageRank and edge attributes the normalized edge weight. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PageRank public PageRank() Method Detail run public static <VD,ED> Graph<Object,Object> run(Graph<VD,ED> graph, int numIter, double resetProb, scala.reflect.ClassTag<VD> evidence$1, scala.reflect.ClassTag<ED> evidence$2) Run PageRank for a fixed number of iterations returning a graph with vertex attributes containing the PageRank and edge attributes the normalized edge weight. Parameters:graph - the graph on which to compute PageRanknumIter - the number of iterations of PageRank to runresetProb - the random reset probability (alpha) evidence$1 - (undocumented)evidence$2 - (undocumented) Returns:the graph containing with each vertex containing the PageRank and each edge containing the normalized weight. runWithOptions public static <VD,ED> Graph<Object,Object> runWithOptions(Graph<VD,ED> graph, int numIter, double resetProb, scala.Option<Object> srcId, scala.reflect.ClassTag<VD> evidence$3, scala.reflect.ClassTag<ED> evidence$4) Run PageRank for a fixed number of iterations returning a graph with vertex attributes containing the PageRank and edge attributes the normalized edge weight. Parameters:graph - the graph on which to compute PageRanknumIter - the number of iterations of PageRank to runresetProb - the random reset probability (alpha)srcId - the source vertex for a Personalized Page Rank (optional) evidence$3 - (undocumented)evidence$4 - (undocumented) Returns:the graph containing with each vertex containing the PageRank and each edge containing the normalized weight. runUntilConvergence public static <VD,ED> Graph<Object,Object> runUntilConvergence(Graph<VD,ED> graph, double tol, double resetProb, scala.reflect.ClassTag<VD> evidence$5, scala.reflect.ClassTag<ED> evidence$6) Run a dynamic version of PageRank returning a graph with vertex attributes containing the PageRank and edge attributes containing the normalized edge weight. Parameters:graph - the graph on which to compute PageRanktol - the tolerance allowed at convergence (smaller => more accurate).resetProb - the random reset probability (alpha) evidence$5 - (undocumented)evidence$6 - (undocumented) Returns:the graph containing with each vertex containing the PageRank and each edge containing the normalized weight. runUntilConvergenceWithOptions public static <VD,ED> Graph<Object,Object> runUntilConvergenceWithOptions(Graph<VD,ED> graph, double tol, double resetProb, scala.Option<Object> srcId, scala.reflect.ClassTag<VD> evidence$7, scala.reflect.ClassTag<ED> evidence$8) Run a dynamic version of PageRank returning a graph with vertex attributes containing the PageRank and edge attributes containing the normalized edge weight. Parameters:graph - the graph on which to compute PageRanktol - the tolerance allowed at convergence (smaller => more accurate).resetProb - the random reset probability (alpha)srcId - the source vertex for a Personalized Page Rank (optional) evidence$7 - (undocumented)evidence$8 - (undocumented) Returns:the graph containing with each vertex containing the PageRank and each edge containing the normalized weight. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PairDStreamFunctions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PairDStreamFunctions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.streaming.dstream Class PairDStreamFunctions<K,V> Object org.apache.spark.streaming.dstream.PairDStreamFunctions<K,V> All Implemented Interfaces: java.io.Serializable public class PairDStreamFunctions<K,V> extends Object implements scala.Serializable Extra functions available on DStream of (key, value) pairs through an implicit conversion. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PairDStreamFunctions(DStream<scala.Tuple2<K,V>> self, scala.reflect.ClassTag<K> kt, scala.reflect.ClassTag<V> vt, scala.math.Ordering<K> ord)  Method Summary Methods  Modifier and Type Method and Description <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$12) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$13) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$14) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. <C> DStream<scala.Tuple2<K,C>> combineByKey(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiner, Partitioner partitioner, boolean mapSideCombine, scala.reflect.ClassTag<C> evidence$1) Combine elements of each key in DStream's RDDs using custom functions. <U> DStream<scala.Tuple2<K,U>> flatMapValues(scala.Function1<V,scala.collection.TraversableOnce<U>> flatMapValuesFunc, scala.reflect.ClassTag<U> evidence$11) Return a new DStream by applying a flatmap function to the value of each key-value pairs in 'this' DStream without changing the key. <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$24) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$25) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$26) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey() Return a new DStream by applying groupByKey to each RDD. DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey(int numPartitions) Return a new DStream by applying groupByKey to each RDD. DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey(Partitioner partitioner) Return a new DStream by applying groupByKey on each RDD. DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKeyAndWindow(Duration windowDuration) Return a new DStream by applying groupByKey over a sliding window. DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration) Return a new DStream by applying groupByKey over a sliding window. DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream by applying groupByKey over a sliding window on this DStream. DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, Partitioner partitioner) Create a new DStream by applying groupByKey over a sliding window on this DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<V,W>>> join(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$15) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<V,W>>> join(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$16) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<V,W>>> join(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$17) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$18) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$19) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$20) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. <U> DStream<scala.Tuple2<K,U>> mapValues(scala.Function1<V,U> mapValuesFunc, scala.reflect.ClassTag<U> evidence$10) Return a new DStream by applying a map function to the value of each key-value pairs in 'this' DStream without changing the key. <StateType,MappedType> MapWithStateDStream<K,V,StateType,MappedType> mapWithState(StateSpec<K,V,StateType,MappedType> spec, scala.reflect.ClassTag<StateType> evidence$2, scala.reflect.ClassTag<MappedType> evidence$3) :: Experimental :: Return a MapWithStateDStream by applying a function to every key-value element of this stream, while maintaining some state data for each unique key. DStream<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> reduceFunc) Return a new DStream by applying reduceByKey to each RDD. DStream<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> reduceFunc, int numPartitions) Return a new DStream by applying reduceByKey to each RDD. DStream<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> reduceFunc, Partitioner partitioner) Return a new DStream by applying reduceByKey to each RDD. DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, Duration windowDuration) Return a new DStream by applying reduceByKey over a sliding window on this DStream. DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream by applying reduceByKey over a sliding window. DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream by applying reduceByKey over a sliding window. DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner) Return a new DStream by applying reduceByKey over a sliding window. DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, scala.Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions, scala.Function1<scala.Tuple2<K,V>,Object> filterFunc) Return a new DStream by applying incremental reduceByKey over a sliding window. DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, scala.Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner, scala.Function1<scala.Tuple2<K,V>,Object> filterFunc) Return a new DStream by applying incremental reduceByKey over a sliding window. <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$21) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$22) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$23) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapred.OutputFormat<?,?>> outputFormatClass, org.apache.hadoop.mapred.JobConf conf) Save each RDD in this DStream as a Hadoop file. <F extends org.apache.hadoop.mapred.OutputFormat<K,V>> void saveAsHadoopFiles(String prefix, String suffix, scala.reflect.ClassTag<F> fm) Save each RDD in this DStream as a Hadoop file. void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> outputFormatClass, org.apache.hadoop.conf.Configuration conf) Save each RDD in this DStream as a Hadoop file. <F extends org.apache.hadoop.mapreduce.OutputFormat<K,V>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, scala.reflect.ClassTag<F> fm) Save each RDD in this DStream as a Hadoop file. <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function1<scala.collection.Iterator<scala.Tuple3<K,scala.collection.Seq<V>,scala.Option<S>>>,scala.collection.Iterator<scala.Tuple2<K,S>>> updateFunc, Partitioner partitioner, boolean rememberPartitioner, scala.reflect.ClassTag<S> evidence$7) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function1<scala.collection.Iterator<scala.Tuple3<K,scala.collection.Seq<V>,scala.Option<S>>>,scala.collection.Iterator<scala.Tuple2<K,S>>> updateFunc, Partitioner partitioner, boolean rememberPartitioner, RDD<scala.Tuple2<K,S>> initialRDD, scala.reflect.ClassTag<S> evidence$9) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function2<scala.collection.Seq<V>,scala.Option<S>,scala.Option<S>> updateFunc, scala.reflect.ClassTag<S> evidence$4) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function2<scala.collection.Seq<V>,scala.Option<S>,scala.Option<S>> updateFunc, int numPartitions, scala.reflect.ClassTag<S> evidence$5) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function2<scala.collection.Seq<V>,scala.Option<S>,scala.Option<S>> updateFunc, Partitioner partitioner, scala.reflect.ClassTag<S> evidence$6) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key. <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function2<scala.collection.Seq<V>,scala.Option<S>,scala.Option<S>> updateFunc, Partitioner partitioner, RDD<scala.Tuple2<K,S>> initialRDD, scala.reflect.ClassTag<S> evidence$8) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PairDStreamFunctions public PairDStreamFunctions(DStream<scala.Tuple2<K,V>> self, scala.reflect.ClassTag<K> kt, scala.reflect.ClassTag<V> vt, scala.math.Ordering<K> ord) Method Detail groupByKey public DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey() Return a new DStream by applying groupByKey to each RDD. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Returns:(undocumented) groupByKey public DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey(int numPartitions) Return a new DStream by applying groupByKey to each RDD. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:numPartitions - (undocumented) Returns:(undocumented) groupByKey public DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey(Partitioner partitioner) Return a new DStream by applying groupByKey on each RDD. The supplied org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:partitioner - (undocumented) Returns:(undocumented) reduceByKey public DStream<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> reduceFunc) Return a new DStream by applying reduceByKey to each RDD. The values for each key are merged using the associative and commutative reduce function. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:reduceFunc - (undocumented) Returns:(undocumented) reduceByKey public DStream<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> reduceFunc, int numPartitions) Return a new DStream by applying reduceByKey to each RDD. The values for each key are merged using the supplied reduce function. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:reduceFunc - (undocumented)numPartitions - (undocumented) Returns:(undocumented) reduceByKey public DStream<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> reduceFunc, Partitioner partitioner) Return a new DStream by applying reduceByKey to each RDD. The values for each key are merged using the supplied reduce function. org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:reduceFunc - (undocumented)partitioner - (undocumented) Returns:(undocumented) combineByKey public <C> DStream<scala.Tuple2<K,C>> combineByKey(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiner, Partitioner partitioner, boolean mapSideCombine, scala.reflect.ClassTag<C> evidence$1) Combine elements of each key in DStream's RDDs using custom functions. This is similar to the combineByKey for RDDs. Please refer to combineByKey in org.apache.spark.rdd.PairRDDFunctions in the Spark core documentation for more information. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiner - (undocumented)partitioner - (undocumented)mapSideCombine - (undocumented)evidence$1 - (undocumented) Returns:(undocumented) groupByKeyAndWindow public DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKeyAndWindow(Duration windowDuration) Return a new DStream by applying groupByKey over a sliding window. This is similar to DStream.groupByKey() but applies it over a sliding window. The new DStream generates RDDs with the same interval as this DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching interval Returns:(undocumented) groupByKeyAndWindow public DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration) Return a new DStream by applying groupByKey over a sliding window. Similar to DStream.groupByKey(), but applies it over a sliding window. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) groupByKeyAndWindow public DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream by applying groupByKey over a sliding window on this DStream. Similar to DStream.groupByKey(), but applies it over a sliding window. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalnumPartitions - number of partitions of each RDD in the new DStream; if not specified then Spark's default number of partitions will be used Returns:(undocumented) groupByKeyAndWindow public DStream<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKeyAndWindow(Duration windowDuration, Duration slideDuration, Partitioner partitioner) Create a new DStream by applying groupByKey over a sliding window on this DStream. Similar to DStream.groupByKey(), but applies it over a sliding window. Parameters:windowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalpartitioner - partitioner for controlling the partitioning of each RDD in the new DStream. Returns:(undocumented) reduceByKeyAndWindow public DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, Duration windowDuration) Return a new DStream by applying reduceByKey over a sliding window on this DStream. Similar to DStream.reduceByKey(), but applies it over a sliding window. The new DStream generates RDDs with the same interval as this DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:reduceFunc - associative and commutative reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching interval Returns:(undocumented) reduceByKeyAndWindow public DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration) Return a new DStream by applying reduceByKey over a sliding window. This is similar to DStream.reduceByKey() but applies it over a sliding window. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:reduceFunc - associative and commutative reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching interval Returns:(undocumented) reduceByKeyAndWindow public DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions) Return a new DStream by applying reduceByKey over a sliding window. This is similar to DStream.reduceByKey() but applies it over a sliding window. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:reduceFunc - associative and commutative reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalnumPartitions - number of partitions of each RDD in the new DStream. Returns:(undocumented) reduceByKeyAndWindow public DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner) Return a new DStream by applying reduceByKey over a sliding window. Similar to DStream.reduceByKey(), but applies it over a sliding window. Parameters:reduceFunc - associative and commutative reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalpartitioner - partitioner for controlling the partitioning of each RDD in the new DStream. Returns:(undocumented) reduceByKeyAndWindow public DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, scala.Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, int numPartitions, scala.Function1<scala.Tuple2<K,V>,Object> filterFunc) Return a new DStream by applying incremental reduceByKey over a sliding window. The reduced value of over a new window is calculated using the old window's reduced value : 1. reduce the new values that entered the window (e.g., adding new counts) 2. "inverse reduce" the old values that left the window (e.g., subtracting old counts) This is more efficient than reduceByKeyAndWindow without "inverse reduce" function. However, it is applicable to only "invertible reduce functions". Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:reduceFunc - associative and commutative reduce functioninvReduceFunc - inverse reduce function; such that for all y, invertible x: invReduceFunc(reduceFunc(x, y), x) = ywindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalfilterFunc - Optional function to filter expired key-value pairs; only pairs that satisfy the function are retainednumPartitions - (undocumented) Returns:(undocumented) reduceByKeyAndWindow public DStream<scala.Tuple2<K,V>> reduceByKeyAndWindow(scala.Function2<V,V,V> reduceFunc, scala.Function2<V,V,V> invReduceFunc, Duration windowDuration, Duration slideDuration, Partitioner partitioner, scala.Function1<scala.Tuple2<K,V>,Object> filterFunc) Return a new DStream by applying incremental reduceByKey over a sliding window. The reduced value of over a new window is calculated using the old window's reduced value : 1. reduce the new values that entered the window (e.g., adding new counts) 2. "inverse reduce" the old values that left the window (e.g., subtracting old counts) This is more efficient than reduceByKeyAndWindow without "inverse reduce" function. However, it is applicable to only "invertible reduce functions". Parameters:reduceFunc - associative and commutative reduce functioninvReduceFunc - inverse reduce functionwindowDuration - width of the window; must be a multiple of this DStream's batching intervalslideDuration - sliding interval of the window (i.e., the interval after which the new DStream will generate RDDs); must be a multiple of this DStream's batching intervalpartitioner - partitioner for controlling the partitioning of each RDD in the new DStream.filterFunc - Optional function to filter expired key-value pairs; only pairs that satisfy the function are retained Returns:(undocumented) mapWithState public <StateType,MappedType> MapWithStateDStream<K,V,StateType,MappedType> mapWithState(StateSpec<K,V,StateType,MappedType> spec, scala.reflect.ClassTag<StateType> evidence$2, scala.reflect.ClassTag<MappedType> evidence$3) :: Experimental :: Return a MapWithStateDStream by applying a function to every key-value element of this stream, while maintaining some state data for each unique key. The mapping function and other specification (e.g. partitioners, timeouts, initial state data, etc.) of this transformation can be specified using StateSpec class. The state data is accessible in as a parameter of type State in the mapping function. Example of using mapWithState: // A mapping function that maintains an integer state and return a String def mappingFunction(key: String, value: Option[Int], state: State[Int]): Option[String] = { // Use state.exists(), state.get(), state.update() and state.remove() // to manage state, and return the necessary string } val spec = StateSpec.function(mappingFunction).numPartitions(10) val mapWithStateDStream = keyValueDStream.mapWithState[StateType, MappedType](spec) Parameters:spec - Specification of this transformationevidence$2 - (undocumented)evidence$3 - (undocumented) Returns:(undocumented) updateStateByKey public <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function2<scala.collection.Seq<V>,scala.Option<S>,scala.Option<S>> updateFunc, scala.reflect.ClassTag<S> evidence$4) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:updateFunc - State update function. If this function returns None, then corresponding state key-value pair will be eliminated.evidence$4 - (undocumented) Returns:(undocumented) updateStateByKey public <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function2<scala.collection.Seq<V>,scala.Option<S>,scala.Option<S>> updateFunc, int numPartitions, scala.reflect.ClassTag<S> evidence$5) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:updateFunc - State update function. If this function returns None, then corresponding state key-value pair will be eliminated.numPartitions - Number of partitions of each RDD in the new DStream.evidence$5 - (undocumented) Returns:(undocumented) updateStateByKey public <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function2<scala.collection.Seq<V>,scala.Option<S>,scala.Option<S>> updateFunc, Partitioner partitioner, scala.reflect.ClassTag<S> evidence$6) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key. Partitioner is used to control the partitioning of each RDD. Parameters:updateFunc - State update function. If this function returns None, then corresponding state key-value pair will be eliminated.partitioner - Partitioner for controlling the partitioning of each RDD in the new DStream.evidence$6 - (undocumented) Returns:(undocumented) updateStateByKey public <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function1<scala.collection.Iterator<scala.Tuple3<K,scala.collection.Seq<V>,scala.Option<S>>>,scala.collection.Iterator<scala.Tuple2<K,S>>> updateFunc, Partitioner partitioner, boolean rememberPartitioner, scala.reflect.ClassTag<S> evidence$7) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. Partitioner is used to control the partitioning of each RDD. Parameters:updateFunc - State update function. Note, that this function may generate a different tuple with a different key than the input key. Therefore keys may be removed or added in this way. It is up to the developer to decide whether to remember the partitioner despite the key being changed.partitioner - Partitioner for controlling the partitioning of each RDD in the new DStreamrememberPartitioner - Whether to remember the partitioner object in the generated RDDs.evidence$7 - (undocumented) Returns:(undocumented) updateStateByKey public <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function2<scala.collection.Seq<V>,scala.Option<S>,scala.Option<S>> updateFunc, Partitioner partitioner, RDD<scala.Tuple2<K,S>> initialRDD, scala.reflect.ClassTag<S> evidence$8) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key. org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:updateFunc - State update function. If this function returns None, then corresponding state key-value pair will be eliminated.partitioner - Partitioner for controlling the partitioning of each RDD in the new DStream.initialRDD - initial state value of each key.evidence$8 - (undocumented) Returns:(undocumented) updateStateByKey public <S> DStream<scala.Tuple2<K,S>> updateStateByKey(scala.Function1<scala.collection.Iterator<scala.Tuple3<K,scala.collection.Seq<V>,scala.Option<S>>>,scala.collection.Iterator<scala.Tuple2<K,S>>> updateFunc, Partitioner partitioner, boolean rememberPartitioner, RDD<scala.Tuple2<K,S>> initialRDD, scala.reflect.ClassTag<S> evidence$9) Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:updateFunc - State update function. Note, that this function may generate a different tuple with a different key than the input key. Therefore keys may be removed or added in this way. It is up to the developer to decide whether to remember the partitioner despite the key being changed.partitioner - Partitioner for controlling the partitioning of each RDD in the new DStreamrememberPartitioner - Whether to remember the partitioner object in the generated RDDs.initialRDD - initial state value of each key.evidence$9 - (undocumented) Returns:(undocumented) mapValues public <U> DStream<scala.Tuple2<K,U>> mapValues(scala.Function1<V,U> mapValuesFunc, scala.reflect.ClassTag<U> evidence$10) Return a new DStream by applying a map function to the value of each key-value pairs in 'this' DStream without changing the key. Parameters:mapValuesFunc - (undocumented)evidence$10 - (undocumented) Returns:(undocumented) flatMapValues public <U> DStream<scala.Tuple2<K,U>> flatMapValues(scala.Function1<V,scala.collection.TraversableOnce<U>> flatMapValuesFunc, scala.reflect.ClassTag<U> evidence$11) Return a new DStream by applying a flatmap function to the value of each key-value pairs in 'this' DStream without changing the key. Parameters:flatMapValuesFunc - (undocumented)evidence$11 - (undocumented) Returns:(undocumented) cogroup public <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$12) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented)evidence$12 - (undocumented) Returns:(undocumented) cogroup public <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$13) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented)evidence$13 - (undocumented) Returns:(undocumented) cogroup public <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$14) Return a new DStream by applying 'cogroup' between RDDs of this DStream and other DStream. The supplied org.apache.spark.Partitioner is used to partition the generated RDDs. Parameters:other - (undocumented)partitioner - (undocumented)evidence$14 - (undocumented) Returns:(undocumented) join public <W> DStream<scala.Tuple2<K,scala.Tuple2<V,W>>> join(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$15) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented)evidence$15 - (undocumented) Returns:(undocumented) join public <W> DStream<scala.Tuple2<K,scala.Tuple2<V,W>>> join(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$16) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented)evidence$16 - (undocumented) Returns:(undocumented) join public <W> DStream<scala.Tuple2<K,scala.Tuple2<V,W>>> join(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$17) Return a new DStream by applying 'join' between RDDs of this DStream and other DStream. The supplied org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:other - (undocumented)partitioner - (undocumented)evidence$17 - (undocumented) Returns:(undocumented) leftOuterJoin public <W> DStream<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$18) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented)evidence$18 - (undocumented) Returns:(undocumented) leftOuterJoin public <W> DStream<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$19) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented)evidence$19 - (undocumented) Returns:(undocumented) leftOuterJoin public <W> DStream<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$20) Return a new DStream by applying 'left outer join' between RDDs of this DStream and other DStream. The supplied org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:other - (undocumented)partitioner - (undocumented)evidence$20 - (undocumented) Returns:(undocumented) rightOuterJoin public <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$21) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented)evidence$21 - (undocumented) Returns:(undocumented) rightOuterJoin public <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$22) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented)evidence$22 - (undocumented) Returns:(undocumented) rightOuterJoin public <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$23) Return a new DStream by applying 'right outer join' between RDDs of this DStream and other DStream. The supplied org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:other - (undocumented)partitioner - (undocumented)evidence$23 - (undocumented) Returns:(undocumented) fullOuterJoin public <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(DStream<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$24) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with Spark's default number of partitions. Parameters:other - (undocumented)evidence$24 - (undocumented) Returns:(undocumented) fullOuterJoin public <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(DStream<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$25) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. Hash partitioning is used to generate the RDDs with numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented)evidence$25 - (undocumented) Returns:(undocumented) fullOuterJoin public <W> DStream<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(DStream<scala.Tuple2<K,W>> other, Partitioner partitioner, scala.reflect.ClassTag<W> evidence$26) Return a new DStream by applying 'full outer join' between RDDs of this DStream and other DStream. The supplied org.apache.spark.Partitioner is used to control the partitioning of each RDD. Parameters:other - (undocumented)partitioner - (undocumented)evidence$26 - (undocumented) Returns:(undocumented) saveAsHadoopFiles public <F extends org.apache.hadoop.mapred.OutputFormat<K,V>> void saveAsHadoopFiles(String prefix, String suffix, scala.reflect.ClassTag<F> fm) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix" Parameters:prefix - (undocumented)suffix - (undocumented)fm - (undocumented) saveAsHadoopFiles public void saveAsHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapred.OutputFormat<?,?>> outputFormatClass, org.apache.hadoop.mapred.JobConf conf) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix" Parameters:prefix - (undocumented)suffix - (undocumented)keyClass - (undocumented)valueClass - (undocumented)outputFormatClass - (undocumented)conf - (undocumented) saveAsNewAPIHadoopFiles public <F extends org.apache.hadoop.mapreduce.OutputFormat<K,V>> void saveAsNewAPIHadoopFiles(String prefix, String suffix, scala.reflect.ClassTag<F> fm) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented)fm - (undocumented) saveAsNewAPIHadoopFiles public void saveAsNewAPIHadoopFiles(String prefix, String suffix, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> outputFormatClass, org.apache.hadoop.conf.Configuration conf) Save each RDD in this DStream as a Hadoop file. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS.suffix". Parameters:prefix - (undocumented)suffix - (undocumented)keyClass - (undocumented)valueClass - (undocumented)outputFormatClass - (undocumented)conf - (undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PairFlatMapFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PairFlatMapFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface PairFlatMapFunction<T,K,V> All Superinterfaces: java.io.Serializable public interface PairFlatMapFunction<T,K,V> extends java.io.Serializable A function that returns zero or more key-value pair records from each input record. The key-value pairs are represented as scala.Tuple2 objects. Method Summary Methods  Modifier and Type Method and Description java.util.Iterator<scala.Tuple2<K,V>> call(T t)  Method Detail call java.util.Iterator<scala.Tuple2<K,V>> call(T t) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PairFunction (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PairFunction (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.java.function Interface PairFunction<T,K,V> All Superinterfaces: java.io.Serializable public interface PairFunction<T,K,V> extends java.io.Serializable A function that returns key-value pairs (Tuple2<K, V>), and can be used to construct PairRDDs. Method Summary Methods  Modifier and Type Method and Description scala.Tuple2<K,V> call(T t)  Method Detail call scala.Tuple2<K,V> call(T t) throws Exception Throws: Exception Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PairRDDFunctions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PairRDDFunctions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class PairRDDFunctions<K,V> Object org.apache.spark.rdd.PairRDDFunctions<K,V> All Implemented Interfaces: java.io.Serializable public class PairRDDFunctions<K,V> extends Object implements scala.Serializable Extra functions available on RDDs of (key, value) pairs through an implicit conversion. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PairRDDFunctions(RDD<scala.Tuple2<K,V>> self, scala.reflect.ClassTag<K> kt, scala.reflect.ClassTag<V> vt, scala.math.Ordering<K> ord)  Method Summary Methods  Modifier and Type Method and Description <U> RDD<scala.Tuple2<K,U>> aggregateByKey(U zeroValue, scala.Function2<U,V,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$3) Aggregate the values of each key, using given combine functions and a neutral "zero value". <U> RDD<scala.Tuple2<K,U>> aggregateByKey(U zeroValue, int numPartitions, scala.Function2<U,V,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$2) Aggregate the values of each key, using given combine functions and a neutral "zero value". <U> RDD<scala.Tuple2<K,U>> aggregateByKey(U zeroValue, Partitioner partitioner, scala.Function2<U,V,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$1) Aggregate the values of each key, using given combine functions and a neutral "zero value". <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(RDD<scala.Tuple2<K,W>> other) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(RDD<scala.Tuple2<K,W>> other, int numPartitions) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. <W1,W2> RDD<scala.Tuple2<K,scala.Tuple3<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. <W1,W2> RDD<scala.Tuple2<K,scala.Tuple3<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, int numPartitions) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. <W1,W2> RDD<scala.Tuple2<K,scala.Tuple3<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, Partitioner partitioner) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. <W1,W2,W3> RDD<scala.Tuple2<K,scala.Tuple4<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>,scala.collection.Iterable<W3>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, RDD<scala.Tuple2<K,W3>> other3) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. <W1,W2,W3> RDD<scala.Tuple2<K,scala.Tuple4<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>,scala.collection.Iterable<W3>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, RDD<scala.Tuple2<K,W3>> other3, int numPartitions) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. <W1,W2,W3> RDD<scala.Tuple2<K,scala.Tuple4<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>,scala.collection.Iterable<W3>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, RDD<scala.Tuple2<K,W3>> other3, Partitioner partitioner) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. scala.collection.Map<K,V> collectAsMap() Return the key-value pairs in this RDD to the master as a Map. <C> RDD<scala.Tuple2<K,C>> combineByKey(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners) Simplified version of combineByKeyWithClassTag that hash-partitions the resulting RDD using the existing partitioner/parallelism level. <C> RDD<scala.Tuple2<K,C>> combineByKey(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, int numPartitions) Simplified version of combineByKeyWithClassTag that hash-partitions the output RDD. <C> RDD<scala.Tuple2<K,C>> combineByKey(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine, Serializer serializer) Generic function to combine the elements for each key using a custom set of aggregation functions. <C> RDD<scala.Tuple2<K,C>> combineByKeyWithClassTag(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, scala.reflect.ClassTag<C> ct) :: Experimental :: Simplified version of combineByKeyWithClassTag that hash-partitions the resulting RDD using the existing partitioner/parallelism level. <C> RDD<scala.Tuple2<K,C>> combineByKeyWithClassTag(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, int numPartitions, scala.reflect.ClassTag<C> ct) :: Experimental :: Simplified version of combineByKeyWithClassTag that hash-partitions the output RDD. <C> RDD<scala.Tuple2<K,C>> combineByKeyWithClassTag(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine, Serializer serializer, scala.reflect.ClassTag<C> ct) :: Experimental :: Generic function to combine the elements for each key using a custom set of aggregation functions. RDD<scala.Tuple2<K,Object>> countApproxDistinctByKey(double relativeSD) Return approximate number of distinct values for each key in this RDD. RDD<scala.Tuple2<K,Object>> countApproxDistinctByKey(double relativeSD, int numPartitions) Return approximate number of distinct values for each key in this RDD. RDD<scala.Tuple2<K,Object>> countApproxDistinctByKey(double relativeSD, Partitioner partitioner) Return approximate number of distinct values for each key in this RDD. RDD<scala.Tuple2<K,Object>> countApproxDistinctByKey(int p, int sp, Partitioner partitioner) Return approximate number of distinct values for each key in this RDD. scala.collection.Map<K,Object> countByKey() Count the number of elements for each key, collecting the results to a local Map. PartialResult<scala.collection.Map<K,BoundedDouble>> countByKeyApprox(long timeout, double confidence) Approximate version of countByKey that can return a partial result if it does not finish within a timeout. static scala.util.DynamicVariable<Object> disableOutputSpecValidation() Allows for the spark.hadoop.validateOutputSpecs checks to be disabled on a case-by-case basis; see SPARK-4835 for more details. <U> RDD<scala.Tuple2<K,U>> flatMapValues(scala.Function1<V,scala.collection.TraversableOnce<U>> f) Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD's partitioning. RDD<scala.Tuple2<K,V>> foldByKey(V zeroValue, scala.Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). RDD<scala.Tuple2<K,V>> foldByKey(V zeroValue, int numPartitions, scala.Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). RDD<scala.Tuple2<K,V>> foldByKey(V zeroValue, Partitioner partitioner, scala.Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(RDD<scala.Tuple2<K,W>> other) Perform a full outer join of this and other. <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(RDD<scala.Tuple2<K,W>> other, int numPartitions) Perform a full outer join of this and other. <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) Perform a full outer join of this and other. RDD<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey() Group the values for each key in the RDD into a single sequence. RDD<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey(int numPartitions) Group the values for each key in the RDD into a single sequence. RDD<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey(Partitioner partitioner) Group the values for each key in the RDD into a single sequence. <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> groupWith(RDD<scala.Tuple2<K,W>> other) Alias for cogroup. <W1,W2> RDD<scala.Tuple2<K,scala.Tuple3<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>>>> groupWith(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2) Alias for cogroup. <W1,W2,W3> RDD<scala.Tuple2<K,scala.Tuple4<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>,scala.collection.Iterable<W3>>>> groupWith(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, RDD<scala.Tuple2<K,W3>> other3) Alias for cogroup. <W> RDD<scala.Tuple2<K,scala.Tuple2<V,W>>> join(RDD<scala.Tuple2<K,W>> other) Return an RDD containing all pairs of elements with matching keys in this and other. <W> RDD<scala.Tuple2<K,scala.Tuple2<V,W>>> join(RDD<scala.Tuple2<K,W>> other, int numPartitions) Return an RDD containing all pairs of elements with matching keys in this and other. <W> RDD<scala.Tuple2<K,scala.Tuple2<V,W>>> join(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) Return an RDD containing all pairs of elements with matching keys in this and other. RDD<K> keys() Return an RDD with the keys of each tuple. <W> RDD<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(RDD<scala.Tuple2<K,W>> other) Perform a left outer join of this and other. <W> RDD<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(RDD<scala.Tuple2<K,W>> other, int numPartitions) Perform a left outer join of this and other. <W> RDD<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) Perform a left outer join of this and other. scala.collection.Seq<V> lookup(K key) Return the list of values in the RDD for key key. <U> RDD<scala.Tuple2<K,U>> mapValues(scala.Function1<V,U> f) Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD's partitioning. RDD<scala.Tuple2<K,V>> partitionBy(Partitioner partitioner) Return a copy of the RDD partitioned using the specified partitioner. static int RECORDS_BETWEEN_BYTES_WRITTEN_METRIC_UPDATES()  RDD<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function. RDD<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> func, int numPartitions) Merge the values for each key using an associative and commutative reduce function. RDD<scala.Tuple2<K,V>> reduceByKey(Partitioner partitioner, scala.Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function. scala.collection.Map<K,V> reduceByKeyLocally(scala.Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function, but return the results immediately to the master as a Map. <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(RDD<scala.Tuple2<K,W>> other) Perform a right outer join of this and other. <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(RDD<scala.Tuple2<K,W>> other, int numPartitions) Perform a right outer join of this and other. <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) Perform a right outer join of this and other. RDD<scala.Tuple2<K,V>> sampleByKey(boolean withReplacement, scala.collection.Map<K,Object> fractions, long seed) Return a subset of this RDD sampled by key (via stratified sampling). RDD<scala.Tuple2<K,V>> sampleByKeyExact(boolean withReplacement, scala.collection.Map<K,Object> fractions, long seed) Return a subset of this RDD sampled by key (via stratified sampling) containing exactly math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). void saveAsHadoopDataset(org.apache.hadoop.mapred.JobConf conf) Output the RDD to any Hadoop-supported storage system, using a Hadoop JobConf object for that storage system. void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapred.OutputFormat<?,?>> outputFormatClass, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) Output the RDD to any Hadoop-supported file system, using a Hadoop OutputFormat class supporting the key and value types K and V in this RDD. void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapred.OutputFormat<?,?>> outputFormatClass, org.apache.hadoop.mapred.JobConf conf, scala.Option<Class<? extends org.apache.hadoop.io.compress.CompressionCodec>> codec) Output the RDD to any Hadoop-supported file system, using a Hadoop OutputFormat class supporting the key and value types K and V in this RDD. <F extends org.apache.hadoop.mapred.OutputFormat<K,V>> void saveAsHadoopFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec, scala.reflect.ClassTag<F> fm) Output the RDD to any Hadoop-supported file system, using a Hadoop OutputFormat class supporting the key and value types K and V in this RDD. <F extends org.apache.hadoop.mapred.OutputFormat<K,V>> void saveAsHadoopFile(String path, scala.reflect.ClassTag<F> fm) Output the RDD to any Hadoop-supported file system, using a Hadoop OutputFormat class supporting the key and value types K and V in this RDD. void saveAsNewAPIHadoopDataset(org.apache.hadoop.conf.Configuration conf) Output the RDD to any Hadoop-supported storage system with new Hadoop API, using a Hadoop Configuration object for that storage system. void saveAsNewAPIHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> outputFormatClass, org.apache.hadoop.conf.Configuration conf) Output the RDD to any Hadoop-supported file system, using a new Hadoop API OutputFormat (mapreduce.OutputFormat) object supporting the key and value types K and V in this RDD. <F extends org.apache.hadoop.mapreduce.OutputFormat<K,V>> void saveAsNewAPIHadoopFile(String path, scala.reflect.ClassTag<F> fm) Output the RDD to any Hadoop-supported file system, using a new Hadoop API OutputFormat (mapreduce.OutputFormat) object supporting the key and value types K and V in this RDD. <W> RDD<scala.Tuple2<K,V>> subtractByKey(RDD<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$4) Return an RDD with the pairs from this whose keys are not in other. <W> RDD<scala.Tuple2<K,V>> subtractByKey(RDD<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$5) Return an RDD with the pairs from `this` whose keys are not in `other`. <W> RDD<scala.Tuple2<K,V>> subtractByKey(RDD<scala.Tuple2<K,W>> other, Partitioner p, scala.reflect.ClassTag<W> evidence$6) Return an RDD with the pairs from `this` whose keys are not in `other`. RDD<V> values() Return an RDD with the values of each tuple. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PairRDDFunctions public PairRDDFunctions(RDD<scala.Tuple2<K,V>> self, scala.reflect.ClassTag<K> kt, scala.reflect.ClassTag<V> vt, scala.math.Ordering<K> ord) Method Detail RECORDS_BETWEEN_BYTES_WRITTEN_METRIC_UPDATES public static int RECORDS_BETWEEN_BYTES_WRITTEN_METRIC_UPDATES() disableOutputSpecValidation public static scala.util.DynamicVariable<Object> disableOutputSpecValidation() Allows for the spark.hadoop.validateOutputSpecs checks to be disabled on a case-by-case basis; see SPARK-4835 for more details. Returns:(undocumented) combineByKeyWithClassTag public <C> RDD<scala.Tuple2<K,C>> combineByKeyWithClassTag(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine, Serializer serializer, scala.reflect.ClassTag<C> ct) :: Experimental :: Generic function to combine the elements for each key using a custom set of aggregation functions. Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined type" C Note that V and C can be different -- for example, one might group an RDD of type (Int, Int) into an RDD of type (Int, Seq[Int]). Users provide three functions: - createCombiner, which turns a V into a C (e.g., creates a one-element list) - mergeValue, to merge a V into a C (e.g., adds it to the end of a list) - mergeCombiners, to combine two C's into a single one. In addition, users can control the partitioning of the output RDD, and whether to perform map-side aggregation (if a mapper can produce multiple items with the same key). Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)partitioner - (undocumented)mapSideCombine - (undocumented)serializer - (undocumented)ct - (undocumented) Returns:(undocumented) combineByKey public <C> RDD<scala.Tuple2<K,C>> combineByKey(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, Partitioner partitioner, boolean mapSideCombine, Serializer serializer) Generic function to combine the elements for each key using a custom set of aggregation functions. This method is here for backward compatibility. It does not provide combiner classtag information to the shuffle. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)partitioner - (undocumented)mapSideCombine - (undocumented)serializer - (undocumented) Returns:(undocumented)See Also:combineByKeyWithClassTag combineByKey public <C> RDD<scala.Tuple2<K,C>> combineByKey(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, int numPartitions) Simplified version of combineByKeyWithClassTag that hash-partitions the output RDD. This method is here for backward compatibility. It does not provide combiner classtag information to the shuffle. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)numPartitions - (undocumented) Returns:(undocumented)See Also:combineByKeyWithClassTag combineByKeyWithClassTag public <C> RDD<scala.Tuple2<K,C>> combineByKeyWithClassTag(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, int numPartitions, scala.reflect.ClassTag<C> ct) :: Experimental :: Simplified version of combineByKeyWithClassTag that hash-partitions the output RDD. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)numPartitions - (undocumented)ct - (undocumented) Returns:(undocumented) aggregateByKey public <U> RDD<scala.Tuple2<K,U>> aggregateByKey(U zeroValue, Partitioner partitioner, scala.Function2<U,V,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$1) Aggregate the values of each key, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U. Parameters:zeroValue - (undocumented)partitioner - (undocumented)seqOp - (undocumented)combOp - (undocumented)evidence$1 - (undocumented) Returns:(undocumented) aggregateByKey public <U> RDD<scala.Tuple2<K,U>> aggregateByKey(U zeroValue, int numPartitions, scala.Function2<U,V,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$2) Aggregate the values of each key, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U. Parameters:zeroValue - (undocumented)numPartitions - (undocumented)seqOp - (undocumented)combOp - (undocumented)evidence$2 - (undocumented) Returns:(undocumented) aggregateByKey public <U> RDD<scala.Tuple2<K,U>> aggregateByKey(U zeroValue, scala.Function2<U,V,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$3) Aggregate the values of each key, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U. Parameters:zeroValue - (undocumented)seqOp - (undocumented)combOp - (undocumented)evidence$3 - (undocumented) Returns:(undocumented) foldByKey public RDD<scala.Tuple2<K,V>> foldByKey(V zeroValue, Partitioner partitioner, scala.Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). Parameters:zeroValue - (undocumented)partitioner - (undocumented)func - (undocumented) Returns:(undocumented) foldByKey public RDD<scala.Tuple2<K,V>> foldByKey(V zeroValue, int numPartitions, scala.Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). Parameters:zeroValue - (undocumented)numPartitions - (undocumented)func - (undocumented) Returns:(undocumented) foldByKey public RDD<scala.Tuple2<K,V>> foldByKey(V zeroValue, scala.Function2<V,V,V> func) Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). Parameters:zeroValue - (undocumented)func - (undocumented) Returns:(undocumented) sampleByKey public RDD<scala.Tuple2<K,V>> sampleByKey(boolean withReplacement, scala.collection.Map<K,Object> fractions, long seed) Return a subset of this RDD sampled by key (via stratified sampling). Create a sample of this RDD using variable sampling rates for different keys as specified by fractions, a key to sampling rate map, via simple random sampling with one pass over the RDD, to produce a sample of size that's approximately equal to the sum of math.ceil(numItems * samplingRate) over all key values. Parameters:withReplacement - whether to sample with or without replacementfractions - map of specific keys to sampling ratesseed - seed for the random number generator Returns:RDD containing the sampled subset sampleByKeyExact public RDD<scala.Tuple2<K,V>> sampleByKeyExact(boolean withReplacement, scala.collection.Map<K,Object> fractions, long seed) Return a subset of this RDD sampled by key (via stratified sampling) containing exactly math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). This method differs from sampleByKey in that we make additional passes over the RDD to create a sample size that's exactly equal to the sum of math.ceil(numItems * samplingRate) over all key values with a 99.99% confidence. When sampling without replacement, we need one additional pass over the RDD to guarantee sample size; when sampling with replacement, we need two additional passes. Parameters:withReplacement - whether to sample with or without replacementfractions - map of specific keys to sampling ratesseed - seed for the random number generator Returns:RDD containing the sampled subset reduceByKey public RDD<scala.Tuple2<K,V>> reduceByKey(Partitioner partitioner, scala.Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Parameters:partitioner - (undocumented)func - (undocumented) Returns:(undocumented) reduceByKey public RDD<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> func, int numPartitions) Merge the values for each key using an associative and commutative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Output will be hash-partitioned with numPartitions partitions. Parameters:func - (undocumented)numPartitions - (undocumented) Returns:(undocumented) reduceByKey public RDD<scala.Tuple2<K,V>> reduceByKey(scala.Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/ parallelism level. Parameters:func - (undocumented) Returns:(undocumented) reduceByKeyLocally public scala.collection.Map<K,V> reduceByKeyLocally(scala.Function2<V,V,V> func) Merge the values for each key using an associative and commutative reduce function, but return the results immediately to the master as a Map. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Parameters:func - (undocumented) Returns:(undocumented) countByKey public scala.collection.Map<K,Object> countByKey() Count the number of elements for each key, collecting the results to a local Map. Note that this method should only be used if the resulting map is expected to be small, as the whole thing is loaded into the driver's memory. To handle very large results, consider using rdd.mapValues(_ => 1L).reduceByKey(_ + _), which returns an RDD[T, Long] instead of a map. Returns:(undocumented) countByKeyApprox public PartialResult<scala.collection.Map<K,BoundedDouble>> countByKeyApprox(long timeout, double confidence) Approximate version of countByKey that can return a partial result if it does not finish within a timeout. The confidence is the probability that the error bounds of the result will contain the true value. That is, if countApprox were called repeatedly with confidence 0.9, we would expect 90% of the results to contain the true count. The confidence must be in the range [0,1] or an exception will be thrown. Parameters:timeout - maximum time to wait for the job, in millisecondsconfidence - the desired statistical confidence in the result Returns:a potentially incomplete result, with error bounds countApproxDistinctByKey public RDD<scala.Tuple2<K,Object>> countApproxDistinctByKey(int p, int sp, Partitioner partitioner) Return approximate number of distinct values for each key in this RDD. The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here. The relative accuracy is approximately 1.054 / sqrt(2^p). Setting a nonzero sp > p would trigger sparse representation of registers, which may reduce the memory consumption and increase accuracy when the cardinality is small. Parameters:p - The precision value for the normal set. p must be a value between 4 and sp if sp is not zero (32 max).sp - The precision value for the sparse set, between 0 and 32. If sp equals 0, the sparse representation is skipped.partitioner - Partitioner to use for the resulting RDD. Returns:(undocumented) countApproxDistinctByKey public RDD<scala.Tuple2<K,Object>> countApproxDistinctByKey(double relativeSD, Partitioner partitioner) Return approximate number of distinct values for each key in this RDD. The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here. Parameters:relativeSD - Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017.partitioner - partitioner of the resulting RDD Returns:(undocumented) countApproxDistinctByKey public RDD<scala.Tuple2<K,Object>> countApproxDistinctByKey(double relativeSD, int numPartitions) Return approximate number of distinct values for each key in this RDD. The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here. Parameters:relativeSD - Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017.numPartitions - number of partitions of the resulting RDD Returns:(undocumented) countApproxDistinctByKey public RDD<scala.Tuple2<K,Object>> countApproxDistinctByKey(double relativeSD) Return approximate number of distinct values for each key in this RDD. The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available here. Parameters:relativeSD - Relative accuracy. Smaller values create counters that require more space. It must be greater than 0.000017. Returns:(undocumented) groupByKey public RDD<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey(Partitioner partitioner) Group the values for each key in the RDD into a single sequence. Allows controlling the partitioning of the resulting key-value pair RDD by passing a Partitioner. The ordering of elements within each group is not guaranteed, and may even differ each time the resulting RDD is evaluated. Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance. Note: As currently implemented, groupByKey must be able to hold all the key-value pairs for any key in memory. If a key has too many values, it can result in an OutOfMemoryError. Parameters:partitioner - (undocumented) Returns:(undocumented) groupByKey public RDD<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey(int numPartitions) Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with into numPartitions partitions. The ordering of elements within each group is not guaranteed, and may even differ each time the resulting RDD is evaluated. Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance. Note: As currently implemented, groupByKey must be able to hold all the key-value pairs for any key in memory. If a key has too many values, it can result in an OutOfMemoryError. Parameters:numPartitions - (undocumented) Returns:(undocumented) partitionBy public RDD<scala.Tuple2<K,V>> partitionBy(Partitioner partitioner) Return a copy of the RDD partitioned using the specified partitioner. Parameters:partitioner - (undocumented) Returns:(undocumented) join public <W> RDD<scala.Tuple2<K,scala.Tuple2<V,W>>> join(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Uses the given Partitioner to partition the output RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) leftOuterJoin public <W> RDD<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) Perform a left outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (v, Some(w))) for w in other, or the pair (k, (v, None)) if no elements in other have key k. Uses the given Partitioner to partition the output RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) rightOuterJoin public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) Perform a right outer join of this and other. For each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), w)) for v in this, or the pair (k, (None, w)) if no elements in this have key k. Uses the given Partitioner to partition the output RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) fullOuterJoin public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) Perform a full outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in other, or the pair (k, (Some(v), None)) if no elements in other have key k. Similarly, for each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for v in this, or the pair (k, (None, Some(w))) if no elements in this have key k. Uses the given Partitioner to partition the output RDD. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) combineByKey public <C> RDD<scala.Tuple2<K,C>> combineByKey(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners) Simplified version of combineByKeyWithClassTag that hash-partitions the resulting RDD using the existing partitioner/parallelism level. This method is here for backward compatibility. It does not provide combiner classtag information to the shuffle. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented) Returns:(undocumented)See Also:combineByKeyWithClassTag combineByKeyWithClassTag public <C> RDD<scala.Tuple2<K,C>> combineByKeyWithClassTag(scala.Function1<V,C> createCombiner, scala.Function2<C,V,C> mergeValue, scala.Function2<C,C,C> mergeCombiners, scala.reflect.ClassTag<C> ct) :: Experimental :: Simplified version of combineByKeyWithClassTag that hash-partitions the resulting RDD using the existing partitioner/parallelism level. Parameters:createCombiner - (undocumented)mergeValue - (undocumented)mergeCombiners - (undocumented)ct - (undocumented) Returns:(undocumented) groupByKey public RDD<scala.Tuple2<K,scala.collection.Iterable<V>>> groupByKey() Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with the existing partitioner/parallelism level. The ordering of elements within each group is not guaranteed, and may even differ each time the resulting RDD is evaluated. Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance. Returns:(undocumented) join public <W> RDD<scala.Tuple2<K,scala.Tuple2<V,W>>> join(RDD<scala.Tuple2<K,W>> other) Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster. Parameters:other - (undocumented) Returns:(undocumented) join public <W> RDD<scala.Tuple2<K,scala.Tuple2<V,W>>> join(RDD<scala.Tuple2<K,W>> other, int numPartitions) Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) leftOuterJoin public <W> RDD<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(RDD<scala.Tuple2<K,W>> other) Perform a left outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (v, Some(w))) for w in other, or the pair (k, (v, None)) if no elements in other have key k. Hash-partitions the output using the existing partitioner/parallelism level. Parameters:other - (undocumented) Returns:(undocumented) leftOuterJoin public <W> RDD<scala.Tuple2<K,scala.Tuple2<V,scala.Option<W>>>> leftOuterJoin(RDD<scala.Tuple2<K,W>> other, int numPartitions) Perform a left outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (v, Some(w))) for w in other, or the pair (k, (v, None)) if no elements in other have key k. Hash-partitions the output into numPartitions partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) rightOuterJoin public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(RDD<scala.Tuple2<K,W>> other) Perform a right outer join of this and other. For each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), w)) for v in this, or the pair (k, (None, w)) if no elements in this have key k. Hash-partitions the resulting RDD using the existing partitioner/parallelism level. Parameters:other - (undocumented) Returns:(undocumented) rightOuterJoin public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,W>>> rightOuterJoin(RDD<scala.Tuple2<K,W>> other, int numPartitions) Perform a right outer join of this and other. For each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), w)) for v in this, or the pair (k, (None, w)) if no elements in this have key k. Hash-partitions the resulting RDD into the given number of partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) fullOuterJoin public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(RDD<scala.Tuple2<K,W>> other) Perform a full outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in other, or the pair (k, (Some(v), None)) if no elements in other have key k. Similarly, for each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for v in this, or the pair (k, (None, Some(w))) if no elements in this have key k. Hash-partitions the resulting RDD using the existing partitioner/ parallelism level. Parameters:other - (undocumented) Returns:(undocumented) fullOuterJoin public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.Option<V>,scala.Option<W>>>> fullOuterJoin(RDD<scala.Tuple2<K,W>> other, int numPartitions) Perform a full outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in other, or the pair (k, (Some(v), None)) if no elements in other have key k. Similarly, for each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for v in this, or the pair (k, (None, Some(w))) if no elements in this have key k. Hash-partitions the resulting RDD into the given number of partitions. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) collectAsMap public scala.collection.Map<K,V> collectAsMap() Return the key-value pairs in this RDD to the master as a Map. Warning: this doesn't return a multimap (so if you have multiple values to the same key, only one value per key is preserved in the map returned) Returns:(undocumented) mapValues public <U> RDD<scala.Tuple2<K,U>> mapValues(scala.Function1<V,U> f) Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD's partitioning. Parameters:f - (undocumented) Returns:(undocumented) flatMapValues public <U> RDD<scala.Tuple2<K,U>> flatMapValues(scala.Function1<V,scala.collection.TraversableOnce<U>> f) Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD's partitioning. Parameters:f - (undocumented) Returns:(undocumented) cogroup public <W1,W2,W3> RDD<scala.Tuple2<K,scala.Tuple4<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>,scala.collection.Iterable<W3>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, RDD<scala.Tuple2<K,W3>> other3, Partitioner partitioner) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. Parameters:other1 - (undocumented)other2 - (undocumented)other3 - (undocumented)partitioner - (undocumented) Returns:(undocumented) cogroup public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(RDD<scala.Tuple2<K,W>> other, Partitioner partitioner) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. Parameters:other - (undocumented)partitioner - (undocumented) Returns:(undocumented) cogroup public <W1,W2> RDD<scala.Tuple2<K,scala.Tuple3<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, Partitioner partitioner) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. Parameters:other1 - (undocumented)other2 - (undocumented)partitioner - (undocumented) Returns:(undocumented) cogroup public <W1,W2,W3> RDD<scala.Tuple2<K,scala.Tuple4<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>,scala.collection.Iterable<W3>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, RDD<scala.Tuple2<K,W3>> other3) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. Parameters:other1 - (undocumented)other2 - (undocumented)other3 - (undocumented) Returns:(undocumented) cogroup public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(RDD<scala.Tuple2<K,W>> other) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. Parameters:other - (undocumented) Returns:(undocumented) cogroup public <W1,W2> RDD<scala.Tuple2<K,scala.Tuple3<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. Parameters:other1 - (undocumented)other2 - (undocumented) Returns:(undocumented) cogroup public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> cogroup(RDD<scala.Tuple2<K,W>> other, int numPartitions) For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other. Parameters:other - (undocumented)numPartitions - (undocumented) Returns:(undocumented) cogroup public <W1,W2> RDD<scala.Tuple2<K,scala.Tuple3<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, int numPartitions) For each key k in this or other1 or other2, return a resulting RDD that contains a tuple with the list of values for that key in this, other1 and other2. Parameters:other1 - (undocumented)other2 - (undocumented)numPartitions - (undocumented) Returns:(undocumented) cogroup public <W1,W2,W3> RDD<scala.Tuple2<K,scala.Tuple4<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>,scala.collection.Iterable<W3>>>> cogroup(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, RDD<scala.Tuple2<K,W3>> other3, int numPartitions) For each key k in this or other1 or other2 or other3, return a resulting RDD that contains a tuple with the list of values for that key in this, other1, other2 and other3. Parameters:other1 - (undocumented)other2 - (undocumented)other3 - (undocumented)numPartitions - (undocumented) Returns:(undocumented) groupWith public <W> RDD<scala.Tuple2<K,scala.Tuple2<scala.collection.Iterable<V>,scala.collection.Iterable<W>>>> groupWith(RDD<scala.Tuple2<K,W>> other) Alias for cogroup. groupWith public <W1,W2> RDD<scala.Tuple2<K,scala.Tuple3<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>>>> groupWith(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2) Alias for cogroup. groupWith public <W1,W2,W3> RDD<scala.Tuple2<K,scala.Tuple4<scala.collection.Iterable<V>,scala.collection.Iterable<W1>,scala.collection.Iterable<W2>,scala.collection.Iterable<W3>>>> groupWith(RDD<scala.Tuple2<K,W1>> other1, RDD<scala.Tuple2<K,W2>> other2, RDD<scala.Tuple2<K,W3>> other3) Alias for cogroup. subtractByKey public <W> RDD<scala.Tuple2<K,V>> subtractByKey(RDD<scala.Tuple2<K,W>> other, scala.reflect.ClassTag<W> evidence$4) Return an RDD with the pairs from this whose keys are not in other. Uses this partitioner/partition size, because even if other is huge, the resulting RDD will be <= us. Parameters:other - (undocumented)evidence$4 - (undocumented) Returns:(undocumented) subtractByKey public <W> RDD<scala.Tuple2<K,V>> subtractByKey(RDD<scala.Tuple2<K,W>> other, int numPartitions, scala.reflect.ClassTag<W> evidence$5) Return an RDD with the pairs from `this` whose keys are not in `other`. subtractByKey public <W> RDD<scala.Tuple2<K,V>> subtractByKey(RDD<scala.Tuple2<K,W>> other, Partitioner p, scala.reflect.ClassTag<W> evidence$6) Return an RDD with the pairs from `this` whose keys are not in `other`. lookup public scala.collection.Seq<V> lookup(K key) Return the list of values in the RDD for key key. This operation is done efficiently if the RDD has a known partitioner by only searching the partition that the key maps to. Parameters:key - (undocumented) Returns:(undocumented) saveAsHadoopFile public <F extends org.apache.hadoop.mapred.OutputFormat<K,V>> void saveAsHadoopFile(String path, scala.reflect.ClassTag<F> fm) Output the RDD to any Hadoop-supported file system, using a Hadoop OutputFormat class supporting the key and value types K and V in this RDD. Parameters:path - (undocumented)fm - (undocumented) saveAsHadoopFile public <F extends org.apache.hadoop.mapred.OutputFormat<K,V>> void saveAsHadoopFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec, scala.reflect.ClassTag<F> fm) Output the RDD to any Hadoop-supported file system, using a Hadoop OutputFormat class supporting the key and value types K and V in this RDD. Compress the result with the supplied codec. Parameters:path - (undocumented)codec - (undocumented)fm - (undocumented) saveAsNewAPIHadoopFile public <F extends org.apache.hadoop.mapreduce.OutputFormat<K,V>> void saveAsNewAPIHadoopFile(String path, scala.reflect.ClassTag<F> fm) Output the RDD to any Hadoop-supported file system, using a new Hadoop API OutputFormat (mapreduce.OutputFormat) object supporting the key and value types K and V in this RDD. Parameters:path - (undocumented)fm - (undocumented) saveAsNewAPIHadoopFile public void saveAsNewAPIHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapreduce.OutputFormat<?,?>> outputFormatClass, org.apache.hadoop.conf.Configuration conf) Output the RDD to any Hadoop-supported file system, using a new Hadoop API OutputFormat (mapreduce.OutputFormat) object supporting the key and value types K and V in this RDD. Parameters:path - (undocumented)keyClass - (undocumented)valueClass - (undocumented)outputFormatClass - (undocumented)conf - (undocumented) saveAsHadoopFile public void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapred.OutputFormat<?,?>> outputFormatClass, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) Output the RDD to any Hadoop-supported file system, using a Hadoop OutputFormat class supporting the key and value types K and V in this RDD. Compress with the supplied codec. Parameters:path - (undocumented)keyClass - (undocumented)valueClass - (undocumented)outputFormatClass - (undocumented)codec - (undocumented) saveAsHadoopFile public void saveAsHadoopFile(String path, Class<?> keyClass, Class<?> valueClass, Class<? extends org.apache.hadoop.mapred.OutputFormat<?,?>> outputFormatClass, org.apache.hadoop.mapred.JobConf conf, scala.Option<Class<? extends org.apache.hadoop.io.compress.CompressionCodec>> codec) Output the RDD to any Hadoop-supported file system, using a Hadoop OutputFormat class supporting the key and value types K and V in this RDD. Note that, we should make sure our tasks are idempotent when speculation is enabled, i.e. do not use output committer that writes data directly. There is an example in https://issues.apache.org/jira/browse/SPARK-10063 to show the bad result of using direct output committer with speculation enabled. Parameters:path - (undocumented)keyClass - (undocumented)valueClass - (undocumented)outputFormatClass - (undocumented)conf - (undocumented)codec - (undocumented) saveAsNewAPIHadoopDataset public void saveAsNewAPIHadoopDataset(org.apache.hadoop.conf.Configuration conf) Output the RDD to any Hadoop-supported storage system with new Hadoop API, using a Hadoop Configuration object for that storage system. The Conf should set an OutputFormat and any output paths required (e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. Note that, we should make sure our tasks are idempotent when speculation is enabled, i.e. do not use output committer that writes data directly. There is an example in https://issues.apache.org/jira/browse/SPARK-10063 to show the bad result of using direct output committer with speculation enabled. Parameters:conf - (undocumented) saveAsHadoopDataset public void saveAsHadoopDataset(org.apache.hadoop.mapred.JobConf conf) Output the RDD to any Hadoop-supported storage system, using a Hadoop JobConf object for that storage system. The JobConf should set an OutputFormat and any output paths required (e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. Parameters:conf - (undocumented) keys public RDD<K> keys() Return an RDD with the keys of each tuple. Returns:(undocumented) values public RDD<V> values() Return an RDD with the values of each tuple. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PairwiseRRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PairwiseRRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.api.r Class PairwiseRRDD<T> Object org.apache.spark.rdd.RDD<U> org.apache.spark.api.r.BaseRRDD<T,scala.Tuple2<Object,byte[]>> org.apache.spark.api.r.PairwiseRRDD<T> All Implemented Interfaces: java.io.Serializable public class PairwiseRRDD<T> extends BaseRRDD<T,scala.Tuple2<Object,byte[]>> Form an RDD[(Int, Array[Byte])] from key-value pairs returned from R. This is used by SparkR's shuffle operations. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PairwiseRRDD(RDD<T> parent, int numPartitions, byte[] hashFunc, String deserializer, byte[] packageNames, Object[] broadcastVars, scala.reflect.ClassTag<T> evidence$3)  Method Summary Methods  Modifier and Type Method and Description JavaPairRDD<Object,byte[]> asJavaPairRDD()  Methods inherited from class org.apache.spark.api.r.BaseRRDD compute, getPartitions Methods inherited from class org.apache.spark.rdd.RDD aggregate, cache, cartesian, checkpoint, coalesce, collect, collect, context, count, countApprox, countApproxDistinct, countApproxDistinct, countByValue, countByValueApprox, dependencies, distinct, distinct, doubleRDDToDoubleRDDFunctions, filter, first, flatMap, fold, foreach, foreachPartition, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupBy, id, intersection, intersection, intersection, isCheckpointed, isEmpty, iterator, keyBy, localCheckpoint, map, mapPartitions, mapPartitionsWithIndex, max, min, name, numericRDDToDoubleRDDFunctions, partitioner, partitions, persist, persist, pipe, pipe, pipe, preferredLocations, randomSplit, rddToAsyncRDDActions, rddToOrderedRDDFunctions, rddToPairRDDFunctions, rddToSequenceFileRDDFunctions, reduce, repartition, sample, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortBy, sparkContext, subtract, subtract, subtract, take, takeOrdered, takeSample, toDebugString, toJavaRDD, toLocalIterator, top, toString, treeAggregate, treeReduce, union, unpersist, zip, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail PairwiseRRDD public PairwiseRRDD(RDD<T> parent, int numPartitions, byte[] hashFunc, String deserializer, byte[] packageNames, Object[] broadcastVars, scala.reflect.ClassTag<T> evidence$3) Method Detail asJavaPairRDD public JavaPairRDD<Object,byte[]> asJavaPairRDD() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Param (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Param (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class Param<T> Object org.apache.spark.ml.param.Param<T> All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: BooleanParam, DoubleArrayParam, DoubleParam, FloatParam, IntArrayParam, IntParam, LongParam, StringArrayParam public class Param<T> extends Object implements scala.Serializable :: DeveloperApi :: A param with self-contained documentation and optionally default value. Primitive-typed param should use the specialized versions, which are more friendly to Java users. param: parent parent object param: name param name param: doc documentation param: isValid optional validation method which indicates if a value is valid. See ParamValidators for factory methods for common validation functions. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Param(Identifiable parent, String name, String doc)  Param(Identifiable parent, String name, String doc, scala.Function1<T,Object> isValid)  Param(String parent, String name, String doc)  Param(String parent, String name, String doc, scala.Function1<T,Object> isValid)  Method Summary Methods  Modifier and Type Method and Description String doc()  boolean equals(Object obj)  int hashCode()  scala.Function1<T,Object> isValid()  T jsonDecode(String json) Decodes a param value from JSON. String jsonEncode(T value) Encodes a param value into JSON, which can be decoded by jsonDecode(). String name()  String parent()  String toString()  ParamPair<T> w(T value) Creates a param pair with the given value (for Java). Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail Param public Param(String parent, String name, String doc, scala.Function1<T,Object> isValid) Param public Param(Identifiable parent, String name, String doc, scala.Function1<T,Object> isValid) Param public Param(String parent, String name, String doc) Param public Param(Identifiable parent, String name, String doc) Method Detail parent public String parent() name public String name() doc public String doc() isValid public scala.Function1<T,Object> isValid() w public ParamPair<T> w(T value) Creates a param pair with the given value (for Java). jsonEncode public String jsonEncode(T value) Encodes a param value into JSON, which can be decoded by jsonDecode(). jsonDecode public T jsonDecode(String json) Decodes a param value from JSON. toString public final String toString() Overrides: toString in class Object hashCode public final int hashCode() Overrides: hashCode in class Object equals public final boolean equals(Object obj) Overrides: equals in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ParamGridBuilder (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ParamGridBuilder (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.tuning Class ParamGridBuilder Object org.apache.spark.ml.tuning.ParamGridBuilder public class ParamGridBuilder extends Object Builder for a param grid used in grid search-based model selection. Constructor Summary Constructors  Constructor and Description ParamGridBuilder()  Method Summary Methods  Modifier and Type Method and Description ParamGridBuilder addGrid(BooleanParam param) Adds a boolean param with true and false. ParamGridBuilder addGrid(DoubleParam param, double[] values) Adds a double param with multiple values. ParamGridBuilder addGrid(FloatParam param, float[] values) Adds a float param with multiple values. ParamGridBuilder addGrid(IntParam param, int[] values) Adds an int param with multiple values. ParamGridBuilder addGrid(LongParam param, long[] values) Adds a long param with multiple values. <T> ParamGridBuilder addGrid(Param<T> param, scala.collection.Iterable<T> values) Adds a param with multiple values (overwrites if the input param exists). ParamGridBuilder baseOn(ParamMap paramMap) Sets the given parameters in this grid to fixed values. ParamGridBuilder baseOn(ParamPair<?>... paramPairs) Sets the given parameters in this grid to fixed values. ParamGridBuilder baseOn(scala.collection.Seq<ParamPair<?>> paramPairs) Sets the given parameters in this grid to fixed values. ParamMap[] build() Builds and returns all combinations of parameters specified by the param grid. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ParamGridBuilder public ParamGridBuilder() Method Detail baseOn public ParamGridBuilder baseOn(ParamPair<?>... paramPairs) Sets the given parameters in this grid to fixed values. Parameters:paramPairs - (undocumented) Returns:(undocumented) baseOn public ParamGridBuilder baseOn(ParamMap paramMap) Sets the given parameters in this grid to fixed values. Parameters:paramMap - (undocumented) Returns:(undocumented) baseOn public ParamGridBuilder baseOn(scala.collection.Seq<ParamPair<?>> paramPairs) Sets the given parameters in this grid to fixed values. Parameters:paramPairs - (undocumented) Returns:(undocumented) addGrid public <T> ParamGridBuilder addGrid(Param<T> param, scala.collection.Iterable<T> values) Adds a param with multiple values (overwrites if the input param exists). Parameters:param - (undocumented)values - (undocumented) Returns:(undocumented) addGrid public ParamGridBuilder addGrid(DoubleParam param, double[] values) Adds a double param with multiple values. Parameters:param - (undocumented)values - (undocumented) Returns:(undocumented) addGrid public ParamGridBuilder addGrid(IntParam param, int[] values) Adds an int param with multiple values. Parameters:param - (undocumented)values - (undocumented) Returns:(undocumented) addGrid public ParamGridBuilder addGrid(FloatParam param, float[] values) Adds a float param with multiple values. Parameters:param - (undocumented)values - (undocumented) Returns:(undocumented) addGrid public ParamGridBuilder addGrid(LongParam param, long[] values) Adds a long param with multiple values. Parameters:param - (undocumented)values - (undocumented) Returns:(undocumented) addGrid public ParamGridBuilder addGrid(BooleanParam param) Adds a boolean param with true and false. Parameters:param - (undocumented) Returns:(undocumented) build public ParamMap[] build() Builds and returns all combinations of parameters specified by the param grid. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ParamMap (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ParamMap (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class ParamMap Object org.apache.spark.ml.param.ParamMap All Implemented Interfaces: java.io.Serializable public final class ParamMap extends Object implements scala.Serializable A param to value map. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ParamMap() Creates an empty param map. Method Summary Methods  Modifier and Type Method and Description <T> T apply(Param<T> param) Gets the value of the input param or its default value if it does not exist. boolean contains(Param<?> param) Checks whether a parameter is explicitly specified. ParamMap copy() Creates a copy of this param map. static ParamMap empty() Returns an empty param map. ParamMap filter(Params parent) Filters this param map for the given parent. <T> scala.Option<T> get(Param<T> param) Optionally returns the value associated with a param. <T> T getOrElse(Param<T> param, T default_) Returns the value associated with a param or a default value. <T> ParamMap put(Param<T> param, T value) Puts a (param, value) pair (overwrites if the input param exists). ParamMap put(ParamPair<?>... paramPairs) Puts a list of param pairs (overwrites if the input params exists). ParamMap put(scala.collection.Seq<ParamPair<?>> paramPairs) Puts a list of param pairs (overwrites if the input params exists). <T> scala.Option<T> remove(Param<T> param) Removes a key from this map and returns its value associated previously as an option. int size() Number of param pairs in this map. scala.collection.Seq<ParamPair<?>> toSeq() Converts this param map to a sequence of param pairs. String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail ParamMap public ParamMap() Creates an empty param map. Method Detail empty public static ParamMap empty() Returns an empty param map. Returns:(undocumented) put public ParamMap put(ParamPair<?>... paramPairs) Puts a list of param pairs (overwrites if the input params exists). Parameters:paramPairs - (undocumented) Returns:(undocumented) put public <T> ParamMap put(Param<T> param, T value) Puts a (param, value) pair (overwrites if the input param exists). Parameters:param - (undocumented)value - (undocumented) Returns:(undocumented) put public ParamMap put(scala.collection.Seq<ParamPair<?>> paramPairs) Puts a list of param pairs (overwrites if the input params exists). Parameters:paramPairs - (undocumented) Returns:(undocumented) get public <T> scala.Option<T> get(Param<T> param) Optionally returns the value associated with a param. Parameters:param - (undocumented) Returns:(undocumented) getOrElse public <T> T getOrElse(Param<T> param, T default_) Returns the value associated with a param or a default value. Parameters:param - (undocumented)default_ - (undocumented) Returns:(undocumented) apply public <T> T apply(Param<T> param) Gets the value of the input param or its default value if it does not exist. Raises a NoSuchElementException if there is no value associated with the input param. Parameters:param - (undocumented) Returns:(undocumented) contains public boolean contains(Param<?> param) Checks whether a parameter is explicitly specified. Parameters:param - (undocumented) Returns:(undocumented) remove public <T> scala.Option<T> remove(Param<T> param) Removes a key from this map and returns its value associated previously as an option. Parameters:param - (undocumented) Returns:(undocumented) filter public ParamMap filter(Params parent) Filters this param map for the given parent. Parameters:parent - (undocumented) Returns:(undocumented) copy public ParamMap copy() Creates a copy of this param map. Returns:(undocumented) toString public String toString() Overrides: toString in class Object toSeq public scala.collection.Seq<ParamPair<?>> toSeq() Converts this param map to a sequence of param pairs. Returns:(undocumented) size public int size() Number of param pairs in this map. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ParamPair (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ParamPair (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class ParamPair<T> Object org.apache.spark.ml.param.ParamPair<T> All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class ParamPair<T> extends Object implements scala.Product, scala.Serializable A param and its value. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ParamPair(Param<T> param, T value)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  Param<T> param()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  T value()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail ParamPair public ParamPair(Param<T> param, T value) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() param public Param<T> param() value public T value() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ParamValidators (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ParamValidators (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Class ParamValidators Object org.apache.spark.ml.param.ParamValidators public class ParamValidators extends Object :: DeveloperApi :: Factory methods for common validation functions for Param.isValid. The numerical methods only support Int, Long, Float, and Double. Constructor Summary Constructors  Constructor and Description ParamValidators()  Method Summary Methods  Modifier and Type Method and Description static <T> scala.Function1<Object,Object> arrayLengthGt(double lowerBound) Check that the array length is greater than lowerBound. static <T> scala.Function1<T,Object> gt(double lowerBound) Check if value > lowerBound static <T> scala.Function1<T,Object> gtEq(double lowerBound) Check if value >= lowerBound static <T> scala.Function1<T,Object> inArray(java.util.List<T> allowed) Check for value in an allowed set of values. static <T> scala.Function1<T,Object> inArray(Object allowed) Check for value in an allowed set of values. static <T> scala.Function1<T,Object> inRange(double lowerBound, double upperBound) Version of inRange() which uses inclusive be default: [lowerBound, upperBound] static <T> scala.Function1<T,Object> inRange(double lowerBound, double upperBound, boolean lowerInclusive, boolean upperInclusive) Check for value in range lowerBound to upperBound. static <T> scala.Function1<T,Object> lt(double upperBound) Check if value < upperBound static <T> scala.Function1<T,Object> ltEq(double upperBound) Check if value <= upperBound Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail ParamValidators public ParamValidators() Method Detail gt public static <T> scala.Function1<T,Object> gt(double lowerBound) Check if value > lowerBound gtEq public static <T> scala.Function1<T,Object> gtEq(double lowerBound) Check if value >= lowerBound lt public static <T> scala.Function1<T,Object> lt(double upperBound) Check if value < upperBound ltEq public static <T> scala.Function1<T,Object> ltEq(double upperBound) Check if value <= upperBound inRange public static <T> scala.Function1<T,Object> inRange(double lowerBound, double upperBound, boolean lowerInclusive, boolean upperInclusive) Check for value in range lowerBound to upperBound. Parameters:lowerInclusive - If true, check for value >= lowerBound. If false, check for value > lowerBound.upperInclusive - If true, check for value <= upperBound. If false, check for value < upperBound.lowerBound - (undocumented)upperBound - (undocumented) Returns:(undocumented) inRange public static <T> scala.Function1<T,Object> inRange(double lowerBound, double upperBound) Version of inRange() which uses inclusive be default: [lowerBound, upperBound] inArray public static <T> scala.Function1<T,Object> inArray(Object allowed) Check for value in an allowed set of values. inArray public static <T> scala.Function1<T,Object> inArray(java.util.List<T> allowed) Check for value in an allowed set of values. arrayLengthGt public static <T> scala.Function1<Object,Object> arrayLengthGt(double lowerBound) Check that the array length is greater than lowerBound. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Params (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Params (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.param Interface Params All Superinterfaces: Identifiable, java.io.Serializable All Known Implementing Classes: AFTSurvivalRegression, AFTSurvivalRegressionModel, ALS, ALSModel, Binarizer, BinaryClassificationEvaluator, BisectingKMeans, BisectingKMeansModel, Bucketizer, ChiSqSelector, ChiSqSelectorModel, ClassificationModel, Classifier, ColumnPruner, CountVectorizer, CountVectorizerModel, CrossValidator, CrossValidatorModel, DCT, DecisionTreeClassificationModel, DecisionTreeClassifier, DecisionTreeRegressionModel, DecisionTreeRegressor, DistributedLDAModel, ElementwiseProduct, Estimator, Evaluator, GaussianMixture, GaussianMixtureModel, GBTClassificationModel, GBTClassifier, GBTRegressionModel, GBTRegressor, GeneralizedLinearRegression, GeneralizedLinearRegressionModel, HashingTF, IDF, IDFModel, IndexToString, Interaction, IsotonicRegression, IsotonicRegressionModel, JavaParams, KMeans, KMeansModel, LDA, LDAModel, LinearRegression, LinearRegressionModel, LocalLDAModel, LogisticRegression, LogisticRegressionModel, MaxAbsScaler, MaxAbsScalerModel, MinMaxScaler, MinMaxScalerModel, Model, MulticlassClassificationEvaluator, MultilayerPerceptronClassificationModel, MultilayerPerceptronClassifier, NaiveBayes, NaiveBayesModel, NGram, Normalizer, OneHotEncoder, OneVsRest, OneVsRestModel, PCA, PCAModel, Pipeline, PipelineModel, PipelineStage, PolynomialExpansion, PredictionModel, Predictor, ProbabilisticClassificationModel, ProbabilisticClassifier, QuantileDiscretizer, RandomForestClassificationModel, RandomForestClassifier, RandomForestRegressionModel, RandomForestRegressor, RegexTokenizer, RegressionEvaluator, RegressionModel, RFormula, RFormulaModel, SQLTransformer, StandardScaler, StandardScalerModel, StopWordsRemover, StringIndexer, StringIndexerModel, Tokenizer, TrainValidationSplit, TrainValidationSplitModel, Transformer, UnaryTransformer, VectorAssembler, VectorAttributeRewriter, VectorIndexer, VectorIndexerModel, VectorSlicer, Word2Vec, Word2VecModel public interface Params extends Identifiable, scala.Serializable :: DeveloperApi :: Trait for components that take parameters. This also provides an internal param map to store parameter values attached to the instance. Method Summary Methods  Modifier and Type Method and Description Params clear(Param<?> param) Clears the user-supplied value for the input param. Params copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. <T extends Params> T copyValues(T to, ParamMap extra) Copies param values from this instance to another instance for params shared by them. <T extends Params> T defaultCopy(ParamMap extra) Default implementation of copy with extra params. ParamMap defaultParamMap() Internal param map for default values. String explainParam(Param<?> param) Explains a param. String explainParams() Explains all params of this instance. ParamMap extractParamMap() extractParamMap with no extra values. ParamMap extractParamMap(ParamMap extra) Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values less than user-supplied values less than extra. <T> scala.Option<T> get(Param<T> param) Optionally returns the user-supplied value of a param. <T> scala.Option<T> getDefault(Param<T> param) Gets the default value of a parameter. <T> T getOrDefault(Param<T> param) Gets the value of a param in the embedded param map or its default value. Param<Object> getParam(String paramName) Gets a param by its name. <T> boolean hasDefault(Param<T> param) Tests whether the input param has a default value set. boolean hasParam(String paramName) Tests whether this instance contains a param with a given name. boolean isDefined(Param<?> param) Checks whether a param is explicitly set or has a default value. boolean isSet(Param<?> param) Checks whether a param is explicitly set. ParamMap paramMap() Internal param map for user-supplied values. Param<?>[] params() Returns all params sorted by their names. <T> Params set(Param<T> param, T value) Sets a parameter in the embedded param map. Params set(ParamPair<?> paramPair) Sets a parameter in the embedded param map. Params set(String param, Object value) Sets a parameter (by name) in the embedded param map. <T> Params setDefault(Param<T> param, T value) Sets a default value for a param. Params setDefault(scala.collection.Seq<ParamPair<?>> paramPairs) Sets default values for a list of params. void shouldOwn(Param<?> param) Validates that the input param belongs to this instance. void validateParams() Deprecated.  Will be removed in 2.1.0. All the checks should be merged into transformSchema Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Method Detail params Param<?>[] params() Returns all params sorted by their names. The default implementation uses Java reflection to list all public methods that have no arguments and return Param. Note: Developer should not use this method in constructor because we cannot guarantee that this variable gets initialized before other params. Returns:(undocumented) validateParams void validateParams() Deprecated. Will be removed in 2.1.0. All the checks should be merged into transformSchema Validates parameter values stored internally. Raise an exception if any parameter value is invalid. This only needs to check for interactions between parameters. Parameter value checks which do not depend on other parameters are handled by Param.validate(). This method does not handle input/output column parameters; those are checked during schema validation. explainParam String explainParam(Param<?> param) Explains a param. Parameters:param - input param, must belong to this instance. Returns:a string that contains the input param name, doc, and optionally its default value and the user-supplied value explainParams String explainParams() Explains all params of this instance. See explainParam(). Returns:(undocumented) isSet boolean isSet(Param<?> param) Checks whether a param is explicitly set. isDefined boolean isDefined(Param<?> param) Checks whether a param is explicitly set or has a default value. hasParam boolean hasParam(String paramName) Tests whether this instance contains a param with a given name. getParam Param<Object> getParam(String paramName) Gets a param by its name. set <T> Params set(Param<T> param, T value) Sets a parameter in the embedded param map. Parameters:param - (undocumented)value - (undocumented) Returns:(undocumented) set Params set(String param, Object value) Sets a parameter (by name) in the embedded param map. Parameters:param - (undocumented)value - (undocumented) Returns:(undocumented) set Params set(ParamPair<?> paramPair) Sets a parameter in the embedded param map. Parameters:paramPair - (undocumented) Returns:(undocumented) get <T> scala.Option<T> get(Param<T> param) Optionally returns the user-supplied value of a param. Parameters:param - (undocumented) Returns:(undocumented) clear Params clear(Param<?> param) Clears the user-supplied value for the input param. Parameters:param - (undocumented) Returns:(undocumented) getOrDefault <T> T getOrDefault(Param<T> param) Gets the value of a param in the embedded param map or its default value. Throws an exception if neither is set. Parameters:param - (undocumented) Returns:(undocumented) setDefault <T> Params setDefault(Param<T> param, T value) Sets a default value for a param. Parameters:param - param to set the default value. Make sure that this param is initialized before this method gets called.value - the default value Returns:(undocumented) setDefault Params setDefault(scala.collection.Seq<ParamPair<?>> paramPairs) Sets default values for a list of params. Note: Java developers should use the single-parameter setDefault. Annotating this with varargs can cause compilation failures due to a Scala compiler bug. See SPARK-9268. Parameters:paramPairs - a list of param pairs that specify params and their default values to set respectively. Make sure that the params are initialized before this method gets called. Returns:(undocumented) getDefault <T> scala.Option<T> getDefault(Param<T> param) Gets the default value of a parameter. Parameters:param - (undocumented) Returns:(undocumented) hasDefault <T> boolean hasDefault(Param<T> param) Tests whether the input param has a default value set. Parameters:param - (undocumented) Returns:(undocumented) copy Params copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Parameters:extra - (undocumented) Returns:(undocumented) defaultCopy <T extends Params> T defaultCopy(ParamMap extra) Default implementation of copy with extra params. It tries to create a new instance with the same UID. Then it copies the embedded and extra parameters over and returns the new instance. Parameters:extra - (undocumented) Returns:(undocumented) extractParamMap ParamMap extractParamMap(ParamMap extra) Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values less than user-supplied values less than extra. Parameters:extra - (undocumented) Returns:(undocumented) extractParamMap ParamMap extractParamMap() extractParamMap with no extra values. Returns:(undocumented) paramMap ParamMap paramMap() Internal param map for user-supplied values. defaultParamMap ParamMap defaultParamMap() Internal param map for default values. shouldOwn void shouldOwn(Param<?> param) Validates that the input param belongs to this instance. copyValues <T extends Params> T copyValues(T to, ParamMap extra) Copies param values from this instance to another instance for params shared by them. This handles default Params and explicitly set Params separately. Default Params are copied from and to defaultParamMap, and explicitly set Params are copied from and to paramMap. Warning: This implicitly assumes that this Params instance and the target instance share the same set of default Params. Parameters:to - the target instance, which should work with the same set of default Params as this source instanceextra - extra params to be copied to the target's paramMap Returns:the target instance with param values copied Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PartialResult (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PartialResult (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.partial Class PartialResult<R> Object org.apache.spark.partial.PartialResult<R> public class PartialResult<R> extends Object Constructor Summary Constructors  Constructor and Description PartialResult(R initialVal, boolean isFinal)  Method Summary Methods  Modifier and Type Method and Description R getFinalValue() Blocking method to wait for and return the final value. R initialValue()  boolean isInitialValueFinal()  <T> PartialResult<T> map(scala.Function1<R,T> f) Transform this PartialResult into a PartialResult of type T. PartialResult<R> onComplete(scala.Function1<R,scala.runtime.BoxedUnit> handler) Set a handler to be called when this PartialResult completes. void onFail(scala.Function1<Exception,scala.runtime.BoxedUnit> handler) Set a handler to be called if this PartialResult's job fails. String toString()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail PartialResult public PartialResult(R initialVal, boolean isFinal) Method Detail initialValue public R initialValue() isInitialValueFinal public boolean isInitialValueFinal() getFinalValue public R getFinalValue() Blocking method to wait for and return the final value. Returns:(undocumented) onComplete public PartialResult<R> onComplete(scala.Function1<R,scala.runtime.BoxedUnit> handler) Set a handler to be called when this PartialResult completes. Only one completion handler is supported per PartialResult. Parameters:handler - (undocumented) Returns:(undocumented) onFail public void onFail(scala.Function1<Exception,scala.runtime.BoxedUnit> handler) Set a handler to be called if this PartialResult's job fails. Only one failure handler is supported per PartialResult. Parameters:handler - (undocumented) map public <T> PartialResult<T> map(scala.Function1<R,T> f) Transform this PartialResult into a PartialResult of type T. Parameters:f - (undocumented) Returns:(undocumented) toString public String toString() Overrides: toString in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Partition (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Partition (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Interface Partition All Superinterfaces: java.io.Serializable public interface Partition extends scala.Serializable An identifier for a partition in an RDD. Method Summary Methods  Modifier and Type Method and Description boolean equals(Object other)  int hashCode()  int index() Get the partition's index within its parent RDD Method Detail index int index() Get the partition's index within its parent RDD Returns:(undocumented) hashCode int hashCode() Overrides: hashCode in class Object equals boolean equals(Object other) Overrides: equals in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PartitionCoalescer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PartitionCoalescer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Interface PartitionCoalescer All Known Implementing Classes: DefaultPartitionCoalescer public interface PartitionCoalescer ::DeveloperApi:: A PartitionCoalescer defines how to coalesce the partitions of a given RDD. Method Summary Methods  Modifier and Type Method and Description PartitionGroup[] coalesce(int maxPartitions, RDD<?> parent) Coalesce the partitions of the given RDD. Method Detail coalesce PartitionGroup[] coalesce(int maxPartitions, RDD<?> parent) Coalesce the partitions of the given RDD. Parameters:maxPartitions - the maximum number of partitions to have after coalescingparent - the parent RDD whose partitions to coalesce Returns:an array of PartitionGroups, where each element is itself an array of Partitions and represents a partition after coalescing is performed. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PartitionGroup (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PartitionGroup (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class PartitionGroup Object org.apache.spark.rdd.PartitionGroup public class PartitionGroup extends Object ::DeveloperApi:: A group of Partitions param: prefLoc preferred location for the partition group Constructor Summary Constructors  Constructor and Description PartitionGroup(scala.Option<String> prefLoc)  Method Summary Methods  Modifier and Type Method and Description int numPartitions()  scala.collection.mutable.ArrayBuffer<Partition> partitions()  scala.Option<String> prefLoc()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PartitionGroup public PartitionGroup(scala.Option<String> prefLoc) Method Detail prefLoc public scala.Option<String> prefLoc() partitions public scala.collection.mutable.ArrayBuffer<Partition> partitions() numPartitions public int numPartitions() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PartitionPruningRDD (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PartitionPruningRDD (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.rdd Class PartitionPruningRDD<T> Object org.apache.spark.rdd.RDD<T> org.apache.spark.rdd.PartitionPruningRDD<T> All Implemented Interfaces: java.io.Serializable public class PartitionPruningRDD<T> extends RDD<T> :: DeveloperApi :: A RDD used to prune RDD partitions/partitions so we can avoid launching tasks on all partitions. An example use case: If we know the RDD is partitioned by range, and the execution DAG has a filter on the key, we can avoid launching tasks on partitions that don't have the range covering the key. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PartitionPruningRDD(RDD<T> prev, scala.Function1<Object,Object> partitionFilterFunc, scala.reflect.ClassTag<T> evidence$1)  Method Summary Methods  Modifier and Type Method and Description static RDD<T> $plus$plus(RDD<T> other)  static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29)  static RDD<T> cache()  static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5)  static void checkpoint()  static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord)  static boolean coalesce$default$2()  static scala.Option<PartitionCoalescer> coalesce$default$3()  static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer)  static Object collect()  static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28)  scala.collection.Iterator<T> compute(Partition split, TaskContext context) :: DeveloperApi :: Implemented by subclasses to compute a given partition. static SparkContext context()  static long count()  static PartialResult<BoundedDouble> countApprox(long timeout, double confidence)  static double countApprox$default$2()  static long countApproxDistinct(double relativeSD)  static long countApproxDistinct(int p, int sp)  static double countApproxDistinct$default$1()  static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord)  static scala.math.Ordering<T> countByValue$default$1()  static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord)  static double countByValueApprox$default$2()  static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence)  static <T> PartitionPruningRDD<T> create(RDD<T> rdd, scala.Function1<Object,Object> partitionFilterFunc) Create a PartitionPruningRDD. static scala.collection.Seq<Dependency<?>> dependencies()  static RDD<T> distinct()  static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> distinct$default$2(int numPartitions)  static RDD<T> filter(scala.Function1<T,Object> f)  static T first()  static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4)  static T fold(T zeroValue, scala.Function2<T,T,T> op)  static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f)  static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f)  static scala.Option<String> getCheckpointFile()  static int getNumPartitions()  static StorageLevel getStorageLevel()  static RDD<Object> glom()  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt)  static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord)  static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p)  static int id()  static RDD<T> intersection(RDD<T> other)  static RDD<T> intersection(RDD<T> other, int numPartitions)  static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner)  static boolean isCheckpointed()  static boolean isEmpty()  static scala.collection.Iterator<T> iterator(Partition split, TaskContext context)  static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f)  static RDD<T> localCheckpoint()  static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3)  static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6)  static <U> boolean mapPartitions$default$2()  static <U> boolean mapPartitionsInternal$default$2()  static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8)  static <U> boolean mapPartitionsWithIndex$default$2()  static T max(scala.math.Ordering<T> ord)  static T min(scala.math.Ordering<T> ord)  static void name_$eq(String x$1)  static String name()  static scala.Option<Partitioner> partitioner()  static Partition[] partitions()  static RDD<T> persist()  static RDD<T> persist(StorageLevel newLevel)  static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding)  static RDD<String> pipe(String command)  static RDD<String> pipe(String command, scala.collection.Map<String,String> env)  static scala.collection.Map<String,String> pipe$default$2()  static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3()  static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4()  static boolean pipe$default$5()  static int pipe$default$6()  static String pipe$default$7()  static scala.collection.Seq<String> preferredLocations(Partition split)  static RDD<T>[] randomSplit(double[] weights, long seed)  static long randomSplit$default$2()  static T reduce(scala.Function2<T,T,T> f)  static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> repartition$default$2(int numPartitions)  static RDD<T> sample(boolean withReplacement, double fraction, long seed)  static long sample$default$3()  static void saveAsObjectFile(String path)  static void saveAsTextFile(String path)  static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec)  static RDD<T> setName(String _name)  static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag)  static <K> boolean sortBy$default$2()  static <K> int sortBy$default$3()  static SparkContext sparkContext()  static RDD<T> subtract(RDD<T> other)  static RDD<T> subtract(RDD<T> other, int numPartitions)  static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord)  static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p)  static Object take(int num)  static Object takeOrdered(int num, scala.math.Ordering<T> ord)  static Object takeSample(boolean withReplacement, int num, long seed)  static long takeSample$default$3()  static String toDebugString()  static JavaRDD<T> toJavaRDD()  static scala.collection.Iterator<T> toLocalIterator()  static Object top(int num, scala.math.Ordering<T> ord)  static String toString()  static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30)  static <U> int treeAggregate$default$4(U zeroValue)  static T treeReduce(scala.Function2<T,T,T> f, int depth)  static int treeReduce$default$2()  static RDD<T> union(RDD<T> other)  static RDD<T> unpersist(boolean blocking)  static boolean unpersist$default$1()  static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11)  static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16)  static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23)  static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27)  static RDD<scala.Tuple2<T,Object>> zipWithIndex()  static RDD<scala.Tuple2<T,Object>> zipWithUniqueId()  Methods inherited from class org.apache.spark.rdd.RDD aggregate, cache, cartesian, checkpoint, coalesce, collect, collect, context, count, countApprox, countApproxDistinct, countApproxDistinct, countByValue, countByValueApprox, dependencies, distinct, distinct, doubleRDDToDoubleRDDFunctions, filter, first, flatMap, fold, foreach, foreachPartition, getCheckpointFile, getNumPartitions, getStorageLevel, glom, groupBy, groupBy, groupBy, id, intersection, intersection, intersection, isCheckpointed, isEmpty, iterator, keyBy, localCheckpoint, map, mapPartitions, mapPartitionsWithIndex, max, min, name, numericRDDToDoubleRDDFunctions, partitioner, partitions, persist, persist, pipe, pipe, pipe, preferredLocations, randomSplit, rddToAsyncRDDActions, rddToOrderedRDDFunctions, rddToPairRDDFunctions, rddToSequenceFileRDDFunctions, reduce, repartition, sample, saveAsObjectFile, saveAsTextFile, saveAsTextFile, setName, sortBy, sparkContext, subtract, subtract, subtract, take, takeOrdered, takeSample, toDebugString, toJavaRDD, toLocalIterator, top, toString, treeAggregate, treeReduce, union, unpersist, zip, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipPartitions, zipWithIndex, zipWithUniqueId Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Constructor Detail PartitionPruningRDD public PartitionPruningRDD(RDD<T> prev, scala.Function1<Object,Object> partitionFilterFunc, scala.reflect.ClassTag<T> evidence$1) Method Detail create public static <T> PartitionPruningRDD<T> create(RDD<T> rdd, scala.Function1<Object,Object> partitionFilterFunc) Create a PartitionPruningRDD. This function can be used to create the PartitionPruningRDD when its type T is not known at compile time. Parameters:rdd - (undocumented)partitionFilterFunc - (undocumented) Returns:(undocumented) partitioner public static scala.Option<Partitioner> partitioner() sparkContext public static SparkContext sparkContext() id public static int id() name public static String name() name_$eq public static void name_$eq(String x$1) setName public static RDD<T> setName(String _name) persist public static RDD<T> persist(StorageLevel newLevel) persist public static RDD<T> persist() cache public static RDD<T> cache() unpersist public static RDD<T> unpersist(boolean blocking) getStorageLevel public static StorageLevel getStorageLevel() dependencies public static final scala.collection.Seq<Dependency<?>> dependencies() partitions public static final Partition[] partitions() getNumPartitions public static final int getNumPartitions() preferredLocations public static final scala.collection.Seq<String> preferredLocations(Partition split) iterator public static final scala.collection.Iterator<T> iterator(Partition split, TaskContext context) map public static <U> RDD<U> map(scala.Function1<T,U> f, scala.reflect.ClassTag<U> evidence$3) flatMap public static <U> RDD<U> flatMap(scala.Function1<T,scala.collection.TraversableOnce<U>> f, scala.reflect.ClassTag<U> evidence$4) filter public static RDD<T> filter(scala.Function1<T,Object> f) distinct public static RDD<T> distinct(int numPartitions, scala.math.Ordering<T> ord) distinct public static RDD<T> distinct() repartition public static RDD<T> repartition(int numPartitions, scala.math.Ordering<T> ord) coalesce public static RDD<T> coalesce(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer, scala.math.Ordering<T> ord) sample public static RDD<T> sample(boolean withReplacement, double fraction, long seed) randomSplit public static RDD<T>[] randomSplit(double[] weights, long seed) takeSample public static Object takeSample(boolean withReplacement, int num, long seed) union public static RDD<T> union(RDD<T> other) $plus$plus public static RDD<T> $plus$plus(RDD<T> other) sortBy public static <K> RDD<T> sortBy(scala.Function1<T,K> f, boolean ascending, int numPartitions, scala.math.Ordering<K> ord, scala.reflect.ClassTag<K> ctag) intersection public static RDD<T> intersection(RDD<T> other) intersection public static RDD<T> intersection(RDD<T> other, Partitioner partitioner, scala.math.Ordering<T> ord) intersection public static RDD<T> intersection(RDD<T> other, int numPartitions) glom public static RDD<Object> glom() cartesian public static <U> RDD<scala.Tuple2<T,U>> cartesian(RDD<U> other, scala.reflect.ClassTag<U> evidence$5) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, int numPartitions, scala.reflect.ClassTag<K> kt) groupBy public static <K> RDD<scala.Tuple2<K,scala.collection.Iterable<T>>> groupBy(scala.Function1<T,K> f, Partitioner p, scala.reflect.ClassTag<K> kt, scala.math.Ordering<K> ord) pipe public static RDD<String> pipe(String command) pipe public static RDD<String> pipe(String command, scala.collection.Map<String,String> env) pipe public static RDD<String> pipe(scala.collection.Seq<String> command, scala.collection.Map<String,String> env, scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printPipeContext, scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> printRDDElement, boolean separateWorkingDir, int bufferSize, String encoding) mapPartitions public static <U> RDD<U> mapPartitions(scala.Function1<scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$6) mapPartitionsWithIndex public static <U> RDD<U> mapPartitionsWithIndex(scala.Function2<Object,scala.collection.Iterator<T>,scala.collection.Iterator<U>> f, boolean preservesPartitioning, scala.reflect.ClassTag<U> evidence$8) zip public static <U> RDD<scala.Tuple2<T,U>> zip(RDD<U> other, scala.reflect.ClassTag<U> evidence$9) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, boolean preservesPartitioning, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$10, scala.reflect.ClassTag<V> evidence$11) zipPartitions public static <B,V> RDD<V> zipPartitions(RDD<B> rdd2, scala.Function2<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$12, scala.reflect.ClassTag<V> evidence$13) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, boolean preservesPartitioning, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$14, scala.reflect.ClassTag<C> evidence$15, scala.reflect.ClassTag<V> evidence$16) zipPartitions public static <B,C,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, scala.Function3<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$17, scala.reflect.ClassTag<C> evidence$18, scala.reflect.ClassTag<V> evidence$19) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, boolean preservesPartitioning, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$20, scala.reflect.ClassTag<C> evidence$21, scala.reflect.ClassTag<D> evidence$22, scala.reflect.ClassTag<V> evidence$23) zipPartitions public static <B,C,D,V> RDD<V> zipPartitions(RDD<B> rdd2, RDD<C> rdd3, RDD<D> rdd4, scala.Function4<scala.collection.Iterator<T>,scala.collection.Iterator<B>,scala.collection.Iterator<C>,scala.collection.Iterator<D>,scala.collection.Iterator<V>> f, scala.reflect.ClassTag<B> evidence$24, scala.reflect.ClassTag<C> evidence$25, scala.reflect.ClassTag<D> evidence$26, scala.reflect.ClassTag<V> evidence$27) foreach public static void foreach(scala.Function1<T,scala.runtime.BoxedUnit> f) foreachPartition public static void foreachPartition(scala.Function1<scala.collection.Iterator<T>,scala.runtime.BoxedUnit> f) collect public static Object collect() toLocalIterator public static scala.collection.Iterator<T> toLocalIterator() collect public static <U> RDD<U> collect(scala.PartialFunction<T,U> f, scala.reflect.ClassTag<U> evidence$28) subtract public static RDD<T> subtract(RDD<T> other) subtract public static RDD<T> subtract(RDD<T> other, int numPartitions) subtract public static RDD<T> subtract(RDD<T> other, Partitioner p, scala.math.Ordering<T> ord) reduce public static T reduce(scala.Function2<T,T,T> f) treeReduce public static T treeReduce(scala.Function2<T,T,T> f, int depth) fold public static T fold(T zeroValue, scala.Function2<T,T,T> op) aggregate public static <U> U aggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, scala.reflect.ClassTag<U> evidence$29) treeAggregate public static <U> U treeAggregate(U zeroValue, scala.Function2<U,T,U> seqOp, scala.Function2<U,U,U> combOp, int depth, scala.reflect.ClassTag<U> evidence$30) count public static long count() countApprox public static PartialResult<BoundedDouble> countApprox(long timeout, double confidence) countByValue public static scala.collection.Map<T,Object> countByValue(scala.math.Ordering<T> ord) countByValueApprox public static PartialResult<scala.collection.Map<T,BoundedDouble>> countByValueApprox(long timeout, double confidence, scala.math.Ordering<T> ord) countApproxDistinct public static long countApproxDistinct(int p, int sp) countApproxDistinct public static long countApproxDistinct(double relativeSD) zipWithIndex public static RDD<scala.Tuple2<T,Object>> zipWithIndex() zipWithUniqueId public static RDD<scala.Tuple2<T,Object>> zipWithUniqueId() take public static Object take(int num) first public static T first() top public static Object top(int num, scala.math.Ordering<T> ord) takeOrdered public static Object takeOrdered(int num, scala.math.Ordering<T> ord) max public static T max(scala.math.Ordering<T> ord) min public static T min(scala.math.Ordering<T> ord) isEmpty public static boolean isEmpty() saveAsTextFile public static void saveAsTextFile(String path) saveAsTextFile public static void saveAsTextFile(String path, Class<? extends org.apache.hadoop.io.compress.CompressionCodec> codec) saveAsObjectFile public static void saveAsObjectFile(String path) keyBy public static <K> RDD<scala.Tuple2<K,T>> keyBy(scala.Function1<T,K> f) checkpoint public static void checkpoint() localCheckpoint public static RDD<T> localCheckpoint() isCheckpointed public static boolean isCheckpointed() getCheckpointFile public static scala.Option<String> getCheckpointFile() context public static SparkContext context() toDebugString public static String toDebugString() toString public static String toString() toJavaRDD public static JavaRDD<T> toJavaRDD() sample$default$3 public static long sample$default$3() mapPartitionsWithIndex$default$2 public static <U> boolean mapPartitionsWithIndex$default$2() unpersist$default$1 public static boolean unpersist$default$1() distinct$default$2 public static scala.math.Ordering<T> distinct$default$2(int numPartitions) coalesce$default$2 public static boolean coalesce$default$2() coalesce$default$3 public static scala.Option<PartitionCoalescer> coalesce$default$3() coalesce$default$4 public static scala.math.Ordering<T> coalesce$default$4(int numPartitions, boolean shuffle, scala.Option<PartitionCoalescer> partitionCoalescer) repartition$default$2 public static scala.math.Ordering<T> repartition$default$2(int numPartitions) subtract$default$3 public static scala.math.Ordering<T> subtract$default$3(RDD<T> other, Partitioner p) intersection$default$3 public static scala.math.Ordering<T> intersection$default$3(RDD<T> other, Partitioner partitioner) randomSplit$default$2 public static long randomSplit$default$2() sortBy$default$2 public static <K> boolean sortBy$default$2() sortBy$default$3 public static <K> int sortBy$default$3() mapPartitions$default$2 public static <U> boolean mapPartitions$default$2() groupBy$default$4 public static <K> scala.runtime.Null$ groupBy$default$4(scala.Function1<T,K> f, Partitioner p) pipe$default$2 public static scala.collection.Map<String,String> pipe$default$2() pipe$default$3 public static scala.Function1<scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$3() pipe$default$4 public static scala.Function2<T,scala.Function1<String,scala.runtime.BoxedUnit>,scala.runtime.BoxedUnit> pipe$default$4() pipe$default$5 public static boolean pipe$default$5() pipe$default$6 public static int pipe$default$6() pipe$default$7 public static String pipe$default$7() treeReduce$default$2 public static int treeReduce$default$2() treeAggregate$default$4 public static <U> int treeAggregate$default$4(U zeroValue) countApprox$default$2 public static double countApprox$default$2() countByValue$default$1 public static scala.math.Ordering<T> countByValue$default$1() countByValueApprox$default$2 public static double countByValueApprox$default$2() countByValueApprox$default$3 public static scala.math.Ordering<T> countByValueApprox$default$3(long timeout, double confidence) takeSample$default$3 public static long takeSample$default$3() countApproxDistinct$default$1 public static double countApproxDistinct$default$1() mapPartitionsInternal$default$2 public static <U> boolean mapPartitionsInternal$default$2() compute public scala.collection.Iterator<T> compute(Partition split, TaskContext context) Description copied from class: RDD :: DeveloperApi :: Implemented by subclasses to compute a given partition. Specified by: compute in class RDD<T> Parameters:split - (undocumented)context - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PartitionStrategy.CanonicalRandomVertexCut$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PartitionStrategy.CanonicalRandomVertexCut$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class PartitionStrategy.CanonicalRandomVertexCut$ Object org.apache.spark.graphx.PartitionStrategy.CanonicalRandomVertexCut$ All Implemented Interfaces: java.io.Serializable, PartitionStrategy, scala.Equals, scala.Product Enclosing interface: PartitionStrategy public static class PartitionStrategy.CanonicalRandomVertexCut$ extends Object implements PartitionStrategy, scala.Product, scala.Serializable Assigns edges to partitions by hashing the source and destination vertex IDs in a canonical direction, resulting in a random vertex cut that colocates all edges between two vertices, regardless of direction. See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface org.apache.spark.graphx.PartitionStrategy PartitionStrategy.CanonicalRandomVertexCut$, PartitionStrategy.EdgePartition1D$, PartitionStrategy.EdgePartition2D$, PartitionStrategy.RandomVertexCut$ Field Summary Fields  Modifier and Type Field and Description static PartitionStrategy.CanonicalRandomVertexCut$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description PartitionStrategy.CanonicalRandomVertexCut$()  Method Summary Methods  Modifier and Type Method and Description int getPartition(long src, long dst, int numParts) Returns the partition number for a given edge. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final PartitionStrategy.CanonicalRandomVertexCut$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail PartitionStrategy.CanonicalRandomVertexCut$ public PartitionStrategy.CanonicalRandomVertexCut$() Method Detail getPartition public int getPartition(long src, long dst, int numParts) Description copied from interface: PartitionStrategy Returns the partition number for a given edge. Specified by: getPartition in interface PartitionStrategy Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PartitionStrategy.EdgePartition1D$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PartitionStrategy.EdgePartition1D$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class PartitionStrategy.EdgePartition1D$ Object org.apache.spark.graphx.PartitionStrategy.EdgePartition1D$ All Implemented Interfaces: java.io.Serializable, PartitionStrategy, scala.Equals, scala.Product Enclosing interface: PartitionStrategy public static class PartitionStrategy.EdgePartition1D$ extends Object implements PartitionStrategy, scala.Product, scala.Serializable Assigns edges to partitions using only the source vertex ID, colocating edges with the same source. See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface org.apache.spark.graphx.PartitionStrategy PartitionStrategy.CanonicalRandomVertexCut$, PartitionStrategy.EdgePartition1D$, PartitionStrategy.EdgePartition2D$, PartitionStrategy.RandomVertexCut$ Field Summary Fields  Modifier and Type Field and Description static PartitionStrategy.EdgePartition1D$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description PartitionStrategy.EdgePartition1D$()  Method Summary Methods  Modifier and Type Method and Description int getPartition(long src, long dst, int numParts) Returns the partition number for a given edge. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final PartitionStrategy.EdgePartition1D$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail PartitionStrategy.EdgePartition1D$ public PartitionStrategy.EdgePartition1D$() Method Detail getPartition public int getPartition(long src, long dst, int numParts) Description copied from interface: PartitionStrategy Returns the partition number for a given edge. Specified by: getPartition in interface PartitionStrategy Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PartitionStrategy.EdgePartition2D$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PartitionStrategy.EdgePartition2D$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class PartitionStrategy.EdgePartition2D$ Object org.apache.spark.graphx.PartitionStrategy.EdgePartition2D$ All Implemented Interfaces: java.io.Serializable, PartitionStrategy, scala.Equals, scala.Product Enclosing interface: PartitionStrategy public static class PartitionStrategy.EdgePartition2D$ extends Object implements PartitionStrategy, scala.Product, scala.Serializable Assigns edges to partitions using a 2D partitioning of the sparse edge adjacency matrix, guaranteeing a 2 * sqrt(numParts) bound on vertex replication. Suppose we have a graph with 12 vertices that we want to partition over 9 machines. We can use the following sparse matrix representation: __________________________________ v0 | P0 * | P1 | P2 * | v1 | **** | * | | v2 | ******* | ** | **** | v3 | ***** | * * | * | ---------------------------------- v4 | P3 * | P4 *** | P5 ** * | v5 | * * | * | | v6 | * | ** | **** | v7 | * * * | * * | * | ---------------------------------- v8 | P6 * | P7 * | P8 * *| v9 | * | * * | | v10 | * | ** | * * | v11 | * <-E | *** | ** | ---------------------------------- The edge denoted by E connects v11 with v1 and is assigned to processor P6. To get the processor number we divide the matrix into sqrt(numParts) by sqrt(numParts) blocks. Notice that edges adjacent to v11 can only be in the first column of blocks (P0, P3, P6) or the last row of blocks (P6, P7, P8). As a consequence we can guarantee that v11 will need to be replicated to at most 2 * sqrt(numParts) machines. Notice that P0 has many edges and as a consequence this partitioning would lead to poor work balance. To improve balance we first multiply each vertex id by a large prime to shuffle the vertex locations. When the number of partitions requested is not a perfect square we use a slightly different method where the last column can have a different number of rows than the others while still maintaining the same size per block. See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface org.apache.spark.graphx.PartitionStrategy PartitionStrategy.CanonicalRandomVertexCut$, PartitionStrategy.EdgePartition1D$, PartitionStrategy.EdgePartition2D$, PartitionStrategy.RandomVertexCut$ Field Summary Fields  Modifier and Type Field and Description static PartitionStrategy.EdgePartition2D$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description PartitionStrategy.EdgePartition2D$()  Method Summary Methods  Modifier and Type Method and Description int getPartition(long src, long dst, int numParts) Returns the partition number for a given edge. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final PartitionStrategy.EdgePartition2D$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail PartitionStrategy.EdgePartition2D$ public PartitionStrategy.EdgePartition2D$() Method Detail getPartition public int getPartition(long src, long dst, int numParts) Description copied from interface: PartitionStrategy Returns the partition number for a given edge. Specified by: getPartition in interface PartitionStrategy Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PartitionStrategy.RandomVertexCut$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PartitionStrategy.RandomVertexCut$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class PartitionStrategy.RandomVertexCut$ Object org.apache.spark.graphx.PartitionStrategy.RandomVertexCut$ All Implemented Interfaces: java.io.Serializable, PartitionStrategy, scala.Equals, scala.Product Enclosing interface: PartitionStrategy public static class PartitionStrategy.RandomVertexCut$ extends Object implements PartitionStrategy, scala.Product, scala.Serializable Assigns edges to partitions by hashing the source and destination vertex IDs, resulting in a random vertex cut that colocates all same-direction edges between two vertices. See Also:Serialized Form Nested Class Summary Nested classes/interfaces inherited from interface org.apache.spark.graphx.PartitionStrategy PartitionStrategy.CanonicalRandomVertexCut$, PartitionStrategy.EdgePartition1D$, PartitionStrategy.EdgePartition2D$, PartitionStrategy.RandomVertexCut$ Field Summary Fields  Modifier and Type Field and Description static PartitionStrategy.RandomVertexCut$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description PartitionStrategy.RandomVertexCut$()  Method Summary Methods  Modifier and Type Method and Description int getPartition(long src, long dst, int numParts) Returns the partition number for a given edge. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Field Detail MODULE$ public static final PartitionStrategy.RandomVertexCut$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail PartitionStrategy.RandomVertexCut$ public PartitionStrategy.RandomVertexCut$() Method Detail getPartition public int getPartition(long src, long dst, int numParts) Description copied from interface: PartitionStrategy Returns the partition number for a given edge. Specified by: getPartition in interface PartitionStrategy Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PartitionStrategy (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PartitionStrategy (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Interface PartitionStrategy All Superinterfaces: java.io.Serializable All Known Implementing Classes: PartitionStrategy.CanonicalRandomVertexCut$, PartitionStrategy.EdgePartition1D$, PartitionStrategy.EdgePartition2D$, PartitionStrategy.RandomVertexCut$ public interface PartitionStrategy extends scala.Serializable Represents the way edges are assigned to edge partitions based on their source and destination vertex IDs. Nested Class Summary Nested Classes  Modifier and Type Interface and Description static class  PartitionStrategy.CanonicalRandomVertexCut$ Assigns edges to partitions by hashing the source and destination vertex IDs in a canonical direction, resulting in a random vertex cut that colocates all edges between two vertices, regardless of direction. static class  PartitionStrategy.EdgePartition1D$ Assigns edges to partitions using only the source vertex ID, colocating edges with the same source. static class  PartitionStrategy.EdgePartition2D$ Assigns edges to partitions using a 2D partitioning of the sparse edge adjacency matrix, guaranteeing a 2 * sqrt(numParts) bound on vertex replication. static class  PartitionStrategy.RandomVertexCut$ Assigns edges to partitions by hashing the source and destination vertex IDs, resulting in a random vertex cut that colocates all same-direction edges between two vertices. Method Summary Methods  Modifier and Type Method and Description int getPartition(long src, long dst, int numParts) Returns the partition number for a given edge. Method Detail getPartition int getPartition(long src, long dst, int numParts) Returns the partition number for a given edge. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Partitioner (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Partitioner (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark Class Partitioner Object org.apache.spark.Partitioner All Implemented Interfaces: java.io.Serializable Direct Known Subclasses: HashPartitioner, RangePartitioner public abstract class Partitioner extends Object implements scala.Serializable An object that defines how the elements in a key-value pair RDD are partitioned by key. Maps each key to a partition ID, from 0 to numPartitions - 1. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Partitioner()  Method Summary Methods  Modifier and Type Method and Description static Partitioner defaultPartitioner(RDD<?> rdd, scala.collection.Seq<RDD<?>> others) Choose a partitioner to use for a cogroup-like operation between a number of RDDs. abstract int getPartition(Object key)  abstract int numPartitions()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Partitioner public Partitioner() Method Detail defaultPartitioner public static Partitioner defaultPartitioner(RDD<?> rdd, scala.collection.Seq<RDD<?>> others) Choose a partitioner to use for a cogroup-like operation between a number of RDDs. If any of the RDDs already has a partitioner, choose that one. Otherwise, we use a default HashPartitioner. For the number of partitions, if spark.default.parallelism is set, then we'll use the value from SparkContext defaultParallelism, otherwise we'll use the max number of upstream partitions. Unless spark.default.parallelism is set, the number of partitions will be the same as the number of partitions in the largest upstream RDD, as this should be least likely to cause out-of-memory errors. We use two method parameters (rdd, others) to enforce callers passing at least 1 RDD. Parameters:rdd - (undocumented)others - (undocumented) Returns:(undocumented) numPartitions public abstract int numPartitions() getPartition public abstract int getPartition(Object key) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PearsonCorrelation (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PearsonCorrelation (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.stat.correlation Class PearsonCorrelation Object org.apache.spark.mllib.stat.correlation.PearsonCorrelation public class PearsonCorrelation extends Object Compute Pearson correlation for two RDDs of the type RDD[Double] or the correlation matrix for an RDD of the type RDD[Vector]. Definition of Pearson correlation can be found at http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient Constructor Summary Constructors  Constructor and Description PearsonCorrelation()  Method Summary Methods  Modifier and Type Method and Description static double computeCorrelation(RDD<Object> x, RDD<Object> y) Compute the Pearson correlation for two datasets. static Matrix computeCorrelationMatrix(RDD<Vector> X) Compute the Pearson correlation matrix S, for the input matrix, where S(i, j) is the correlation between column i and j. static Matrix computeCorrelationMatrixFromCovariance(Matrix covarianceMatrix) Compute the Pearson correlation matrix from the covariance matrix. static double computeCorrelationWithMatrixImpl(RDD<Object> x, RDD<Object> y)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PearsonCorrelation public PearsonCorrelation() Method Detail computeCorrelation public static double computeCorrelation(RDD<Object> x, RDD<Object> y) Compute the Pearson correlation for two datasets. NaN if either vector has 0 variance. Parameters:x - (undocumented)y - (undocumented) Returns:(undocumented) computeCorrelationMatrix public static Matrix computeCorrelationMatrix(RDD<Vector> X) Compute the Pearson correlation matrix S, for the input matrix, where S(i, j) is the correlation between column i and j. 0 covariance results in a correlation value of Double.NaN. Parameters:X - (undocumented) Returns:(undocumented) computeCorrelationMatrixFromCovariance public static Matrix computeCorrelationMatrixFromCovariance(Matrix covarianceMatrix) Compute the Pearson correlation matrix from the covariance matrix. 0 variance results in a correlation value of Double.NaN. Parameters:covarianceMatrix - (undocumented) Returns:(undocumented) computeCorrelationWithMatrixImpl public static double computeCorrelationWithMatrixImpl(RDD<Object> x, RDD<Object> y) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Pipeline.SharedReadWrite$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Pipeline.SharedReadWrite$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml Class Pipeline.SharedReadWrite$ Object org.apache.spark.ml.Pipeline.SharedReadWrite$ Enclosing class: Pipeline public static class Pipeline.SharedReadWrite$ extends Object Methods for `MLReader` and `MLWriter` shared between Pipeline and PipelineModel Field Summary Fields  Modifier and Type Field and Description static Pipeline.SharedReadWrite$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description Pipeline.SharedReadWrite$()  Method Summary Methods  Modifier and Type Method and Description String getStagePath(String stageUid, int stageIdx, int numStages, String stagesDir) Get path for saving the given stage. scala.Tuple2<String,PipelineStage[]> load(String expectedClassName, SparkContext sc, String path) Load metadata and stages for a Pipeline or PipelineModel void saveImpl(Params instance, PipelineStage[] stages, SparkContext sc, String path) Save metadata and stages for a Pipeline or PipelineModel - save metadata to path/metadata - save stages to stages/IDX_UID void validateStages(PipelineStage[] stages) Check that all stages are Writable Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final Pipeline.SharedReadWrite$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail Pipeline.SharedReadWrite$ public Pipeline.SharedReadWrite$() Method Detail validateStages public void validateStages(PipelineStage[] stages) Check that all stages are Writable saveImpl public void saveImpl(Params instance, PipelineStage[] stages, SparkContext sc, String path) Save metadata and stages for a Pipeline or PipelineModel - save metadata to path/metadata - save stages to stages/IDX_UID Parameters:instance - (undocumented)stages - (undocumented)sc - (undocumented)path - (undocumented) load public scala.Tuple2<String,PipelineStage[]> load(String expectedClassName, SparkContext sc, String path) Load metadata and stages for a Pipeline or PipelineModel Parameters:expectedClassName - (undocumented)sc - (undocumented)path - (undocumented) Returns:(UID, list of stages) getStagePath public String getStagePath(String stageUid, int stageIdx, int numStages, String stagesDir) Get path for saving the given stage. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Pipeline (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Pipeline (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml Class Pipeline Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<PipelineModel> org.apache.spark.ml.Pipeline All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class Pipeline extends Estimator<PipelineModel> implements MLWritable A simple pipeline, which acts as an estimator. A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer. When fit(org.apache.spark.sql.Dataset<?>) is called, the stages are executed in order. If a stage is an Estimator, its Estimator.fit(org.apache.spark.sql.Dataset<?>, org.apache.spark.ml.param.ParamPair<?>, org.apache.spark.ml.param.ParamPair<?>...) method will be called on the input dataset to fit a model. Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage. If a stage is a Transformer, its Transformer.transform(org.apache.spark.sql.Dataset<?>, org.apache.spark.ml.param.ParamPair<?>, org.apache.spark.ml.param.ParamPair<?>...) method will be called to produce the dataset for the next stage. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages. If there are no stages, the pipeline acts as an identity transformer. See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  Pipeline.SharedReadWrite$ Methods for `MLReader` and `MLWriter` shared between Pipeline and PipelineModel Constructor Summary Constructors  Constructor and Description Pipeline()  Pipeline(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  Pipeline copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  PipelineModel fit(Dataset<?> dataset) Fits the pipeline to the input dataset with additional parameters. static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  PipelineStage[] getStages()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Pipeline load(String path)  static Param<?>[] params()  static MLReader<Pipeline> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  Pipeline setStages(PipelineStage[] value)  Param<PipelineStage[]> stages() param for pipeline stages static String toString()  StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail Pipeline public Pipeline(String uid) Pipeline public Pipeline() Method Detail read public static MLReader<Pipeline> read() load public static Pipeline load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) stages public Param<PipelineStage[]> stages() param for pipeline stages Returns:(undocumented) setStages public Pipeline setStages(PipelineStage[] value) getStages public PipelineStage[] getStages() fit public PipelineModel fit(Dataset<?> dataset) Fits the pipeline to the input dataset with additional parameters. If a stage is an Estimator, its Estimator.fit(org.apache.spark.sql.Dataset<?>, org.apache.spark.ml.param.ParamPair<?>, org.apache.spark.ml.param.ParamPair<?>...) method will be called on the input dataset to fit a model. Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage. If a stage is a Transformer, its Transformer.transform(org.apache.spark.sql.Dataset<?>, org.apache.spark.ml.param.ParamPair<?>, org.apache.spark.ml.param.ParamPair<?>...) method will be called to produce the dataset for the next stage. The fitted model from a Pipeline is an PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages. If there are no stages, the output model acts as an identity transformer. Specified by: fit in class Estimator<PipelineModel> Parameters:dataset - input dataset Returns:fitted pipeline copy public Pipeline copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Estimator<PipelineModel> Parameters:extra - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PipelineModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PipelineModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml Class PipelineModel Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<PipelineModel> org.apache.spark.ml.PipelineModel All Implemented Interfaces: java.io.Serializable, Params, Identifiable, MLWritable public class PipelineModel extends Model<PipelineModel> implements MLWritable Represents a fitted pipeline. See Also:Serialized Form Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  PipelineModel copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static PipelineModel load(String path)  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static MLReader<PipelineModel> read()  static void save(String path)  static <T> Params set(Param<T> param, T value)  static M setParent(Estimator<M> parent)  Transformer[] stages()  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms the input dataset. StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  MLWriter write() Returns an MLWriter instance for this ML instance. Methods inherited from class org.apache.spark.ml.Model hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Method Detail read public static MLReader<PipelineModel> read() load public static PipelineModel load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) stages public Transformer[] stages() transform public Dataset<Row> transform(Dataset<?> dataset) Description copied from class: Transformer Transforms the input dataset. Specified by: transform in class Transformer Parameters:dataset - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) copy public PipelineModel copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Model<PipelineModel> Parameters:extra - (undocumented) Returns:(undocumented) write public MLWriter write() Description copied from interface: MLWritable Returns an MLWriter instance for this ML instance. Specified by: write in interface MLWritable Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PipelineStage (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PipelineStage (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml Class PipelineStage Object org.apache.spark.ml.PipelineStage All Implemented Interfaces: java.io.Serializable, Params, Identifiable Direct Known Subclasses: Estimator, Transformer public abstract class PipelineStage extends Object implements Params :: DeveloperApi :: A stage in a pipeline, either an Estimator or a Transformer. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PipelineStage()  Method Summary Methods  Modifier and Type Method and Description abstract PipelineStage copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. abstract StructType transformSchema(StructType schema) :: DeveloperApi :: Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail PipelineStage public PipelineStage() Method Detail transformSchema public abstract StructType transformSchema(StructType schema) :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Parameters:schema - (undocumented) Returns:(undocumented) copy public abstract PipelineStage copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PoissonBounds (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PoissonBounds (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.random Class PoissonBounds Object org.apache.spark.util.random.PoissonBounds public class PoissonBounds extends Object Utility functions that help us determine bounds on adjusted sampling rate to guarantee exact sample sizes with high confidence when sampling with replacement. Constructor Summary Constructors  Constructor and Description PoissonBounds()  Method Summary Methods  Modifier and Type Method and Description static double getLowerBound(double s) Returns a lambda such that Pr[X > s] is very small, where X ~ Pois(lambda). static double getUpperBound(double s) Returns a lambda such that Pr[X < s] is very small, where X ~ Pois(lambda). Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PoissonBounds public PoissonBounds() Method Detail getLowerBound public static double getLowerBound(double s) Returns a lambda such that Pr[X > s] is very small, where X ~ Pois(lambda). Parameters:s - (undocumented) Returns:(undocumented) getUpperBound public static double getUpperBound(double s) Returns a lambda such that Pr[X < s] is very small, where X ~ Pois(lambda). Parameters:s - sample size Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PoissonGenerator (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PoissonGenerator (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.random Class PoissonGenerator Object org.apache.spark.mllib.random.PoissonGenerator All Implemented Interfaces: java.io.Serializable, RandomDataGenerator<Object>, Pseudorandom public class PoissonGenerator extends Object implements RandomDataGenerator<Object> :: DeveloperApi :: Generates i.i.d. samples from the Poisson distribution with the given mean. param: mean mean for the Poisson distribution. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PoissonGenerator(double mean)  Method Summary Methods  Modifier and Type Method and Description PoissonGenerator copy() Returns a copy of the RandomDataGenerator with a new instance of the rng object used in the class when applicable for non-locking concurrent usage. double mean()  double nextValue() Returns an i.i.d. void setSeed(long seed) Set random seed. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PoissonGenerator public PoissonGenerator(double mean) Method Detail mean public double mean() nextValue public double nextValue() Description copied from interface: RandomDataGenerator Returns an i.i.d. sample as a generic type from an underlying distribution. Specified by: nextValue in interface RandomDataGenerator<Object> Returns:(undocumented) setSeed public void setSeed(long seed) Description copied from interface: Pseudorandom Set random seed. Specified by: setSeed in interface Pseudorandom copy public PoissonGenerator copy() Description copied from interface: RandomDataGenerator Returns a copy of the RandomDataGenerator with a new instance of the rng object used in the class when applicable for non-locking concurrent usage. Specified by: copy in interface RandomDataGenerator<Object> Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PoissonSampler (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PoissonSampler (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.random Class PoissonSampler<T> Object org.apache.spark.util.random.PoissonSampler<T> All Implemented Interfaces: java.io.Serializable, Cloneable, Pseudorandom, RandomSampler<T,T> public class PoissonSampler<T> extends Object implements RandomSampler<T,T> :: DeveloperApi :: A sampler for sampling with replacement, based on values drawn from Poisson distribution. param: fraction the sampling fraction (with replacement) param: useGapSamplingIfPossible if true, use gap sampling when sampling ratio is low. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PoissonSampler(double fraction)  PoissonSampler(double fraction, boolean useGapSamplingIfPossible)  Method Summary Methods  Modifier and Type Method and Description PoissonSampler<T> clone() return a copy of the RandomSampler object int sample() Whether to sample the next item or not. scala.collection.Iterator<T> sample(scala.collection.Iterator<T> items) take a random sample void setSeed(long seed) Set random seed. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PoissonSampler public PoissonSampler(double fraction, boolean useGapSamplingIfPossible) PoissonSampler public PoissonSampler(double fraction) Method Detail setSeed public void setSeed(long seed) Description copied from interface: Pseudorandom Set random seed. Specified by: setSeed in interface Pseudorandom sample public int sample() Description copied from interface: RandomSampler Whether to sample the next item or not. Return how many times the next item will be sampled. Return 0 if it is not sampled. Specified by: sample in interface RandomSampler<T,T> Returns:(undocumented) sample public scala.collection.Iterator<T> sample(scala.collection.Iterator<T> items) Description copied from interface: RandomSampler take a random sample Specified by: sample in interface RandomSampler<T,T> clone public PoissonSampler<T> clone() Description copied from interface: RandomSampler return a copy of the RandomSampler object Specified by: clone in interface RandomSampler<T,T> Overrides: clone in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PolynomialExpansion (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PolynomialExpansion (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class PolynomialExpansion Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.UnaryTransformer<Vector,Vector,PolynomialExpansion> org.apache.spark.ml.feature.PolynomialExpansion All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public class PolynomialExpansion extends UnaryTransformer<Vector,Vector,PolynomialExpansion> implements DefaultParamsWritable Perform feature expansion in a polynomial space. As said in wikipedia of Polynomial Expansion, which is available at http://en.wikipedia.org/wiki/Polynomial_expansion, "In mathematics, an expansion of a product of sums expresses it as a sum of products by using the fact that multiplication distributes over addition". Take a 2-variable feature vector as an example: (x, y), if we want to expand it with degree 2, then we get (x, x * x, y, x * y, y * y). See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PolynomialExpansion()  PolynomialExpansion(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  PolynomialExpansion copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. IntParam degree() The polynomial degree to expand, which should be >= 1. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  int getDegree()  static String getInputCol()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static PolynomialExpansion load(String path)  static Param<String> outputCol()  static Param<?>[] params()  static void save(String path)  static <T> Params set(Param<T> param, T value)  PolynomialExpansion setDegree(int value)  static T setInputCol(String value)  static T setOutputCol(String value)  static String toString()  static Dataset<Row> transform(Dataset<?> dataset)  static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs)  static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs)  static StructType transformSchema(StructType schema)  String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.UnaryTransformer setInputCol, setOutputCol, transform, transformSchema Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Constructor Detail PolynomialExpansion public PolynomialExpansion(String uid) PolynomialExpansion public PolynomialExpansion() Method Detail load public static PolynomialExpansion load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, scala.collection.Seq<ParamPair<?>> otherParamPairs) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamMap paramMap) transform public static Dataset<Row> transform(Dataset<?> dataset, ParamPair<?> firstParamPair, ParamPair<?>... otherParamPairs) inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() setInputCol public static T setInputCol(String value) setOutputCol public static T setOutputCol(String value) transformSchema public static StructType transformSchema(StructType schema) transform public static Dataset<Row> transform(Dataset<?> dataset) save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) degree public IntParam degree() The polynomial degree to expand, which should be >= 1. A value of 1 means no expansion. Default: 2 Returns:(undocumented) getDegree public int getDegree() setDegree public PolynomialExpansion setDegree(int value) copy public PolynomialExpansion copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Overrides: copy in class UnaryTransformer<Vector,Vector,PolynomialExpansion> Parameters:extra - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PortableDataStream (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PortableDataStream (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.input Class PortableDataStream Object org.apache.spark.input.PortableDataStream All Implemented Interfaces: java.io.Serializable public class PortableDataStream extends Object implements scala.Serializable A class that allows DataStreams to be serialized and moved around by not creating them until they need to be read See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PortableDataStream(org.apache.hadoop.mapreduce.lib.input.CombineFileSplit isplit, org.apache.hadoop.mapreduce.TaskAttemptContext context, Integer index)  Method Summary Methods  Modifier and Type Method and Description String getPath()  java.io.DataInputStream open() Create a new DataInputStream from the split and context. byte[] toArray() Read the file as a byte array Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PortableDataStream public PortableDataStream(org.apache.hadoop.mapreduce.lib.input.CombineFileSplit isplit, org.apache.hadoop.mapreduce.TaskAttemptContext context, Integer index) Method Detail open public java.io.DataInputStream open() Create a new DataInputStream from the split and context. The user of this method is responsible for closing the stream after usage. Returns:(undocumented) toArray public byte[] toArray() Read the file as a byte array Returns:(undocumented) getPath public String getPath() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PostgresDialect (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PostgresDialect (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.jdbc Class PostgresDialect Object org.apache.spark.sql.jdbc.PostgresDialect public class PostgresDialect extends Object Constructor Summary Constructors  Constructor and Description PostgresDialect()  Method Summary Methods  Modifier and Type Method and Description static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties)  static boolean canHandle(String url)  static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md)  static scala.Option<JdbcType> getJDBCType(DataType dt)  static String getTableExistsQuery(String table)  static String quoteIdentifier(String colName)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PostgresDialect public PostgresDialect() Method Detail canHandle public static boolean canHandle(String url) getCatalystType public static scala.Option<DataType> getCatalystType(int sqlType, String typeName, int size, MetadataBuilder md) getJDBCType public static scala.Option<JdbcType> getJDBCType(DataType dt) getTableExistsQuery public static String getTableExistsQuery(String table) beforeFetch public static void beforeFetch(java.sql.Connection connection, scala.collection.immutable.Map<String,String> properties) quoteIdentifier public static String quoteIdentifier(String colName) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PowerIterationClustering.Assignment$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PowerIterationClustering.Assignment$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class PowerIterationClustering.Assignment$ Object scala.runtime.AbstractFunction2<Object,Object,PowerIterationClustering.Assignment> org.apache.spark.mllib.clustering.PowerIterationClustering.Assignment$ All Implemented Interfaces: java.io.Serializable, scala.Function2<Object,Object,PowerIterationClustering.Assignment> Enclosing class: PowerIterationClustering public static class PowerIterationClustering.Assignment$ extends scala.runtime.AbstractFunction2<Object,Object,PowerIterationClustering.Assignment> implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static PowerIterationClustering.Assignment$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description PowerIterationClustering.Assignment$()  Method Summary Methods inherited from class scala.runtime.AbstractFunction2 apply$mcDDD$sp, apply$mcDDI$sp, apply$mcDDJ$sp, apply$mcDID$sp, apply$mcDII$sp, apply$mcDIJ$sp, apply$mcDJD$sp, apply$mcDJI$sp, apply$mcDJJ$sp, apply$mcFDD$sp, apply$mcFDI$sp, apply$mcFDJ$sp, apply$mcFID$sp, apply$mcFII$sp, apply$mcFIJ$sp, apply$mcFJD$sp, apply$mcFJI$sp, apply$mcFJJ$sp, apply$mcIDD$sp, apply$mcIDI$sp, apply$mcIDJ$sp, apply$mcIID$sp, apply$mcIII$sp, apply$mcIIJ$sp, apply$mcIJD$sp, apply$mcIJI$sp, apply$mcIJJ$sp, apply$mcJDD$sp, apply$mcJDI$sp, apply$mcJDJ$sp, apply$mcJID$sp, apply$mcJII$sp, apply$mcJIJ$sp, apply$mcJJD$sp, apply$mcJJI$sp, apply$mcJJJ$sp, apply$mcVDD$sp, apply$mcVDI$sp, apply$mcVDJ$sp, apply$mcVID$sp, apply$mcVII$sp, apply$mcVIJ$sp, apply$mcVJD$sp, apply$mcVJI$sp, apply$mcVJJ$sp, apply$mcZDD$sp, apply$mcZDI$sp, apply$mcZDJ$sp, apply$mcZID$sp, apply$mcZII$sp, apply$mcZIJ$sp, apply$mcZJD$sp, apply$mcZJI$sp, apply$mcZJJ$sp, curried, toString, tupled Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, wait, wait, wait Methods inherited from interface scala.Function2 apply Field Detail MODULE$ public static final PowerIterationClustering.Assignment$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail PowerIterationClustering.Assignment$ public PowerIterationClustering.Assignment$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PowerIterationClustering.Assignment (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PowerIterationClustering.Assignment (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class PowerIterationClustering.Assignment Object org.apache.spark.mllib.clustering.PowerIterationClustering.Assignment All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product Enclosing class: PowerIterationClustering public static class PowerIterationClustering.Assignment extends Object implements scala.Product, scala.Serializable Cluster assignment. param: id node id param: cluster assigned cluster id See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PowerIterationClustering.Assignment(long id, int cluster)  Method Summary Methods  Modifier and Type Method and Description int cluster()  long id()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail PowerIterationClustering.Assignment public PowerIterationClustering.Assignment(long id, int cluster) Method Detail id public long id() cluster public int cluster() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PowerIterationClustering (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PowerIterationClustering (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class PowerIterationClustering Object org.apache.spark.mllib.clustering.PowerIterationClustering All Implemented Interfaces: java.io.Serializable public class PowerIterationClustering extends Object implements scala.Serializable Power Iteration Clustering (PIC), a scalable graph clustering algorithm developed by Lin and Cohen. From the abstract: PIC finds a very low-dimensional embedding of a dataset using truncated power iteration on a normalized pair-wise similarity matrix of the data. param: k Number of clusters. param: maxIterations Maximum number of iterations of the PIC algorithm. param: initMode Set the initialization mode. This can be either "random" to use a random vector as vertex properties, or "degree" to use normalized sum similarities. Default: random. See Also:, Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  PowerIterationClustering.Assignment Cluster assignment. static class  PowerIterationClustering.Assignment$  Constructor Summary Constructors  Constructor and Description PowerIterationClustering() Constructs a PIC instance with default parameters: {k: 2, maxIterations: 100, initMode: "random"}. Method Summary Methods  Modifier and Type Method and Description PowerIterationClusteringModel run(Graph<Object,Object> graph) Run the PIC algorithm on Graph. PowerIterationClusteringModel run(JavaRDD<scala.Tuple3<Long,Long,Double>> similarities) A Java-friendly version of PowerIterationClustering.run. PowerIterationClusteringModel run(RDD<scala.Tuple3<Object,Object,Object>> similarities) Run the PIC algorithm. PowerIterationClustering setInitializationMode(String mode) Set the initialization mode. PowerIterationClustering setK(int k) Set the number of clusters. PowerIterationClustering setMaxIterations(int maxIterations) Set maximum number of iterations of the power iteration loop Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PowerIterationClustering public PowerIterationClustering() Constructs a PIC instance with default parameters: {k: 2, maxIterations: 100, initMode: "random"}. Method Detail setK public PowerIterationClustering setK(int k) Set the number of clusters. Parameters:k - (undocumented) Returns:(undocumented) setMaxIterations public PowerIterationClustering setMaxIterations(int maxIterations) Set maximum number of iterations of the power iteration loop Parameters:maxIterations - (undocumented) Returns:(undocumented) setInitializationMode public PowerIterationClustering setInitializationMode(String mode) Set the initialization mode. This can be either "random" to use a random vector as vertex properties, or "degree" to use normalized sum similarities. Default: random. Parameters:mode - (undocumented) Returns:(undocumented) run public PowerIterationClusteringModel run(Graph<Object,Object> graph) Run the PIC algorithm on Graph. Parameters:graph - an affinity matrix represented as graph, which is the matrix A in the PIC paper. The similarity s,,ij,, represented as the edge between vertices (i, j) must be nonnegative. This is a symmetric matrix and hence s,,ij,, = s,,ji,,. For any (i, j) with nonzero similarity, there should be either (i, j, s,,ij,,) or (j, i, s,,ji,,) in the input. Tuples with i = j are ignored, because we assume s,,ij,, = 0.0. Returns:a PowerIterationClusteringModel that contains the clustering result run public PowerIterationClusteringModel run(RDD<scala.Tuple3<Object,Object,Object>> similarities) Run the PIC algorithm. Parameters:similarities - an RDD of (i, j, s,,ij,,) tuples representing the affinity matrix, which is the matrix A in the PIC paper. The similarity s,,ij,, must be nonnegative. This is a symmetric matrix and hence s,,ij,, = s,,ji,,. For any (i, j) with nonzero similarity, there should be either (i, j, s,,ij,,) or (j, i, s,,ji,,) in the input. Tuples with i = j are ignored, because we assume s,,ij,, = 0.0. Returns:a PowerIterationClusteringModel that contains the clustering result run public PowerIterationClusteringModel run(JavaRDD<scala.Tuple3<Long,Long,Double>> similarities) A Java-friendly version of PowerIterationClustering.run. Parameters:similarities - (undocumented) Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PowerIterationClusteringModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PowerIterationClusteringModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class PowerIterationClusteringModel.SaveLoadV1_0$ Object org.apache.spark.mllib.clustering.PowerIterationClusteringModel.SaveLoadV1_0$ Enclosing class: PowerIterationClusteringModel public static class PowerIterationClusteringModel.SaveLoadV1_0$ extends Object Field Summary Fields  Modifier and Type Field and Description static PowerIterationClusteringModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description PowerIterationClusteringModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description PowerIterationClusteringModel load(SparkContext sc, String path)  void save(SparkContext sc, PowerIterationClusteringModel model, String path)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final PowerIterationClusteringModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail PowerIterationClusteringModel.SaveLoadV1_0$ public PowerIterationClusteringModel.SaveLoadV1_0$() Method Detail save public void save(SparkContext sc, PowerIterationClusteringModel model, String path) load public PowerIterationClusteringModel load(SparkContext sc, String path) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PowerIterationClusteringModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PowerIterationClusteringModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.clustering Class PowerIterationClusteringModel Object org.apache.spark.mllib.clustering.PowerIterationClusteringModel All Implemented Interfaces: java.io.Serializable, Saveable public class PowerIterationClusteringModel extends Object implements Saveable, scala.Serializable Model produced by PowerIterationClustering. param: k number of clusters param: assignments an RDD of clustering PowerIterationClustering#Assignments See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  PowerIterationClusteringModel.SaveLoadV1_0$  Constructor Summary Constructors  Constructor and Description PowerIterationClusteringModel(int k, RDD<PowerIterationClustering.Assignment> assignments)  Method Summary Methods  Modifier and Type Method and Description RDD<PowerIterationClustering.Assignment> assignments()  int k()  static PowerIterationClusteringModel load(SparkContext sc, String path)  void save(SparkContext sc, String path) Save this model to the given path. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PowerIterationClusteringModel public PowerIterationClusteringModel(int k, RDD<PowerIterationClustering.Assignment> assignments) Method Detail load public static PowerIterationClusteringModel load(SparkContext sc, String path) k public int k() assignments public RDD<PowerIterationClustering.Assignment> assignments() save public void save(SparkContext sc, String path) Description copied from interface: Saveable Save this model to the given path. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using Loader.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Precision (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Precision (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.evaluation.binary Class Precision Object org.apache.spark.mllib.evaluation.binary.Precision public class Precision extends Object Precision. Defined as 1.0 when there are no positive examples. Constructor Summary Constructors  Constructor and Description Precision()  Method Summary Methods  Modifier and Type Method and Description static double apply(org.apache.spark.mllib.evaluation.binary.BinaryConfusionMatrix c)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Precision public Precision() Method Detail apply public static double apply(org.apache.spark.mllib.evaluation.binary.BinaryConfusionMatrix c) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Predict (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Predict (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.model Class Predict Object org.apache.spark.mllib.tree.model.Predict All Implemented Interfaces: java.io.Serializable public class Predict extends Object implements scala.Serializable :: DeveloperApi :: Predicted value for a node param: predict predicted value param: prob probability of the label (classification only) See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Predict(double predict, double prob)  Method Summary Methods  Modifier and Type Method and Description boolean equals(Object other)  int hashCode()  double predict()  double prob()  String toString()  Methods inherited from class Object getClass, notify, notifyAll, wait, wait, wait Constructor Detail Predict public Predict(double predict, double prob) Method Detail predict public double predict() prob public double prob() toString public String toString() Overrides: toString in class Object equals public boolean equals(Object other) Overrides: equals in class Object hashCode public int hashCode() Overrides: hashCode in class Object Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PredictionModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PredictionModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml Class PredictionModel<FeaturesType,M extends PredictionModel<FeaturesType,M>> Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<M> org.apache.spark.ml.PredictionModel<FeaturesType,M> All Implemented Interfaces: java.io.Serializable, Params, Identifiable Direct Known Subclasses: ClassificationModel, DecisionTreeRegressionModel, GBTClassificationModel, GBTRegressionModel, MultilayerPerceptronClassificationModel, RandomForestRegressionModel, RegressionModel public abstract class PredictionModel<FeaturesType,M extends PredictionModel<FeaturesType,M>> extends Model<M> :: DeveloperApi :: Abstraction for a model for prediction tasks (regression and classification). See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PredictionModel()  Method Summary Methods  Modifier and Type Method and Description Param<String> featuresCol() Param for features column name. String getFeaturesCol()  String getLabelCol()  String getPredictionCol()  Param<String> labelCol() Param for label column name. int numFeatures() Returns the number of features the model was trained on. Param<String> predictionCol() Param for prediction column name. M setFeaturesCol(String value)  M setPredictionCol(String value)  Dataset<Row> transform(Dataset<?> dataset) Transforms dataset by reading from featuresCol, calling predict, and storing the predictions as a new column predictionCol. StructType transformSchema(StructType schema) :: DeveloperApi :: StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Methods inherited from class org.apache.spark.ml.Model copy, hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copy, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail PredictionModel public PredictionModel() Method Detail setFeaturesCol public M setFeaturesCol(String value) setPredictionCol public M setPredictionCol(String value) numFeatures public int numFeatures() Returns the number of features the model was trained on. If unknown, returns -1 transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) transform public Dataset<Row> transform(Dataset<?> dataset) Transforms dataset by reading from featuresCol, calling predict, and storing the predictions as a new column predictionCol. Specified by: transform in class Transformer Parameters:dataset - input dataset Returns:transformed dataset with predictionCol of type Double validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Predictor (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Predictor (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml Class Predictor<FeaturesType,Learner extends Predictor<FeaturesType,Learner,M>,M extends PredictionModel<FeaturesType,M>> Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<FeaturesType,Learner,M> All Implemented Interfaces: java.io.Serializable, Params, Identifiable Direct Known Subclasses: Classifier, DecisionTreeRegressor, GBTClassifier, GBTRegressor, GeneralizedLinearRegression, LinearRegression, MultilayerPerceptronClassifier, RandomForestRegressor public abstract class Predictor<FeaturesType,Learner extends Predictor<FeaturesType,Learner,M>,M extends PredictionModel<FeaturesType,M>> extends Estimator<M> :: DeveloperApi :: Abstraction for prediction problems (regression and classification). See Also:Serialized Form Constructor Summary Constructors  Constructor and Description Predictor()  Method Summary Methods  Modifier and Type Method and Description abstract Learner copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. Param<String> featuresCol() Param for features column name. M fit(Dataset<?> dataset) Fits a model to the input data. String getFeaturesCol()  String getLabelCol()  String getPredictionCol()  Param<String> labelCol() Param for label column name. Param<String> predictionCol() Param for prediction column name. Learner setFeaturesCol(String value)  Learner setLabelCol(String value)  Learner setPredictionCol(String value)  StructType transformSchema(StructType schema) :: DeveloperApi :: StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail Predictor public Predictor() Method Detail setLabelCol public Learner setLabelCol(String value) setFeaturesCol public Learner setFeaturesCol(String value) setPredictionCol public Learner setPredictionCol(String value) fit public M fit(Dataset<?> dataset) Description copied from class: Estimator Fits a model to the input data. Specified by: fit in class Estimator<M extends PredictionModel<FeaturesType,M>> Parameters:dataset - (undocumented) Returns:(undocumented) copy public abstract Learner copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Estimator<M extends PredictionModel<FeaturesType,M>> Parameters:extra - (undocumented) Returns:(undocumented) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PrefixSpan.FreqSequence (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PrefixSpan.FreqSequence (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class PrefixSpan.FreqSequence<Item> Object org.apache.spark.mllib.fpm.PrefixSpan.FreqSequence<Item> All Implemented Interfaces: java.io.Serializable Enclosing class: PrefixSpan public static class PrefixSpan.FreqSequence<Item> extends Object implements scala.Serializable Represents a frequent sequence. param: sequence a sequence of itemsets stored as an Array of Arrays param: freq frequency See Also:Serialized Form Constructor Summary Constructors  Constructor and Description PrefixSpan.FreqSequence(Object[] sequence, long freq)  Method Summary Methods  Modifier and Type Method and Description long freq()  java.util.List<java.util.List<Item>> javaSequence() Returns sequence as a Java List of lists for Java users. Object[] sequence()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PrefixSpan.FreqSequence public PrefixSpan.FreqSequence(Object[] sequence, long freq) Method Detail sequence public Object[] sequence() freq public long freq() javaSequence public java.util.List<java.util.List<Item>> javaSequence() Returns sequence as a Java List of lists for Java users. Returns:(undocumented) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PrefixSpan.Postfix$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PrefixSpan.Postfix$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class PrefixSpan.Postfix$ Object org.apache.spark.mllib.fpm.PrefixSpan.Postfix$ All Implemented Interfaces: java.io.Serializable Enclosing class: PrefixSpan public static class PrefixSpan.Postfix$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static PrefixSpan.Postfix$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description PrefixSpan.Postfix$()  Method Summary Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final PrefixSpan.Postfix$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail PrefixSpan.Postfix$ public PrefixSpan.Postfix$() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PrefixSpan.Prefix$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PrefixSpan.Prefix$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class PrefixSpan.Prefix$ Object org.apache.spark.mllib.fpm.PrefixSpan.Prefix$ All Implemented Interfaces: java.io.Serializable Enclosing class: PrefixSpan public static class PrefixSpan.Prefix$ extends Object implements scala.Serializable See Also:Serialized Form Field Summary Fields  Modifier and Type Field and Description static PrefixSpan.Prefix$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description PrefixSpan.Prefix$()  Method Summary Methods  Modifier and Type Method and Description org.apache.spark.mllib.fpm.PrefixSpan.Prefix empty() An empty Prefix instance. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final PrefixSpan.Prefix$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail PrefixSpan.Prefix$ public PrefixSpan.Prefix$() Method Detail empty public org.apache.spark.mllib.fpm.PrefixSpan.Prefix empty() An empty Prefix instance. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PrefixSpan (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PrefixSpan (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class PrefixSpan Object org.apache.spark.mllib.fpm.PrefixSpan All Implemented Interfaces: java.io.Serializable public class PrefixSpan extends Object implements scala.Serializable A parallel PrefixSpan algorithm to mine frequent sequential patterns. The PrefixSpan algorithm is described in J. Pei, et al., PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth (http://doi.org/10.1109/ICDE.2001.914830). param: minSupport the minimal support level of the sequential pattern, any pattern that appears more than (minSupport * size-of-the-dataset) times will be output param: maxPatternLength the maximal length of the sequential pattern, any pattern that appears less than maxPatternLength will be output param: maxLocalProjDBSize The maximum number of items (including delimiters used in the internal storage format) allowed in a projected database before local processing. If a projected database exceeds this size, another iteration of distributed prefix growth is run. See Also:https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining (Wikipedia)}, Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  PrefixSpan.FreqSequence<Item> Represents a frequent sequence. static class  PrefixSpan.Postfix$  static class  PrefixSpan.Prefix$  Constructor Summary Constructors  Constructor and Description PrefixSpan() Constructs a default instance with default parameters {minSupport: 0.1, maxPatternLength: 10, maxLocalProjDBSize: 32000000L}. Method Summary Methods  Modifier and Type Method and Description long getMaxLocalProjDBSize() Gets the maximum number of items allowed in a projected database before local processing. int getMaxPatternLength() Gets the maximal pattern length (i.e. double getMinSupport() Get the minimal support (i.e. <Item,Itemset extends Iterable<Item>,Sequence extends Iterable<Itemset>> PrefixSpanModel<Item> run(JavaRDD<Sequence> data) A Java-friendly version of run() that reads sequences from a JavaRDD and returns frequent sequences in a PrefixSpanModel. <Item> PrefixSpanModel<Item> run(RDD<Object[]> data, scala.reflect.ClassTag<Item> evidence$1) Finds the complete set of frequent sequential patterns in the input sequences of itemsets. PrefixSpan setMaxLocalProjDBSize(long maxLocalProjDBSize) Sets the maximum number of items (including delimiters used in the internal storage format) allowed in a projected database before local processing (default: 32000000L). PrefixSpan setMaxPatternLength(int maxPatternLength) Sets maximal pattern length (default: 10). PrefixSpan setMinSupport(double minSupport) Sets the minimal support level (default: 0.1). Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PrefixSpan public PrefixSpan() Constructs a default instance with default parameters {minSupport: 0.1, maxPatternLength: 10, maxLocalProjDBSize: 32000000L}. Method Detail getMinSupport public double getMinSupport() Get the minimal support (i.e. the frequency of occurrence before a pattern is considered frequent). Returns:(undocumented) setMinSupport public PrefixSpan setMinSupport(double minSupport) Sets the minimal support level (default: 0.1). Parameters:minSupport - (undocumented) Returns:(undocumented) getMaxPatternLength public int getMaxPatternLength() Gets the maximal pattern length (i.e. the length of the longest sequential pattern to consider. Returns:(undocumented) setMaxPatternLength public PrefixSpan setMaxPatternLength(int maxPatternLength) Sets maximal pattern length (default: 10). Parameters:maxPatternLength - (undocumented) Returns:(undocumented) getMaxLocalProjDBSize public long getMaxLocalProjDBSize() Gets the maximum number of items allowed in a projected database before local processing. Returns:(undocumented) setMaxLocalProjDBSize public PrefixSpan setMaxLocalProjDBSize(long maxLocalProjDBSize) Sets the maximum number of items (including delimiters used in the internal storage format) allowed in a projected database before local processing (default: 32000000L). Parameters:maxLocalProjDBSize - (undocumented) Returns:(undocumented) run public <Item> PrefixSpanModel<Item> run(RDD<Object[]> data, scala.reflect.ClassTag<Item> evidence$1) Finds the complete set of frequent sequential patterns in the input sequences of itemsets. Parameters:data - sequences of itemsets.evidence$1 - (undocumented) Returns:a PrefixSpanModel that contains the frequent patterns run public <Item,Itemset extends Iterable<Item>,Sequence extends Iterable<Itemset>> PrefixSpanModel<Item> run(JavaRDD<Sequence> data) A Java-friendly version of run() that reads sequences from a JavaRDD and returns frequent sequences in a PrefixSpanModel. Parameters:data - ordered sequences of itemsets stored as Java Iterable of Iterables Returns:a PrefixSpanModel that contains the frequent sequential patterns Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PrefixSpanModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PrefixSpanModel.SaveLoadV1_0$ (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class PrefixSpanModel.SaveLoadV1_0$ Object org.apache.spark.mllib.fpm.PrefixSpanModel.SaveLoadV1_0$ Enclosing class: PrefixSpanModel<Item> public static class PrefixSpanModel.SaveLoadV1_0$ extends Object Field Summary Fields  Modifier and Type Field and Description static PrefixSpanModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Summary Constructors  Constructor and Description PrefixSpanModel.SaveLoadV1_0$()  Method Summary Methods  Modifier and Type Method and Description PrefixSpanModel<?> load(SparkContext sc, String path)  <Item> PrefixSpanModel<Item> loadImpl(Dataset<Row> freqSequences, Item sample, scala.reflect.ClassTag<Item> evidence$2)  void save(PrefixSpanModel<?> model, String path)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Field Detail MODULE$ public static final PrefixSpanModel.SaveLoadV1_0$ MODULE$ Static reference to the singleton instance of this Scala object. Constructor Detail PrefixSpanModel.SaveLoadV1_0$ public PrefixSpanModel.SaveLoadV1_0$() Method Detail save public void save(PrefixSpanModel<?> model, String path) load public PrefixSpanModel<?> load(SparkContext sc, String path) loadImpl public <Item> PrefixSpanModel<Item> loadImpl(Dataset<Row> freqSequences, Item sample, scala.reflect.ClassTag<Item> evidence$2) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PrefixSpanModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PrefixSpanModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.fpm Class PrefixSpanModel<Item> Object org.apache.spark.mllib.fpm.PrefixSpanModel<Item> All Implemented Interfaces: java.io.Serializable, Saveable public class PrefixSpanModel<Item> extends Object implements Saveable, scala.Serializable Model fitted by PrefixSpan param: freqSequences frequent sequences See Also:Serialized Form Nested Class Summary Nested Classes  Modifier and Type Class and Description static class  PrefixSpanModel.SaveLoadV1_0$  Constructor Summary Constructors  Constructor and Description PrefixSpanModel(RDD<PrefixSpan.FreqSequence<Item>> freqSequences)  Method Summary Methods  Modifier and Type Method and Description RDD<PrefixSpan.FreqSequence<Item>> freqSequences()  static PrefixSpanModel<?> load(SparkContext sc, String path)  void save(SparkContext sc, String path) Save this model to the given path. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail PrefixSpanModel public PrefixSpanModel(RDD<PrefixSpan.FreqSequence<Item>> freqSequences) Method Detail load public static PrefixSpanModel<?> load(SparkContext sc, String path) freqSequences public RDD<PrefixSpan.FreqSequence<Item>> freqSequences() save public void save(SparkContext sc, String path) Save this model to the given path. It only works for Item datatypes supported by DataFrames. This saves: - human-readable (JSON) model metadata to path/metadata/ - Parquet formatted data to path/data/ The model may be loaded using PrefixSpanModel.load. Specified by: save in interface Saveable Parameters:sc - Spark context used to save model data.path - Path specifying the directory in which to save this model. If the directory already exists, this method throws an exception. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Pregel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Pregel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.graphx Class Pregel Object org.apache.spark.graphx.Pregel public class Pregel extends Object Implements a Pregel-like bulk-synchronous message-passing API. Unlike the original Pregel API, the GraphX Pregel API factors the sendMessage computation over edges, enables the message sending computation to read both vertex attributes, and constrains messages to the graph structure. These changes allow for substantially more efficient distributed execution while also exposing greater flexibility for graph-based computation. Constructor Summary Constructors  Constructor and Description Pregel()  Method Summary Methods  Modifier and Type Method and Description static <VD,ED,A> Graph<VD,ED> apply(Graph<VD,ED> graph, A initialMsg, int maxIterations, EdgeDirection activeDirection, scala.Function3<Object,VD,A,VD> vprog, scala.Function1<EdgeTriplet<VD,ED>,scala.collection.Iterator<scala.Tuple2<Object,A>>> sendMsg, scala.Function2<A,A,A> mergeMsg, scala.reflect.ClassTag<VD> evidence$1, scala.reflect.ClassTag<ED> evidence$2, scala.reflect.ClassTag<A> evidence$3) Execute a Pregel-like iterative vertex-parallel abstraction. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail Pregel public Pregel() Method Detail apply public static <VD,ED,A> Graph<VD,ED> apply(Graph<VD,ED> graph, A initialMsg, int maxIterations, EdgeDirection activeDirection, scala.Function3<Object,VD,A,VD> vprog, scala.Function1<EdgeTriplet<VD,ED>,scala.collection.Iterator<scala.Tuple2<Object,A>>> sendMsg, scala.Function2<A,A,A> mergeMsg, scala.reflect.ClassTag<VD> evidence$1, scala.reflect.ClassTag<ED> evidence$2, scala.reflect.ClassTag<A> evidence$3) Execute a Pregel-like iterative vertex-parallel abstraction. The user-defined vertex-program vprog is executed in parallel on each vertex receiving any inbound messages and computing a new value for the vertex. The sendMsg function is then invoked on all out-edges and is used to compute an optional message to the destination vertex. The mergeMsg function is a commutative associative function used to combine messages destined to the same vertex. On the first iteration all vertices receive the initialMsg and on subsequent iterations if a vertex does not receive a message then the vertex-program is not invoked. This function iterates until there are no remaining messages, or for maxIterations iterations. Parameters:graph - the input graph. initialMsg - the message each vertex will receive at the first iteration maxIterations - the maximum number of iterations to run for activeDirection - the direction of edges incident to a vertex that received a message in the previous round on which to run sendMsg. For example, if this is EdgeDirection.Out, only out-edges of vertices that received a message in the previous round will run. The default is EdgeDirection.Either, which will run sendMsg on edges where either side received a message in the previous round. If this is EdgeDirection.Both, sendMsg will only run on edges where *both* vertices received a message. vprog - the user-defined vertex program which runs on each vertex and receives the inbound message and computes a new vertex value. On the first iteration the vertex program is invoked on all vertices and is passed the default message. On subsequent iterations the vertex program is only invoked on those vertices that receive messages. sendMsg - a user supplied function that is applied to out edges of vertices that received messages in the current iteration mergeMsg - a user supplied function that takes two incoming messages of type A and merges them into a single message of type A. ''This function must be commutative and associative and ideally the size of A should not increase.'' evidence$1 - (undocumented)evidence$2 - (undocumented)evidence$3 - (undocumented) Returns:the resulting graph at the end of the computation Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ProbabilisticClassificationModel (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ProbabilisticClassificationModel (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class ProbabilisticClassificationModel<FeaturesType,M extends ProbabilisticClassificationModel<FeaturesType,M>> Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Transformer org.apache.spark.ml.Model<M> org.apache.spark.ml.PredictionModel<FeaturesType,M> org.apache.spark.ml.classification.ClassificationModel<FeaturesType,M> org.apache.spark.ml.classification.ProbabilisticClassificationModel<FeaturesType,M> All Implemented Interfaces: java.io.Serializable, Params, Identifiable Direct Known Subclasses: DecisionTreeClassificationModel, LogisticRegressionModel, NaiveBayesModel, RandomForestClassificationModel public abstract class ProbabilisticClassificationModel<FeaturesType,M extends ProbabilisticClassificationModel<FeaturesType,M>> extends ClassificationModel<FeaturesType,M> :: DeveloperApi :: Model produced by a ProbabilisticClassifier. Classes are indexed {0, 1, ..., numClasses - 1}. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ProbabilisticClassificationModel()  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  abstract static M copy(ParamMap extra)  static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  static Param<String> featuresCol()  Param<String> featuresCol() Param for features column name. static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getFeaturesCol()  String getFeaturesCol()  static String getLabelCol()  String getLabelCol()  static <T> T getOrDefault(Param<T> param)  static Param<Object> getParam(String paramName)  static String getPredictionCol()  String getPredictionCol()  static String getProbabilityCol()  static String getRawPredictionCol()  String getRawPredictionCol()  static double[] getThresholds()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static boolean hasParent()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static Param<String> labelCol()  Param<String> labelCol() Param for label column name. static void normalizeToProbabilitiesInPlace(DenseVector v) Normalize a vector of raw predictions to be a multinomial probability vector, in place. abstract static int numClasses()  static int numFeatures()  static Param<?>[] params()  static void parent_$eq(Estimator<M> x$1)  static Estimator<M> parent()  static Param<String> predictionCol()  Param<String> predictionCol() Param for prediction column name. static Param<String> probabilityCol()  static Param<String> rawPredictionCol()  Param<String> rawPredictionCol() Param for raw prediction (a.k.a. static <T> Params set(Param<T> param, T value)  static M setFeaturesCol(String value)  static M setParent(Estimator<M> parent)  static M setPredictionCol(String value)  M setProbabilityCol(String value)  static M setRawPredictionCol(String value)  M setThresholds(double[] value)  static DoubleArrayParam thresholds()  static String toString()  Dataset<Row> transform(Dataset<?> dataset) Transforms dataset by reading from featuresCol, and appending new columns as specified by parameters: - predicted labels as predictionCol of type Double - raw predictions (confidences) as rawPredictionCol of type Vector - probability of each class as probabilityCol of type Vector. static StructType transformSchema(StructType schema)  abstract static String uid()  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. static void validateParams()  Methods inherited from class org.apache.spark.ml.classification.ClassificationModel numClasses, setRawPredictionCol Methods inherited from class org.apache.spark.ml.PredictionModel numFeatures, setFeaturesCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Model copy, hasParent, parent, setParent Methods inherited from class org.apache.spark.ml.Transformer transform, transform, transform Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copy, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail ProbabilisticClassificationModel public ProbabilisticClassificationModel() Method Detail normalizeToProbabilitiesInPlace public static void normalizeToProbabilitiesInPlace(DenseVector v) Normalize a vector of raw predictions to be a multinomial probability vector, in place. The input raw predictions should be nonnegative. The output vector sums to 1, unless the input vector is all-0 (in which case the output is all-0 too). NOTE: This is NOT applicable to all models, only ones which effectively use class instance counts for raw predictions. Parameters:v - (undocumented) uid public abstract static String uid() toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() parent public static Estimator<M> parent() parent_$eq public static void parent_$eq(Estimator<M> x$1) setParent public static M setParent(Estimator<M> parent) hasParent public static boolean hasParent() copy public abstract static M copy(ParamMap extra) labelCol public static final Param<String> labelCol() getLabelCol public static final String getLabelCol() featuresCol public static final Param<String> featuresCol() getFeaturesCol public static final String getFeaturesCol() predictionCol public static final Param<String> predictionCol() getPredictionCol public static final String getPredictionCol() setFeaturesCol public static M setFeaturesCol(String value) setPredictionCol public static M setPredictionCol(String value) numFeatures public static int numFeatures() transformSchema public static StructType transformSchema(StructType schema) rawPredictionCol public static final Param<String> rawPredictionCol() getRawPredictionCol public static final String getRawPredictionCol() setRawPredictionCol public static M setRawPredictionCol(String value) numClasses public abstract static int numClasses() probabilityCol public static final Param<String> probabilityCol() getProbabilityCol public static final String getProbabilityCol() thresholds public static final DoubleArrayParam thresholds() getThresholds public static double[] getThresholds() setProbabilityCol public M setProbabilityCol(String value) setThresholds public M setThresholds(double[] value) transform public Dataset<Row> transform(Dataset<?> dataset) Transforms dataset by reading from featuresCol, and appending new columns as specified by parameters: - predicted labels as predictionCol of type Double - raw predictions (confidences) as rawPredictionCol of type Vector - probability of each class as probabilityCol of type Vector. Overrides: transform in class ClassificationModel<FeaturesType,M extends ProbabilisticClassificationModel<FeaturesType,M>> Parameters:dataset - input dataset Returns:transformed dataset validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) rawPredictionCol public Param<String> rawPredictionCol() Param for raw prediction (a.k.a. confidence) column name. Returns:(undocumented) getRawPredictionCol public String getRawPredictionCol() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ProbabilisticClassifier (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ProbabilisticClassifier (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.classification Class ProbabilisticClassifier<FeaturesType,E extends ProbabilisticClassifier<FeaturesType,E,M>,M extends ProbabilisticClassificationModel<FeaturesType,M>> Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<M> org.apache.spark.ml.Predictor<FeaturesType,E,M> org.apache.spark.ml.classification.Classifier<FeaturesType,E,M> org.apache.spark.ml.classification.ProbabilisticClassifier<FeaturesType,E,M> All Implemented Interfaces: java.io.Serializable, Params, Identifiable Direct Known Subclasses: DecisionTreeClassifier, LogisticRegression, NaiveBayes, RandomForestClassifier public abstract class ProbabilisticClassifier<FeaturesType,E extends ProbabilisticClassifier<FeaturesType,E,M>,M extends ProbabilisticClassificationModel<FeaturesType,M>> extends Classifier<FeaturesType,E,M> :: DeveloperApi :: Single-label binary or multiclass classifier which can output class conditional probabilities. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ProbabilisticClassifier()  Method Summary Methods  Modifier and Type Method and Description Param<String> featuresCol() Param for features column name. String getFeaturesCol()  String getLabelCol()  String getPredictionCol()  String getRawPredictionCol()  Param<String> labelCol() Param for label column name. Param<String> predictionCol() Param for prediction column name. Param<String> rawPredictionCol() Param for raw prediction (a.k.a. E setProbabilityCol(String value)  E setThresholds(double[] value)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType)  StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Methods inherited from class org.apache.spark.ml.classification.Classifier setRawPredictionCol Methods inherited from class org.apache.spark.ml.Predictor copy, fit, setFeaturesCol, setLabelCol, setPredictionCol, transformSchema Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copy, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString, uid Constructor Detail ProbabilisticClassifier public ProbabilisticClassifier() Method Detail setProbabilityCol public E setProbabilityCol(String value) setThresholds public E setThresholds(double[] value) validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) rawPredictionCol public Param<String> rawPredictionCol() Param for raw prediction (a.k.a. confidence) column name. Returns:(undocumented) getRawPredictionCol public String getRawPredictionCol() validateAndTransformSchema public StructType validateAndTransformSchema(StructType schema, boolean fitting, DataType featuresDataType) Validates and transforms the input schema with the provided param map. Parameters:schema - input schemafitting - whether this is in fittingfeaturesDataType - SQL DataType for FeaturesType. E.g., VectorUDT for vector features. Returns:output schema labelCol public Param<String> labelCol() Param for label column name. Returns:(undocumented) getLabelCol public String getLabelCol() featuresCol public Param<String> featuresCol() Param for features column name. Returns:(undocumented) getFeaturesCol public String getFeaturesCol() predictionCol public Param<String> predictionCol() Param for prediction column name. Returns:(undocumented) getPredictionCol public String getPredictionCol() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method ProcessingTime (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="ProcessingTime (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.streaming Class ProcessingTime Object org.apache.spark.sql.streaming.ProcessingTime All Implemented Interfaces: java.io.Serializable, Trigger, scala.Equals, scala.Product public class ProcessingTime extends Object implements Trigger, scala.Product, scala.Serializable :: Experimental :: A trigger that runs a query periodically based on the processing time. If interval is 0, the query will run as fast as possible. Scala Example: df.write.trigger(ProcessingTime("10 seconds")) import scala.concurrent.duration._ df.write.trigger(ProcessingTime(10.seconds)) Java Example: df.write.trigger(ProcessingTime.create("10 seconds")) import java.util.concurrent.TimeUnit df.write.trigger(ProcessingTime.create(10, TimeUnit.SECONDS)) Since: 2.0.0 See Also:Serialized Form Constructor Summary Constructors  Constructor and Description ProcessingTime(long intervalMs)  Method Summary Methods  Modifier and Type Method and Description static ProcessingTime apply(scala.concurrent.duration.Duration interval) Create a ProcessingTime. static ProcessingTime apply(String interval) Create a ProcessingTime. abstract static boolean canEqual(Object that)  static ProcessingTime create(long interval, java.util.concurrent.TimeUnit unit) Create a ProcessingTime. static ProcessingTime create(String interval) Create a ProcessingTime. abstract static boolean equals(Object that)  long intervalMs()  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail ProcessingTime public ProcessingTime(long intervalMs) Method Detail apply public static ProcessingTime apply(String interval) Create a ProcessingTime. If interval is 0, the query will run as fast as possible. Example: df.write.trigger(ProcessingTime("10 seconds")) Parameters:interval - (undocumented) Returns:(undocumented)Since: 2.0.0 apply public static ProcessingTime apply(scala.concurrent.duration.Duration interval) Create a ProcessingTime. If interval is 0, the query will run as fast as possible. Example: import scala.concurrent.duration._ df.write.trigger(ProcessingTime(10.seconds)) Parameters:interval - (undocumented) Returns:(undocumented)Since: 2.0.0 create public static ProcessingTime create(String interval) Create a ProcessingTime. If interval is 0, the query will run as fast as possible. Example: df.write.trigger(ProcessingTime.create("10 seconds")) Parameters:interval - (undocumented) Returns:(undocumented)Since: 2.0.0 create public static ProcessingTime create(long interval, java.util.concurrent.TimeUnit unit) Create a ProcessingTime. If interval is 0, the query will run as fast as possible. Example: import java.util.concurrent.TimeUnit df.write.trigger(ProcessingTime.create(10, TimeUnit.SECONDS)) Parameters:interval - (undocumented)unit - (undocumented) Returns:(undocumented)Since: 2.0.0 canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() intervalMs public long intervalMs() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PrunedFilteredScan (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PrunedFilteredScan (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Interface PrunedFilteredScan public interface PrunedFilteredScan ::DeveloperApi:: A BaseRelation that can eliminate unneeded columns and filter using selected predicates before producing an RDD containing all matching tuples as Row objects. The actual filter should be the conjunction of all filters, i.e. they should be "and" together. The pushed down filters are currently purely an optimization as they will all be evaluated again. This means it is safe to use them with methods that produce false positives such as filtering partitions based on a bloom filter. Since: 1.3.0 Method Summary Methods  Modifier and Type Method and Description RDD<Row> buildScan(String[] requiredColumns, Filter[] filters)  Method Detail buildScan RDD<Row> buildScan(String[] requiredColumns, Filter[] filters) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method PrunedScan (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="PrunedScan (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql.sources Interface PrunedScan public interface PrunedScan ::DeveloperApi:: A BaseRelation that can eliminate unneeded columns before producing an RDD containing all of its tuples as Row objects. Since: 1.3.0 Method Summary Methods  Modifier and Type Method and Description RDD<Row> buildScan(String[] requiredColumns)  Method Detail buildScan RDD<Row> buildScan(String[] requiredColumns) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method Pseudorandom (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="Pseudorandom (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.util.random Interface Pseudorandom All Known Subinterfaces: RandomDataGenerator<T>, RandomSampler<T,U> All Known Implementing Classes: BernoulliCellSampler, BernoulliSampler, ExponentialGenerator, GammaGenerator, LogNormalGenerator, PoissonGenerator, PoissonSampler, StandardNormalGenerator, UniformGenerator, WeibullGenerator public interface Pseudorandom :: DeveloperApi :: A class with pseudorandom behavior. Method Summary Methods  Modifier and Type Method and Description void setSeed(long seed) Set random seed. Method Detail setSeed void setSeed(long seed) Set random seed. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method QRDecomposition (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="QRDecomposition (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.linalg Class QRDecomposition<QType,RType> Object org.apache.spark.mllib.linalg.QRDecomposition<QType,RType> All Implemented Interfaces: java.io.Serializable, scala.Equals, scala.Product public class QRDecomposition<QType,RType> extends Object implements scala.Product, scala.Serializable Represents QR factors. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description QRDecomposition(QType Q, RType R)  Method Summary Methods  Modifier and Type Method and Description abstract static boolean canEqual(Object that)  abstract static boolean equals(Object that)  abstract static int productArity()  abstract static Object productElement(int n)  static scala.collection.Iterator<Object> productIterator()  static String productPrefix()  QType Q()  RType R()  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface scala.Product productArity, productElement, productIterator, productPrefix Methods inherited from interface scala.Equals canEqual, equals Constructor Detail QRDecomposition public QRDecomposition(QType Q, RType R) Method Detail canEqual public abstract static boolean canEqual(Object that) equals public abstract static boolean equals(Object that) productElement public abstract static Object productElement(int n) productArity public abstract static int productArity() productIterator public static scala.collection.Iterator<Object> productIterator() productPrefix public static String productPrefix() Q public QType Q() R public RType R() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method QuantileDiscretizer (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="QuantileDiscretizer (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.ml.feature Class QuantileDiscretizer Object org.apache.spark.ml.PipelineStage org.apache.spark.ml.Estimator<Bucketizer> org.apache.spark.ml.feature.QuantileDiscretizer All Implemented Interfaces: java.io.Serializable, Params, DefaultParamsWritable, Identifiable, MLWritable public final class QuantileDiscretizer extends Estimator<Bucketizer> implements DefaultParamsWritable QuantileDiscretizer takes a column with continuous features and outputs a column with binned categorical features. The number of bins can be set using the numBuckets parameter. The bin ranges are chosen using an approximate algorithm (see the documentation for approxQuantile for a detailed description). The precision of the approximation can be controlled with the relativeError parameter. The lower and upper bin bounds will be -Infinity and +Infinity, covering all real values. See Also:Serialized Form Constructor Summary Constructors  Constructor and Description QuantileDiscretizer()  QuantileDiscretizer(String uid)  Method Summary Methods  Modifier and Type Method and Description static Params clear(Param<?> param)  QuantileDiscretizer copy(ParamMap extra) Creates a copy of this instance with the same UID and some extra params. static String explainParam(Param<?> param)  static String explainParams()  static ParamMap extractParamMap()  static ParamMap extractParamMap(ParamMap extra)  Bucketizer fit(Dataset<?> dataset) Fits a model to the input data. static <T> scala.Option<T> get(Param<T> param)  static <T> scala.Option<T> getDefault(Param<T> param)  static String getInputCol()  static int getNumBuckets()  int getNumBuckets()  static <T> T getOrDefault(Param<T> param)  static String getOutputCol()  static Param<Object> getParam(String paramName)  static double getRelativeError()  double getRelativeError()  static <T> boolean hasDefault(Param<T> param)  static boolean hasParam(String paramName)  static Param<String> inputCol()  static boolean isDefined(Param<?> param)  static boolean isSet(Param<?> param)  static QuantileDiscretizer load(String path)  static IntParam numBuckets()  IntParam numBuckets() Number of buckets (quantiles, or categories) into which data points are grouped. static Param<String> outputCol()  static Param<?>[] params()  static DoubleParam relativeError()  DoubleParam relativeError() Relative error (see documentation for approxQuantile for description) Must be in the range [0, 1]. static void save(String path)  static <T> Params set(Param<T> param, T value)  QuantileDiscretizer setInputCol(String value)  QuantileDiscretizer setNumBuckets(int value)  QuantileDiscretizer setOutputCol(String value)  QuantileDiscretizer setRelativeError(double value)  static String toString()  StructType transformSchema(StructType schema) :: DeveloperApi :: String uid() An immutable unique ID for the object and its derivatives. static void validateParams()  static MLWriter write()  Methods inherited from class org.apache.spark.ml.Estimator fit, fit, fit, fit Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Methods inherited from interface org.apache.spark.ml.param.Params clear, copyValues, defaultCopy, defaultParamMap, explainParam, explainParams, extractParamMap, extractParamMap, get, getDefault, getOrDefault, getParam, hasDefault, hasParam, isDefined, isSet, paramMap, params, set, set, set, setDefault, setDefault, shouldOwn, validateParams Methods inherited from interface org.apache.spark.ml.util.Identifiable toString Methods inherited from interface org.apache.spark.ml.util.DefaultParamsWritable write Methods inherited from interface org.apache.spark.ml.util.MLWritable save Constructor Detail QuantileDiscretizer public QuantileDiscretizer(String uid) QuantileDiscretizer public QuantileDiscretizer() Method Detail load public static QuantileDiscretizer load(String path) toString public static String toString() params public static Param<?>[] params() validateParams public static void validateParams() explainParam public static String explainParam(Param<?> param) explainParams public static String explainParams() isSet public static final boolean isSet(Param<?> param) isDefined public static final boolean isDefined(Param<?> param) hasParam public static boolean hasParam(String paramName) getParam public static Param<Object> getParam(String paramName) set public static final <T> Params set(Param<T> param, T value) get public static final <T> scala.Option<T> get(Param<T> param) clear public static final Params clear(Param<?> param) getOrDefault public static final <T> T getOrDefault(Param<T> param) getDefault public static final <T> scala.Option<T> getDefault(Param<T> param) hasDefault public static final <T> boolean hasDefault(Param<T> param) extractParamMap public static final ParamMap extractParamMap(ParamMap extra) extractParamMap public static final ParamMap extractParamMap() inputCol public static final Param<String> inputCol() getInputCol public static final String getInputCol() outputCol public static final Param<String> outputCol() getOutputCol public static final String getOutputCol() numBuckets public static IntParam numBuckets() getNumBuckets public static int getNumBuckets() relativeError public static DoubleParam relativeError() getRelativeError public static double getRelativeError() save public static void save(String path) throws java.io.IOException Throws: java.io.IOException write public static MLWriter write() uid public String uid() Description copied from interface: Identifiable An immutable unique ID for the object and its derivatives. Specified by: uid in interface Identifiable Returns:(undocumented) setRelativeError public QuantileDiscretizer setRelativeError(double value) setNumBuckets public QuantileDiscretizer setNumBuckets(int value) setInputCol public QuantileDiscretizer setInputCol(String value) setOutputCol public QuantileDiscretizer setOutputCol(String value) transformSchema public StructType transformSchema(StructType schema) Description copied from class: PipelineStage :: DeveloperApi :: Check transform validity and derive the output schema from the input schema. Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks. Specified by: transformSchema in class PipelineStage Parameters:schema - (undocumented) Returns:(undocumented) fit public Bucketizer fit(Dataset<?> dataset) Description copied from class: Estimator Fits a model to the input data. Specified by: fit in class Estimator<Bucketizer> Parameters:dataset - (undocumented) Returns:(undocumented) copy public QuantileDiscretizer copy(ParamMap extra) Description copied from interface: Params Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See defaultCopy(). Specified by: copy in interface Params Specified by: copy in class Estimator<Bucketizer> Parameters:extra - (undocumented) Returns:(undocumented) numBuckets public IntParam numBuckets() Number of buckets (quantiles, or categories) into which data points are grouped. Must be >= 2. default: 2 Returns:(undocumented) getNumBuckets public int getNumBuckets() relativeError public DoubleParam relativeError() Relative error (see documentation for approxQuantile for description) Must be in the range [0, 1]. default: 0.001 Returns:(undocumented) getRelativeError public double getRelativeError() Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method QuantileStrategy (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="QuantileStrategy (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.mllib.tree.configuration Class QuantileStrategy Object org.apache.spark.mllib.tree.configuration.QuantileStrategy public class QuantileStrategy extends Object Enum for selecting the quantile calculation strategy Constructor Summary Constructors  Constructor and Description QuantileStrategy()  Method Summary Methods  Modifier and Type Method and Description static scala.Enumeration.Value apply(int x)  static scala.Enumeration.Value ApproxHist()  static int maxId()  static scala.Enumeration.Value MinMax()  static scala.Enumeration.Value Sort()  static String toString()  static scala.Enumeration.ValueSet values()  static scala.Enumeration.Value withName(String s)  Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail QuantileStrategy public QuantileStrategy() Method Detail Sort public static scala.Enumeration.Value Sort() MinMax public static scala.Enumeration.Value MinMax() ApproxHist public static scala.Enumeration.Value ApproxHist() toString public static String toString() values public static scala.Enumeration.ValueSet values() maxId public static final int maxId() apply public static final scala.Enumeration.Value apply(int x) withName public static final scala.Enumeration.Value withName(String s) Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method functions (Spark 2.0.2 JavaDoc) <!-- if (location.href.indexOf('is-external=true') == -1) { parent.document.title="functions (Spark 2.0.2 JavaDoc)"; } //--> JavaScript is disabled on your browser. Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_top"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method org.apache.spark.sql Class functions Object org.apache.spark.sql.functions public class functions extends Object :: Experimental :: Functions available for DataFrame. Since: 1.3.0 Constructor Summary Constructors  Constructor and Description functions()  Method Summary Methods  Modifier and Type Method and Description static Column abs(Column e) Computes the absolute value. static Column acos(Column e) Computes the cosine inverse of the given value; the returned angle is in the range 0.0 through pi. static Column acos(String columnName) Computes the cosine inverse of the given column; the returned angle is in the range 0.0 through pi. static Column add_months(Column startDate, int numMonths) Returns the date that is numMonths after startDate. static Column approxCountDistinct(Column e) Aggregate function: returns the approximate number of distinct items in a group. static Column approxCountDistinct(Column e, double rsd) Aggregate function: returns the approximate number of distinct items in a group. static Column approxCountDistinct(String columnName) Aggregate function: returns the approximate number of distinct items in a group. static Column approxCountDistinct(String columnName, double rsd) Aggregate function: returns the approximate number of distinct items in a group. static Column array_contains(Column column, Object value) Returns true if the array contains value static Column array(Column... cols) Creates a new array column. static Column array(scala.collection.Seq<Column> cols) Creates a new array column. static Column array(String colName, scala.collection.Seq<String> colNames) Creates a new array column. static Column array(String colName, String... colNames) Creates a new array column. static Column asc(String columnName) Returns a sort expression based on ascending order of the column. static Column ascii(Column e) Computes the numeric value of the first character of the string column, and returns the result as an int column. static Column asin(Column e) Computes the sine inverse of the given value; the returned angle is in the range -pi/2 through pi/2. static Column asin(String columnName) Computes the sine inverse of the given column; the returned angle is in the range -pi/2 through pi/2. static Column atan(Column e) Computes the tangent inverse of the given value. static Column atan(String columnName) Computes the tangent inverse of the given column. static Column atan2(Column l, Column r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). static Column atan2(Column l, double r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). static Column atan2(Column l, String rightName) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). static Column atan2(double l, Column r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). static Column atan2(double l, String rightName) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). static Column atan2(String leftName, Column r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). static Column atan2(String leftName, double r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). static Column atan2(String leftName, String rightName) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). static Column avg(Column e) Aggregate function: returns the average of the values in a group. static Column avg(String columnName) Aggregate function: returns the average of the values in a group. static Column base64(Column e) Computes the BASE64 encoding of a binary column and returns it as a string column. static Column bin(Column e) An expression that returns the string representation of the binary value of the given long column. static Column bin(String columnName) An expression that returns the string representation of the binary value of the given long column. static Column bitwiseNOT(Column e) Computes bitwise NOT. static <T> Dataset<T> broadcast(Dataset<T> df) Marks a DataFrame as small enough for use in broadcast joins. static Column bround(Column e) Returns the value of the column e rounded to 0 decimal places with HALF_EVEN round mode. static Column bround(Column e, int scale) Round the value of e to scale decimal places with HALF_EVEN round mode if scale >= 0 or at integral part when scale < 0. static Column callUDF(String udfName, Column... cols) Call an user-defined function. static Column callUDF(String udfName, scala.collection.Seq<Column> cols) Call an user-defined function. static Column cbrt(Column e) Computes the cube-root of the given value. static Column cbrt(String columnName) Computes the cube-root of the given column. static Column ceil(Column e) Computes the ceiling of the given value. static Column ceil(String columnName) Computes the ceiling of the given column. static Column coalesce(Column... e) Returns the first column that is not null, or null if all inputs are null. static Column coalesce(scala.collection.Seq<Column> e) Returns the first column that is not null, or null if all inputs are null. static Column col(String colName) Returns a Column based on the given column name. static Column collect_list(Column e) Aggregate function: returns a list of objects with duplicates. static Column collect_list(String columnName) Aggregate function: returns a list of objects with duplicates. static Column collect_set(Column e) Aggregate function: returns a set of objects with duplicate elements eliminated. static Column collect_set(String columnName) Aggregate function: returns a set of objects with duplicate elements eliminated. static Column column(String colName) Returns a Column based on the given column name. static Column concat_ws(String sep, Column... exprs) Concatenates multiple input string columns together into a single string column, using the given separator. static Column concat_ws(String sep, scala.collection.Seq<Column> exprs) Concatenates multiple input string columns together into a single string column, using the given separator. static Column concat(Column... exprs) Concatenates multiple input string columns together into a single string column. static Column concat(scala.collection.Seq<Column> exprs) Concatenates multiple input string columns together into a single string column. static Column conv(Column num, int fromBase, int toBase) Convert a number in a string column from one base to another. static Column corr(Column column1, Column column2) Aggregate function: returns the Pearson Correlation Coefficient for two columns. static Column corr(String columnName1, String columnName2) Aggregate function: returns the Pearson Correlation Coefficient for two columns. static Column cos(Column e) Computes the cosine of the given value. static Column cos(String columnName) Computes the cosine of the given column. static Column cosh(Column e) Computes the hyperbolic cosine of the given value. static Column cosh(String columnName) Computes the hyperbolic cosine of the given column. static Column count(Column e) Aggregate function: returns the number of items in a group. static TypedColumn<Object,Object> count(String columnName) Aggregate function: returns the number of items in a group. static Column countDistinct(Column expr, Column... exprs) Aggregate function: returns the number of distinct items in a group. static Column countDistinct(Column expr, scala.collection.Seq<Column> exprs) Aggregate function: returns the number of distinct items in a group. static Column countDistinct(String columnName, scala.collection.Seq<String> columnNames) Aggregate function: returns the number of distinct items in a group. static Column countDistinct(String columnName, String... columnNames) Aggregate function: returns the number of distinct items in a group. static Column covar_pop(Column column1, Column column2) Aggregate function: returns the population covariance for two columns. static Column covar_pop(String columnName1, String columnName2) Aggregate function: returns the population covariance for two columns. static Column covar_samp(Column column1, Column column2) Aggregate function: returns the sample covariance for two columns. static Column covar_samp(String columnName1, String columnName2) Aggregate function: returns the sample covariance for two columns. static Column crc32(Column e) Calculates the cyclic redundancy check value (CRC32) of a binary column and returns the value as a bigint. static Column cume_dist() Window function: returns the cumulative distribution of values within a window partition, i.e. static Column current_date() Returns the current date as a date column. static Column current_timestamp() Returns the current timestamp as a timestamp column. static Column date_add(Column start, int days) Returns the date that is days days after start static Column date_format(Column dateExpr, String format) Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. static Column date_sub(Column start, int days) Returns the date that is days days before start static Column datediff(Column end, Column start) Returns the number of days from start to end. static Column dayofmonth(Column e) Extracts the day of the month as an integer from a given date/timestamp/string. static Column dayofyear(Column e) Extracts the day of the year as an integer from a given date/timestamp/string. static Column decode(Column value, String charset) Computes the first argument into a string from a binary using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'). static Column dense_rank() Window function: returns the rank of rows within a window partition, without any gaps. static Column desc(String columnName) Returns a sort expression based on the descending order of the column. static Column encode(Column value, String charset) Computes the first argument into a binary from a string using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'). static Column exp(Column e) Computes the exponential of the given value. static Column exp(String columnName) Computes the exponential of the given column. static Column explode(Column e) Creates a new row for each element in the given array or map column. static Column expm1(Column e) Computes the exponential of the given value minus one. static Column expm1(String columnName) Computes the exponential of the given column. static Column expr(String expr) Parses the expression string into the column that it represents, similar to DataFrame.selectExpr static Column factorial(Column e) Computes the factorial of the given value. static Column first(Column e) Aggregate function: returns the first value in a group. static Column first(Column e, boolean ignoreNulls) Aggregate function: returns the first value in a group. static Column first(String columnName) Aggregate function: returns the first value of a column in a group. static Column first(String columnName, boolean ignoreNulls) Aggregate function: returns the first value of a column in a group. static Column floor(Column e) Computes the floor of the given value. static Column floor(String columnName) Computes the floor of the given column. static Column format_number(Column x, int d) Formats numeric column x to a format like '#,###,###.##', rounded to d decimal places, and returns the result as a string column. static Column format_string(String format, Column... arguments) Formats the arguments in printf-style and returns the result as a string column. static Column format_string(String format, scala.collection.Seq<Column> arguments) Formats the arguments in printf-style and returns the result as a string column. static Column from_unixtime(Column ut) Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format. static Column from_unixtime(Column ut, String f) Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format. static Column from_utc_timestamp(Column ts, String tz) Assumes given timestamp is UTC and converts to given timezone. static Column get_json_object(Column e, String path) Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. static Column greatest(Column... exprs) Returns the greatest value of the list of values, skipping null values. static Column greatest(scala.collection.Seq<Column> exprs) Returns the greatest value of the list of values, skipping null values. static Column greatest(String columnName, scala.collection.Seq<String> columnNames) Returns the greatest value of the list of column names, skipping null values. static Column greatest(String columnName, String... columnNames) Returns the greatest value of the list of column names, skipping null values. static Column grouping_id(scala.collection.Seq<Column> cols) Aggregate function: returns the level of grouping, equals to static Column grouping_id(String colName, scala.collection.Seq<String> colNames) Aggregate function: returns the level of grouping, equals to static Column grouping(Column e) Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set. static Column grouping(String columnName) Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set. static Column hash(Column... cols) Calculates the hash code of given columns, and returns the result as an int column. static Column hash(scala.collection.Seq<Column> cols) Calculates the hash code of given columns, and returns the result as an int column. static Column hex(Column column) Computes hex value of the given column. static Column hour(Column e) Extracts the hours as an integer from a given date/timestamp/string. static Column hypot(Column l, Column r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. static Column hypot(Column l, double r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. static Column hypot(Column l, String rightName) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. static Column hypot(double l, Column r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. static Column hypot(double l, String rightName) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. static Column hypot(String leftName, Column r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. static Column hypot(String leftName, double r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. static Column hypot(String leftName, String rightName) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. static Column initcap(Column e) Returns a new string column by converting the first letter of each word to uppercase. static Column input_file_name() Creates a string column for the file name of the current Spark task. static Column instr(Column str, String substring) Locate the position of the first occurrence of substr column in the given string. static Column isnan(Column e) Return true iff the column is NaN. static Column isnull(Column e) Return true iff the column is null. static Column json_tuple(Column json, scala.collection.Seq<String> fields) Creates a new row for a json column according to the given field names. static Column json_tuple(Column json, String... fields) Creates a new row for a json column according to the given field names. static Column kurtosis(Column e) Aggregate function: returns the kurtosis of the values in a group. static Column kurtosis(String columnName) Aggregate function: returns the kurtosis of the values in a group. static Column lag(Column e, int offset) Window function: returns the value that is offset rows before the current row, and null if there is less than offset rows before the current row. static Column lag(Column e, int offset, Object defaultValue) Window function: returns the value that is offset rows before the current row, and defaultValue if there is less than offset rows before the current row. static Column lag(String columnName, int offset) Window function: returns the value that is offset rows before the current row, and null if there is less than offset rows before the current row. static Column lag(String columnName, int offset, Object defaultValue) Window function: returns the value that is offset rows before the current row, and defaultValue if there is less than offset rows before the current row. static Column last_day(Column e) Given a date column, returns the last day of the month which the given date belongs to. static Column last(Column e) Aggregate function: returns the last value in a group. static Column last(Column e, boolean ignoreNulls) Aggregate function: returns the last value in a group. static Column last(String columnName) Aggregate function: returns the last value of the column in a group. static Column last(String columnName, boolean ignoreNulls) Aggregate function: returns the last value of the column in a group. static Column lead(Column e, int offset) Window function: returns the value that is offset rows after the current row, and null if there is less than offset rows after the current row. static Column lead(Column e, int offset, Object defaultValue) Window function: returns the value that is offset rows after the current row, and defaultValue if there is less than offset rows after the current row. static Column lead(String columnName, int offset) Window function: returns the value that is offset rows after the current row, and null if there is less than offset rows after the current row. static Column lead(String columnName, int offset, Object defaultValue) Window function: returns the value that is offset rows after the current row, and defaultValue if there is less than offset rows after the current row. static Column least(Column... exprs) Returns the least value of the list of values, skipping null values. static Column least(scala.collection.Seq<Column> exprs) Returns the least value of the list of values, skipping null values. static Column least(String columnName, scala.collection.Seq<String> columnNames) Returns the least value of the list of column names, skipping null values. static Column least(String columnName, String... columnNames) Returns the least value of the list of column names, skipping null values. static Column length(Column e) Computes the length of a given string or binary column. static Column levenshtein(Column l, Column r) Computes the Levenshtein distance of the two given string columns. static Column lit(Object literal) Creates a Column of literal value. static Column locate(String substr, Column str) Locate the position of the first occurrence of substr. static Column locate(String substr, Column str, int pos) Locate the position of the first occurrence of substr in a string column, after position pos. static Column log(Column e) Computes the natural logarithm of the given value. static Column log(double base, Column a) Returns the first argument-base logarithm of the second argument. static Column log(double base, String columnName) Returns the first argument-base logarithm of the second argument. static Column log(String columnName) Computes the natural logarithm of the given column. static Column log10(Column e) Computes the logarithm of the given value in base 10. static Column log10(String columnName) Computes the logarithm of the given value in base 10. static Column log1p(Column e) Computes the natural logarithm of the given value plus one. static Column log1p(String columnName) Computes the natural logarithm of the given column plus one. static Column log2(Column expr) Computes the logarithm of the given column in base 2. static Column log2(String columnName) Computes the logarithm of the given value in base 2. static Column lower(Column e) Converts a string column to lower case. static Column lpad(Column str, int len, String pad) Left-pad the string column with static Column ltrim(Column e) Trim the spaces from left end for the specified string value. static Column map(Column... cols) Creates a new map column. static Column map(scala.collection.Seq<Column> cols) Creates a new map column. static Column max(Column e) Aggregate function: returns the maximum value of the expression in a group. static Column max(String columnName) Aggregate function: returns the maximum value of the column in a group. static Column md5(Column e) Calculates the MD5 digest of a binary column and returns the value as a 32 character hex string. static Column mean(Column e) Aggregate function: returns the average of the values in a group. static Column mean(String columnName) Aggregate function: returns the average of the values in a group. static Column min(Column e) Aggregate function: returns the minimum value of the expression in a group. static Column min(String columnName) Aggregate function: returns the minimum value of the column in a group. static Column minute(Column e) Extracts the minutes as an integer from a given date/timestamp/string. static Column monotonically_increasing_id() A column expression that generates monotonically increasing 64-bit integers. static Column monotonicallyIncreasingId() Deprecated.  Use monotonically_increasing_id(). Since 2.0.0. static Column month(Column e) Extracts the month as an integer from a given date/timestamp/string. static Column months_between(Column date1, Column date2) Returns number of months between dates date1 and date2. static Column nanvl(Column col1, Column col2) Returns col1 if it is not NaN, or col2 if col1 is NaN. static Column negate(Column e) Unary minus, i.e. static Column next_day(Column date, String dayOfWeek) Given a date column, returns the first date which is later than the value of the date column that is on the specified day of the week. static Column not(Column e) Inversion of boolean expression, i.e. static Column ntile(int n) Window function: returns the ntile group id (from 1 to n inclusive) in an ordered window partition. static Column percent_rank() Window function: returns the relative rank (i.e. static Column pmod(Column dividend, Column divisor) Returns the positive value of dividend mod divisor. static Column posexplode(Column e) Creates a new row for each element with position in the given array or map column. static Column pow(Column l, Column r) Returns the value of the first argument raised to the power of the second argument. static Column pow(Column l, double r) Returns the value of the first argument raised to the power of the second argument. static Column pow(Column l, String rightName) Returns the value of the first argument raised to the power of the second argument. static Column pow(double l, Column r) Returns the value of the first argument raised to the power of the second argument. static Column pow(double l, String rightName) Returns the value of the first argument raised to the power of the second argument. static Column pow(String leftName, Column r) Returns the value of the first argument raised to the power of the second argument. static Column pow(String leftName, double r) Returns the value of the first argument raised to the power of the second argument. static Column pow(String leftName, String rightName) Returns the value of the first argument raised to the power of the second argument. static Column quarter(Column e) Extracts the quarter as an integer from a given date/timestamp/string. static Column rand() Generate a random column with i.i.d. static Column rand(long seed) Generate a random column with i.i.d. static Column randn() Generate a column with i.i.d. static Column randn(long seed) Generate a column with i.i.d. static Column rank() Window function: returns the rank of rows within a window partition. static Column regexp_extract(Column e, String exp, int groupIdx) Extract a specific group matched by a Java regex, from the specified string column. static Column regexp_replace(Column e, String pattern, String replacement) Replace all substrings of the specified string value that match regexp with rep. static Column repeat(Column str, int n) Repeats a string column n times, and returns it as a new string column. static Column reverse(Column str) Reverses the string column and returns it as a new string column. static Column rint(Column e) Returns the double value that is closest in value to the argument and is equal to a mathematical integer. static Column rint(String columnName) Returns the double value that is closest in value to the argument and is equal to a mathematical integer. static Column round(Column e) Returns the value of the column e rounded to 0 decimal places. static Column round(Column e, int scale) Round the value of e to scale decimal places if scale >= 0 or at integral part when scale < 0. static Column row_number() Window function: returns a sequential number starting at 1 within a window partition. static Column rpad(Column str, int len, String pad) Right-padded with pad to a length of len. static Column rtrim(Column e) Trim the spaces from right end for the specified string value. static Column second(Column e) Extracts the seconds as an integer from a given date/timestamp/string. static Column sha1(Column e) Calculates the SHA-1 digest of a binary column and returns the value as a 40 character hex string. static Column sha2(Column e, int numBits) Calculates the SHA-2 family of hash functions of a binary column and returns the value as a hex string. static Column shiftLeft(Column e, int numBits) Shift the given value numBits left. static Column shiftRight(Column e, int numBits) Shift the given value numBits right. static Column shiftRightUnsigned(Column e, int numBits) Unsigned shift the given value numBits right. static Column signum(Column e) Computes the signum of the given value. static Column signum(String columnName) Computes the signum of the given column. static Column sin(Column e) Computes the sine of the given value. static Column sin(String columnName) Computes the sine of the given column. static Column sinh(Column e) Computes the hyperbolic sine of the given value. static Column sinh(String columnName) Computes the hyperbolic sine of the given column. static Column size(Column e) Returns length of array or map. static Column skewness(Column e) Aggregate function: returns the skewness of the values in a group. static Column skewness(String columnName) Aggregate function: returns the skewness of the values in a group. static Column sort_array(Column e) Sorts the input array for the given column in ascending order, according to the natural ordering of the array elements. static Column sort_array(Column e, boolean asc) Sorts the input array for the given column in ascending / descending order, according to the natural ordering of the array elements. static Column soundex(Column e) * Return the soundex code for the specified expression. static Column spark_partition_id() Partition ID of the Spark task. static Column split(Column str, String pattern) Splits str around pattern (pattern is a regular expression). static Column sqrt(Column e) Computes the square root of the specified float value. static Column sqrt(String colName) Computes the square root of the specified float value. static Column stddev_pop(Column e) Aggregate function: returns the population standard deviation of the expression in a group. static Column stddev_pop(String columnName) Aggregate function: returns the population standard deviation of the expression in a group. static Column stddev_samp(Column e) Aggregate function: returns the sample standard deviation of the expression in a group. static Column stddev_samp(String columnName) Aggregate function: returns the sample standard deviation of the expression in a group. static Column stddev(Column e) Aggregate function: alias for stddev_samp. static Column stddev(String columnName) Aggregate function: alias for stddev_samp. static Column struct(Column... cols) Creates a new struct column. static Column struct(scala.collection.Seq<Column> cols) Creates a new struct column. static Column struct(String colName, scala.collection.Seq<String> colNames) Creates a new struct column that composes multiple input columns. static Column struct(String colName, String... colNames) Creates a new struct column that composes multiple input columns. static Column substring_index(Column str, String delim, int count) Returns the substring from string str before count occurrences of the delimiter delim. static Column substring(Column str, int pos, int len) Substring starts at pos and is of length len when str is String type or returns the slice of byte array that starts at pos in byte and is of length len when str is Binary type static Column sum(Column e) Aggregate function: returns the sum of all values in the expression. static Column sum(String columnName) Aggregate function: returns the sum of all values in the given column. static Column sumDistinct(Column e) Aggregate function: returns the sum of distinct values in the expression. static Column sumDistinct(String columnName) Aggregate function: returns the sum of distinct values in the expression. static Column tan(Column e) Computes the tangent of the given value. static Column tan(String columnName) Computes the tangent of the given column. static Column tanh(Column e) Computes the hyperbolic tangent of the given value. static Column tanh(String columnName) Computes the hyperbolic tangent of the given column. static Column to_date(Column e) Converts the column into DateType. static Column to_utc_timestamp(Column ts, String tz) Assumes given timestamp is in given timezone and converts to UTC. static Column toDegrees(Column e) Converts an angle measured in radians to an approximately equivalent angle measured in degrees. static Column toDegrees(String columnName) Converts an angle measured in radians to an approximately equivalent angle measured in degrees. static Column toRadians(Column e) Converts an angle measured in degrees to an approximately equivalent angle measured in radians. static Column toRadians(String columnName) Converts an angle measured in degrees to an approximately equivalent angle measured in radians. static Column translate(Column src, String matchingString, String replaceString) Translate any character in the src by a character in replaceString. static Column trim(Column e) Trim the spaces from both ends for the specified string column. static Column trunc(Column date, String format) Returns date truncated to the unit specified by the format. static <RT> UserDefinedFunction udf(scala.Function0<RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$1) Defines a user-defined function of 0 arguments as user-defined function (UDF). static <RT,A1> UserDefinedFunction udf(scala.Function1<A1,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$2, scala.reflect.api.TypeTags.TypeTag<A1> evidence$3) Defines a user-defined function of 1 arguments as user-defined function (UDF). static <RT,A1,A2,A3,A4,A5,A6,A7,A8,A9,A10> UserDefinedFunction udf(scala.Function10<A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$56, scala.reflect.api.TypeTags.TypeTag<A1> evidence$57, scala.reflect.api.TypeTags.TypeTag<A2> evidence$58, scala.reflect.api.TypeTags.TypeTag<A3> evidence$59, scala.reflect.api.TypeTags.TypeTag<A4> evidence$60, scala.reflect.api.TypeTags.TypeTag<A5> evidence$61, scala.reflect.api.TypeTags.TypeTag<A6> evidence$62, scala.reflect.api.TypeTags.TypeTag<A7> evidence$63, scala.reflect.api.TypeTags.TypeTag<A8> evidence$64, scala.reflect.api.TypeTags.TypeTag<A9> evidence$65, scala.reflect.api.TypeTags.TypeTag<A10> evidence$66) Defines a user-defined function of 10 arguments as user-defined function (UDF). static <RT,A1,A2> UserDefinedFunction udf(scala.Function2<A1,A2,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$4, scala.reflect.api.TypeTags.TypeTag<A1> evidence$5, scala.reflect.api.TypeTags.TypeTag<A2> evidence$6) Defines a user-defined function of 2 arguments as user-defined function (UDF). static <RT,A1,A2,A3> UserDefinedFunction udf(scala.Function3<A1,A2,A3,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$7, scala.reflect.api.TypeTags.TypeTag<A1> evidence$8, scala.reflect.api.TypeTags.TypeTag<A2> evidence$9, scala.reflect.api.TypeTags.TypeTag<A3> evidence$10) Defines a user-defined function of 3 arguments as user-defined function (UDF). static <RT,A1,A2,A3,A4> UserDefinedFunction udf(scala.Function4<A1,A2,A3,A4,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$11, scala.reflect.api.TypeTags.TypeTag<A1> evidence$12, scala.reflect.api.TypeTags.TypeTag<A2> evidence$13, scala.reflect.api.TypeTags.TypeTag<A3> evidence$14, scala.reflect.api.TypeTags.TypeTag<A4> evidence$15) Defines a user-defined function of 4 arguments as user-defined function (UDF). static <RT,A1,A2,A3,A4,A5> UserDefinedFunction udf(scala.Function5<A1,A2,A3,A4,A5,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$16, scala.reflect.api.TypeTags.TypeTag<A1> evidence$17, scala.reflect.api.TypeTags.TypeTag<A2> evidence$18, scala.reflect.api.TypeTags.TypeTag<A3> evidence$19, scala.reflect.api.TypeTags.TypeTag<A4> evidence$20, scala.reflect.api.TypeTags.TypeTag<A5> evidence$21) Defines a user-defined function of 5 arguments as user-defined function (UDF). static <RT,A1,A2,A3,A4,A5,A6> UserDefinedFunction udf(scala.Function6<A1,A2,A3,A4,A5,A6,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$22, scala.reflect.api.TypeTags.TypeTag<A1> evidence$23, scala.reflect.api.TypeTags.TypeTag<A2> evidence$24, scala.reflect.api.TypeTags.TypeTag<A3> evidence$25, scala.reflect.api.TypeTags.TypeTag<A4> evidence$26, scala.reflect.api.TypeTags.TypeTag<A5> evidence$27, scala.reflect.api.TypeTags.TypeTag<A6> evidence$28) Defines a user-defined function of 6 arguments as user-defined function (UDF). static <RT,A1,A2,A3,A4,A5,A6,A7> UserDefinedFunction udf(scala.Function7<A1,A2,A3,A4,A5,A6,A7,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$29, scala.reflect.api.TypeTags.TypeTag<A1> evidence$30, scala.reflect.api.TypeTags.TypeTag<A2> evidence$31, scala.reflect.api.TypeTags.TypeTag<A3> evidence$32, scala.reflect.api.TypeTags.TypeTag<A4> evidence$33, scala.reflect.api.TypeTags.TypeTag<A5> evidence$34, scala.reflect.api.TypeTags.TypeTag<A6> evidence$35, scala.reflect.api.TypeTags.TypeTag<A7> evidence$36) Defines a user-defined function of 7 arguments as user-defined function (UDF). static <RT,A1,A2,A3,A4,A5,A6,A7,A8> UserDefinedFunction udf(scala.Function8<A1,A2,A3,A4,A5,A6,A7,A8,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$37, scala.reflect.api.TypeTags.TypeTag<A1> evidence$38, scala.reflect.api.TypeTags.TypeTag<A2> evidence$39, scala.reflect.api.TypeTags.TypeTag<A3> evidence$40, scala.reflect.api.TypeTags.TypeTag<A4> evidence$41, scala.reflect.api.TypeTags.TypeTag<A5> evidence$42, scala.reflect.api.TypeTags.TypeTag<A6> evidence$43, scala.reflect.api.TypeTags.TypeTag<A7> evidence$44, scala.reflect.api.TypeTags.TypeTag<A8> evidence$45) Defines a user-defined function of 8 arguments as user-defined function (UDF). static <RT,A1,A2,A3,A4,A5,A6,A7,A8,A9> UserDefinedFunction udf(scala.Function9<A1,A2,A3,A4,A5,A6,A7,A8,A9,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$46, scala.reflect.api.TypeTags.TypeTag<A1> evidence$47, scala.reflect.api.TypeTags.TypeTag<A2> evidence$48, scala.reflect.api.TypeTags.TypeTag<A3> evidence$49, scala.reflect.api.TypeTags.TypeTag<A4> evidence$50, scala.reflect.api.TypeTags.TypeTag<A5> evidence$51, scala.reflect.api.TypeTags.TypeTag<A6> evidence$52, scala.reflect.api.TypeTags.TypeTag<A7> evidence$53, scala.reflect.api.TypeTags.TypeTag<A8> evidence$54, scala.reflect.api.TypeTags.TypeTag<A9> evidence$55) Defines a user-defined function of 9 arguments as user-defined function (UDF). static UserDefinedFunction udf(Object f, DataType dataType) Defines a user-defined function (UDF) using a Scala closure. static Column unbase64(Column e) Decodes a BASE64 encoded string column and returns it as a binary column. static Column unhex(Column column) Inverse of hex. static Column unix_timestamp() Gets current Unix timestamp in seconds. static Column unix_timestamp(Column s) Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds), using the default timezone and the default locale, return null if fail. static Column unix_timestamp(Column s, String p) Convert time string with given pattern (see [http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html]) to Unix time stamp (in seconds), return null if fail. static Column upper(Column e) Converts a string column to upper case. static Column var_pop(Column e) Aggregate function: returns the population variance of the values in a group. static Column var_pop(String columnName) Aggregate function: returns the population variance of the values in a group. static Column var_samp(Column e) Aggregate function: returns the unbiased variance of the values in a group. static Column var_samp(String columnName) Aggregate function: returns the unbiased variance of the values in a group. static Column variance(Column e) Aggregate function: alias for var_samp. static Column variance(String columnName) Aggregate function: alias for var_samp. static Column weekofyear(Column e) Extracts the week number as an integer from a given date/timestamp/string. static Column when(Column condition, Object value) Evaluates a list of conditions and returns one of multiple possible result expressions. static Column window(Column timeColumn, String windowDuration) Generates tumbling time windows given a timestamp specifying column. static Column window(Column timeColumn, String windowDuration, String slideDuration) Bucketize rows into one or more time windows given a timestamp specifying column. static Column window(Column timeColumn, String windowDuration, String slideDuration, String startTime) Bucketize rows into one or more time windows given a timestamp specifying column. static Column year(Column e) Extracts the year as an integer from a given date/timestamp/string. Methods inherited from class Object equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait Constructor Detail functions public functions() Method Detail countDistinct public static Column countDistinct(Column expr, Column... exprs) Aggregate function: returns the number of distinct items in a group. Parameters:expr - (undocumented)exprs - (undocumented) Returns:(undocumented)Since: 1.3.0 countDistinct public static Column countDistinct(String columnName, String... columnNames) Aggregate function: returns the number of distinct items in a group. Parameters:columnName - (undocumented)columnNames - (undocumented) Returns:(undocumented)Since: 1.3.0 array public static Column array(Column... cols) Creates a new array column. The input columns must all have the same data type. Parameters:cols - (undocumented) Returns:(undocumented)Since: 1.4.0 array public static Column array(String colName, String... colNames) Creates a new array column. The input columns must all have the same data type. Parameters:colName - (undocumented)colNames - (undocumented) Returns:(undocumented)Since: 1.4.0 map public static Column map(Column... cols) Creates a new map column. The input columns must be grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...). The key columns must all have the same data type, and can't be null. The value columns must all have the same data type. Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0 coalesce public static Column coalesce(Column... e) Returns the first column that is not null, or null if all inputs are null. For example, coalesce(a, b, c) will return a if a is not null, or b if a is null and b is not null, or c if both a and b are null but c is not null. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 struct public static Column struct(Column... cols) Creates a new struct column. If the input column is a column in a DataFrame, or a derived column expression that is named (i.e. aliased), its name would be remained as the StructField's name, otherwise, the newly generated StructField's name would be auto generated as col${index + 1}, i.e. col1, col2, col3, ... Parameters:cols - (undocumented) Returns:(undocumented)Since: 1.4.0 struct public static Column struct(String colName, String... colNames) Creates a new struct column that composes multiple input columns. Parameters:colName - (undocumented)colNames - (undocumented) Returns:(undocumented)Since: 1.4.0 greatest public static Column greatest(Column... exprs) Returns the greatest value of the list of values, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. Parameters:exprs - (undocumented) Returns:(undocumented)Since: 1.5.0 greatest public static Column greatest(String columnName, String... columnNames) Returns the greatest value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. Parameters:columnName - (undocumented)columnNames - (undocumented) Returns:(undocumented)Since: 1.5.0 least public static Column least(Column... exprs) Returns the least value of the list of values, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. Parameters:exprs - (undocumented) Returns:(undocumented)Since: 1.5.0 least public static Column least(String columnName, String... columnNames) Returns the least value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. Parameters:columnName - (undocumented)columnNames - (undocumented) Returns:(undocumented)Since: 1.5.0 hash public static Column hash(Column... cols) Calculates the hash code of given columns, and returns the result as an int column. Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0 concat public static Column concat(Column... exprs) Concatenates multiple input string columns together into a single string column. Parameters:exprs - (undocumented) Returns:(undocumented)Since: 1.5.0 concat_ws public static Column concat_ws(String sep, Column... exprs) Concatenates multiple input string columns together into a single string column, using the given separator. Parameters:sep - (undocumented)exprs - (undocumented) Returns:(undocumented)Since: 1.5.0 format_string public static Column format_string(String format, Column... arguments) Formats the arguments in printf-style and returns the result as a string column. Parameters:format - (undocumented)arguments - (undocumented) Returns:(undocumented)Since: 1.5.0 json_tuple public static Column json_tuple(Column json, String... fields) Creates a new row for a json column according to the given field names. Parameters:json - (undocumented)fields - (undocumented) Returns:(undocumented)Since: 1.6.0 callUDF public static Column callUDF(String udfName, Column... cols) Call an user-defined function. Example: import org.apache.spark.sql._ val df = Seq(("id1", 1), ("id2", 4), ("id3", 5)).toDF("id", "value") val spark = df.sparkSession spark.udf.register("simpleUDF", (v: Int) => v * v) df.select($"id", callUDF("simpleUDF", $"value")) Parameters:udfName - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.5.0 col public static Column col(String colName) Returns a Column based on the given column name. Parameters:colName - (undocumented) Returns:(undocumented)Since: 1.3.0 column public static Column column(String colName) Returns a Column based on the given column name. Alias of col. Parameters:colName - (undocumented) Returns:(undocumented)Since: 1.3.0 lit public static Column lit(Object literal) Creates a Column of literal value. The passed in object is returned directly if it is already a Column. If the object is a Scala Symbol, it is converted into a Column also. Otherwise, a new Column is created to represent the literal value. Parameters:literal - (undocumented) Returns:(undocumented)Since: 1.3.0 asc public static Column asc(String columnName) Returns a sort expression based on ascending order of the column. // Sort by dept in ascending order, and then age in descending order. df.sort(asc("dept"), desc("age")) Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 desc public static Column desc(String columnName) Returns a sort expression based on the descending order of the column. // Sort by dept in ascending order, and then age in descending order. df.sort(asc("dept"), desc("age")) Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 approxCountDistinct public static Column approxCountDistinct(Column e) Aggregate function: returns the approximate number of distinct items in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 approxCountDistinct public static Column approxCountDistinct(String columnName) Aggregate function: returns the approximate number of distinct items in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 approxCountDistinct public static Column approxCountDistinct(Column e, double rsd) Aggregate function: returns the approximate number of distinct items in a group. Parameters:rsd - maximum estimation error allowed (default = 0.05) e - (undocumented) Returns:(undocumented)Since: 1.3.0 approxCountDistinct public static Column approxCountDistinct(String columnName, double rsd) Aggregate function: returns the approximate number of distinct items in a group. Parameters:rsd - maximum estimation error allowed (default = 0.05) columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 avg public static Column avg(Column e) Aggregate function: returns the average of the values in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 avg public static Column avg(String columnName) Aggregate function: returns the average of the values in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 collect_list public static Column collect_list(Column e) Aggregate function: returns a list of objects with duplicates. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 collect_list public static Column collect_list(String columnName) Aggregate function: returns a list of objects with duplicates. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 collect_set public static Column collect_set(Column e) Aggregate function: returns a set of objects with duplicate elements eliminated. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 collect_set public static Column collect_set(String columnName) Aggregate function: returns a set of objects with duplicate elements eliminated. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 corr public static Column corr(Column column1, Column column2) Aggregate function: returns the Pearson Correlation Coefficient for two columns. Parameters:column1 - (undocumented)column2 - (undocumented) Returns:(undocumented)Since: 1.6.0 corr public static Column corr(String columnName1, String columnName2) Aggregate function: returns the Pearson Correlation Coefficient for two columns. Parameters:columnName1 - (undocumented)columnName2 - (undocumented) Returns:(undocumented)Since: 1.6.0 count public static Column count(Column e) Aggregate function: returns the number of items in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 count public static TypedColumn<Object,Object> count(String columnName) Aggregate function: returns the number of items in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 countDistinct public static Column countDistinct(Column expr, scala.collection.Seq<Column> exprs) Aggregate function: returns the number of distinct items in a group. Parameters:expr - (undocumented)exprs - (undocumented) Returns:(undocumented)Since: 1.3.0 countDistinct public static Column countDistinct(String columnName, scala.collection.Seq<String> columnNames) Aggregate function: returns the number of distinct items in a group. Parameters:columnName - (undocumented)columnNames - (undocumented) Returns:(undocumented)Since: 1.3.0 covar_pop public static Column covar_pop(Column column1, Column column2) Aggregate function: returns the population covariance for two columns. Parameters:column1 - (undocumented)column2 - (undocumented) Returns:(undocumented)Since: 2.0.0 covar_pop public static Column covar_pop(String columnName1, String columnName2) Aggregate function: returns the population covariance for two columns. Parameters:columnName1 - (undocumented)columnName2 - (undocumented) Returns:(undocumented)Since: 2.0.0 covar_samp public static Column covar_samp(Column column1, Column column2) Aggregate function: returns the sample covariance for two columns. Parameters:column1 - (undocumented)column2 - (undocumented) Returns:(undocumented)Since: 2.0.0 covar_samp public static Column covar_samp(String columnName1, String columnName2) Aggregate function: returns the sample covariance for two columns. Parameters:columnName1 - (undocumented)columnName2 - (undocumented) Returns:(undocumented)Since: 2.0.0 first public static Column first(Column e, boolean ignoreNulls) Aggregate function: returns the first value in a group. The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned. Parameters:e - (undocumented)ignoreNulls - (undocumented) Returns:(undocumented)Since: 2.0.0 first public static Column first(String columnName, boolean ignoreNulls) Aggregate function: returns the first value of a column in a group. The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned. Parameters:columnName - (undocumented)ignoreNulls - (undocumented) Returns:(undocumented)Since: 2.0.0 first public static Column first(Column e) Aggregate function: returns the first value in a group. The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 first public static Column first(String columnName) Aggregate function: returns the first value of a column in a group. The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 grouping public static Column grouping(Column e) Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set. Parameters:e - (undocumented) Returns:(undocumented)Since: 2.0.0 grouping public static Column grouping(String columnName) Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 2.0.0 grouping_id public static Column grouping_id(scala.collection.Seq<Column> cols) Aggregate function: returns the level of grouping, equals to (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn) Note: the list of columns should match with grouping columns exactly, or empty (means all the grouping columns). Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0.0 grouping_id public static Column grouping_id(String colName, scala.collection.Seq<String> colNames) Aggregate function: returns the level of grouping, equals to (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn) Note: the list of columns should match with grouping columns exactly. Parameters:colName - (undocumented)colNames - (undocumented) Returns:(undocumented)Since: 2.0.0 kurtosis public static Column kurtosis(Column e) Aggregate function: returns the kurtosis of the values in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 kurtosis public static Column kurtosis(String columnName) Aggregate function: returns the kurtosis of the values in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 last public static Column last(Column e, boolean ignoreNulls) Aggregate function: returns the last value in a group. The function by default returns the last values it sees. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned. Parameters:e - (undocumented)ignoreNulls - (undocumented) Returns:(undocumented)Since: 2.0.0 last public static Column last(String columnName, boolean ignoreNulls) Aggregate function: returns the last value of the column in a group. The function by default returns the last values it sees. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned. Parameters:columnName - (undocumented)ignoreNulls - (undocumented) Returns:(undocumented)Since: 2.0.0 last public static Column last(Column e) Aggregate function: returns the last value in a group. The function by default returns the last values it sees. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 last public static Column last(String columnName) Aggregate function: returns the last value of the column in a group. The function by default returns the last values it sees. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 max public static Column max(Column e) Aggregate function: returns the maximum value of the expression in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 max public static Column max(String columnName) Aggregate function: returns the maximum value of the column in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 mean public static Column mean(Column e) Aggregate function: returns the average of the values in a group. Alias for avg. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 mean public static Column mean(String columnName) Aggregate function: returns the average of the values in a group. Alias for avg. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 min public static Column min(Column e) Aggregate function: returns the minimum value of the expression in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 min public static Column min(String columnName) Aggregate function: returns the minimum value of the column in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 skewness public static Column skewness(Column e) Aggregate function: returns the skewness of the values in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 skewness public static Column skewness(String columnName) Aggregate function: returns the skewness of the values in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 stddev public static Column stddev(Column e) Aggregate function: alias for stddev_samp. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 stddev public static Column stddev(String columnName) Aggregate function: alias for stddev_samp. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 stddev_samp public static Column stddev_samp(Column e) Aggregate function: returns the sample standard deviation of the expression in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 stddev_samp public static Column stddev_samp(String columnName) Aggregate function: returns the sample standard deviation of the expression in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 stddev_pop public static Column stddev_pop(Column e) Aggregate function: returns the population standard deviation of the expression in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 stddev_pop public static Column stddev_pop(String columnName) Aggregate function: returns the population standard deviation of the expression in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 sum public static Column sum(Column e) Aggregate function: returns the sum of all values in the expression. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 sum public static Column sum(String columnName) Aggregate function: returns the sum of all values in the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 sumDistinct public static Column sumDistinct(Column e) Aggregate function: returns the sum of distinct values in the expression. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 sumDistinct public static Column sumDistinct(String columnName) Aggregate function: returns the sum of distinct values in the expression. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.3.0 variance public static Column variance(Column e) Aggregate function: alias for var_samp. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 variance public static Column variance(String columnName) Aggregate function: alias for var_samp. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 var_samp public static Column var_samp(Column e) Aggregate function: returns the unbiased variance of the values in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 var_samp public static Column var_samp(String columnName) Aggregate function: returns the unbiased variance of the values in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 var_pop public static Column var_pop(Column e) Aggregate function: returns the population variance of the values in a group. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 var_pop public static Column var_pop(String columnName) Aggregate function: returns the population variance of the values in a group. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.6.0 cume_dist public static Column cume_dist() Window function: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row. N = total number of rows in the partition cumeDist(x) = number of values before (and including) x / N Returns:(undocumented)Since: 1.6.0 dense_rank public static Column dense_rank() Window function: returns the rank of rows within a window partition, without any gaps. The difference between rank and denseRank is that denseRank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using denseRank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Returns:(undocumented)Since: 1.6.0 lag public static Column lag(Column e, int offset) Window function: returns the value that is offset rows before the current row, and null if there is less than offset rows before the current row. For example, an offset of one will return the previous row at any given point in the window partition. This is equivalent to the LAG function in SQL. Parameters:e - (undocumented)offset - (undocumented) Returns:(undocumented)Since: 1.4.0 lag public static Column lag(String columnName, int offset) Window function: returns the value that is offset rows before the current row, and null if there is less than offset rows before the current row. For example, an offset of one will return the previous row at any given point in the window partition. This is equivalent to the LAG function in SQL. Parameters:columnName - (undocumented)offset - (undocumented) Returns:(undocumented)Since: 1.4.0 lag public static Column lag(String columnName, int offset, Object defaultValue) Window function: returns the value that is offset rows before the current row, and defaultValue if there is less than offset rows before the current row. For example, an offset of one will return the previous row at any given point in the window partition. This is equivalent to the LAG function in SQL. Parameters:columnName - (undocumented)offset - (undocumented)defaultValue - (undocumented) Returns:(undocumented)Since: 1.4.0 lag public static Column lag(Column e, int offset, Object defaultValue) Window function: returns the value that is offset rows before the current row, and defaultValue if there is less than offset rows before the current row. For example, an offset of one will return the previous row at any given point in the window partition. This is equivalent to the LAG function in SQL. Parameters:e - (undocumented)offset - (undocumented)defaultValue - (undocumented) Returns:(undocumented)Since: 1.4.0 lead public static Column lead(String columnName, int offset) Window function: returns the value that is offset rows after the current row, and null if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition. This is equivalent to the LEAD function in SQL. Parameters:columnName - (undocumented)offset - (undocumented) Returns:(undocumented)Since: 1.4.0 lead public static Column lead(Column e, int offset) Window function: returns the value that is offset rows after the current row, and null if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition. This is equivalent to the LEAD function in SQL. Parameters:e - (undocumented)offset - (undocumented) Returns:(undocumented)Since: 1.4.0 lead public static Column lead(String columnName, int offset, Object defaultValue) Window function: returns the value that is offset rows after the current row, and defaultValue if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition. This is equivalent to the LEAD function in SQL. Parameters:columnName - (undocumented)offset - (undocumented)defaultValue - (undocumented) Returns:(undocumented)Since: 1.4.0 lead public static Column lead(Column e, int offset, Object defaultValue) Window function: returns the value that is offset rows after the current row, and defaultValue if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition. This is equivalent to the LEAD function in SQL. Parameters:e - (undocumented)offset - (undocumented)defaultValue - (undocumented) Returns:(undocumented)Since: 1.4.0 ntile public static Column ntile(int n) Window function: returns the ntile group id (from 1 to n inclusive) in an ordered window partition. Fow example, if n is 4, the first quarter of the rows will get value 1, the second quarter will get 2, the third quarter will get 3, and the last quarter will get 4. This is equivalent to the NTILE function in SQL. Parameters:n - (undocumented) Returns:(undocumented)Since: 1.4.0 percent_rank public static Column percent_rank() Window function: returns the relative rank (i.e. percentile) of rows within a window partition. This is computed by: (rank of row in its partition - 1) / (number of rows in the partition - 1) This is equivalent to the PERCENT_RANK function in SQL. Returns:(undocumented)Since: 1.6.0 rank public static Column rank() Window function: returns the rank of rows within a window partition. The difference between rank and denseRank is that denseRank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using denseRank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. This is equivalent to the RANK function in SQL. Returns:(undocumented)Since: 1.4.0 row_number public static Column row_number() Window function: returns a sequential number starting at 1 within a window partition. Returns:(undocumented)Since: 1.6.0 abs public static Column abs(Column e) Computes the absolute value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 array public static Column array(scala.collection.Seq<Column> cols) Creates a new array column. The input columns must all have the same data type. Parameters:cols - (undocumented) Returns:(undocumented)Since: 1.4.0 array public static Column array(String colName, scala.collection.Seq<String> colNames) Creates a new array column. The input columns must all have the same data type. Parameters:colName - (undocumented)colNames - (undocumented) Returns:(undocumented)Since: 1.4.0 map public static Column map(scala.collection.Seq<Column> cols) Creates a new map column. The input columns must be grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...). The key columns must all have the same data type, and can't be null. The value columns must all have the same data type. Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0 broadcast public static <T> Dataset<T> broadcast(Dataset<T> df) Marks a DataFrame as small enough for use in broadcast joins. The following example marks the right DataFrame for broadcast hash join using joinKey. // left and right are DataFrames left.join(broadcast(right), "joinKey") Parameters:df - (undocumented) Returns:(undocumented)Since: 1.5.0 coalesce public static Column coalesce(scala.collection.Seq<Column> e) Returns the first column that is not null, or null if all inputs are null. For example, coalesce(a, b, c) will return a if a is not null, or b if a is null and b is not null, or c if both a and b are null but c is not null. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 input_file_name public static Column input_file_name() Creates a string column for the file name of the current Spark task. Returns:(undocumented)Since: 1.6.0 isnan public static Column isnan(Column e) Return true iff the column is NaN. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 isnull public static Column isnull(Column e) Return true iff the column is null. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.6.0 monotonicallyIncreasingId public static Column monotonicallyIncreasingId() Deprecated. Use monotonically_increasing_id(). Since 2.0.0. A column expression that generates monotonically increasing 64-bit integers. The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the record number within each partition in the lower 33 bits. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records. As an example, consider a DataFrame with two partitions, each with 3 records. This expression would return the following IDs: 0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594. Returns:(undocumented)Since: 1.4.0 monotonically_increasing_id public static Column monotonically_increasing_id() A column expression that generates monotonically increasing 64-bit integers. The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the record number within each partition in the lower 33 bits. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records. As an example, consider a DataFrame with two partitions, each with 3 records. This expression would return the following IDs: 0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594. Returns:(undocumented)Since: 1.6.0 nanvl public static Column nanvl(Column col1, Column col2) Returns col1 if it is not NaN, or col2 if col1 is NaN. Both inputs should be floating point columns (DoubleType or FloatType). Parameters:col1 - (undocumented)col2 - (undocumented) Returns:(undocumented)Since: 1.5.0 negate public static Column negate(Column e) Unary minus, i.e. negate the expression. // Select the amount column and negates all values. // Scala: df.select( -df("amount") ) // Java: df.select( negate(df.col("amount")) ); Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 not public static Column not(Column e) Inversion of boolean expression, i.e. NOT. // Scala: select rows that are not active (isActive === false) df.filter( !df("isActive") ) // Java: df.filter( not(df.col("isActive")) ); Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 rand public static Column rand(long seed) Generate a random column with i.i.d. samples from U[0.0, 1.0]. Note that this is indeterministic when data partitions are not fixed. Parameters:seed - (undocumented) Returns:(undocumented)Since: 1.4.0 rand public static Column rand() Generate a random column with i.i.d. samples from U[0.0, 1.0]. Returns:(undocumented)Since: 1.4.0 randn public static Column randn(long seed) Generate a column with i.i.d. samples from the standard normal distribution. Note that this is indeterministic when data partitions are not fixed. Parameters:seed - (undocumented) Returns:(undocumented)Since: 1.4.0 randn public static Column randn() Generate a column with i.i.d. samples from the standard normal distribution. Returns:(undocumented)Since: 1.4.0 spark_partition_id public static Column spark_partition_id() Partition ID of the Spark task. Note that this is indeterministic because it depends on data partitioning and task scheduling. Returns:(undocumented)Since: 1.6.0 sqrt public static Column sqrt(Column e) Computes the square root of the specified float value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 sqrt public static Column sqrt(String colName) Computes the square root of the specified float value. Parameters:colName - (undocumented) Returns:(undocumented)Since: 1.5.0 struct public static Column struct(scala.collection.Seq<Column> cols) Creates a new struct column. If the input column is a column in a DataFrame, or a derived column expression that is named (i.e. aliased), its name would be remained as the StructField's name, otherwise, the newly generated StructField's name would be auto generated as col${index + 1}, i.e. col1, col2, col3, ... Parameters:cols - (undocumented) Returns:(undocumented)Since: 1.4.0 struct public static Column struct(String colName, scala.collection.Seq<String> colNames) Creates a new struct column that composes multiple input columns. Parameters:colName - (undocumented)colNames - (undocumented) Returns:(undocumented)Since: 1.4.0 when public static Column when(Column condition, Object value) Evaluates a list of conditions and returns one of multiple possible result expressions. If otherwise is not defined at the end, null is returned for unmatched conditions. // Example: encoding gender string column into integer. // Scala: people.select(when(people("gender") === "male", 0) .when(people("gender") === "female", 1) .otherwise(2)) // Java: people.select(when(col("gender").equalTo("male"), 0) .when(col("gender").equalTo("female"), 1) .otherwise(2)) Parameters:condition - (undocumented)value - (undocumented) Returns:(undocumented)Since: 1.4.0 bitwiseNOT public static Column bitwiseNOT(Column e) Computes bitwise NOT. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 expr public static Column expr(String expr) Parses the expression string into the column that it represents, similar to DataFrame.selectExpr // get the number of words of each length df.groupBy(expr("length(word)")).count() Parameters:expr - (undocumented) Returns:(undocumented) acos public static Column acos(Column e) Computes the cosine inverse of the given value; the returned angle is in the range 0.0 through pi. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 acos public static Column acos(String columnName) Computes the cosine inverse of the given column; the returned angle is in the range 0.0 through pi. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 asin public static Column asin(Column e) Computes the sine inverse of the given value; the returned angle is in the range -pi/2 through pi/2. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 asin public static Column asin(String columnName) Computes the sine inverse of the given column; the returned angle is in the range -pi/2 through pi/2. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 atan public static Column atan(Column e) Computes the tangent inverse of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 atan public static Column atan(String columnName) Computes the tangent inverse of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 atan2 public static Column atan2(Column l, Column r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 atan2 public static Column atan2(Column l, String rightName) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). Parameters:l - (undocumented)rightName - (undocumented) Returns:(undocumented)Since: 1.4.0 atan2 public static Column atan2(String leftName, Column r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). Parameters:leftName - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 atan2 public static Column atan2(String leftName, String rightName) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). Parameters:leftName - (undocumented)rightName - (undocumented) Returns:(undocumented)Since: 1.4.0 atan2 public static Column atan2(Column l, double r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 atan2 public static Column atan2(String leftName, double r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). Parameters:leftName - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 atan2 public static Column atan2(double l, Column r) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 atan2 public static Column atan2(double l, String rightName) Returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). Parameters:l - (undocumented)rightName - (undocumented) Returns:(undocumented)Since: 1.4.0 bin public static Column bin(Column e) An expression that returns the string representation of the binary value of the given long column. For example, bin("12") returns "1100". Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 bin public static Column bin(String columnName) An expression that returns the string representation of the binary value of the given long column. For example, bin("12") returns "1100". Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.5.0 cbrt public static Column cbrt(Column e) Computes the cube-root of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 cbrt public static Column cbrt(String columnName) Computes the cube-root of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 ceil public static Column ceil(Column e) Computes the ceiling of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 ceil public static Column ceil(String columnName) Computes the ceiling of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 conv public static Column conv(Column num, int fromBase, int toBase) Convert a number in a string column from one base to another. Parameters:num - (undocumented)fromBase - (undocumented)toBase - (undocumented) Returns:(undocumented)Since: 1.5.0 cos public static Column cos(Column e) Computes the cosine of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 cos public static Column cos(String columnName) Computes the cosine of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 cosh public static Column cosh(Column e) Computes the hyperbolic cosine of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 cosh public static Column cosh(String columnName) Computes the hyperbolic cosine of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 exp public static Column exp(Column e) Computes the exponential of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 exp public static Column exp(String columnName) Computes the exponential of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 expm1 public static Column expm1(Column e) Computes the exponential of the given value minus one. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 expm1 public static Column expm1(String columnName) Computes the exponential of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 factorial public static Column factorial(Column e) Computes the factorial of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 floor public static Column floor(Column e) Computes the floor of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 floor public static Column floor(String columnName) Computes the floor of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 greatest public static Column greatest(scala.collection.Seq<Column> exprs) Returns the greatest value of the list of values, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. Parameters:exprs - (undocumented) Returns:(undocumented)Since: 1.5.0 greatest public static Column greatest(String columnName, scala.collection.Seq<String> columnNames) Returns the greatest value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. Parameters:columnName - (undocumented)columnNames - (undocumented) Returns:(undocumented)Since: 1.5.0 hex public static Column hex(Column column) Computes hex value of the given column. Parameters:column - (undocumented) Returns:(undocumented)Since: 1.5.0 unhex public static Column unhex(Column column) Inverse of hex. Interprets each pair of characters as a hexadecimal number and converts to the byte representation of number. Parameters:column - (undocumented) Returns:(undocumented)Since: 1.5.0 hypot public static Column hypot(Column l, Column r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 hypot public static Column hypot(Column l, String rightName) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. Parameters:l - (undocumented)rightName - (undocumented) Returns:(undocumented)Since: 1.4.0 hypot public static Column hypot(String leftName, Column r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. Parameters:leftName - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 hypot public static Column hypot(String leftName, String rightName) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. Parameters:leftName - (undocumented)rightName - (undocumented) Returns:(undocumented)Since: 1.4.0 hypot public static Column hypot(Column l, double r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 hypot public static Column hypot(String leftName, double r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. Parameters:leftName - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 hypot public static Column hypot(double l, Column r) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 hypot public static Column hypot(double l, String rightName) Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow. Parameters:l - (undocumented)rightName - (undocumented) Returns:(undocumented)Since: 1.4.0 least public static Column least(scala.collection.Seq<Column> exprs) Returns the least value of the list of values, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. Parameters:exprs - (undocumented) Returns:(undocumented)Since: 1.5.0 least public static Column least(String columnName, scala.collection.Seq<String> columnNames) Returns the least value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. Parameters:columnName - (undocumented)columnNames - (undocumented) Returns:(undocumented)Since: 1.5.0 log public static Column log(Column e) Computes the natural logarithm of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 log public static Column log(String columnName) Computes the natural logarithm of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 log public static Column log(double base, Column a) Returns the first argument-base logarithm of the second argument. Parameters:base - (undocumented)a - (undocumented) Returns:(undocumented)Since: 1.4.0 log public static Column log(double base, String columnName) Returns the first argument-base logarithm of the second argument. Parameters:base - (undocumented)columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 log10 public static Column log10(Column e) Computes the logarithm of the given value in base 10. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 log10 public static Column log10(String columnName) Computes the logarithm of the given value in base 10. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 log1p public static Column log1p(Column e) Computes the natural logarithm of the given value plus one. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 log1p public static Column log1p(String columnName) Computes the natural logarithm of the given column plus one. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 log2 public static Column log2(Column expr) Computes the logarithm of the given column in base 2. Parameters:expr - (undocumented) Returns:(undocumented)Since: 1.5.0 log2 public static Column log2(String columnName) Computes the logarithm of the given value in base 2. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.5.0 pow public static Column pow(Column l, Column r) Returns the value of the first argument raised to the power of the second argument. Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 pow public static Column pow(Column l, String rightName) Returns the value of the first argument raised to the power of the second argument. Parameters:l - (undocumented)rightName - (undocumented) Returns:(undocumented)Since: 1.4.0 pow public static Column pow(String leftName, Column r) Returns the value of the first argument raised to the power of the second argument. Parameters:leftName - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 pow public static Column pow(String leftName, String rightName) Returns the value of the first argument raised to the power of the second argument. Parameters:leftName - (undocumented)rightName - (undocumented) Returns:(undocumented)Since: 1.4.0 pow public static Column pow(Column l, double r) Returns the value of the first argument raised to the power of the second argument. Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 pow public static Column pow(String leftName, double r) Returns the value of the first argument raised to the power of the second argument. Parameters:leftName - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 pow public static Column pow(double l, Column r) Returns the value of the first argument raised to the power of the second argument. Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.4.0 pow public static Column pow(double l, String rightName) Returns the value of the first argument raised to the power of the second argument. Parameters:l - (undocumented)rightName - (undocumented) Returns:(undocumented)Since: 1.4.0 pmod public static Column pmod(Column dividend, Column divisor) Returns the positive value of dividend mod divisor. Parameters:dividend - (undocumented)divisor - (undocumented) Returns:(undocumented)Since: 1.5.0 rint public static Column rint(Column e) Returns the double value that is closest in value to the argument and is equal to a mathematical integer. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 rint public static Column rint(String columnName) Returns the double value that is closest in value to the argument and is equal to a mathematical integer. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 round public static Column round(Column e) Returns the value of the column e rounded to 0 decimal places. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 round public static Column round(Column e, int scale) Round the value of e to scale decimal places if scale >= 0 or at integral part when scale < 0. Parameters:e - (undocumented)scale - (undocumented) Returns:(undocumented)Since: 1.5.0 bround public static Column bround(Column e) Returns the value of the column e rounded to 0 decimal places with HALF_EVEN round mode. Parameters:e - (undocumented) Returns:(undocumented)Since: 2.0.0 bround public static Column bround(Column e, int scale) Round the value of e to scale decimal places with HALF_EVEN round mode if scale >= 0 or at integral part when scale < 0. Parameters:e - (undocumented)scale - (undocumented) Returns:(undocumented)Since: 2.0.0 shiftLeft public static Column shiftLeft(Column e, int numBits) Shift the given value numBits left. If the given value is a long value, this function will return a long value else it will return an integer value. Parameters:e - (undocumented)numBits - (undocumented) Returns:(undocumented)Since: 1.5.0 shiftRight public static Column shiftRight(Column e, int numBits) Shift the given value numBits right. If the given value is a long value, it will return a long value else it will return an integer value. Parameters:e - (undocumented)numBits - (undocumented) Returns:(undocumented)Since: 1.5.0 shiftRightUnsigned public static Column shiftRightUnsigned(Column e, int numBits) Unsigned shift the given value numBits right. If the given value is a long value, it will return a long value else it will return an integer value. Parameters:e - (undocumented)numBits - (undocumented) Returns:(undocumented)Since: 1.5.0 signum public static Column signum(Column e) Computes the signum of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 signum public static Column signum(String columnName) Computes the signum of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 sin public static Column sin(Column e) Computes the sine of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 sin public static Column sin(String columnName) Computes the sine of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 sinh public static Column sinh(Column e) Computes the hyperbolic sine of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 sinh public static Column sinh(String columnName) Computes the hyperbolic sine of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 tan public static Column tan(Column e) Computes the tangent of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 tan public static Column tan(String columnName) Computes the tangent of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 tanh public static Column tanh(Column e) Computes the hyperbolic tangent of the given value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 tanh public static Column tanh(String columnName) Computes the hyperbolic tangent of the given column. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 toDegrees public static Column toDegrees(Column e) Converts an angle measured in radians to an approximately equivalent angle measured in degrees. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 toDegrees public static Column toDegrees(String columnName) Converts an angle measured in radians to an approximately equivalent angle measured in degrees. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 toRadians public static Column toRadians(Column e) Converts an angle measured in degrees to an approximately equivalent angle measured in radians. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.4.0 toRadians public static Column toRadians(String columnName) Converts an angle measured in degrees to an approximately equivalent angle measured in radians. Parameters:columnName - (undocumented) Returns:(undocumented)Since: 1.4.0 md5 public static Column md5(Column e) Calculates the MD5 digest of a binary column and returns the value as a 32 character hex string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 sha1 public static Column sha1(Column e) Calculates the SHA-1 digest of a binary column and returns the value as a 40 character hex string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 sha2 public static Column sha2(Column e, int numBits) Calculates the SHA-2 family of hash functions of a binary column and returns the value as a hex string. Parameters:e - column to compute SHA-2 on.numBits - one of 224, 256, 384, or 512. Returns:(undocumented)Since: 1.5.0 crc32 public static Column crc32(Column e) Calculates the cyclic redundancy check value (CRC32) of a binary column and returns the value as a bigint. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 hash public static Column hash(scala.collection.Seq<Column> cols) Calculates the hash code of given columns, and returns the result as an int column. Parameters:cols - (undocumented) Returns:(undocumented)Since: 2.0 ascii public static Column ascii(Column e) Computes the numeric value of the first character of the string column, and returns the result as an int column. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 base64 public static Column base64(Column e) Computes the BASE64 encoding of a binary column and returns it as a string column. This is the reverse of unbase64. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 concat public static Column concat(scala.collection.Seq<Column> exprs) Concatenates multiple input string columns together into a single string column. Parameters:exprs - (undocumented) Returns:(undocumented)Since: 1.5.0 concat_ws public static Column concat_ws(String sep, scala.collection.Seq<Column> exprs) Concatenates multiple input string columns together into a single string column, using the given separator. Parameters:sep - (undocumented)exprs - (undocumented) Returns:(undocumented)Since: 1.5.0 decode public static Column decode(Column value, String charset) Computes the first argument into a string from a binary using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'). If either argument is null, the result will also be null. Parameters:value - (undocumented)charset - (undocumented) Returns:(undocumented)Since: 1.5.0 encode public static Column encode(Column value, String charset) Computes the first argument into a binary from a string using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'). If either argument is null, the result will also be null. Parameters:value - (undocumented)charset - (undocumented) Returns:(undocumented)Since: 1.5.0 format_number public static Column format_number(Column x, int d) Formats numeric column x to a format like '#,###,###.##', rounded to d decimal places, and returns the result as a string column. If d is 0, the result has no decimal point or fractional part. If d < 0, the result will be null. Parameters:x - (undocumented)d - (undocumented) Returns:(undocumented)Since: 1.5.0 format_string public static Column format_string(String format, scala.collection.Seq<Column> arguments) Formats the arguments in printf-style and returns the result as a string column. Parameters:format - (undocumented)arguments - (undocumented) Returns:(undocumented)Since: 1.5.0 initcap public static Column initcap(Column e) Returns a new string column by converting the first letter of each word to uppercase. Words are delimited by whitespace. For example, "hello world" will become "Hello World". Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 instr public static Column instr(Column str, String substring) Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null. NOTE: The position is not zero based, but 1 based index, returns 0 if substr could not be found in str. Parameters:str - (undocumented)substring - (undocumented) Returns:(undocumented)Since: 1.5.0 length public static Column length(Column e) Computes the length of a given string or binary column. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 lower public static Column lower(Column e) Converts a string column to lower case. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 levenshtein public static Column levenshtein(Column l, Column r) Computes the Levenshtein distance of the two given string columns. Parameters:l - (undocumented)r - (undocumented) Returns:(undocumented)Since: 1.5.0 locate public static Column locate(String substr, Column str) Locate the position of the first occurrence of substr. NOTE: The position is not zero based, but 1 based index, returns 0 if substr could not be found in str. Parameters:substr - (undocumented)str - (undocumented) Returns:(undocumented)Since: 1.5.0 locate public static Column locate(String substr, Column str, int pos) Locate the position of the first occurrence of substr in a string column, after position pos. NOTE: The position is not zero based, but 1 based index. returns 0 if substr could not be found in str. Parameters:substr - (undocumented)str - (undocumented)pos - (undocumented) Returns:(undocumented)Since: 1.5.0 lpad public static Column lpad(Column str, int len, String pad) Left-pad the string column with Parameters:str - (undocumented)len - (undocumented)pad - (undocumented) Returns:(undocumented)Since: 1.5.0 ltrim public static Column ltrim(Column e) Trim the spaces from left end for the specified string value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 regexp_extract public static Column regexp_extract(Column e, String exp, int groupIdx) Extract a specific group matched by a Java regex, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned. Parameters:e - (undocumented)exp - (undocumented)groupIdx - (undocumented) Returns:(undocumented)Since: 1.5.0 regexp_replace public static Column regexp_replace(Column e, String pattern, String replacement) Replace all substrings of the specified string value that match regexp with rep. Parameters:e - (undocumented)pattern - (undocumented)replacement - (undocumented) Returns:(undocumented)Since: 1.5.0 unbase64 public static Column unbase64(Column e) Decodes a BASE64 encoded string column and returns it as a binary column. This is the reverse of base64. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 rpad public static Column rpad(Column str, int len, String pad) Right-padded with pad to a length of len. Parameters:str - (undocumented)len - (undocumented)pad - (undocumented) Returns:(undocumented)Since: 1.5.0 repeat public static Column repeat(Column str, int n) Repeats a string column n times, and returns it as a new string column. Parameters:str - (undocumented)n - (undocumented) Returns:(undocumented)Since: 1.5.0 reverse public static Column reverse(Column str) Reverses the string column and returns it as a new string column. Parameters:str - (undocumented) Returns:(undocumented)Since: 1.5.0 rtrim public static Column rtrim(Column e) Trim the spaces from right end for the specified string value. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 soundex public static Column soundex(Column e) * Return the soundex code for the specified expression. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 split public static Column split(Column str, String pattern) Splits str around pattern (pattern is a regular expression). NOTE: pattern is a string representation of the regular expression. Parameters:str - (undocumented)pattern - (undocumented) Returns:(undocumented)Since: 1.5.0 substring public static Column substring(Column str, int pos, int len) Substring starts at pos and is of length len when str is String type or returns the slice of byte array that starts at pos in byte and is of length len when str is Binary type Parameters:str - (undocumented)pos - (undocumented)len - (undocumented) Returns:(undocumented)Since: 1.5.0 substring_index public static Column substring_index(Column str, String delim, int count) Returns the substring from string str before count occurrences of the delimiter delim. If count is positive, everything the left of the final delimiter (counting from left) is returned. If count is negative, every to the right of the final delimiter (counting from the right) is returned. substring_index performs a case-sensitive match when searching for delim. Parameters:str - (undocumented)delim - (undocumented)count - (undocumented) Returns:(undocumented) translate public static Column translate(Column src, String matchingString, String replaceString) Translate any character in the src by a character in replaceString. The characters in replaceString correspond to the characters in matchingString. The translate will happen when any character in the string matches the character in the matchingString. Parameters:src - (undocumented)matchingString - (undocumented)replaceString - (undocumented) Returns:(undocumented)Since: 1.5.0 trim public static Column trim(Column e) Trim the spaces from both ends for the specified string column. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 upper public static Column upper(Column e) Converts a string column to upper case. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 add_months public static Column add_months(Column startDate, int numMonths) Returns the date that is numMonths after startDate. Parameters:startDate - (undocumented)numMonths - (undocumented) Returns:(undocumented)Since: 1.5.0 current_date public static Column current_date() Returns the current date as a date column. Returns:(undocumented)Since: 1.5.0 current_timestamp public static Column current_timestamp() Returns the current timestamp as a timestamp column. Returns:(undocumented)Since: 1.5.0 date_format public static Column date_format(Column dateExpr, String format) Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. A pattern could be for instance dd.MM.yyyy and could return a string like '18.03.1993'. All pattern letters of SimpleDateFormat can be used. NOTE: Use when ever possible specialized functions like year. These benefit from a specialized implementation. Parameters:dateExpr - (undocumented)format - (undocumented) Returns:(undocumented)Since: 1.5.0 date_add public static Column date_add(Column start, int days) Returns the date that is days days after start Parameters:start - (undocumented)days - (undocumented) Returns:(undocumented)Since: 1.5.0 date_sub public static Column date_sub(Column start, int days) Returns the date that is days days before start Parameters:start - (undocumented)days - (undocumented) Returns:(undocumented)Since: 1.5.0 datediff public static Column datediff(Column end, Column start) Returns the number of days from start to end. Parameters:end - (undocumented)start - (undocumented) Returns:(undocumented)Since: 1.5.0 year public static Column year(Column e) Extracts the year as an integer from a given date/timestamp/string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 quarter public static Column quarter(Column e) Extracts the quarter as an integer from a given date/timestamp/string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 month public static Column month(Column e) Extracts the month as an integer from a given date/timestamp/string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 dayofmonth public static Column dayofmonth(Column e) Extracts the day of the month as an integer from a given date/timestamp/string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 dayofyear public static Column dayofyear(Column e) Extracts the day of the year as an integer from a given date/timestamp/string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 hour public static Column hour(Column e) Extracts the hours as an integer from a given date/timestamp/string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 last_day public static Column last_day(Column e) Given a date column, returns the last day of the month which the given date belongs to. For example, input "2015-07-27" returns "2015-07-31" since July 31 is the last day of the month in July 2015. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 minute public static Column minute(Column e) Extracts the minutes as an integer from a given date/timestamp/string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 months_between public static Column months_between(Column date1, Column date2) Returns number of months between dates date1 and date2. Parameters:date1 - (undocumented)date2 - (undocumented) Returns:(undocumented)Since: 1.5.0 next_day public static Column next_day(Column date, String dayOfWeek) Given a date column, returns the first date which is later than the value of the date column that is on the specified day of the week. For example, next_day('2015-07-27', "Sunday") returns 2015-08-02 because that is the first Sunday after 2015-07-27. Day of the week parameter is case insensitive, and accepts: "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun". Parameters:date - (undocumented)dayOfWeek - (undocumented) Returns:(undocumented)Since: 1.5.0 second public static Column second(Column e) Extracts the seconds as an integer from a given date/timestamp/string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 weekofyear public static Column weekofyear(Column e) Extracts the week number as an integer from a given date/timestamp/string. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 from_unixtime public static Column from_unixtime(Column ut) Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format. Parameters:ut - (undocumented) Returns:(undocumented)Since: 1.5.0 from_unixtime public static Column from_unixtime(Column ut, String f) Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format. Parameters:ut - (undocumented)f - (undocumented) Returns:(undocumented)Since: 1.5.0 unix_timestamp public static Column unix_timestamp() Gets current Unix timestamp in seconds. Returns:(undocumented)Since: 1.5.0 unix_timestamp public static Column unix_timestamp(Column s) Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds), using the default timezone and the default locale, return null if fail. Parameters:s - (undocumented) Returns:(undocumented)Since: 1.5.0 unix_timestamp public static Column unix_timestamp(Column s, String p) Convert time string with given pattern (see [http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html]) to Unix time stamp (in seconds), return null if fail. Parameters:s - (undocumented)p - (undocumented) Returns:(undocumented)Since: 1.5.0 to_date public static Column to_date(Column e) Converts the column into DateType. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 trunc public static Column trunc(Column date, String format) Returns date truncated to the unit specified by the format. Parameters:format: - 'year', 'yyyy', 'yy' for truncate by year, or 'month', 'mon', 'mm' for truncate by month date - (undocumented) Returns:(undocumented)Since: 1.5.0 from_utc_timestamp public static Column from_utc_timestamp(Column ts, String tz) Assumes given timestamp is UTC and converts to given timezone. Parameters:ts - (undocumented)tz - (undocumented) Returns:(undocumented)Since: 1.5.0 to_utc_timestamp public static Column to_utc_timestamp(Column ts, String tz) Assumes given timestamp is in given timezone and converts to UTC. Parameters:ts - (undocumented)tz - (undocumented) Returns:(undocumented)Since: 1.5.0 window public static Column window(Column timeColumn, String windowDuration, String slideDuration, String startTime) Bucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported. The following example takes the average stock price for a one minute window every 10 seconds starting 5 seconds after the hour: val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType df.groupBy(window($"time", "1 minute", "10 seconds", "5 seconds"), $"stockId") .agg(mean("price")) The windows will look like: 09:00:05-09:01:05 09:00:15-09:01:15 09:00:25-09:01:25 ... For a streaming query, you may use the function current_timestamp to generate windows on processing time. Parameters:timeColumn - The column or the expression to use as the timestamp for windowing by time. The time column must be of TimestampType.windowDuration - A string specifying the width of the window, e.g. 10 minutes, 1 second. Check CalendarInterval for valid duration identifiers. Note that the duration is a fixed length of time, and does not vary over time according to a calendar. For example, 1 day always means 86,400,000 milliseconds, not a calendar day.slideDuration - A string specifying the sliding interval of the window, e.g. 1 minute. A new window will be generated every slideDuration. Must be less than or equal to the windowDuration. Check CalendarInterval for valid duration identifiers. This duration is likewise absolute, and does not vary according to a calendar.startTime - The offset with respect to 1970-01-01 00:00:00 UTC with which to start window intervals. For example, in order to have hourly tumbling windows that start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15... provide startTime as 15 minutes. Returns:(undocumented)Since: 2.0.0 window public static Column window(Column timeColumn, String windowDuration, String slideDuration) Bucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC. The following example takes the average stock price for a one minute window every 10 seconds: val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType df.groupBy(window($"time", "1 minute", "10 seconds"), $"stockId") .agg(mean("price")) The windows will look like: 09:00:00-09:01:00 09:00:10-09:01:10 09:00:20-09:01:20 ... For a streaming query, you may use the function current_timestamp to generate windows on processing time. Parameters:timeColumn - The column or the expression to use as the timestamp for windowing by time. The time column must be of TimestampType.windowDuration - A string specifying the width of the window, e.g. 10 minutes, 1 second. Check CalendarInterval for valid duration identifiers. Note that the duration is a fixed length of time, and does not vary over time according to a calendar. For example, 1 day always means 86,400,000 milliseconds, not a calendar day.slideDuration - A string specifying the sliding interval of the window, e.g. 1 minute. A new window will be generated every slideDuration. Must be less than or equal to the windowDuration. Check CalendarInterval for valid duration identifiers. This duration is likewise absolute, and does not vary according to a calendar. Returns:(undocumented)Since: 2.0.0 window public static Column window(Column timeColumn, String windowDuration) Generates tumbling time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC. The following example takes the average stock price for a one minute tumbling window: val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType df.groupBy(window($"time", "1 minute"), $"stockId") .agg(mean("price")) The windows will look like: 09:00:00-09:01:00 09:01:00-09:02:00 09:02:00-09:03:00 ... For a streaming query, you may use the function current_timestamp to generate windows on processing time. Parameters:timeColumn - The column or the expression to use as the timestamp for windowing by time. The time column must be of TimestampType.windowDuration - A string specifying the width of the window, e.g. 10 minutes, 1 second. Check CalendarInterval for valid duration identifiers. Returns:(undocumented)Since: 2.0.0 array_contains public static Column array_contains(Column column, Object value) Returns true if the array contains value Parameters:column - (undocumented)value - (undocumented) Returns:(undocumented)Since: 1.5.0 explode public static Column explode(Column e) Creates a new row for each element in the given array or map column. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.3.0 posexplode public static Column posexplode(Column e) Creates a new row for each element with position in the given array or map column. Parameters:e - (undocumented) Returns:(undocumented)Since: 2.1.0 get_json_object public static Column get_json_object(Column e, String path) Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid. Parameters:e - (undocumented)path - (undocumented) Returns:(undocumented)Since: 1.6.0 json_tuple public static Column json_tuple(Column json, scala.collection.Seq<String> fields) Creates a new row for a json column according to the given field names. Parameters:json - (undocumented)fields - (undocumented) Returns:(undocumented)Since: 1.6.0 size public static Column size(Column e) Returns length of array or map. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 sort_array public static Column sort_array(Column e) Sorts the input array for the given column in ascending order, according to the natural ordering of the array elements. Parameters:e - (undocumented) Returns:(undocumented)Since: 1.5.0 sort_array public static Column sort_array(Column e, boolean asc) Sorts the input array for the given column in ascending / descending order, according to the natural ordering of the array elements. Parameters:e - (undocumented)asc - (undocumented) Returns:(undocumented)Since: 1.5.0 udf public static <RT> UserDefinedFunction udf(scala.Function0<RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$1) Defines a user-defined function of 0 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$1 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1> UserDefinedFunction udf(scala.Function1<A1,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$2, scala.reflect.api.TypeTags.TypeTag<A1> evidence$3) Defines a user-defined function of 1 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$2 - (undocumented)evidence$3 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1,A2> UserDefinedFunction udf(scala.Function2<A1,A2,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$4, scala.reflect.api.TypeTags.TypeTag<A1> evidence$5, scala.reflect.api.TypeTags.TypeTag<A2> evidence$6) Defines a user-defined function of 2 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$4 - (undocumented)evidence$5 - (undocumented)evidence$6 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1,A2,A3> UserDefinedFunction udf(scala.Function3<A1,A2,A3,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$7, scala.reflect.api.TypeTags.TypeTag<A1> evidence$8, scala.reflect.api.TypeTags.TypeTag<A2> evidence$9, scala.reflect.api.TypeTags.TypeTag<A3> evidence$10) Defines a user-defined function of 3 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$7 - (undocumented)evidence$8 - (undocumented)evidence$9 - (undocumented)evidence$10 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1,A2,A3,A4> UserDefinedFunction udf(scala.Function4<A1,A2,A3,A4,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$11, scala.reflect.api.TypeTags.TypeTag<A1> evidence$12, scala.reflect.api.TypeTags.TypeTag<A2> evidence$13, scala.reflect.api.TypeTags.TypeTag<A3> evidence$14, scala.reflect.api.TypeTags.TypeTag<A4> evidence$15) Defines a user-defined function of 4 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$11 - (undocumented)evidence$12 - (undocumented)evidence$13 - (undocumented)evidence$14 - (undocumented)evidence$15 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1,A2,A3,A4,A5> UserDefinedFunction udf(scala.Function5<A1,A2,A3,A4,A5,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$16, scala.reflect.api.TypeTags.TypeTag<A1> evidence$17, scala.reflect.api.TypeTags.TypeTag<A2> evidence$18, scala.reflect.api.TypeTags.TypeTag<A3> evidence$19, scala.reflect.api.TypeTags.TypeTag<A4> evidence$20, scala.reflect.api.TypeTags.TypeTag<A5> evidence$21) Defines a user-defined function of 5 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$16 - (undocumented)evidence$17 - (undocumented)evidence$18 - (undocumented)evidence$19 - (undocumented)evidence$20 - (undocumented)evidence$21 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1,A2,A3,A4,A5,A6> UserDefinedFunction udf(scala.Function6<A1,A2,A3,A4,A5,A6,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$22, scala.reflect.api.TypeTags.TypeTag<A1> evidence$23, scala.reflect.api.TypeTags.TypeTag<A2> evidence$24, scala.reflect.api.TypeTags.TypeTag<A3> evidence$25, scala.reflect.api.TypeTags.TypeTag<A4> evidence$26, scala.reflect.api.TypeTags.TypeTag<A5> evidence$27, scala.reflect.api.TypeTags.TypeTag<A6> evidence$28) Defines a user-defined function of 6 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$22 - (undocumented)evidence$23 - (undocumented)evidence$24 - (undocumented)evidence$25 - (undocumented)evidence$26 - (undocumented)evidence$27 - (undocumented)evidence$28 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1,A2,A3,A4,A5,A6,A7> UserDefinedFunction udf(scala.Function7<A1,A2,A3,A4,A5,A6,A7,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$29, scala.reflect.api.TypeTags.TypeTag<A1> evidence$30, scala.reflect.api.TypeTags.TypeTag<A2> evidence$31, scala.reflect.api.TypeTags.TypeTag<A3> evidence$32, scala.reflect.api.TypeTags.TypeTag<A4> evidence$33, scala.reflect.api.TypeTags.TypeTag<A5> evidence$34, scala.reflect.api.TypeTags.TypeTag<A6> evidence$35, scala.reflect.api.TypeTags.TypeTag<A7> evidence$36) Defines a user-defined function of 7 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$29 - (undocumented)evidence$30 - (undocumented)evidence$31 - (undocumented)evidence$32 - (undocumented)evidence$33 - (undocumented)evidence$34 - (undocumented)evidence$35 - (undocumented)evidence$36 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1,A2,A3,A4,A5,A6,A7,A8> UserDefinedFunction udf(scala.Function8<A1,A2,A3,A4,A5,A6,A7,A8,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$37, scala.reflect.api.TypeTags.TypeTag<A1> evidence$38, scala.reflect.api.TypeTags.TypeTag<A2> evidence$39, scala.reflect.api.TypeTags.TypeTag<A3> evidence$40, scala.reflect.api.TypeTags.TypeTag<A4> evidence$41, scala.reflect.api.TypeTags.TypeTag<A5> evidence$42, scala.reflect.api.TypeTags.TypeTag<A6> evidence$43, scala.reflect.api.TypeTags.TypeTag<A7> evidence$44, scala.reflect.api.TypeTags.TypeTag<A8> evidence$45) Defines a user-defined function of 8 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$37 - (undocumented)evidence$38 - (undocumented)evidence$39 - (undocumented)evidence$40 - (undocumented)evidence$41 - (undocumented)evidence$42 - (undocumented)evidence$43 - (undocumented)evidence$44 - (undocumented)evidence$45 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1,A2,A3,A4,A5,A6,A7,A8,A9> UserDefinedFunction udf(scala.Function9<A1,A2,A3,A4,A5,A6,A7,A8,A9,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$46, scala.reflect.api.TypeTags.TypeTag<A1> evidence$47, scala.reflect.api.TypeTags.TypeTag<A2> evidence$48, scala.reflect.api.TypeTags.TypeTag<A3> evidence$49, scala.reflect.api.TypeTags.TypeTag<A4> evidence$50, scala.reflect.api.TypeTags.TypeTag<A5> evidence$51, scala.reflect.api.TypeTags.TypeTag<A6> evidence$52, scala.reflect.api.TypeTags.TypeTag<A7> evidence$53, scala.reflect.api.TypeTags.TypeTag<A8> evidence$54, scala.reflect.api.TypeTags.TypeTag<A9> evidence$55) Defines a user-defined function of 9 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$46 - (undocumented)evidence$47 - (undocumented)evidence$48 - (undocumented)evidence$49 - (undocumented)evidence$50 - (undocumented)evidence$51 - (undocumented)evidence$52 - (undocumented)evidence$53 - (undocumented)evidence$54 - (undocumented)evidence$55 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static <RT,A1,A2,A3,A4,A5,A6,A7,A8,A9,A10> UserDefinedFunction udf(scala.Function10<A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,RT> f, scala.reflect.api.TypeTags.TypeTag<RT> evidence$56, scala.reflect.api.TypeTags.TypeTag<A1> evidence$57, scala.reflect.api.TypeTags.TypeTag<A2> evidence$58, scala.reflect.api.TypeTags.TypeTag<A3> evidence$59, scala.reflect.api.TypeTags.TypeTag<A4> evidence$60, scala.reflect.api.TypeTags.TypeTag<A5> evidence$61, scala.reflect.api.TypeTags.TypeTag<A6> evidence$62, scala.reflect.api.TypeTags.TypeTag<A7> evidence$63, scala.reflect.api.TypeTags.TypeTag<A8> evidence$64, scala.reflect.api.TypeTags.TypeTag<A9> evidence$65, scala.reflect.api.TypeTags.TypeTag<A10> evidence$66) Defines a user-defined function of 10 arguments as user-defined function (UDF). The data types are automatically inferred based on the function's signature. Parameters:f - (undocumented)evidence$56 - (undocumented)evidence$57 - (undocumented)evidence$58 - (undocumented)evidence$59 - (undocumented)evidence$60 - (undocumented)evidence$61 - (undocumented)evidence$62 - (undocumented)evidence$63 - (undocumented)evidence$64 - (undocumented)evidence$65 - (undocumented)evidence$66 - (undocumented) Returns:(undocumented)Since: 1.3.0 udf public static UserDefinedFunction udf(Object f, DataType dataType) Defines a user-defined function (UDF) using a Scala closure. For this variant, the caller must specify the output data type, and there is no automatic input type coercion. Parameters:f - A closure in ScaladataType - The output data type of the UDF Returns:(undocumented)Since: 2.0.0 callUDF public static Column callUDF(String udfName, scala.collection.Seq<Column> cols) Call an user-defined function. Example: import org.apache.spark.sql._ val df = Seq(("id1", 1), ("id2", 4), ("id3", 5)).toDF("id", "value") val spark = df.sparkSession spark.udf.register("simpleUDF", (v: Int) => v * v) df.select($"id", callUDF("simpleUDF", $"value")) Parameters:udfName - (undocumented)cols - (undocumented) Returns:(undocumented)Since: 1.5.0 Overview Package Class Tree Deprecated Index Help Prev Class Next Class Frames No Frames All Classes <!-- allClassesLink = document.getElementById("allclasses_navbar_bottom"); if(window==top) { allClassesLink.style.display = "block"; } else { allClassesLink.style.display = "none"; } //--> Summary:  Nested |  Field |  Constr |  Method Detail:  Field |  Constr |  Method 