createAHSClient()
getApplicationAttempts(org.apache.hadoop.yarn.api.records.ApplicationId)
getApplications()
getContainers(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
getApplicationAttemptReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
getApplicationReport(org.apache.hadoop.yarn.api.records.ApplicationId)
getContainerReport(org.apache.hadoop.yarn.api.records.ContainerId)
createAHSProxy(org.apache.hadoop.conf.Configuration, java.lang.Class, java.net.InetSocketAddress)
getProxy(org.apache.hadoop.conf.Configuration, java.lang.Class, java.net.InetSocketAddress)
valueOf(java.lang.String)
values()AMRMClient(java.lang.String)
addContainerRequest(T)
createAMRMClient()
getClusterNodeCount()
getNMTokenCache()
releaseAssignedContainer(org.apache.hadoop.yarn.api.records.ContainerId)
setNMTokenCache(org.apache.hadoop.yarn.client.api.NMTokenCache)
updateBlacklist(java.util.List, java.util.List)
waitFor(com.google.common.base.Supplier, int)
allocate(float)
getAvailableResources()
getMatchingRequests(org.apache.hadoop.yarn.api.records.Priority, java.lang.String, org.apache.hadoop.yarn.api.records.Resource)
registerApplicationMaster(java.lang.String, int, java.lang.String)
removeContainerRequest(T)
unregisterApplicationMaster(org.apache.hadoop.yarn.api.records.FinalApplicationStatus, java.lang.String, java.lang.String)
waitFor(com.google.common.base.Supplier)
waitFor(com.google.common.base.Supplier, int, int)
client
heartbeatIntervalMs
AMRMClientAsync(org.apache.hadoop.yarn.client.api.AMRMClient, int, org.apache.hadoop.yarn.client.api.async.AMRMClientAsync.CallbackHandler)
addContainerRequest(T)
createAMRMClientAsync(int, org.apache.hadoop.yarn.client.api.async.AMRMClientAsync.CallbackHandler)
getClusterNodeCount()
registerApplicationMaster(java.lang.String, int, java.lang.String)
removeContainerRequest(T)
unregisterApplicationMaster(org.apache.hadoop.yarn.api.records.FinalApplicationStatus, java.lang.String, java.lang.String)
waitFor(com.google.common.base.Supplier)
waitFor(com.google.common.base.Supplier, int, int)
handler
AMRMClientAsync(int, org.apache.hadoop.yarn.client.api.async.AMRMClientAsync.CallbackHandler)
createAMRMClientAsync(org.apache.hadoop.yarn.client.api.AMRMClient, int, org.apache.hadoop.yarn.client.api.async.AMRMClientAsync.CallbackHandler)
getAvailableResources()
getMatchingRequests(org.apache.hadoop.yarn.api.records.Priority, java.lang.String, org.apache.hadoop.yarn.api.records.Resource)
releaseAssignedContainer(org.apache.hadoop.yarn.api.records.ContainerId)
setHeartbeatInterval(int)
updateBlacklist(java.util.List, java.util.List)
waitFor(com.google.common.base.Supplier, int)
KIND_NAME
equals(java.lang.Object)
getKind()
getUser()
readFields(java.io.DataInput)
write(java.io.DataOutput)
getKeyId()
getProto()
hashCode()
toString()selectToken(org.apache.hadoop.io.Text, java.util.Collection)
LOG
countCounters()
findCounter(java.lang.Enum)
getGroup(java.lang.String)
hashCode()
iterator()
toString()
equals(java.lang.Object)
findCounter(java.lang.String, java.lang.String)
getGroupNames()
incrAllCounters(org.apache.hadoop.mapreduce.counters.AbstractCounters)
readFields(java.io.DataInput)
write(java.io.DataOutput)
AbstractDNSToSwitchMapping()
dumpTopology()
getSwitchMap()
isSingleSwitch()
setConf(org.apache.hadoop.conf.Configuration)
AbstractDNSToSwitchMapping(org.apache.hadoop.conf.Configuration)
getConf()
isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping)
isSingleSwitchByScriptPolicy()getTimestamp()
toString()
getType()statistics
checkPath(org.apache.hadoop.fs.Path)
clearStatistics()
createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
equals(java.lang.Object)
getAclStatus(org.apache.hadoop.fs.Path)
getCanonicalServiceName()
getFileChecksum(org.apache.hadoop.fs.Path)
getFileChecksum(org.apache.hadoop.fs.Path)
getFileStatus(org.apache.hadoop.fs.Path)
getFileStatus(org.apache.hadoop.fs.Path)
getFsStatus(org.apache.hadoop.fs.Path)
getFsStatus(org.apache.hadoop.fs.Path)
getInitialWorkingDirectory()
getServerDefaults()
getStatistics(java.net.URI)
getUriDefaultPort()
getXAttr(org.apache.hadoop.fs.Path, java.lang.String)
getXAttrs(org.apache.hadoop.fs.Path, java.util.List)
isValidName(java.lang.String)
listLocatedStatus(org.apache.hadoop.fs.Path)
listLocatedStatus(org.apache.hadoop.fs.Path)
listStatusIterator(org.apache.hadoop.fs.Path)
listStatus(org.apache.hadoop.fs.Path)
makeQualified(org.apache.hadoop.fs.Path)
modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List)
open(org.apache.hadoop.fs.Path, int)
open(org.apache.hadoop.fs.Path, int)
removeAcl(org.apache.hadoop.fs.Path)
removeDefaultAcl(org.apache.hadoop.fs.Path)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options.Rename...)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options.Rename...)
renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options.Rename...)
setAcl(org.apache.hadoop.fs.Path, java.util.List)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
setTimes(org.apache.hadoop.fs.Path, long, long)
setTimes(org.apache.hadoop.fs.Path, long, long)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[])
supportsSymlinks()
checkScheme(java.net.URI, java.lang.String)
create(org.apache.hadoop.fs.Path, java.util.EnumSet, org.apache.hadoop.fs.Options.CreateOpts...)
create(org.apache.hadoop.fs.Path, java.util.EnumSet, org.apache.hadoop.fs.Options.CreateOpts...)
createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options.ChecksumOpt, boolean)
create(org.apache.hadoop.fs.Path, java.util.EnumSet, org.apache.hadoop.fs.Options.CreateOpts...)
delete(org.apache.hadoop.fs.Path, boolean)
delete(org.apache.hadoop.fs.Path, boolean)
get(java.net.URI, org.apache.hadoop.conf.Configuration)
getAllStatistics()
getFileBlockLocations(org.apache.hadoop.fs.Path, long, long)
getFileBlockLocations(org.apache.hadoop.fs.Path, long, long)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getFsStatus()
getFsStatus(org.apache.hadoop.fs.Path)
getHomeDirectory()
getLinkTarget(org.apache.hadoop.fs.Path)
getStatistics()
getUri()
getUriPath(org.apache.hadoop.fs.Path)
getXAttrs(org.apache.hadoop.fs.Path)
hashCode()
listCorruptFileBlocks(org.apache.hadoop.fs.Path)
listStatus(org.apache.hadoop.fs.Path)
listXAttrs(org.apache.hadoop.fs.Path)
mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean)
mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean)
open(org.apache.hadoop.fs.Path)
open(org.apache.hadoop.fs.Path)
printStatistics()
removeAclEntries(org.apache.hadoop.fs.Path, java.util.List)
removeXAttr(org.apache.hadoop.fs.Path, java.lang.String)
renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options.Rename...)
resolvePath(org.apache.hadoop.fs.Path)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setReplication(org.apache.hadoop.fs.Path, short)
setReplication(org.apache.hadoop.fs.Path, short)
setVerifyChecksum(boolean)
setVerifyChecksum(boolean, org.apache.hadoop.fs.Path)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet)
truncate(org.apache.hadoop.fs.Path, long)
truncate(org.apache.hadoop.fs.Path, long)DEFAULT_EXPIRE
expire(O)
register(O)
serviceStart()
setExpireInterval(int)
unregister(O)
receivedPing(O)
resetTimer()
serviceStop()
setMonitorInterval(int)AbstractMapWritable()
addToMap(java.lang.Class)
getClass(byte)
getId(java.lang.Class)
setConf(org.apache.hadoop.conf.Configuration)
copy(org.apache.hadoop.io.Writable)
getConf()
readFields(java.io.DataInput)
write(java.io.DataOutput)
AbstractMetric(org.apache.hadoop.metrics2.MetricsInfo)
description()
hashCode()
name()
type()
visit(org.apache.hadoop.metrics2.MetricsVisitor)
equals(java.lang.Object)
info()
toString()
value()AbstractMetricsContext()
close()
emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord)
getAllRecords()
getAttributeTable(java.lang.String)
getContextName()
init(java.lang.String, org.apache.hadoop.metrics.ContextFactory)
newRecord(java.lang.String)
registerUpdater(org.apache.hadoop.metrics.Updater)
setPeriod(int)
stopMonitoring()
update(org.apache.hadoop.metrics.spi.MetricsRecordImpl)
createRecord(java.lang.String)
flush()
getAttribute(java.lang.String)
getContextFactory()
getPeriod()
isMonitoring()
parseAndSetPeriod(java.lang.String)
remove(org.apache.hadoop.metrics.spi.MetricsRecordImpl)
startMonitoring()
unregisterUpdater(org.apache.hadoop.metrics.Updater)close()
stop()
getConfig()
getFailureState()
getFailureCause()
getName()
getStartTime()
isInState(org.apache.hadoop.service.Service.STATE)
putBlocker(java.lang.String, java.lang.String)
registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)
serviceInit(org.apache.hadoop.conf.Configuration)
serviceStop()
start()
toString()
unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)
getBlockers()
getFailureCause()
getLifecycleHistory()
getServiceState()
init(org.apache.hadoop.conf.Configuration)
noteFailure(java.lang.Exception)
registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener)
removeBlocker(java.lang.String)
serviceStart()
setConfig(org.apache.hadoop.conf.Configuration)
stop()
unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener)
waitForServiceToStop(long)
aclSpecToString(java.util.List)
getName()
getScope()
hashCode()
parseAclSpec(java.lang.String, boolean)
equals(java.lang.Object)
getPermission()
getType()
parseAclEntry(java.lang.String, boolean)
toString()valueOf(java.lang.String)
values()valueOf(java.lang.String)
values()equals(java.lang.Object)
getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry, org.apache.hadoop.fs.permission.FsPermission)
getGroup()
getPermission()
isStickyBit()
getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry)
getEntries()
getOwner()
hashCode()
toString()addService(org.apache.hadoop.service.Service)
removeService(org.apache.hadoop.service.Service)
getKerberosInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
getTokenInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
equals(java.lang.Object)
toString()
hashCode()close()
getApplicationOwner()
readAcontainerLogs(java.io.DataInputStream, java.io.Writer)
readAContainerLogsForALogType(java.io.DataInputStream, java.io.PrintStream)
getApplicationAcls()
next(org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat.LogKey)
readAcontainerLogs(java.io.DataInputStream, java.io.Writer, long)
readAContainerLogsForALogType(java.io.DataInputStream, java.io.PrintStream, long)
getAskList()
getProgress()
getResourceBlacklistRequest()
newInstance(int, float, java.util.List, java.util.List, org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest)
setAskList(java.util.List)
setProgress(float)
setResourceBlacklistRequest(org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest)
getIncreaseRequests()
getReleaseList()
getResponseId()
newInstance(int, float, java.util.List, java.util.List, org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest, java.util.List)
setIncreaseRequests(java.util.List)
setReleaseList(java.util.List)
setResponseId(int)getAllocatedContainers()
getAMRMToken()
getCompletedContainersStatuses()
getIncreasedContainers()
getNumClusterNodes()
getResponseId()
newInstance(int, java.util.List, java.util.List, java.util.List, org.apache.hadoop.yarn.api.records.Resource, org.apache.hadoop.yarn.api.records.AMCommand, int, org.apache.hadoop.yarn.api.records.PreemptionMessage, java.util.List)
getAMCommand()
getAvailableResources()
getDecreasedContainers()
getNMTokens()
getPreemptionMessage()
getUpdatedNodes()
newInstance(int, java.util.List, java.util.List, java.util.List, org.apache.hadoop.yarn.api.records.Resource, org.apache.hadoop.yarn.api.records.AMCommand, int, org.apache.hadoop.yarn.api.records.PreemptionMessage, java.util.List, java.util.List, java.util.List)
valueOf(java.lang.String)
values()build()
equals(java.lang.Object)
getAttemptId()
toString()
compareTo(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
getApplicationId()
hashCode()
getAMContainerId()
getDiagnostics()
getOriginalTrackingUrl()
getTrackingUrl()
getApplicationAttemptId()
getHost()
getRpcPort()
getYarnApplicationAttemptState()
deleteReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationDeleteRequest)
getClusterMetrics(org.apache.hadoop.yarn.api.protocolrecords.GetClusterMetricsRequest)
getClusterNodes(org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodesRequest)
getNewApplication(org.apache.hadoop.yarn.api.protocolrecords.GetNewApplicationRequest)
getQueueInfo(org.apache.hadoop.yarn.api.protocolrecords.GetQueueInfoRequest)
moveApplicationAcrossQueues(org.apache.hadoop.yarn.api.protocolrecords.MoveApplicationAcrossQueuesRequest)
submitReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest)
forceKillApplication(org.apache.hadoop.yarn.api.protocolrecords.KillApplicationRequest)
getClusterNodeLabels(org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodeLabelsRequest)
getLabelsToNodes(org.apache.hadoop.yarn.api.protocolrecords.GetLabelsToNodesRequest)
getNodeToLabels(org.apache.hadoop.yarn.api.protocolrecords.GetNodesToLabelsRequest)
getQueueUserAcls(org.apache.hadoop.yarn.api.protocolrecords.GetQueueUserAclsInfoRequest)
submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest)
updateReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationUpdateRequest)
build()
equals(java.lang.Object)
getId()
toString()
compareTo(org.apache.hadoop.yarn.api.records.ApplicationId)
getClusterTimestamp()
hashCode()
appAttemptID
numRequestedContainers
finish()
main(java.lang.String[])
numAllocatedContainers
numTotalContainers
init(java.lang.String[])
run()allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)
registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest)
finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest)
getAMRMToken()
getApplicationResourceUsageReport()
getApplicationType()
getCurrentApplicationAttemptId()
getFinalApplicationStatus()
getHost()
getProgress()
getRpcPort()
getTrackingUrl()
getYarnApplicationState()
getApplicationId()
getApplicationTags()
getClientToAMToken()
getDiagnostics()
getFinishTime()
getName()
getQueue()
getStartTime()
getUser()getMemorySeconds()
getNumUsedContainers()
getUsedResources()
getNeededResources()
getReservedResources()
getVcoreSeconds()getAMContainerResourceRequest()
getApplicationId()
getApplicationTags()
getAttemptFailuresValidityInterval()
getLogAggregationContext()
getNodeLabelExpression()
getQueue()
getResource()
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String, java.lang.String, org.apache.hadoop.yarn.api.records.ContainerLaunchContext, boolean, boolean, int, java.lang.String, boolean, java.lang.String, org.apache.hadoop.yarn.api.records.ResourceRequest)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String, java.lang.String, org.apache.hadoop.yarn.api.records.Priority, org.apache.hadoop.yarn.api.records.ContainerLaunchContext, boolean, boolean, int, org.apache.hadoop.yarn.api.records.Resource, java.lang.String)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String, java.lang.String, org.apache.hadoop.yarn.api.records.Priority, org.apache.hadoop.yarn.api.records.ContainerLaunchContext, boolean, boolean, int, org.apache.hadoop.yarn.api.records.Resource, java.lang.String, boolean, org.apache.hadoop.yarn.api.records.LogAggregationContext)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String, java.lang.String, org.apache.hadoop.yarn.api.records.Priority, org.apache.hadoop.yarn.api.records.ContainerLaunchContext, boolean, boolean, int, org.apache.hadoop.yarn.api.records.Resource, java.lang.String, boolean, java.lang.String, java.lang.String)
setAMContainerSpec(org.apache.hadoop.yarn.api.records.ContainerLaunchContext)
setApplicationName(java.lang.String)
setApplicationType(java.lang.String)
setKeepContainersAcrossApplicationAttempts(boolean)
setMaxAppAttempts(int)
setQueue(java.lang.String)
setResource(org.apache.hadoop.yarn.api.records.Resource)
getAMContainerSpec()
getApplicationName()
getApplicationType()
getKeepContainersAcrossApplicationAttempts()
getMaxAppAttempts()
getPriority()
getReservationID()
getUnmanagedAM()
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String, java.lang.String, org.apache.hadoop.yarn.api.records.Priority, org.apache.hadoop.yarn.api.records.ContainerLaunchContext, boolean, boolean, int, org.apache.hadoop.yarn.api.records.Resource)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String, java.lang.String, org.apache.hadoop.yarn.api.records.Priority, org.apache.hadoop.yarn.api.records.ContainerLaunchContext, boolean, boolean, int, org.apache.hadoop.yarn.api.records.Resource, java.lang.String, boolean)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String, java.lang.String, org.apache.hadoop.yarn.api.records.Priority, org.apache.hadoop.yarn.api.records.ContainerLaunchContext, boolean, boolean, int, org.apache.hadoop.yarn.api.records.Resource, java.lang.String, boolean, long)
setAMContainerResourceRequest(org.apache.hadoop.yarn.api.records.ResourceRequest)
setApplicationId(org.apache.hadoop.yarn.api.records.ApplicationId)
setApplicationTags(java.util.Set)
setAttemptFailuresValidityInterval(long)
setLogAggregationContext(org.apache.hadoop.yarn.api.records.LogAggregationContext)
setNodeLabelExpression(java.lang.String)
setReservationID(org.apache.hadoop.yarn.api.records.ReservationId)
setUnmanagedAM(boolean)valueOf(java.lang.String)
values()
add(X)
close()
next(X)
reset()
clear()
hasNext()
replay(X)
get()
getDeclaredComponentType()
readFields(java.io.DataInput)
write(java.io.DataOutput)
getComponentType()
isDeclaredComponentType(java.lang.Class)
set(java.lang.Object)
get()
readFields(java.io.DataInput)
toArray()
write(java.io.DataOutput)
getValueClass()
set(org.apache.hadoop.io.Writable[])
toStrings()eventDispatchers
dispatch(org.apache.hadoop.yarn.event.Event)
isDrained()
register(java.lang.Class, org.apache.hadoop.yarn.event.EventHandler)
serviceStart()
setDrainEventsOnStop()
getEventHandler()
isEventThreadWaiting()
serviceInit(org.apache.hadoop.conf.Configuration)
serviceStop()close()
read(byte[], int, int)
tell()
length()
seek(long)
METRIC_TAG_ACCOUNT_NAME
METRIC_TAG_FILESYSTEM_ID
WASB_BYTES_WRITTEN
WASB_DIRECTORIES_CREATED
WASB_DOWNLOAD_LATENCY
WASB_FILES_CREATED
WASB_RAW_BYTES_DOWNLOADED
WASB_SERVER_ERRORS
WASB_UPLOAD_RATE
blockDownloaded(long)
clientErrorEncountered()
currentUploadBytesPerSecond(long)
directoryDeleted()
fileDeleted()
getBlockUploadLatency()
getCurrentMaximumUploadBandwidth()
getFileSystemInstanceId()
getMetricsRegistryInfo()
rawBytesUploaded(long)
setAccountName(java.lang.String)
updateBytesReadInLastSecond(long)
webResponse()
METRIC_TAG_CONTAINTER_NAME
WASB_BYTES_READ
WASB_CLIENT_ERRORS
WASB_DIRECTORIES_DELETED
WASB_DOWNLOAD_RATE
WASB_FILES_DELETED
WASB_RAW_BYTES_UPLOADED
WASB_UPLOAD_LATENCY
WASB_WEB_RESPONSES
blockUploaded(long)
currentDownloadBytesPerSecond(long)
directoryCreated()
fileCreated()
getBlockDownloadLatency()
getCurrentMaximumDownloadBandwidth()
getCurrentWebResponses()
getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean)
rawBytesDownloaded(long)
serverErrorEncountered()
setContainerName(java.lang.String)
updateBytesWrittenInLastSecond(long)createCompressor()
createInputStream(java.io.InputStream)
createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, long, long, org.apache.hadoop.io.compress.SplittableCompressionCodec.READ_MODE)
createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor)
getConf()
getDefaultExtension()
createDecompressor()
createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor)
createOutputStream(java.io.OutputStream)
getCompressorType()
getDecompressorType()
setConf(org.apache.hadoop.conf.Configuration)
split(org.apache.hadoop.conf.Configuration, java.sql.ResultSet, java.lang.String)
tryDivide(java.math.BigDecimal, java.math.BigDecimal)
compareTo(org.apache.hadoop.io.BinaryComparable)
equals(java.lang.Object)
getLength()
compareTo(byte[], int, int)
getBytes()
hashCode()LEFT_OFFSET_PROPERTY_NAME
getConf()
setConf(org.apache.hadoop.conf.Configuration)
setOffsets(org.apache.hadoop.conf.Configuration, int, int)
RIGHT_OFFSET_PROPERTY_NAME
getPartition(org.apache.hadoop.io.BinaryComparable, V, int)
getBytes()
setLeftOffset(org.apache.hadoop.conf.Configuration, int)
setRightOffset(org.apache.hadoop.conf.Configuration, int)
endMap(java.lang.String)
endVector(java.lang.String)
readBool(java.lang.String)
readByte(java.lang.String)
readFloat(java.lang.String)
readLong(java.lang.String)
startMap(java.lang.String)
startVector(java.lang.String)
endRecord(java.lang.String)
get(java.io.DataInput)
readBuffer(java.lang.String)
readDouble(java.lang.String)
readInt(java.lang.String)
readString(java.lang.String)
startRecord(java.lang.String)
endMap(java.util.TreeMap, java.lang.String)
endVector(java.util.ArrayList, java.lang.String)
startMap(java.util.TreeMap, java.lang.String)
startVector(java.util.ArrayList, java.lang.String)
writeBuffer(org.apache.hadoop.record.Buffer, java.lang.String)
writeDouble(double, java.lang.String)
writeInt(int, java.lang.String)
writeString(java.lang.String, java.lang.String)
endRecord(org.apache.hadoop.record.Record, java.lang.String)
get(java.io.DataOutput)
startRecord(org.apache.hadoop.record.Record, java.lang.String)
writeBool(boolean, java.lang.String)
writeByte(byte, java.lang.String)
writeFloat(float, java.lang.String)
writeLong(long, java.lang.String)
compress()
write(byte[], int, int)
finish()BlockDecompressorStream(java.io.InputStream)
BlockDecompressorStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, int)
decompress(byte[], int, int)
resetState()
BlockDecompressorStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor)
getCompressedData()getCachedHosts()
getLength()
getOffset()
isCorrupt()
setCorrupt(boolean)
setLength(long)
setOffset(long)
toString()
getHosts()
getNames()
getTopologyPaths()
setCachedHosts(java.lang.String[])
setHosts(java.lang.String[])
setNames(java.lang.String[])
setTopologyPaths(java.lang.String[])
getVolumeIds()
add(org.apache.hadoop.util.bloom.Key)
getVectorSize()
not()
readFields(java.io.DataInput)
write(java.io.DataOutput)
and(org.apache.hadoop.util.bloom.Filter)
membershipTest(org.apache.hadoop.util.bloom.Key)
or(org.apache.hadoop.util.bloom.Filter)
toString()
xor(org.apache.hadoop.util.bloom.Filter)BLOOM_FILE_NAME
delete(org.apache.hadoop.fs.FileSystem, java.lang.String)
HASH_COUNTsplit(org.apache.hadoop.conf.Configuration, java.sql.ResultSet, java.lang.String)
compareTo(org.apache.hadoop.io.BooleanWritable)
get()
readFields(java.io.DataInput)
toString()
equals(java.lang.Object)
hashCode()
set(boolean)
write(java.io.DataOutput)
append(byte[])
clone()
copy(byte[], int, int)
get()
getCount()
reset()
setCapacity(int)
toString(java.lang.String)
append(byte[], int, int)
compareTo(java.lang.Object)
equals(java.lang.Object)
getCapacity()
hashCode()
set(byte[])
toString()
truncate()getBuffer(boolean, int)
putBuffer(java.nio.ByteBuffer)
compareTo(org.apache.hadoop.io.ByteWritable)
get()
readFields(java.io.DataInput)
toString()
equals(java.lang.Object)
hashCode()
set(byte)
write(java.io.DataOutput)
copyBytes()
get()
getBytes()
getCapacity()
getSize()
getLength()
readFields(java.io.DataInput)
set(org.apache.hadoop.io.BytesWritable)
setSize(int)
write(java.io.DataOutput)
equals(java.lang.Object)
getBytes()
getLength()
hashCode()
set(byte[], int, int)
setCapacity(int)
toString()cluster
displayJobList(org.apache.hadoop.mapreduce.JobStatus[])
getCounter(org.apache.hadoop.mapreduce.Counters, java.lang.String, java.lang.String)
main(java.lang.String[])
displayTasks(org.apache.hadoop.mapreduce.Job, java.lang.String, java.lang.String)
getTaskLogURL(org.apache.hadoop.mapreduce.TaskAttemptID, java.lang.String)
run(java.lang.String[])
valueOf(java.lang.String)
values()rawMapping
getSwitchMap()
reloadCachedMappings()
resolve(java.util.List)
isSingleSwitch()
isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping)
reloadCachedMappings(java.util.List)
toString()setDropBehind(java.lang.Boolean)
setReadahead(java.lang.Long)
getDelegationToken()
setDelegationToken(org.apache.hadoop.yarn.api.records.Token)
addMapper(org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, org.apache.hadoop.conf.Configuration)
setup(org.apache.hadoop.mapreduce.Mapper.Context)
run(org.apache.hadoop.mapreduce.Mapper.Context)addMapper(org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, org.apache.hadoop.conf.Configuration)
setReducer(org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, org.apache.hadoop.conf.Configuration)
run(org.apache.hadoop.mapreduce.Reducer.Context)
run(org.apache.hadoop.mapreduce.Reducer.Context)
setup(org.apache.hadoop.mapreduce.Reducer.Context)
getPos()
append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable)
copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
getApproxChkSumLength(long)
getChecksumFile(org.apache.hadoop.fs.Path)
getChecksumLength(long, int)
isChecksumFile(org.apache.hadoop.fs.Path)
listStatus(org.apache.hadoop.fs.Path)
open(org.apache.hadoop.fs.Path, int)
reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long)
setReplication(org.apache.hadoop.fs.Path, short)
setWriteChecksum(boolean)
truncate(org.apache.hadoop.fs.Path, long)
completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
delete(org.apache.hadoop.fs.Path, boolean)
getBytesPerSum()
getChecksumFileLength(org.apache.hadoop.fs.Path, long)
getRawFileSystem()
listLocatedStatus(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
setConf(org.apache.hadoop.conf.Configuration)
setVerifyChecksum(boolean)
startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
SCRIPT_PATH
init(java.lang.String[])
run()
main(java.lang.String[])
createRMProxy(org.apache.hadoop.conf.Configuration, java.lang.Class)
getRMDelegationTokenService(org.apache.hadoop.conf.Configuration)
getAMRMTokenService(org.apache.hadoop.conf.Configuration)
getTokenService(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, int)
getKerberosInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
getTokenInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
release(org.apache.hadoop.yarn.api.protocolrecords.ReleaseSharedCacheResourceRequest)
use(org.apache.hadoop.yarn.api.protocolrecords.UseSharedCacheResourceRequest)
getKerberosInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
getTokenInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
KIND_NAME
equals(java.lang.Object)
getClientName()
getProto()
hashCode()
toString()
getApplicationAttemptID()
getKind()
getUser()
readFields(java.io.DataInput)
write(java.io.DataOutput)
getMasterKey(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
setMasterKey(byte[])
retrievePassword(org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier)
getTime()
cancelDelegationToken(org.apache.hadoop.security.token.Token)
getActiveTaskTrackers()
getAllJobStatuses()
getChildQueues(java.lang.String)
getDelegationToken(org.apache.hadoop.io.Text)
getJob(org.apache.hadoop.mapreduce.JobID)
getJobTrackerStatus()
getQueue(java.lang.String)
getQueues()
getStagingAreaDir()
getTaskTrackerExpiryInterval()
close()
getAllJobs()
getAllJobStatuses()
getBlackListedTaskTrackers()
getClusterStatus()
getFileSystem()
getJobHistoryUrl(org.apache.hadoop.mapreduce.JobID)
getLogParams(org.apache.hadoop.mapreduce.JobID, org.apache.hadoop.mapreduce.TaskAttemptID)
getQueueAclsForCurrentUser()
getRootQueues()
getSystemDir()
renewDelegationToken(org.apache.hadoop.security.token.Token)getBlackListedTaskTrackerCount()
getGrayListedTaskTrackerCount()
getOccupiedMapSlots()
getReduceSlotCapacity()
getReservedReduceSlots()
getRunningReduces()
getTotalJobSubmissions()
write(java.io.DataOutput)
getDecommissionedTaskTrackerCount()
getMapSlotCapacity()
getOccupiedReduceSlots()
getReservedMapSlots()
getRunningMaps()
getTaskTrackerCount()
readFields(java.io.DataInput)
UNINITIALIZED_MEMORY_VALUE
getActiveTrackerNames()
getBlacklistedTrackers()
getGraylistedTrackerNames()
getJobTrackerState()
getMapTasks()
getMaxMemory()
getNumExcludedNodes()
getTaskTrackers()
getUsedMemory()
write(java.io.DataOutput)
getBlacklistedTrackerNames()
getBlackListedTrackersInfo()
getGraylistedTrackers()
getJobTrackerStatus()
getMaxMapTasks()
getMaxReduceTasks()
getReduceTasks()
getTTExpiryInterval()
readFields(java.io.DataInput)
toString()
getCompressor(org.apache.hadoop.io.compress.CompressionCodec)
getDecompressor(org.apache.hadoop.io.compress.CompressionCodec)
getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)
returnDecompressor(org.apache.hadoop.io.compress.Decompressor)
getCompressor(org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.conf.Configuration)
getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)
returnCompressor(org.apache.hadoop.io.compress.Compressor)
SPLIT_MINSIZE_PERNODE
createPool(java.util.List)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
getSplits(org.apache.hadoop.mapreduce.JobContext)
setMaxSplitSize(long)
setMinSplitSizeRack(long)
SPLIT_MINSIZE_PERRACK
createPool(org.apache.hadoop.fs.PathFilter...)
getFileBlockLocations(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus)
isSplitable(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.fs.Path)
setMinSplitSizeNode(long)context
idx
rrConstructor
close()
getCurrentValue()
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
nextKeyValue()
curReader
progress
split
getCurrentKey()
getProgress()
initNextRecordReader()CombineFileRecordReaderWrapper(org.apache.hadoop.mapreduce.lib.input.FileInputFormat, org.apache.hadoop.mapreduce.lib.input.CombineFileSplit, org.apache.hadoop.mapreduce.TaskAttemptContext, java.lang.Integer)
close()
getCurrentValue()
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
getCurrentKey()
getProgress()
nextKeyValue()getLength()
getLengths()
getNumPaths()
getPath(int)
getStartOffsets()
toString()
getLength(int)
getLocations()
getOffset(int)
getPaths()
readFields(java.io.DataInput)
write(java.io.DataOutput)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
JOIN_COMPARATOR
addDefaults()
compose(java.lang.String, java.lang.Class, org.apache.hadoop.fs.Path...)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
setFormat(org.apache.hadoop.conf.Configuration)
JOIN_EXPR
compose(java.lang.Class, java.lang.String)
compose(java.lang.String, java.lang.Class, java.lang.String...)
getSplits(org.apache.hadoop.mapreduce.JobContext)
add(org.apache.hadoop.mapreduce.InputSplit)
getLength()
getLocation(int)
readFields(java.io.DataInput)
get(int)
getLength(int)
getLocations()
write(java.io.DataOutput)
conf
key
kids
accept(org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader.JoinCollector, K)
close()
compareTo(org.apache.hadoop.mapreduce.lib.join.ComposableRecordReader)
createTupleWritable()
getComparator()
getCurrentKey()
getDelegate()
getRecordReaderQueue()
id()
key()
setConf(org.apache.hadoop.conf.Configuration)
jc
keyclass
value
add(org.apache.hadoop.mapreduce.lib.join.ComposableRecordReader)
combine(java.lang.Object[], org.apache.hadoop.mapreduce.lib.join.TupleWritable)
createKey()
fillJoinCollector(K)
getConf()
getCurrentValue()
getProgress()
hasNext()
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
key(K)
skip(K)
STOP_ONLY_STARTED_SERVICES
addIfService(java.lang.Object)
getServices()
serviceInit(org.apache.hadoop.conf.Configuration)
serviceStop()
addService(org.apache.hadoop.service.Service)
removeService(org.apache.hadoop.service.Service)
serviceStart()ensureInflated()
readFieldsCompressed(java.io.DataInput)
readFields(java.io.DataInput)
writeCompressed(java.io.DataOutput)
write(java.io.DataOutput)
readFields(java.io.DataInput)
write(java.io.DataOutput)
createCompressor()
createInputStream(java.io.InputStream)
createOutputStream(java.io.OutputStream)
getCompressorType()
getDefaultExtension()
createDecompressor()
createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor)
createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor)
getDecompressorType()
LOG
getCodec(org.apache.hadoop.fs.Path)
getCodecByName(java.lang.String)
getCodecClasses(org.apache.hadoop.conf.Configuration)
removeSuffix(java.lang.String, java.lang.String)
toString()
getCodecByClassName(java.lang.String)
getCodecClassByName(java.lang.String)
main(java.lang.String[])
setCodecClasses(org.apache.hadoop.conf.Configuration, java.util.List)
in
CompressionInputStream(java.io.InputStream)
close()
read(byte[], int, int)
seek(long)
maxAvailableData
getPos()
resetState()
seekToNewSource(long)out
CompressionOutputStream(java.io.OutputStream)
close()
flush()
write(byte[], int, int)
finish()
resetState()compress(byte[], int, int)
finish()
getBytesRead()
needsInput()
reset()
setInput(byte[], int, int)
end()
finished()
getBytesWritten()
reinit(org.apache.hadoop.conf.Configuration)
setDictionary(byte[], int, int)buffer
compressor
CompressorStream(java.io.OutputStream)
CompressorStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor, int)
close()
finish()
write(byte[], int, int)
closed
CompressorStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor)
compress()
resetState()
write(int)getConf()
setConf(org.apache.hadoop.conf.Configuration)
addDefaultResource(java.lang.String)
addDeprecation(java.lang.String, java.lang.String[])
addDeprecation(java.lang.String, java.lang.String)
addDeprecation(java.lang.String, java.lang.String, java.lang.String)
addResource(org.apache.hadoop.conf.Configuration)
addResource(java.io.InputStream, java.lang.String)
addResource(java.lang.String)
clear()
dumpDeprecatedKeys()
get(java.lang.String, java.lang.String)
getClass(java.lang.String, java.lang.Class)
getClassByName(java.lang.String)
getClasses(java.lang.String, java.lang.Class...)
getConfResourceAsInputStream(java.lang.String)
getDouble(java.lang.String, double)
getFile(java.lang.String, java.lang.String)
getFloat(java.lang.String, float)
getInt(java.lang.String, int)
getLocalPath(java.lang.String, java.lang.String)
getLongBytes(java.lang.String, long)
getPasswordFromConfig(java.lang.String)
getPattern(java.lang.String, java.util.regex.Pattern)
getProps()
getRaw(java.lang.String)
VariableExpansion
getSocketAddr(java.lang.String, java.lang.String, int)
getStringCollection(java.lang.String)
getStrings(java.lang.String, java.lang.String...)
getTrimmed(java.lang.String)
getTrimmedStringCollection(java.lang.String)
getTrimmedStrings(java.lang.String, java.lang.String...)
hasWarnedDeprecation(java.lang.String)
iterator()
onlyKeyExists(java.lang.String)
reloadConfiguration()
set(java.lang.String, java.lang.String, java.lang.String)
setBoolean(java.lang.String, boolean)
setClass(java.lang.String, java.lang.Class, java.lang.Class)
setDeprecatedProperties()
setEnum(java.lang.String, T)
setIfUnset(java.lang.String, java.lang.String)
setLong(java.lang.String, long)
setQuietMode(boolean)
setStrings(java.lang.String, java.lang.String...)
size()
unset(java.lang.String)
updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress)
writeXml(java.io.OutputStream)
addDeprecation(java.lang.String, java.lang.String)
addDeprecation(java.lang.String, java.lang.String[], java.lang.String)
addDeprecation(java.lang.String, java.lang.String, java.lang.String)
addDeprecations(org.apache.hadoop.conf.Configuration.DeprecationDelta[])
addResource(java.io.InputStream)
addResource(org.apache.hadoop.fs.Path)
addResource(java.net.URL)
dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer)
get(java.lang.String)
getBoolean(java.lang.String, boolean)
getClass(java.lang.String, java.lang.Class, java.lang.Class)
getClassByNameOrNull(java.lang.String)
getClassLoader()
getConfResourceAsReader(java.lang.String)
getEnum(java.lang.String, T)
getFinalParameters()
getInstances(java.lang.String, java.lang.Class)
getInts(java.lang.String)
getLong(java.lang.String, long)
getPassword(java.lang.String)
getPasswordFromCredentialProviders(java.lang.String)
getPropertySources(java.lang.String)
getRange(java.lang.String, java.lang.String)
getResource(java.lang.String)
getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int)
getStrings(java.lang.String)
getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit)
getTrimmed(java.lang.String, java.lang.String)
getTrimmedStrings(java.lang.String)
getValByRegex(java.lang.String)
isDeprecated(java.lang.String)
main(java.lang.String[])
readFields(java.io.DataInput)
set(java.lang.String, java.lang.String)
setAllowNullValueProperties(boolean)
setBooleanIfUnset(java.lang.String, boolean)
setClassLoader(java.lang.ClassLoader)
setDouble(java.lang.String, double)
setFloat(java.lang.String, float)
setInt(java.lang.String, int)
setPattern(java.lang.String, java.util.regex.Pattern)
setSocketAddr(java.lang.String, java.net.InetSocketAddress)
setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit)
toString()
updateConnectAddr(java.lang.String, java.net.InetSocketAddress)
write(java.io.DataOutput)
writeXml(java.io.Writer)
getConf()
setConf(org.apache.hadoop.conf.Configuration)
getContainerToken()
getNodeHttpAddress()
getPriority()
getId()
getNodeId()
getResource()
CONTAINER_ID_BITMASK
build()
equals(java.lang.Object)
getApplicationAttemptId()
getId()
toString()
compareTo(org.apache.hadoop.yarn.api.records.ContainerId)
fromString(java.lang.String)
getContainerId()
hashCode()getApplicationACLs()
getEnvironment()
getServiceData()
newInstance(java.util.Map, java.util.Map, java.util.List, java.util.Map, java.nio.ByteBuffer, java.util.Map)
setCommands(java.util.List)
setLocalResources(java.util.Map)
setTokens(java.nio.ByteBuffer)
getCommands()
getLocalResources()
getTokens()
setApplicationACLs(java.util.Map)
setEnvironment(java.util.Map)
setServiceData(java.util.Map)
activateOptions()
close()
getContainerLogDir()
getTotalLogFileSize()
setContainerLogFile(java.lang.String)
append(org.apache.log4j.spi.LoggingEvent)
flush()
getContainerLogFile()
setContainerLogDir(java.lang.String)
setTotalLogFileSize(long)getContainerStatuses(org.apache.hadoop.yarn.api.protocolrecords.GetContainerStatusesRequest)
stopContainers(org.apache.hadoop.yarn.api.protocolrecords.StopContainersRequest)
startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest)
getKerberosInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
getTokenInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
getAllocatedResource()
getContainerExitStatus()
getContainerState()
getDiagnosticsInfo()
getLogUrl()
getPriority()
setAssignedNode(org.apache.hadoop.yarn.api.records.NodeId)
setContainerId(org.apache.hadoop.yarn.api.records.ContainerId)
setCreationTime(long)
setFinishTime(long)
setPriority(org.apache.hadoop.yarn.api.records.Priority)
getAssignedNode()
getContainerId()
getCreationTime()
getFinishTime()
getNodeHttpAddress()
setAllocatedResource(org.apache.hadoop.yarn.api.records.Resource)
setContainerExitStatus(int)
setContainerState(org.apache.hadoop.yarn.api.records.ContainerState)
setDiagnosticsInfo(java.lang.String)
setLogUrl(java.lang.String)
equals(java.lang.Object)
getContainerId()
newInstance(org.apache.hadoop.yarn.api.records.ContainerId, org.apache.hadoop.yarn.api.records.Resource)
setContainerId(org.apache.hadoop.yarn.api.records.ContainerId)
getCapability()
hashCode()
setCapability(org.apache.hadoop.yarn.api.records.Resource)
activateOptions()
getContainerLogDir()
setContainerLogDir(java.lang.String)
flush()
getContainerLogFile()
setContainerLogFile(java.lang.String)
valueOf(java.lang.String)
values()getContainerId()
getExitStatus()
getDiagnostics()
getState()KIND
equals(java.lang.Object)
getContainerID()
getExpiryTimeStamp()
getLogAggregationContext()
getNmHostAddress()
getProto()
getRMIdentifier()
hashCode()
toString()
getApplicationSubmitter()
getCreationTime()
getKind()
getMasterKeyId()
getPriority()
getResource()
getUser()
readFields(java.io.DataInput)
write(java.io.DataOutput)
selectToken(org.apache.hadoop.io.Text, java.util.Collection)
getDirectoryCount()
getHeader(boolean)
getQuota()
getSpaceQuota()
getTypeQuota(org.apache.hadoop.fs.StorageType)
isTypeQuotaSet()
toString(boolean)
getFileCount()
getLength()
getSpaceConsumed()
getTypeConsumed(org.apache.hadoop.fs.StorageType)
isTypeConsumedAvailable()
toString()
toString(boolean, boolean)CREATE_DIR
addDependingJob(org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob)
getDependentJobs()
getJobID()
getJobState()
getMessage()
isReady()
setJob(org.apache.hadoop.mapreduce.Job)
setJobName(java.lang.String)
setMessage(java.lang.String)
toString()
failJob(java.lang.String)
getJob()
getJobName()
getMapredJobId()
isCompleted()
killJob()
setJobID(java.lang.String)
setJobState(org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.State)
submit()getDisplayName()
getValue()
setDisplayName(java.lang.String)
getName()
increment(long)
setValue(long)
addCounter(java.lang.String, java.lang.String, long)
findCounter(java.lang.String)
findCounter(java.lang.String, java.lang.String)
getName()
setDisplayName(java.lang.String)
addCounter(T)
findCounter(java.lang.String, boolean)
getDisplayName()
incrAllCounters(org.apache.hadoop.mapreduce.counters.CounterGroupBase)
size()contentEquals(org.apache.hadoop.mapred.Counters.Counter)
getCounter()
getName()
getValue()
increment(long)
readFields(java.io.DataInput)
setValue(long)
equals(java.lang.Object)
getDisplayName()
getUnderlyingCounter()
hashCode()
makeEscapedCompactString()
setDisplayName(java.lang.String)
write(java.io.DataOutput)
Counters.Group()
addCounter(org.apache.hadoop.mapred.Counters.Counter)
equals(java.lang.Object)
findCounter(java.lang.String, boolean)
getCounter(int, java.lang.String)
findCounter(java.lang.String)
getCounterForName(java.lang.String)
getName()
hashCode()
iterator()
readFields(java.io.DataInput)
size()
addCounter(java.lang.String, java.lang.String, long)
findCounter(java.lang.String)
findCounter(java.lang.String, java.lang.String)
getCounter(java.lang.String)
getDisplayName()
getUnderlyingGroup()
incrAllCounters(org.apache.hadoop.mapreduce.counters.CounterGroupBase)
makeEscapedCompactString()
setDisplayName(java.lang.String)
write(java.io.DataOutput)
add(org.apache.hadoop.util.bloom.Key)
approximateCount(org.apache.hadoop.util.bloom.Key)
membershipTest(org.apache.hadoop.util.bloom.Key)
or(org.apache.hadoop.util.bloom.Filter)
toString()
xor(org.apache.hadoop.util.bloom.Filter)
and(org.apache.hadoop.util.bloom.Filter)
delete(org.apache.hadoop.util.bloom.Key)
not()
readFields(java.io.DataInput)
write(java.io.DataOutput)
validate(java.util.EnumSet)
validateForAppend(java.util.EnumSet)
values()
validate(java.lang.Object, boolean, java.util.EnumSet)
valueOf(java.lang.String)
CLEAR_TEXT_FALLBACK
createCredentialEntry(java.lang.String, char[])
flush()
getCredentialEntry(java.lang.String)
deleteCredentialEntry(java.lang.String)
getAliases()
isTransient()CREDENTIAL_PROVIDER_PATH
createProvider(java.net.URI, org.apache.hadoop.conf.Configuration)
getProviders(org.apache.hadoop.conf.Configuration)
endMap(java.lang.String)
endVector(java.lang.String)
readBuffer(java.lang.String)
readDouble(java.lang.String)
readInt(java.lang.String)
readString(java.lang.String)
startRecord(java.lang.String)
endRecord(java.lang.String)
readBool(java.lang.String)
readByte(java.lang.String)
readFloat(java.lang.String)
readLong(java.lang.String)
startMap(java.lang.String)
startVector(java.lang.String)
endMap(java.util.TreeMap, java.lang.String)
endVector(java.util.ArrayList, java.lang.String)
startRecord(org.apache.hadoop.record.Record, java.lang.String)
writeBool(boolean, java.lang.String)
writeByte(byte, java.lang.String)
writeFloat(float, java.lang.String)
writeLong(long, java.lang.String)
endRecord(org.apache.hadoop.record.Record, java.lang.String)
startMap(java.util.TreeMap, java.lang.String)
startVector(java.util.ArrayList, java.lang.String)
writeBuffer(org.apache.hadoop.record.Buffer, java.lang.String)
writeDouble(double, java.lang.String)
writeInt(int, java.lang.String)
writeString(java.lang.String, java.lang.String)
DRIVER_CLASS_PROPERTY
INPUT_CLASS_PROPERTY
INPUT_COUNT_QUERY
INPUT_ORDER_BY_PROPERTY
INPUT_TABLE_NAME_PROPERTY
OUTPUT_FIELD_NAMES_PROPERTY
PASSWORD_PROPERTY
USERNAME_PROPERTY
configureDB(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String)
getConf()
getInputBoundingQuery()
getInputConditions()
getInputFieldNames()
getInputQuery()
getOutputFieldCount()
getOutputTableName()
setInputClass(java.lang.Class)
setInputCountQuery(java.lang.String)
setInputOrderBy(java.lang.String)
setInputTableName(java.lang.String)
setOutputFieldNames(java.lang.String...)
INPUT_BOUNDING_QUERY
INPUT_CONDITIONS_PROPERTY
INPUT_FIELD_NAMES_PROPERTY
INPUT_QUERY
OUTPUT_FIELD_COUNT_PROPERTY
OUTPUT_TABLE_NAME_PROPERTY
URL_PROPERTY
configureDB(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String, java.lang.String)
getConnection()
getInputClass()
getInputCountQuery()
getInputOrderBy()
getInputTableName()
getOutputFieldNames()
setInputBoundingQuery(java.lang.String)
setInputConditions(java.lang.String)
setInputFieldNames(java.lang.String...)
setInputQuery(java.lang.String)
setOutputFieldCount(int)
setOutputTableName(java.lang.String)
conditions
dbConf
fieldNames
closeConnection()
createDBRecordReader(org.apache.hadoop.mapreduce.lib.db.DBInputFormat.DBInputSplit, org.apache.hadoop.conf.Configuration)
getConf()
getCountQuery()
getDBProductName()
setConf(org.apache.hadoop.conf.Configuration)
setInput(org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.String, java.lang.String, java.lang.String, java.lang.String...)
connection
dbProductName
tableName
createConnection()
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
getConnection()
getDBConf()
getSplits(org.apache.hadoop.mapreduce.JobContext)
setInput(org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.String, java.lang.String)
checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
setOutput(org.apache.hadoop.mapreduce.Job, java.lang.String, int)
constructQuery(java.lang.String, java.lang.String[])
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
setOutput(org.apache.hadoop.mapreduce.Job, java.lang.String, java.lang.String...)
statement
close()
executeQuery(java.lang.String)
getConnection()
getCurrentValue()
getFieldNames()
getProgress()
getSplit()
getTableName()
next(org.apache.hadoop.io.LongWritable, T)
nextKeyValue()
setStatement(java.sql.PreparedStatement)
createValue()
getConditions()
getCurrentKey()
getDBConf()
getPos()
getSelectQuery()
getStatement()
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
nextKeyValue()split(org.apache.hadoop.conf.Configuration, java.sql.ResultSet, java.lang.String)
readFields(java.sql.ResultSet)
write(java.sql.PreparedStatement)
reloadCachedMappings()
resolve(java.util.List)
reloadCachedMappings(java.util.List)
SUBSTITUTE_TOKEN
createDBRecordReader(org.apache.hadoop.mapreduce.lib.db.DBInputFormat.DBInputSplit, org.apache.hadoop.conf.Configuration)
getSplits(org.apache.hadoop.mapreduce.JobContext)
setBoundingQuery(org.apache.hadoop.conf.Configuration, java.lang.String)
setInput(org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.String, java.lang.String, java.lang.String, java.lang.String...)
getBoundingValsQuery()
getSplitter(int)
setInput(org.apache.hadoop.mapreduce.Job, java.lang.Class, java.lang.String, java.lang.String)
getSelectQuery()
constructOutputStream(java.io.DataOutput)
write(byte[], int, int)
write(byte[])
write(int)dateToString(java.util.Date)
split(org.apache.hadoop.conf.Configuration, java.sql.ResultSet, java.lang.String)
decompress(byte[], int, int)
finished()
needsDictionary()
reset()
setInput(byte[], int, int)
end()
getRemaining()
needsInput()
setInput(byte[], int, int)
setDictionary(byte[], int, int)buffer
decompressor
DecompressorStream(java.io.InputStream)
DecompressorStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, int)
available()
close()
getCompressedData()
markSupported()
read(byte[], int, int)
resetState()
closed
eof
DecompressorStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor)
checkStream()
decompress(byte[], int, int)
mark(int)
read()
reset()
skip(long)createCompressor()
createDirectDecompressor()
createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor)
createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor)
getConf()
getDefaultExtension()
createDecompressor()
createInputStream(java.io.InputStream)
createOutputStream(java.io.OutputStream)
getCompressorType()
getDecompressorType()
setConf(org.apache.hadoop.conf.Configuration)
initialize(java.lang.String)
shutdown()
values()
instance()
valueOf(java.lang.String)
close()
load(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.Class)
store(org.apache.hadoop.conf.Configuration, K, java.lang.String)
toString(T)
fromString(java.lang.String)
loadArray(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.Class)
storeArray(org.apache.hadoop.conf.Configuration, K[], java.lang.String)
getDelegationToken()
setDelegationToken(org.apache.hadoop.security.token.Token)
cancelDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.Token)
getDefaultDelegationTokenAuthenticator()
getDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.Token, java.lang.String, java.lang.String)
openConnection(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.Token)
renewDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.Token)
setDefaultDelegationTokenAuthenticator(java.lang.Class)
useQueryStringForDelegationToken()
cancelDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.Token, java.lang.String)
getDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.Token, java.lang.String)
openConnection(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL.Token)
openConnection(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.Token, java.lang.String)
renewDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.Token, java.lang.String)
setUseQueryStringForDelegationToken(boolean)DELEGATION_PARAM
DELEGATION_TOKEN_JSON
OP_PARAM
RENEWER_PARAM
authenticate(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL.Token)
cancelDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL.Token, org.apache.hadoop.security.token.Token, java.lang.String)
getDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL.Token, java.lang.String, java.lang.String)
renewDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL.Token, org.apache.hadoop.security.token.Token, java.lang.String)
DELEGATION_TOKEN_HEADER
DELEGATION_TOKEN_URL_STRING_JSON
RENEW_DELEGATION_TOKEN_JSON
TOKEN_PARAM
cancelDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL.Token, org.apache.hadoop.security.token.Token)
getDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL.Token, java.lang.String)
renewDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL.Token, org.apache.hadoop.security.token.Token)
setConnectionConfigurator(org.apache.hadoop.security.authentication.client.ConnectionConfigurator)createDirectDecompressor()
decompress(java.nio.ByteBuffer, java.nio.ByteBuffer)
DEFAULT_DISPATCHER_EXIT_ON_ERROR
getEventHandler()
DISPATCHER_EXIT_ON_ERROR_KEY
register(java.lang.Class, org.apache.hadoop.yarn.event.EventHandler)
CACHE_ARCHIVES
CACHE_ARCHIVES_TIMESTAMPS
CACHE_FILES_SIZES
CACHE_LOCALARCHIVES
CACHE_SYMLINK
addLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)
createAllSymlink(org.apache.hadoop.conf.Configuration, java.io.File, java.io.File)
getTimestamp(org.apache.hadoop.conf.Configuration, java.net.URI)
setFileTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)
setLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)
CACHE_ARCHIVES_SIZES
CACHE_FILES
CACHE_FILES_TIMESTAMPS
CACHE_LOCALFILES
addLocalFiles(org.apache.hadoop.conf.Configuration, java.lang.String)
getFileStatus(org.apache.hadoop.conf.Configuration, java.net.URI)
setArchiveTimestamps(org.apache.hadoop.conf.Configuration, java.lang.String)
setLocalArchives(org.apache.hadoop.conf.Configuration, java.lang.String)
addNextValue(double)
getCombinerOutput()
getSum()
addNextValue(java.lang.Object)
getReport()
reset()compareTo(org.apache.hadoop.io.DoubleWritable)
get()
readFields(java.io.DataInput)
toString()
equals(java.lang.Object)
hashCode()
set(double)
write(java.io.DataOutput)
add(org.apache.hadoop.util.bloom.Key)
membershipTest(org.apache.hadoop.util.bloom.Key)
or(org.apache.hadoop.util.bloom.Filter)
toString()
xor(org.apache.hadoop.util.bloom.Filter)
and(org.apache.hadoop.util.bloom.Filter)
not()
readFields(java.io.DataInput)
write(java.io.DataOutput)
getBuffer(boolean, int)
putBuffer(java.nio.ByteBuffer)
addresses
api
clone()
validate()
addressType
protocolType
toString()add(E)
get()
getElementType()
iterator()
set(java.util.EnumSet, java.lang.Class)
size()
write(java.io.DataOutput)
equals(java.lang.Object)
getConf()
hashCode()
readFields(java.io.DataInput)
setConf(org.apache.hadoop.conf.Configuration)
toString()getTimestamp()
toString()
getType()append(org.apache.log4j.spi.LoggingEvent)
requiresLayout()
close()handle(T)
getFileDescriptor()
read(java.nio.ByteBuffer)
read(org.apache.hadoop.io.ByteBufferPool, int, java.util.EnumSet)
readFully(long, byte[])
readFully(long, byte[], int, int)
releaseBuffer(java.nio.ByteBuffer)
seekToNewSource(long)
setReadahead(java.lang.Long)
getPos()
read(org.apache.hadoop.io.ByteBufferPool, int)
read(long, byte[], int, int)
readFully(long, byte[], int, int)
seek(long)
setDropBehind(java.lang.Boolean)
unbuffer()close()
hflush()
setDropBehind(java.lang.Boolean)
getPos()
hsync()
sync()
DEFAULT_BLOCK_SIZE
E_SAME_DIRECTORY_ONLY
FS_FTP_HOST_PORT
FS_FTP_USER_PREFIX
append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable)
delete(org.apache.hadoop.fs.Path, boolean)
getFileStatus(org.apache.hadoop.fs.Path)
getScheme()
getWorkingDirectory()
listStatus(org.apache.hadoop.fs.Path)
open(org.apache.hadoop.fs.Path, int)
setWorkingDirectory(org.apache.hadoop.fs.Path)
DEFAULT_BUFFER_SIZE
FS_FTP_HOST
FS_FTP_PASSWORD_PREFIX
LOG
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
getDefaultPort()
getHomeDirectory()
getUri()
initialize(java.net.URI, org.apache.hadoop.conf.Configuration)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
checkArgs(java.lang.String)
tryFence(org.apache.hadoop.ha.HAServiceTarget, java.lang.String)
DATA_FIELD_SEPERATOR
MAP_OUTPUT_KEY_VALUE_SPEC
extractOutputKeyValue(java.lang.String, java.lang.String, java.lang.String, java.util.List, java.util.List, int, boolean, boolean)
getValue()
specToString(java.lang.String, java.lang.String, int, java.util.List, java.util.List)
emptyText
REDUCE_OUTPUT_KEY_VALUE_SPEC
getKey()
parseOutputKeyValueSpec(java.lang.String, java.util.List, java.util.List)
LOG
close()
map(K, V, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)
configure(org.apache.hadoop.mapred.JobConf)
reduce(org.apache.hadoop.io.Text, java.util.Iterator, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)
LOG
map(K, V, org.apache.hadoop.mapreduce.Mapper.Context)
setup(org.apache.hadoop.mapreduce.Mapper.Context)LOG
reduce(org.apache.hadoop.io.Text, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer.Context)
setup(org.apache.hadoop.mapreduce.Reducer.Context)equals(org.apache.hadoop.record.meta.FieldTypeInfo)
getFieldID()
hashCode()
equals(java.lang.Object)
getTypeID()
equals(java.lang.Object)
getBytes()
getLength()
getAlgorithmName()
getChecksumOpt()
hashCode()DEFAULT_PERM
FILE_DEFAULT_PERM
SHUTDOWN_HOOK_PRIORITY
clearStatistics()
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
deleteOnExit(org.apache.hadoop.fs.Path)
getAllStatistics()
getFileContext()
getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration)
getFileContext(java.net.URI)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getFSofPath(org.apache.hadoop.fs.Path)
getHomeDirectory()
getLocalFSFileContext()
getStatistics(java.net.URI)
getUMask()
getXAttr(org.apache.hadoop.fs.Path, java.lang.String)
getXAttrs(org.apache.hadoop.fs.Path, java.util.List)
listLocatedStatus(org.apache.hadoop.fs.Path)
listXAttrs(org.apache.hadoop.fs.Path)
mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean)
open(org.apache.hadoop.fs.Path)
printStatistics()
removeAclEntries(org.apache.hadoop.fs.Path, java.util.List)
removeXAttr(org.apache.hadoop.fs.Path, java.lang.String)
resolve(org.apache.hadoop.fs.Path)
resolvePath(org.apache.hadoop.fs.Path)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setReplication(org.apache.hadoop.fs.Path, short)
setUMask(org.apache.hadoop.fs.permission.FsPermission)
setWorkingDirectory(org.apache.hadoop.fs.Path)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet)
util()
DIR_DEFAULT_PERM
LOG
create(org.apache.hadoop.fs.Path, java.util.EnumSet, org.apache.hadoop.fs.Options.CreateOpts...)
delete(org.apache.hadoop.fs.Path, boolean)
getAclStatus(org.apache.hadoop.fs.Path)
getFileChecksum(org.apache.hadoop.fs.Path)
getFileContext(org.apache.hadoop.fs.AbstractFileSystem)
getFileContext(org.apache.hadoop.conf.Configuration)
getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration)
getFileStatus(org.apache.hadoop.fs.Path)
getFsStatus(org.apache.hadoop.fs.Path)
getLinkTarget(org.apache.hadoop.fs.Path)
getLocalFSFileContext(org.apache.hadoop.conf.Configuration)
getUgi()
getWorkingDirectory()
getXAttrs(org.apache.hadoop.fs.Path)
listCorruptFileBlocks(org.apache.hadoop.fs.Path)
listStatus(org.apache.hadoop.fs.Path)
makeQualified(org.apache.hadoop.fs.Path)
modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List)
open(org.apache.hadoop.fs.Path, int)
removeAcl(org.apache.hadoop.fs.Path)
removeDefaultAcl(org.apache.hadoop.fs.Path)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options.Rename...)
resolveIntermediate(org.apache.hadoop.fs.Path)
setAcl(org.apache.hadoop.fs.Path, java.util.List)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
setTimes(org.apache.hadoop.fs.Path, long, long)
setVerifyChecksum(boolean, org.apache.hadoop.fs.Path)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[])
truncate(org.apache.hadoop.fs.Path, long)
DEFAULT_LIST_STATUS_NUM_THREADS
INPUT_DIR_RECURSIVE
NUM_INPUT_FILES
SPLIT_MAXSIZE
addInputPath(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.fs.Path)
addInputPaths(org.apache.hadoop.mapreduce.Job, java.lang.String)
getBlockIndex(org.apache.hadoop.fs.BlockLocation[], long)
getInputDirRecursive(org.apache.hadoop.mapreduce.JobContext)
getInputPaths(org.apache.hadoop.mapreduce.JobContext)
getMinSplitSize(org.apache.hadoop.mapreduce.JobContext)
isSplitable(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.fs.Path)
makeSplit(org.apache.hadoop.fs.Path, long, long, java.lang.String[])
setInputDirRecursive(org.apache.hadoop.mapreduce.Job, boolean)
setInputPaths(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.fs.Path...)
setMaxInputSplitSize(org.apache.hadoop.mapreduce.Job, long)
INPUT_DIR
LIST_STATUS_NUM_THREADS
PATHFILTER_CLASS
SPLIT_MINSIZE
addInputPathRecursively(java.util.List, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter)
computeSplitSize(long, long, long)
getFormatMinSplitSize()
getInputPathFilter(org.apache.hadoop.mapreduce.JobContext)
getMaxSplitSize(org.apache.hadoop.mapreduce.JobContext)
getSplits(org.apache.hadoop.mapreduce.JobContext)
listStatus(org.apache.hadoop.mapreduce.JobContext)
makeSplit(org.apache.hadoop.fs.Path, long, long, java.lang.String[], java.lang.String[])
setInputPathFilter(org.apache.hadoop.mapreduce.Job, java.lang.Class)
setInputPaths(org.apache.hadoop.mapreduce.Job, java.lang.String)
setMinInputSplitSize(org.apache.hadoop.mapreduce.Job, long)
valueOf(java.lang.String)
values()FILEOUTPUTCOMMITTER_ALGORITHM_VERSION
PENDING_DIR_NAME
SUCCESSFUL_JOB_OUTPUT_DIR_MARKER
abortJob(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.mapreduce.JobStatus.State)
cleanupJob(org.apache.hadoop.mapreduce.JobContext)
commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
getCommittedTaskPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
getJobAttemptPath(int)
getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.fs.Path)
getTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext, org.apache.hadoop.fs.Path)
isRecoverySupported()
recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
FILEOUTPUTCOMMITTER_ALGORITHM_VERSION_DEFAULT
SUCCEEDED_FILE_NAME
TEMP_DIR_NAME
abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
commitJob(org.apache.hadoop.mapreduce.JobContext)
getCommittedTaskPath(int, org.apache.hadoop.mapreduce.TaskAttemptContext)
getCommittedTaskPath(org.apache.hadoop.mapreduce.TaskAttemptContext, org.apache.hadoop.fs.Path)
getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext)
getTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
getWorkPath()
needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)
setupJob(org.apache.hadoop.mapreduce.JobContext)
BASE_OUTPUT_NAME
COMPRESS_CODEC
OUTDIR
checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
getDefaultWorkFile(org.apache.hadoop.mapreduce.TaskAttemptContext, java.lang.String)
getOutputCompressorClass(org.apache.hadoop.mapreduce.JobContext, java.lang.Class)
getOutputPath(org.apache.hadoop.mapreduce.JobContext)
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
getWorkOutputPath(org.apache.hadoop.mapreduce.TaskInputOutputContext)
setOutputCompressorClass(org.apache.hadoop.mapreduce.Job, java.lang.Class)
setOutputPath(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.fs.Path)
COMPRESS
COMPRESS_TYPE
PART
getCompressOutput(org.apache.hadoop.mapreduce.JobContext)
getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
getOutputName(org.apache.hadoop.mapreduce.JobContext)
getPathForWorkFile(org.apache.hadoop.mapreduce.TaskInputOutputContext, java.lang.String, java.lang.String)
getUniqueFile(org.apache.hadoop.mapreduce.TaskAttemptContext, java.lang.String, java.lang.String)
setCompressOutput(org.apache.hadoop.mapreduce.Job, boolean)
setOutputName(org.apache.hadoop.mapreduce.JobContext, java.lang.String)
valueOf(java.lang.String)
values()close()
init(org.apache.commons.configuration.SubsetConfiguration)
flush()
putMetrics(org.apache.hadoop.metrics2.MetricsRecord)
getLength()
getLocations()
getStart()
toString()
getLocationInfo()
getPath()
readFields(java.io.DataInput)
write(java.io.DataOutput)
compareTo(java.lang.Object)
getAccessTime()
getGroup()
getModificationTime()
getPath()
getReplication()
hashCode()
isDirectory()
isFile()
readFields(java.io.DataInput)
setOwner(java.lang.String)
setPermission(org.apache.hadoop.fs.permission.FsPermission)
toString()
equals(java.lang.Object)
getBlockSize()
getLen()
getOwner()
getPermission()
getSymlink()
isDir()
isFile()
isDirectory()
isSymlink()
isEncrypted()
isSymlink()
setGroup(java.lang.String)
setPath(org.apache.hadoop.fs.Path)
setSymlink(org.apache.hadoop.fs.Path)
write(java.io.DataOutput)
DEFAULT_FS
LOG
statistics
FileSystem()
append(org.apache.hadoop.fs.Path)
append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable)
cancelDeleteOnExit(org.apache.hadoop.fs.Path)
checkPath(org.apache.hadoop.fs.Path)
close()
closeAllForUGI(org.apache.hadoop.security.UserGroupInformation)
concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[])
copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
create(org.apache.hadoop.fs.Path, boolean)
create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable)
create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable)
create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable)
createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable)
createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable)
createSnapshot(org.apache.hadoop.fs.Path, java.lang.String)
delete(org.apache.hadoop.fs.Path)
delete(org.apache.hadoop.fs.Path, boolean)
deleteOnExit(org.apache.hadoop.fs.Path)
enableSymlinks()
fixRelativePart(org.apache.hadoop.fs.Path)
fixRelativePart(org.apache.hadoop.fs.Path)
get(java.net.URI, org.apache.hadoop.conf.Configuration)
getAclStatus(org.apache.hadoop.fs.Path)
getBlockSize(org.apache.hadoop.fs.Path)
getContentSummary(org.apache.hadoop.fs.Path)
getDefaultBlockSize(org.apache.hadoop.fs.Path)
getDefaultReplication()
getDefaultReplication(org.apache.hadoop.fs.Path)
getDefaultUri(org.apache.hadoop.conf.Configuration)
getFileBlockLocations(org.apache.hadoop.fs.Path, long, long)
getFileChecksum(org.apache.hadoop.fs.Path, long)
getFileStatus(org.apache.hadoop.fs.Path)
getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)
getInitialWorkingDirectory()
getLinkTarget(org.apache.hadoop.fs.Path)
getLinkTarget(org.apache.hadoop.fs.Path)
getName()
getReplication(org.apache.hadoop.fs.Path)
getServerDefaults()
getServerDefaults(org.apache.hadoop.fs.Path)
getStatistics()
getAllStatistics()
getStatus()
getUri()
getWorkingDirectory()
getXAttrs(org.apache.hadoop.fs.Path)
globStatus(org.apache.hadoop.fs.Path)
initialize(java.net.URI, org.apache.hadoop.conf.Configuration)
isFile(org.apache.hadoop.fs.Path)
listFiles(org.apache.hadoop.fs.Path, boolean)
listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter)
listStatus(org.apache.hadoop.fs.Path[])
listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter)
listXAttrs(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path)
moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
newInstance(java.net.URI, org.apache.hadoop.conf.Configuration)
newInstanceLocal(org.apache.hadoop.conf.Configuration)
open(org.apache.hadoop.fs.Path, int)
primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
printStatistics()
removeAcl(org.apache.hadoop.fs.Path)
removeDefaultAcl(org.apache.hadoop.fs.Path)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
resolvePath(org.apache.hadoop.fs.Path)
setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setReplication(org.apache.hadoop.fs.Path, short)
setVerifyChecksum(boolean)
setWriteChecksum(boolean)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet)
supportsSymlinks()
supportsSymlinks()
FS_DEFAULT_NAME_KEY
SHUTDOWN_HOOK_PRIORITY
append(org.apache.hadoop.fs.Path, int)
areSymlinksEnabled()
canonicalizeUri(java.net.URI)
clearStatistics()
closeAll()
completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path)
copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
create(org.apache.hadoop.fs.Path)
create(org.apache.hadoop.fs.Path, boolean, int)
create(org.apache.hadoop.fs.Path, boolean, int, short, long)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options.ChecksumOpt)
create(org.apache.hadoop.fs.Path, short)
createNewFile(org.apache.hadoop.fs.Path)
createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
createSnapshot(org.apache.hadoop.fs.Path)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
delete(org.apache.hadoop.fs.Path, boolean)
deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String)
exists(org.apache.hadoop.fs.Path)
get(org.apache.hadoop.conf.Configuration)
get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String)
getAllStatistics()
getCanonicalUri()
getDefaultBlockSize()
getDefaultBlockSize(org.apache.hadoop.fs.Path)
getDefaultPort()
getDefaultReplication(org.apache.hadoop.fs.Path)
getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long)
getFileChecksum(org.apache.hadoop.fs.Path)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration)
getHomeDirectory()
getLength(org.apache.hadoop.fs.Path)
getLocal(org.apache.hadoop.conf.Configuration)
getNamed(java.lang.String, org.apache.hadoop.conf.Configuration)
getScheme()
getServerDefaults(org.apache.hadoop.fs.Path)
getStatistics(java.lang.String, java.lang.Class)
getStatus(org.apache.hadoop.fs.Path)
getUsed()
getXAttr(org.apache.hadoop.fs.Path, java.lang.String)
getXAttrs(org.apache.hadoop.fs.Path, java.util.List)
globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter)
isDirectory(org.apache.hadoop.fs.Path)
listCorruptFileBlocks(org.apache.hadoop.fs.Path)
listLocatedStatus(org.apache.hadoop.fs.Path)
listStatus(org.apache.hadoop.fs.Path)
listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter)
listStatusIterator(org.apache.hadoop.fs.Path)
makeQualified(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List)
moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
newInstance(org.apache.hadoop.conf.Configuration)
newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String)
open(org.apache.hadoop.fs.Path)
primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options.ChecksumOpt)
primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean)
processDeleteOnExit()
removeAclEntries(org.apache.hadoop.fs.Path, java.util.List)
removeXAttr(org.apache.hadoop.fs.Path, java.lang.String)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options.Rename...)
resolveLink(org.apache.hadoop.fs.Path)
getLinkTarget(org.apache.hadoop.fs.Path)
setAcl(org.apache.hadoop.fs.Path, java.util.List)
setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
setTimes(org.apache.hadoop.fs.Path, long, long)
setWorkingDirectory(org.apache.hadoop.fs.Path)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[])
startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
truncate(org.apache.hadoop.fs.Path, long)
SYMLINK_NO_PRIVILEGE
canExecute(java.io.File)
canExecute()
canWrite(java.io.File)
canWrite()
chmod(java.lang.String, java.lang.String, boolean)
copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, org.apache.hadoop.conf.Configuration)
copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.io.File, boolean, org.apache.hadoop.conf.Configuration)
copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration)
createJarWithClassPath(java.lang.String, org.apache.hadoop.fs.Path, java.util.Map)
createLocalTempFile(java.io.File, java.lang.String, boolean)
fullyDelete(java.io.File, boolean)
fullyDeleteContents(java.io.File)
getDU(java.io.File)
listFiles(java.io.File)
listFiles()
makeShellPath(java.io.File, boolean)
readLink(java.io.File)
setExecutable(java.io.File, boolean)
setExecutable(boolean)
setPermission(java.io.File, org.apache.hadoop.fs.permission.FsPermission)
setWritable(java.io.File, boolean)
setWritable(boolean)
stat2Paths(org.apache.hadoop.fs.FileStatus[], org.apache.hadoop.fs.Path)
unTar(java.io.File, java.io.File)
canRead(java.io.File)
canRead()
chmod(java.lang.String, java.lang.String)
copy(java.io.File, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration)
copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, org.apache.hadoop.conf.Configuration)
copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, org.apache.hadoop.conf.Configuration)
copyMerge(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration, java.lang.String)
createJarWithClassPath(java.lang.String, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.util.Map)
fullyDelete(java.io.File)
fullyDelete(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)
delete(org.apache.hadoop.fs.Path, boolean)
fullyDeleteContents(java.io.File, boolean)
list(java.io.File)
list()
makeShellPath(java.io.File)
makeShellPath(java.lang.String)
replaceFile(java.io.File, java.io.File)
setOwner(java.io.File, java.lang.String, java.lang.String)
setReadable(java.io.File, boolean)
setReadable(boolean)
stat2Paths(org.apache.hadoop.fs.FileStatus[])
symLink(java.lang.String, java.lang.String)
unZip(java.io.File, java.io.File)
fs
access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction)
canonicalizeUri(java.net.URI)
close()
concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[])
copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options.ChecksumOpt)
createSnapshot(org.apache.hadoop.fs.Path, java.lang.String)
delete(org.apache.hadoop.fs.Path, boolean)
getAclStatus(org.apache.hadoop.fs.Path)
getChildFileSystems()
getDefaultBlockSize()
getDefaultReplication()
getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long)
getFileChecksum(org.apache.hadoop.fs.Path, long)
getFileStatus(org.apache.hadoop.fs.Path)
getInitialWorkingDirectory()
getRawFileSystem()
getServerDefaults(org.apache.hadoop.fs.Path)
getUri()
getWorkingDirectory()
getXAttrs(org.apache.hadoop.fs.Path)
initialize(java.net.URI, org.apache.hadoop.conf.Configuration)
listLocatedStatus(org.apache.hadoop.fs.Path)
listStatusIterator(org.apache.hadoop.fs.Path)
makeQualified(org.apache.hadoop.fs.Path)
modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List)
primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options.ChecksumOpt)
removeAcl(org.apache.hadoop.fs.Path)
removeDefaultAcl(org.apache.hadoop.fs.Path)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
resolveLink(org.apache.hadoop.fs.Path)
getLinkTarget(org.apache.hadoop.fs.Path)
setAcl(org.apache.hadoop.fs.Path, java.util.List)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
setTimes(org.apache.hadoop.fs.Path, long, long)
setWorkingDirectory(org.apache.hadoop.fs.Path)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[])
startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
truncate(org.apache.hadoop.fs.Path, long)
swapScheme
append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable)
checkPath(org.apache.hadoop.fs.Path)
completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path)
copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String)
getCanonicalUri()
getConf()
getDefaultBlockSize(org.apache.hadoop.fs.Path)
getDefaultReplication(org.apache.hadoop.fs.Path)
getFileChecksum(org.apache.hadoop.fs.Path)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getHomeDirectory()
getLinkTarget(org.apache.hadoop.fs.Path)
getLinkTarget(org.apache.hadoop.fs.Path)
getServerDefaults()
getStatus(org.apache.hadoop.fs.Path)
getUsed()
getXAttr(org.apache.hadoop.fs.Path, java.lang.String)
getXAttrs(org.apache.hadoop.fs.Path, java.util.List)
listCorruptFileBlocks(org.apache.hadoop.fs.Path)
listStatus(org.apache.hadoop.fs.Path)
listXAttrs(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
open(org.apache.hadoop.fs.Path, int)
primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
removeAclEntries(org.apache.hadoop.fs.Path, java.util.List)
removeXAttr(org.apache.hadoop.fs.Path, java.lang.String)
renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
resolvePath(org.apache.hadoop.fs.Path)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setReplication(org.apache.hadoop.fs.Path, short)
setVerifyChecksum(boolean)
setWriteChecksum(boolean)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet)
supportsSymlinks()
supportsSymlinks()baseOut
checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
valueOf(java.lang.String)
values()getDiagnostics()
getTrackingUrl()
setDiagnostics(java.lang.String)
setTrackingUrl(java.lang.String)
getFinalApplicationStatus()
newInstance(org.apache.hadoop.yarn.api.records.FinalApplicationStatus, java.lang.String, java.lang.String)
setFinalApplicationStatus(org.apache.hadoop.yarn.api.records.FinalApplicationStatus)
getIsUnregistered()
FIXED_RECORD_LENGTH
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
isSplitable(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.fs.Path)
getRecordLength(org.apache.hadoop.conf.Configuration)
setRecordLength(org.apache.hadoop.conf.Configuration, int)
split(org.apache.hadoop.conf.Configuration, java.sql.ResultSet, java.lang.String)
compareTo(org.apache.hadoop.io.FloatWritable)
get()
readFields(java.io.DataInput)
toString()
equals(java.lang.Object)
hashCode()
set(float)
write(java.io.DataOutput)
SYMBOL
and(org.apache.hadoop.fs.permission.FsAction)
implies(org.apache.hadoop.fs.permission.FsAction)
or(org.apache.hadoop.fs.permission.FsAction)
values()
getFsAction(java.lang.String)
not()
valueOf(java.lang.String)
DEFAULT_UMASK
MAX_PERMISSION_LENGTH
applyUMask(org.apache.hadoop.fs.permission.FsPermission)
equals(java.lang.Object)
getAclBit()
getDefault()
getEncryptedBit()
getGroupAction()
getStickyBit()
getUserAction()
read(java.io.DataInput)
setUMask(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.permission.FsPermission)
toShort()
valueOf(java.lang.String)
DEPRECATED_UMASK_LABEL
UMASK_LABEL
createImmutable(short)
fromShort(short)
getCachePoolDefault()
getDirDefault()
getFileDefault()
getOtherAction()
getUMask(org.apache.hadoop.conf.Configuration)
hashCode()
readFields(java.io.DataInput)
toExtendedShort()
toString()
write(java.io.DataOutput)
getBlockSize()
getChecksumType()
getFileBufferSize()
getTrashInterval()
getBytesPerChecksum()
getEncryptDataTransfer()
getReplication()
getWritePacketSize()getCapacity()
getUsed()
write(java.io.DataOutput)
getRemaining()
readFields(java.io.DataInput)
buffer
metricsServers
close()
getDmax(java.lang.String)
getTmax(java.lang.String)
xdr_int(int)
datagramSocket
offset
emitMetric(java.lang.String, java.lang.String, java.lang.String)
getSlope(java.lang.String)
getUnits(java.lang.String)
xdr_string(java.lang.String)
get()
getTypes()
set(org.apache.hadoop.io.Writable)
toString()
getConf()
readFields(java.io.DataInput)
setConf(org.apache.hadoop.conf.Configuration)
write(java.io.DataOutput)
getApplicationAttemptId()
setApplicationAttemptId(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
getApplicationAttemptReport()
setApplicationAttemptReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptReport)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationAttemptReport)
getApplicationId()
setApplicationId(org.apache.hadoop.yarn.api.records.ApplicationId)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId)
getApplicationAttemptList()
setApplicationAttemptList(java.util.List)
newInstance(java.util.List)
getApplicationId()
setApplicationId(org.apache.hadoop.yarn.api.records.ApplicationId)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId)
getApplicationReport()
getApplicationStates()
newInstance()
newInstance(org.apache.hadoop.yarn.api.protocolrecords.ApplicationsRequestScope, java.util.Set, java.util.Set, java.util.Set, java.util.Set, java.util.EnumSet, org.apache.commons.lang.math.LongRange, org.apache.commons.lang.math.LongRange, java.lang.Long)
newInstance(java.util.Set)
getApplicationTypes()
newInstance(org.apache.hadoop.yarn.api.protocolrecords.ApplicationsRequestScope)
newInstance(java.util.EnumSet)
newInstance(java.util.Set, java.util.EnumSet)
getApplicationList()
newInstance()
getClusterMetrics()
newInstance()
getNodeLabels()
setNodeLabels(java.util.Set)
newInstance(java.util.Set)
getNodeStates()
newInstance(java.util.EnumSet)
newInstance()
setNodeStates(java.util.EnumSet)
getNodeReports()
getContainerId()
setContainerId(org.apache.hadoop.yarn.api.records.ContainerId)
newInstance(org.apache.hadoop.yarn.api.records.ContainerId)
getContainerReport()
setContainerReport(org.apache.hadoop.yarn.api.records.ContainerReport)
newInstance(org.apache.hadoop.yarn.api.records.ContainerReport)
getContainerIds()
setContainerIds(java.util.List)
newInstance(java.util.List)
getContainerStatuses()
getFailedRequests()getApplicationAttemptId()
setApplicationAttemptId(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
getContainerList()
setContainerList(java.util.List)
newInstance(java.util.List)
getRenewer()
setRenewer(java.lang.String)
newInstance(java.lang.String)
getRMDelegationToken()
newInstance()
getApplicationId()
getMaximumResourceCapability()
getIncludeApplications()
getQueueName()
newInstance(java.lang.String, boolean, boolean, boolean)
setIncludeChildQueues(boolean)
setRecursive(boolean)
getIncludeChildQueues()
getRecursive()
setIncludeApplications(boolean)
setQueueName(java.lang.String)
getQueueInfo()
newInstance()
getUserAclsInfoList()
compile(java.lang.String)
close()
init(org.apache.commons.configuration.SubsetConfiguration)
flush()
putMetrics(org.apache.hadoop.metrics2.MetricsRecord)
GROUP_MAPPING_CONFIG_PREFIX
cacheGroupsAdd(java.util.List)
getGroups(java.lang.String)
cacheGroupsRefresh()createCompressor()
createDirectDecompressor()
createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor)
createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor)
getDecompressorType()
createDecompressor()
createInputStream(java.io.InputStream)
createOutputStream(java.io.OutputStream)
getCompressorType()
getDefaultExtension()versionID
getServiceStatus()
transitionToActive(org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo)
monitorHealth()
transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo)monitorHealth(org.apache.hadoop.ha.HAServiceProtocol, org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo)
transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol, org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo)
transitionToActive(org.apache.hadoop.ha.HAServiceProtocol, org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo)
addFencingParameters(java.util.Map)
getAddress()
getFencingParameters()
getZKFCAddress()
isAutoFailoverEnabled()
checkFencingConfigured()
getFencer()
getProxy(org.apache.hadoop.conf.Configuration, int)
getZKFCProxy(org.apache.hadoop.conf.Configuration, int)
clear()
hash(org.apache.hadoop.util.bloom.Key)getPartition(K, V, int)
hashCode()
compareTo(org.apache.hadoop.fs.VolumeId)
hashCode()
equals(java.lang.Object)
toString()
jobListCache
createJobListCache()
findTimestampedDirectories()
getFileInfo(org.apache.hadoop.mapreduce.v2.api.records.JobId)
scanDirectoryForHistoryFiles(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileContext)
serviceStop()
moveToDoneExecutor
deleteDir(org.apache.hadoop.fs.FileStatus)
getAllFileInfo()
scanDirectory(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileContext, org.apache.hadoop.fs.PathFilter)
serviceInit(org.apache.hadoop.conf.Configuration)
setMaxHistoryAge(long)getAllPartialJobs()
getPartialJobs(java.lang.Long, java.lang.Long, java.lang.String, java.lang.String, java.lang.Long, java.lang.Long, java.lang.Long, java.lang.Long, org.apache.hadoop.mapreduce.v2.api.records.JobState)
getFullJob(org.apache.hadoop.mapreduce.v2.api.records.JobId)
setHistoryFileManager(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager)
id
ID()
compareTo(org.apache.hadoop.mapreduce.ID)
getId()
readFields(java.io.DataInput)
write(java.io.DataOutput)
SEPARATOR
ID(int)
equals(java.lang.Object)
hashCode()
toString()cleanup(org.apache.commons.logging.Log, java.io.Closeable...)
closeStream(java.io.Closeable)
copyBytes(java.io.InputStream, java.io.OutputStream, org.apache.hadoop.conf.Configuration, boolean)
copyBytes(java.io.InputStream, java.io.OutputStream, int, boolean)
listDirectory(java.io.File, java.io.FilenameFilter)
skipFully(java.io.InputStream, long)
writeFully(java.nio.channels.FileChannel, java.nio.ByteBuffer, long)
closeSocket(java.net.Socket)
copyBytes(java.io.InputStream, java.io.OutputStream, org.apache.hadoop.conf.Configuration)
copyBytes(java.io.InputStream, java.io.OutputStream, int)
copyBytes(java.io.InputStream, java.io.OutputStream, long, boolean)
readFully(java.io.InputStream, byte[], int, int)
wrappedReadForCompressedData(java.io.InputStream, byte[], int, int)
writeFully(java.nio.channels.WritableByteChannel, java.nio.ByteBuffer)
isIn(java.lang.String)
getGid(java.lang.String)
getGroupName(int, java.lang.String)
getUidAllowingUnknown(java.lang.String)
getGidAllowingUnknown(java.lang.String)
getUid(java.lang.String)
getUserName(int, java.lang.String)
map(K, V, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)
reduce(K, java.util.Iterator, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)
done()
incr()combine(java.lang.Object[], org.apache.hadoop.mapreduce.lib.join.TupleWritable)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
getSplits(org.apache.hadoop.mapreduce.JobContext)
main(java.lang.String[])
writePartitionFile(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.mapreduce.lib.partition.InputSampler.Sampler)
run(java.lang.String[])
getLength()
getLocations()
getLocationInfo()getLocationInfo()
reduce(Key, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer.Context)
compareTo(org.apache.hadoop.io.IntWritable)
get()
readFields(java.io.DataInput)
toString()
equals(java.lang.Object)
hashCode()
set(int)
write(java.io.DataOutput)
split(org.apache.hadoop.conf.Configuration, java.sql.ResultSet, java.lang.String)
info(java.lang.String, java.lang.String)
tag(java.lang.String, java.lang.String, java.lang.String)
tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String)
getMessage()
getProblems()
getCurrentState()
getEvent()map(K, V, org.apache.hadoop.mapreduce.Mapper.Context)
genCode(java.lang.String, java.lang.String, java.util.ArrayList)
COMPLETION_POLL_INTERVAL_KEY
OUTPUT_FILTER
SUBMIT_REPLICATION
addArchiveToClassPath(org.apache.hadoop.fs.Path)
addCacheFile(java.net.URI)
cleanupProgress()
failTask(org.apache.hadoop.mapreduce.TaskAttemptID)
getCounters()
getHistoryUrl()
getInstance(org.apache.hadoop.mapreduce.Cluster)
getInstance()
getInstance(org.apache.hadoop.conf.Configuration)
getInstance(org.apache.hadoop.mapreduce.JobStatus, org.apache.hadoop.conf.Configuration)
getJobName()
getPriority()
getReservationId()
getStartTime()
getTaskCompletionEvents(int)
getTaskDiagnostics(org.apache.hadoop.mapreduce.TaskAttemptID)
getTaskReports(org.apache.hadoop.mapreduce.TaskType)
isComplete()
isSuccessful()
killJob()
mapProgress()
reduceProgress()
setCacheFiles(java.net.URI[])
setCombinerClass(java.lang.Class)
setGroupingComparatorClass(java.lang.Class)
reduce(KEYIN, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer.Context)
setJar(java.lang.String)
setJobName(java.lang.String)
setMapOutputKeyClass(java.lang.Class)
setMapperClass(java.lang.Class)
setMaxMapAttempts(int)
setNumReduceTasks(int)
setOutputKeyClass(java.lang.Class)
setPartitionerClass(java.lang.Class)
setProfileEnabled(boolean)
setProfileTaskRange(boolean, java.lang.String)
setReduceSpeculativeExecution(boolean)
setSortComparatorClass(java.lang.Class)
setTaskOutputFilter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.mapreduce.Job.TaskStatusFilter)
setUser(java.lang.String)
submit()
waitForCompletion(boolean)
DEFAULT_SUBMIT_REPLICATION
PROGRESS_MONITOR_POLL_INTERVAL_KEY
USED_GENERIC_PARSER
addCacheArchive(java.net.URI)
addFileToClassPath(org.apache.hadoop.fs.Path)
createSymlink()
getCompletionPollInterval(org.apache.hadoop.conf.Configuration)
getFinishTime()
getInstance()
getInstance(org.apache.hadoop.mapreduce.Cluster, org.apache.hadoop.conf.Configuration)
getInstance(org.apache.hadoop.conf.Configuration)
getInstance(org.apache.hadoop.conf.Configuration, java.lang.String)
getJobFile()
getJobState()
getProgressPollInterval(org.apache.hadoop.conf.Configuration)
getSchedulingInfo()
getStatus()
getTaskCompletionEvents(int, int)
getTaskOutputFilter(org.apache.hadoop.conf.Configuration)
getTrackingURL()
isRetired()
isUber()
killTask(org.apache.hadoop.mapreduce.TaskAttemptID)
monitorAndPrintJob()
setCacheArchives(java.net.URI[])
setCancelDelegationTokenUponJobCompletion(boolean)
setCombinerKeyGroupingComparatorClass(java.lang.Class)
reduce(KEYIN, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer.Context)
setInputFormatClass(java.lang.Class)
setJarByClass(java.lang.Class)
setJobSetupCleanupNeeded(boolean)
setMapOutputValueClass(java.lang.Class)
setMapSpeculativeExecution(boolean)
setMaxReduceAttempts(int)
setOutputFormatClass(java.lang.Class)
setOutputValueClass(java.lang.Class)
setPriority(org.apache.hadoop.mapreduce.JobPriority)
setProfileParams(java.lang.String)
setReducerClass(java.lang.Class)
setReservationId(org.apache.hadoop.yarn.api.records.ReservationId)
setSpeculativeExecution(boolean)
setupProgress()
setWorkingDirectory(org.apache.hadoop.fs.Path)
toString()cancelDelegationToken(org.apache.hadoop.security.token.Token)
displayTasks(org.apache.hadoop.mapred.JobID, java.lang.String, java.lang.String)
getChildQueues(java.lang.String)
getClusterHandle()
getClusterStatus(boolean)
getDefaultMaps()
getDelegationToken(org.apache.hadoop.io.Text)
getJob(org.apache.hadoop.mapred.JobID)
getJobInner(org.apache.hadoop.mapred.JobID)
getMapTaskReports(org.apache.hadoop.mapred.JobID)
getQueueAclsForCurrentUser()
getQueues()
getReduceTaskReports(java.lang.String)
getReduceTaskReports(org.apache.hadoop.mapred.JobID)
getSetupTaskReports(org.apache.hadoop.mapred.JobID)
getSystemDir()
getTaskOutputFilter(org.apache.hadoop.mapred.JobConf)
isJobDirValid(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem)
main(java.lang.String[])
renewDelegationToken(org.apache.hadoop.security.token.Token)
setTaskOutputFilter(org.apache.hadoop.mapred.JobClient.TaskStatusFilter)
submitJob(org.apache.hadoop.mapred.JobConf)
close()
getAllJobs()
getCleanupTaskReports(org.apache.hadoop.mapred.JobID)
getClusterStatus()
getCounter(org.apache.hadoop.mapreduce.Counters, java.lang.String, java.lang.String)
getDefaultReduces()
getFs()
getJob(java.lang.String)
getJob(org.apache.hadoop.mapred.JobID)
getJobsFromQueue(java.lang.String)
getMapTaskReports(java.lang.String)
getMapTaskReports(org.apache.hadoop.mapred.JobID)
getQueueInfo(java.lang.String)
getReduceTaskReports(org.apache.hadoop.mapred.JobID)
getRootQueues()
getStagingAreaDir()
getTaskOutputFilter()
init(org.apache.hadoop.mapred.JobConf)
jobsToComplete()
monitorAndPrintJob(org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.RunningJob)
runJob(org.apache.hadoop.mapred.JobConf)
setTaskOutputFilter(org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.JobClient.TaskStatusFilter)
submitJob(java.lang.String)
DEFAULT_LOG_LEVEL
DEFAULT_MAPREDUCE_RECOVER_JOB
DISABLED_MEMORY_LIMIT
MAPRED_JOB_REDUCE_MEMORY_MB_PROPERTY
MAPRED_MAP_TASK_ENV
MAPRED_MAP_TASK_LOG_LEVEL
MAPRED_REDUCE_TASK_ENV
MAPRED_REDUCE_TASK_LOG_LEVEL
MAPRED_TASK_DEFAULT_MAXVMEM_PROPERTY
MAPRED_TASK_JAVA_OPTS
MAPRED_MAP_TASK_JAVA_OPTS
MAPRED_REDUCE_TASK_JAVA_OPTS
MAPRED_TASK_MAXVMEM_PROPERTY
MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY
MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY
MAPREDUCE_RECOVER_JOB
UPPER_LIMIT_ON_TASK_VMEM_PROPERTY
WORKFLOW_ADJACENCY_PREFIX_STRING
WORKFLOW_NAME
WORKFLOW_TAGS
deleteLocalFiles()
findContainingJar(java.lang.Class)
getCombinerKeyGroupingComparator()
getCredentials()
getJar()
getJobEndNotificationURI()
getJobName()
getKeepFailedTaskFiles()
getKeyFieldComparatorOption()
getLocalDirs()
getMapDebugScript()
getMapOutputKeyClass()
getMapperClass()
getMapSpeculativeExecution()
getMaxMapTaskFailuresPercent()
getMaxReduceAttempts()
getMaxTaskFailuresPerTracker()
getMemoryForMapTask()
getNumMapTasks()
getNumTasksToExecutePerJvm()
getOutputFormat()
getOutputKeyComparator()
getOutputValueGroupingComparator()
getProfileEnabled()
getProfileTaskRange(boolean)
getReduceDebugScript()
getReduceSpeculativeExecution()
getSpeculativeExecution()
getUseNewReducer()
getWorkingDirectory()
setCombinerClass(java.lang.Class)
setCompressMapOutput(boolean)
setJar(java.lang.String)
setJobEndNotificationURI(java.lang.String)
setJobPriority(org.apache.hadoop.mapred.JobPriority)
setKeepTaskFilesPattern(java.lang.String)
setKeyFieldPartitionerOptions(java.lang.String)
setMapOutputCompressorClass(java.lang.Class)
setMapOutputValueClass(java.lang.Class)
setMapRunnerClass(java.lang.Class)
setMaxMapAttempts(int)
setMaxPhysicalMemoryForTask(long)
setMaxReduceTaskFailuresPercent(int)
setMaxVirtualMemoryForTask(long)
setMemoryForMapTask(long)
setMemoryForReduceTask(long)
setMemoryForReduceTask(long)
setNumReduceTasks(int)
setOutputCommitter(java.lang.Class)
setOutputKeyClass(java.lang.Class)
setOutputValueClass(java.lang.Class)
setPartitionerClass(java.lang.Class)
setProfileParams(java.lang.String)
setQueueName(java.lang.String)
setReducerClass(java.lang.Class)
setSessionId(java.lang.String)
setUseNewMapper(boolean)
setUser(java.lang.String)
DEFAULT_MAPRED_TASK_JAVA_OPTS
DEFAULT_QUEUE_NAME
MAPRED_JOB_MAP_MEMORY_MB_PROPERTY
MAPRED_LOCAL_DIR_PROPERTY
MAPRED_MAP_TASK_JAVA_OPTS
MAPRED_MAP_TASK_ULIMIT
MAPRED_REDUCE_TASK_JAVA_OPTS
MAPRED_REDUCE_TASK_ULIMIT
MAPRED_TASK_ENV
MAPRED_MAP_TASK_ENV
MAPRED_REDUCE_TASK_ENV
MAPRED_TASK_MAXPMEM_PROPERTY
MAPRED_TASK_ULIMIT
UNPACK_JAR_PATTERN_DEFAULT
WORKFLOW_ADJACENCY_PREFIX_PATTERN
WORKFLOW_ID
WORKFLOW_NODE_NAME
deleteLocalFiles(java.lang.String)
getCombinerClass()
getCompressMapOutput()
getInputFormat()
getJarUnpackPattern()
getJobLocalDir()
getJobPriority()
getKeepTaskFilesPattern()
getKeyFieldPartitionerOption()
getLocalPath(java.lang.String)
getMapOutputCompressorClass(java.lang.Class)
getMapOutputValueClass()
getMapRunnerClass()
getMaxMapAttempts()
getMaxPhysicalMemoryForTask()
getMaxReduceTaskFailuresPercent()
getMaxVirtualMemoryForTask()
getMemoryForMapTask()
getMemoryForReduceTask()
getMemoryForReduceTask()
getNumReduceTasks()
getOutputCommitter()
getOutputKeyClass()
getOutputValueClass()
getPartitionerClass()
getProfileParams()
getQueueName()
getReducerClass()
getSessionId()
getUseNewMapper()
getUser()
normalizeMemoryConfigValue(long)
setCombinerKeyGroupingComparator(java.lang.Class)
setInputFormat(java.lang.Class)
setJarByClass(java.lang.Class)
setJobName(java.lang.String)
setKeepFailedTaskFiles(boolean)
setKeyFieldComparatorOptions(java.lang.String)
setMapDebugScript(java.lang.String)
setMapOutputKeyClass(java.lang.Class)
setMapperClass(java.lang.Class)
setMapSpeculativeExecution(boolean)
setMaxMapTaskFailuresPercent(int)
setMaxReduceAttempts(int)
setMaxTaskFailuresPerTracker(int)
setMemoryForMapTask(long)
setNumMapTasks(int)
setNumTasksToExecutePerJvm(int)
setOutputFormat(java.lang.Class)
setOutputKeyComparatorClass(java.lang.Class)
setOutputValueGroupingComparator(java.lang.Class)
setProfileEnabled(boolean)
setProfileTaskRange(boolean, java.lang.String)
setReduceDebugScript(java.lang.String)
setReduceSpeculativeExecution(boolean)
setSpeculativeExecution(boolean)
setUseNewReducer(boolean)
setWorkingDirectory(org.apache.hadoop.fs.Path)
configure(org.apache.hadoop.mapred.JobConf)
getArchiveClassPaths()
getCacheArchives()
getCombinerClass()
getConfiguration()
getFileClassPaths()
getGroupingComparator()
getJar()
getJobName()
getLocalCacheArchives()
getCacheArchives()
getMapOutputKeyClass()
getMapperClass()
getMaxReduceAttempts()
getOutputFormatClass()
getOutputValueClass()
getProfileEnabled()
getProfileTaskRange(boolean)
getSortComparator()
getTaskCleanupNeeded()
getWorkingDirectory()
getArchiveTimestamps()
getCacheFiles()
getCombinerKeyGroupingComparator()
getCredentials()
getFileTimestamps()
getInputFormatClass()
getJobID()
getJobSetupCleanupNeeded()
getLocalCacheFiles()
getCacheFiles()
getMapOutputValueClass()
getMaxMapAttempts()
getNumReduceTasks()
getOutputKeyClass()
getPartitionerClass()
getProfileParams()
getReducerClass()
getSymlink()
getUser()addJob(org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob)
addJobCollection(java.util.Collection)
getFailedJobList()
getRunningJobList()
getThreadState()
resume()
stop()
addJob(org.apache.hadoop.mapred.jobcontrol.Job)
allFinished()
getReadyJobsList()
getSuccessfulJobList()
getWaitingJobList()
run()
suspend()valueOf(java.lang.String)
values()idFormat
JOBID_REGEX
appendTo(java.lang.StringBuilder)
equals(java.lang.Object)
getJtIdentifier()
readFields(java.io.DataInput)
write(java.io.DataOutput)
JOB
compareTo(org.apache.hadoop.mapreduce.ID)
forName(java.lang.String)
hashCode()
toString()valueOf(java.lang.String)
values()getChildren()
getQueueState()clone()
getFailureInfo()
getHistoryFile()
getJobFile()
getJobName()
getNeededMem()
getNumUsedSlots()
getQueue()
getReservedMem()
getSetupProgress()
getState()
getUsedMem()
isJobComplete()
isUber()
setCleanupProgress(float)
setFinishTime(long)
setJobACLs(java.util.Map)
setNeededMem(int)
setNumUsedSlots(int)
setQueue(java.lang.String)
setReservedMem(int)
setSchedulingInfo(java.lang.String)
setStartTime(long)
setTrackingUrl(java.lang.String)
setUsedMem(int)
toString()
getCleanupProgress()
getFinishTime()
getJobACLs()
getJobID()
getMapProgress()
getNumReservedSlots()
getPriority()
getReduceProgress()
getSchedulingInfo()
getStartTime()
getTrackingUrl()
getUsername()
isRetired()
readFields(java.io.DataInput)
setFailureInfo(java.lang.String)
setHistoryFile(java.lang.String)
setMapProgress(float)
setNumReservedSlots(int)
setPriority(org.apache.hadoop.mapreduce.JobPriority)
setReduceProgress(float)
setRetired()
setSetupProgress(float)
setState(org.apache.hadoop.mapreduce.JobStatus.State)
setUber(boolean)
setUsername(java.lang.String)
write(java.io.DataOutput)
createValue()
nextKeyValue()
getDelegate()
COMPARATOR_OPTIONS
compare(byte[], int, int, byte[], int, int)
getKeyFieldComparatorOption(org.apache.hadoop.mapreduce.JobContext)
setKeyFieldComparatorOptions(org.apache.hadoop.mapreduce.Job, java.lang.String)
getConf()
setConf(org.apache.hadoop.conf.Configuration)
PARTITIONER_OPTIONS
getConf()
getPartition(int, int)
hashCode(byte[], int, int, int)
setKeyFieldPartitionerOptions(org.apache.hadoop.mapreduce.Job, java.lang.String)
getKeyFieldPartitionerOption(org.apache.hadoop.mapreduce.JobContext)
getPartition(K2, V2, int)
setConf(org.apache.hadoop.conf.Configuration)
DEFAULT_BITLENGTH
DEFAULT_CIPHER
buildVersionName(java.lang.String, int)
createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider.Options)
deleteKey(java.lang.String)
flush()
getBaseName(java.lang.String)
getCurrentKey(java.lang.String)
getKeysMetadata(java.lang.String...)
getKeyVersions(java.lang.String)
isTransient()
rollNewVersion(java.lang.String)
DEFAULT_BITLENGTH_NAME
DEFAULT_CIPHER_NAME
close()
createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider.Options)
findProvider(java.util.List, java.lang.String)
generateKey(int, java.lang.String)
getConf()
getKeys()
getKeyVersion(java.lang.String)
getMetadata(java.lang.String)
options(org.apache.hadoop.conf.Configuration)
rollNewVersion(java.lang.String, byte[])
KEY_PROVIDER_PATH
createProvider(java.net.URI, org.apache.hadoop.conf.Configuration)
getProviders(org.apache.hadoop.conf.Configuration)
get(java.net.URI, org.apache.hadoop.conf.Configuration)
KEY_VALUE_SEPERATOR
close()
getCurrentKey()
getKeyClass()
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
setKeyValue(org.apache.hadoop.io.Text, org.apache.hadoop.io.Text, byte[], int, int)
findSeparator(byte[], int, int, byte)
getCurrentValue()
getProgress()
nextKeyValue()createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
isSplitable(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.fs.Path)
getApplicationId()
setApplicationId(org.apache.hadoop.yarn.api.records.ApplicationId)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId)
getIsKillCompleted()
OUTPUT_FORMAT
checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
setOutputFormatClass(org.apache.hadoop.mapreduce.Job, java.lang.Class)
copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
getLinkTarget(org.apache.hadoop.fs.Path)
getLinkTarget(org.apache.hadoop.fs.Path)
getScheme()
pathToFile(org.apache.hadoop.fs.Path)
supportsSymlinks()
supportsSymlinks()
copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getRaw()
initialize(java.net.URI, org.apache.hadoop.conf.Configuration)
reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long)
getPattern()
getShouldBeUploadedToSharedCache()
getTimestamp()
getVisibility()
newInstance(org.apache.hadoop.yarn.api.records.URL, org.apache.hadoop.yarn.api.records.LocalResourceType, org.apache.hadoop.yarn.api.records.LocalResourceVisibility, long, long, boolean)
newInstance(org.apache.hadoop.yarn.api.records.URL, org.apache.hadoop.yarn.api.records.LocalResourceType, org.apache.hadoop.yarn.api.records.LocalResourceVisibility, long, long, java.lang.String, boolean)
setResource(org.apache.hadoop.yarn.api.records.URL)
setSize(long)
setType(org.apache.hadoop.yarn.api.records.LocalResourceType)
getResource()
getSize()
getType()
newInstance(org.apache.hadoop.yarn.api.records.URL, org.apache.hadoop.yarn.api.records.LocalResourceType, org.apache.hadoop.yarn.api.records.LocalResourceVisibility, long, long)
newInstance(org.apache.hadoop.yarn.api.records.URL, org.apache.hadoop.yarn.api.records.LocalResourceType, org.apache.hadoop.yarn.api.records.LocalResourceVisibility, long, long, java.lang.String)
setPattern(java.lang.String)
setShouldBeUploadedToSharedCache(boolean)
setTimestamp(long)
setVisibility(org.apache.hadoop.yarn.api.records.LocalResourceVisibility)
valueOf(java.lang.String)
values()valueOf(java.lang.String)
values()compareTo(java.lang.Object)
getBlockLocations()
equals(java.lang.Object)
hashCode()getExcludePattern()
getRolledLogsExcludePattern()
newInstance(java.lang.String, java.lang.String)
setExcludePattern(java.lang.String)
setRolledLogsExcludePattern(java.lang.String)
getIncludePattern()
getRolledLogsIncludePattern()
newInstance(java.lang.String, java.lang.String, java.lang.String, java.lang.String)
setIncludePattern(java.lang.String)
setRolledLogsIncludePattern(java.lang.String)
stateChanged(org.apache.hadoop.service.Service)
HELP_CMD
createYarnClient()
run(java.lang.String[])
main(java.lang.String[])
reduce(KEY, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer.Context)
addNextValue(long)
getCombinerOutput()
getVal()
addNextValue(java.lang.Object)
getReport()
reset()addNextValue(long)
getCombinerOutput()
getVal()
addNextValue(java.lang.Object)
getReport()
reset()addNextValue(long)
getCombinerOutput()
getSum()
addNextValue(java.lang.Object)
getReport()
reset()compareTo(org.apache.hadoop.io.LongWritable)
get()
readFields(java.io.DataInput)
toString()
equals(java.lang.Object)
hashCode()
set(long)
write(java.io.DataOutput)
register(java.lang.String, java.lang.String, java.lang.Object)
unregister(javax.management.ObjectName)
MD5_LEN
compareTo(org.apache.hadoop.io.MD5Hash)
digest(byte[], int, int)
digest(java.lang.String)
equals(java.lang.Object)
getDigester()
hashCode()
read(java.io.DataInput)
set(org.apache.hadoop.io.MD5Hash)
toString()
digest(byte[])
digest(java.io.InputStream)
digest(org.apache.hadoop.io.UTF8)
getDigest()
halfDigest()
quarterDigest()
readFields(java.io.DataInput)
setDigest(java.lang.String)
write(java.io.DataOutput)
getInputSplit()
DATA_FILE_NAME
MapFile()
delete(org.apache.hadoop.fs.FileSystem, java.lang.String)
main(java.lang.String[])
INDEX_FILE_NAME
fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, boolean, org.apache.hadoop.conf.Configuration)
rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String)
getEntry(org.apache.hadoop.io.MapFile.Reader[], org.apache.hadoop.mapreduce.Partitioner, K, V)
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
getReaders(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)
close()
configure(org.apache.hadoop.mapred.JobConf)
run(org.apache.hadoop.mapred.RecordReader, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)
configure(org.apache.hadoop.mapred.JobConf)
run(org.apache.hadoop.mapred.RecordReader, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)
getMapper()equals(java.lang.Object)
getValueTypeID()
getKeyTypeID()
hashCode()clear()
containsValue(java.lang.Object)
equals(java.lang.Object)
hashCode()
keySet()
putAll(java.util.Map)
remove(java.lang.Object)
values()
containsKey(java.lang.Object)
entrySet()
get(java.lang.Object)
isEmpty()
put(org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable)
readFields(java.io.DataInput)
size()
write(java.io.DataOutput)
cleanup(org.apache.hadoop.mapreduce.Mapper.Context)
run(org.apache.hadoop.mapreduce.Mapper.Context)
map(KEYIN, VALUEIN, org.apache.hadoop.mapreduce.Mapper.Context)
setup(org.apache.hadoop.mapreduce.Mapper.Context)clearMark()
mark()
remove()
hasNext()
next()
reset()
ABSOLUTE
getNumber()
isIncrement()
INCREMENT
isAbsolute()
get(java.lang.String, java.util.Collection)
update(org.apache.hadoop.metrics2.MetricsRecord, boolean)
update(org.apache.hadoop.metrics2.MetricsRecord)
addRecord(org.apache.hadoop.metrics2.MetricsInfo)
addRecord(java.lang.String)
accepts(java.lang.Iterable)
accepts(org.apache.hadoop.metrics2.MetricsTag)
accepts(org.apache.hadoop.metrics2.MetricsRecord)
accepts(java.lang.String)
description()
name()init(org.apache.commons.configuration.SubsetConfiguration)
context()
metrics()
tags()
description()
name()
timestamp()add(org.apache.hadoop.metrics2.AbstractMetric)
addCounter(org.apache.hadoop.metrics2.MetricsInfo, int)
addGauge(org.apache.hadoop.metrics2.MetricsInfo, double)
addGauge(org.apache.hadoop.metrics2.MetricsInfo, int)
endRecord()
setContext(java.lang.String)
add(org.apache.hadoop.metrics2.MetricsTag)
addCounter(org.apache.hadoop.metrics2.MetricsInfo, long)
addGauge(org.apache.hadoop.metrics2.MetricsInfo, float)
addGauge(org.apache.hadoop.metrics2.MetricsInfo, long)
parent()
tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String)
MetricsRecordImpl(java.lang.String, org.apache.hadoop.metrics.spi.AbstractMetricsContext)
getRecordName()
incrMetric(java.lang.String, float)
incrMetric(java.lang.String, long)
remove()
setMetric(java.lang.String, byte)
setMetric(java.lang.String, int)
setMetric(java.lang.String, short)
setTag(java.lang.String, int)
setTag(java.lang.String, short)
update()
incrMetric(java.lang.String, byte)
incrMetric(java.lang.String, int)
incrMetric(java.lang.String, short)
removeTag(java.lang.String)
setMetric(java.lang.String, float)
setMetric(java.lang.String, long)
setTag(java.lang.String, byte)
setTag(java.lang.String, long)
setTag(java.lang.String, java.lang.String)
add(java.lang.String, long)
getTag(java.lang.String)
newCounter(org.apache.hadoop.metrics2.MetricsInfo, int)
newCounter(java.lang.String, java.lang.String, int)
newGauge(org.apache.hadoop.metrics2.MetricsInfo, int)
newGauge(java.lang.String, java.lang.String, int)
newQuantiles(java.lang.String, java.lang.String, java.lang.String, java.lang.String, int)
newRate(java.lang.String, java.lang.String)
newStat(java.lang.String, java.lang.String, java.lang.String, java.lang.String)
setContext(java.lang.String)
tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String)
tag(java.lang.String, java.lang.String, java.lang.String)
toString()
get(java.lang.String)
info()
newCounter(org.apache.hadoop.metrics2.MetricsInfo, long)
newCounter(java.lang.String, java.lang.String, long)
newGauge(org.apache.hadoop.metrics2.MetricsInfo, long)
newGauge(java.lang.String, java.lang.String, long)
newRate(java.lang.String)
newRate(java.lang.String, java.lang.String, boolean)
newStat(java.lang.String, java.lang.String, java.lang.String, java.lang.String, boolean)
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean)
tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String, boolean)
tag(java.lang.String, java.lang.String, java.lang.String, boolean)
flush()
putMetrics(org.apache.hadoop.metrics2.MetricsRecord)
getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean)
publishMetricsNow()
register(java.lang.String, java.lang.String, T)
register(T)
unregisterSource(java.lang.String)
register(org.apache.hadoop.metrics2.MetricsSystem.Callback)
register(java.lang.String, java.lang.String, T)
shutdown()currentConfig()
startMetricsMBeans()
stopMetricsMBeans()
start()
stop()description()
hashCode()
name()
value()
equals(java.lang.Object)
info()
toString()counter(org.apache.hadoop.metrics2.MetricsInfo, int)
gauge(org.apache.hadoop.metrics2.MetricsInfo, double)
gauge(org.apache.hadoop.metrics2.MetricsInfo, int)
counter(org.apache.hadoop.metrics2.MetricsInfo, long)
gauge(org.apache.hadoop.metrics2.MetricsInfo, float)
gauge(org.apache.hadoop.metrics2.MetricsInfo, long)
initialize(java.net.URI)
run(java.lang.String[])
main(java.lang.String[])
getApplicationId()
newInstance(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String)
setTargetQueue(java.lang.String)
getTargetQueue()
setApplicationId(org.apache.hadoop.yarn.api.records.ApplicationId)
getRecordReader(org.apache.hadoop.mapred.InputSplit, org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.Reporter)
getSplits(org.apache.hadoop.mapred.JobConf, int)
listStatus(org.apache.hadoop.mapred.JobConf)getLocations()
toString()combine(java.lang.Object[], org.apache.hadoop.mapreduce.lib.join.TupleWritable)
emit(org.apache.hadoop.mapreduce.lib.join.TupleWritable)
getDelegate()
nextKeyValue()
emit(org.apache.hadoop.mapreduce.lib.join.TupleWritable)
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
transition(OPERAND, EVENT)
createIOException(java.util.List)
getExceptions()DIR_FORMATS
addInputPath(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.fs.Path, java.lang.Class)
DIR_MAPPERS
addInputPath(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class)
generateActualKey(K, V)
generateFileNameForKeyValue(K, V, java.lang.String)
getBaseRecordWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.mapred.JobConf, java.lang.String, org.apache.hadoop.util.Progressable)
getRecordWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.mapred.JobConf, java.lang.String, org.apache.hadoop.util.Progressable)
generateActualValue(K, V)
generateLeafFileName(java.lang.String)
getInputFileBasedOutputFileName(org.apache.hadoop.mapred.JobConf, java.lang.String)
addNamedOutput(org.apache.hadoop.mapreduce.Job, java.lang.String, java.lang.Class, java.lang.Class, java.lang.Class)
getCountersEnabled(org.apache.hadoop.mapreduce.JobContext)
write(KEYOUT, VALUEOUT, java.lang.String)
write(java.lang.String, K, V, java.lang.String)
close()
setCountersEnabled(org.apache.hadoop.mapreduce.Job, boolean)
write(java.lang.String, K, V)
getBaseRecordWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.mapred.JobConf, java.lang.String, org.apache.hadoop.util.Progressable)
getBaseRecordWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.mapred.JobConf, java.lang.String, org.apache.hadoop.util.Progressable)
configure(org.apache.hadoop.mapred.JobConf)
run(org.apache.hadoop.mapred.RecordReader, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)
MAP_CLASS
getMapperClass(org.apache.hadoop.mapreduce.JobContext)
run(org.apache.hadoop.mapreduce.Mapper.Context)
setNumberOfThreads(org.apache.hadoop.mapreduce.Job, int)
NUM_THREADS
getNumberOfThreads(org.apache.hadoop.mapreduce.JobContext)
setMapperClass(org.apache.hadoop.mapreduce.Job, java.lang.Class)
MutableCounter(org.apache.hadoop.metrics2.MetricsInfo)
incr()
info()incr()
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean)
incr(int)
value()incr()
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean)
incr(long)
value()MutableGauge(org.apache.hadoop.metrics2.MetricsInfo)
decr()
info()
incr()decr()
incr()
set(int)
value()
decr(int)
incr(int)
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean)
decr()
incr()
set(long)
value()
decr(long)
incr(long)
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean)
changed()
setChanged()
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean)
clearChanged()
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
previousSnapshot
add(long)
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean)
quantiles
getInterval()
add(java.lang.String, long)
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean)
init(java.lang.Class)
add(long)
resetMinMax()
snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean)
add(long, long)
setExtended(boolean)executeQuery(java.lang.String)
executeQuery(java.lang.String)
LINES_PER_MAP
createFileSplit(org.apache.hadoop.fs.Path, long, long)
getNumLinesPerSplit(org.apache.hadoop.mapreduce.JobContext)
getSplitsForFile(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.conf.Configuration, int)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
getSplits(org.apache.hadoop.mapreduce.JobContext)
setNumLinesPerSplit(org.apache.hadoop.mapreduce.Job, int)
NMClient(java.lang.String)
cleanupRunningContainersOnStop(boolean)
createNMClient(java.lang.String)
getNMTokenCache()
startContainer(org.apache.hadoop.yarn.api.records.Container, org.apache.hadoop.yarn.api.records.ContainerLaunchContext)
createNMClient()
getContainerStatus(org.apache.hadoop.yarn.api.records.ContainerId, org.apache.hadoop.yarn.api.records.NodeId)
setNMTokenCache(org.apache.hadoop.yarn.client.api.NMTokenCache)
stopContainer(org.apache.hadoop.yarn.api.records.ContainerId, org.apache.hadoop.yarn.api.records.NodeId)
callbackHandler
NMClientAsync(org.apache.hadoop.yarn.client.api.async.NMClientAsync.CallbackHandler)
NMClientAsync(java.lang.String, org.apache.hadoop.yarn.client.api.NMClient, org.apache.hadoop.yarn.client.api.async.NMClientAsync.CallbackHandler)
createNMClientAsync(org.apache.hadoop.yarn.client.api.async.NMClientAsync.CallbackHandler)
getClient()
setCallbackHandler(org.apache.hadoop.yarn.client.api.async.NMClientAsync.CallbackHandler)
startContainerAsync(org.apache.hadoop.yarn.api.records.Container, org.apache.hadoop.yarn.api.records.ContainerLaunchContext)
client
NMClientAsync(java.lang.String, org.apache.hadoop.yarn.client.api.async.NMClientAsync.CallbackHandler)
getCallbackHandler()
getContainerStatusAsync(org.apache.hadoop.yarn.api.records.ContainerId, org.apache.hadoop.yarn.api.records.NodeId)
setClient(org.apache.hadoop.yarn.client.api.NMClient)
stopContainerAsync(org.apache.hadoop.yarn.api.records.ContainerId, org.apache.hadoop.yarn.api.records.NodeId)
createNMProxy(org.apache.hadoop.conf.Configuration, java.lang.Class, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.yarn.ipc.YarnRPC, java.net.InetSocketAddress)
equals(java.lang.Object)
getToken()
setNodeId(org.apache.hadoop.yarn.api.records.NodeId)
getNodeId()
hashCode()
setToken(org.apache.hadoop.yarn.api.records.Token)
getNMToken(java.lang.String)
getToken(java.lang.String)
setToken(java.lang.String, org.apache.hadoop.yarn.api.records.Token)
getSingleton()
setNMToken(java.lang.String, org.apache.hadoop.yarn.api.records.Token)
getSingleton()KIND
equals(java.lang.Object)
getApplicationSubmitter()
getKind()
getProto()
hashCode()
toString()
getApplicationAttemptId()
getKeyId()
getNodeId()
getUser()
readFields(java.io.DataInput)
write(java.io.DataOutput)
LOG
acquireLease(org.apache.hadoop.fs.Path)
checkPath(org.apache.hadoop.fs.Path)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
delete(org.apache.hadoop.fs.Path)
delete(org.apache.hadoop.fs.Path, boolean, boolean)
finalize()
getFileStatus(org.apache.hadoop.fs.Path)
getScheme()
getUri()
initialize(java.net.URI, org.apache.hadoop.conf.Configuration)
makeAbsolute(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean)
open(org.apache.hadoop.fs.Path, int)
recoverFilesWithDanglingTempData(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setWorkingDirectory(org.apache.hadoop.fs.Path)
SKIP_AZURE_METRICS_PROPERTY_NAME
append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable)
close()
createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable)
createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable)
delete(org.apache.hadoop.fs.Path, boolean)
deleteFilesWithDanglingTempData(org.apache.hadoop.fs.Path)
getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long)
getInstrumentation()
getStore()
getWorkingDirectory()
listStatus(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
newMetricsSourceName()
pathToKey(org.apache.hadoop.fs.Path)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
LOG
append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable)
delete(org.apache.hadoop.fs.Path, boolean)
getDefaultBlockSize()
getScheme()
getWorkingDirectory()
listStatus(org.apache.hadoop.fs.Path)
open(org.apache.hadoop.fs.Path, int)
setWorkingDirectory(org.apache.hadoop.fs.Path)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
getCanonicalServiceName()
getFileStatus(org.apache.hadoop.fs.Path)
getUri()
initialize(java.net.URI, org.apache.hadoop.conf.Configuration)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
build()
equals(java.lang.Object)
getPort()
toString()
compareTo(org.apache.hadoop.yarn.api.records.NodeId)
getHost()
hashCode()getCapability()
getHttpAddress()
getNodeId()
getNodeState()
getUsed()
getHealthReport()
getLastHealthReportTime()
getNodeLabels()
getRackName()isUnusable()
values()
valueOf(java.lang.String)
getMessage()
checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
compareTo(org.apache.hadoop.io.NullWritable)
get()
readFields(java.io.DataInput)
write(java.io.DataOutput)
equals(java.lang.Object)
hashCode()
toString()get()
getDeclaredClass()
readFields(java.io.DataInput)
readObject(java.io.DataInput, org.apache.hadoop.io.ObjectWritable, org.apache.hadoop.conf.Configuration)
setConf(org.apache.hadoop.conf.Configuration)
write(java.io.DataOutput)
writeObject(java.io.DataOutput, java.lang.Object, java.lang.Class, org.apache.hadoop.conf.Configuration, boolean)
getConf()
loadClass(org.apache.hadoop.conf.Configuration, java.lang.String)
readObject(java.io.DataInput, org.apache.hadoop.conf.Configuration)
set(java.lang.Object)
toString()
writeObject(java.io.DataOutput, java.lang.Object, java.lang.Class, org.apache.hadoop.conf.Configuration)
SESSION_TIMEZONE_KEY
getSelectQuery()
setSessionTimeZone(org.apache.hadoop.conf.Configuration, java.sql.Connection)
createDBRecordReader(org.apache.hadoop.mapreduce.lib.db.DBInputFormat.DBInputSplit, org.apache.hadoop.conf.Configuration)
getSplitter(int)
dateToString(java.util.Date)
combine(java.lang.Object[], org.apache.hadoop.mapreduce.lib.join.TupleWritable)
collect(K, V)
abortJob(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.mapreduce.JobStatus.State)
cleanupJob(org.apache.hadoop.mapreduce.JobContext)
commitJob(org.apache.hadoop.mapreduce.JobContext)
abortJob(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.mapreduce.JobStatus.State)
commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
isRecoverySupported(org.apache.hadoop.mapreduce.JobContext)
recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
commitJob(org.apache.hadoop.mapreduce.JobContext)
isRecoverySupported()
isRecoverySupported(org.apache.hadoop.mapreduce.JobContext)
needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)
setupJob(org.apache.hadoop.mapreduce.JobContext)
checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
accept(org.apache.hadoop.fs.Path)
getMetric(java.lang.String)
getMetricsCopy()
getTagNames()
getMetricNames()
getTag(java.lang.String)
getTagsCopy()createValue()
fillJoinCollector(K)
emit(org.apache.hadoop.mapreduce.lib.join.TupleWritable)
currentToken
expectedTokenSequences
tokenImage
add_escapes(java.lang.String)
eol
specialConstructor
getMessage()cmpcl
ident
Parser.Node(java.lang.String)
addIdentifier(java.lang.String, java.lang.Class[], java.lang.Class, java.lang.Class)
setKeyComparator(java.lang.Class)
id
rrCstrMap
setID(int)getNode()
getNum()
getStr()
valueOf(java.lang.String)
values()getNode()
getStr()
getNum()
getType()
cleanUpPartialOutputForTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
getCommittedTaskPath(int, org.apache.hadoop.mapreduce.TaskAttemptContext)
cleanUpPartialOutputForTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
getPartition(KEY, VALUE, int)
CUR_DIR
SEPARATOR_CHAR
compareTo(java.lang.Object)
equals(java.lang.Object)
getName()
getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path)
isAbsolute()
isRoot()
isWindowsAbsolutePath(java.lang.String, boolean)
mergePaths(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
toString()
SEPARATOR
WINDOWS
depth()
getFileSystem(org.apache.hadoop.conf.Configuration)
getParent()
hashCode()
isAbsoluteAndSchemeAuthorityNull()
isUriPathAbsolute()
makeQualified(org.apache.hadoop.fs.FileSystem)
suffix(java.lang.String)
toUri()accept(org.apache.hadoop.fs.Path)
read(long, byte[], int, int)
readFully(long, byte[], int, int)
readFully(long, byte[])getId()
getContainers()
getResourceRequest()
getContainers()getContract()
getStrictContract()getResourceRequest()
UNDEFINED
compareTo(org.apache.hadoop.yarn.api.records.Priority)
getPriority()
newInstance(int)
toString()
equals(java.lang.Object)
hashCode()
setPriority(int)progress()
getValue()
update(byte[], int, int)
reset()
update(int)getValue()
update(byte[], int, int)
reset()
update(int)valueOf(java.lang.String)
values()getOperations()
readFields(java.io.DataInput)
write(java.io.DataOutput)
getQueueName()
setQueueName(java.lang.String)
getAccessibleNodeLabels()
getCapacity()
getCurrentCapacity()
getMaximumCapacity()
getQueueState()
getApplications()
getChildQueues()
getDefaultNodeLabelExpression()
getQueueName()
setDefaultNodeLabelExpression(java.lang.String)
valueOf(java.lang.String)
values()getQueueName()
getUserAcls()KIND_NAME
getKind()
selectToken(org.apache.hadoop.io.Text, java.util.Collection)
RMProxy()
createRMProxy(org.apache.hadoop.conf.Configuration, java.lang.Class, java.net.InetSocketAddress)
buffer()
size()
offset()compare(byte[], int, int, byte[], int, int)
append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable)
completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable)
createOutputStreamWithMode(org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.fs.permission.FsPermission)
delete(org.apache.hadoop.fs.Path, boolean)
getFileStatus(org.apache.hadoop.fs.Path)
getInitialWorkingDirectory()
getStatus(org.apache.hadoop.fs.Path)
getWorkingDirectory()
listStatus(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
mkOneDirWithMode(org.apache.hadoop.fs.Path, java.io.File, org.apache.hadoop.fs.permission.FsPermission)
open(org.apache.hadoop.fs.Path, int)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
setWorkingDirectory(org.apache.hadoop.fs.Path)
supportsSymlinks()
supportsSymlinks()
truncate(org.apache.hadoop.fs.Path, long)
close()
create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable)
createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
createOutputStream(org.apache.hadoop.fs.Path, boolean)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getHomeDirectory()
getLinkTarget(org.apache.hadoop.fs.Path)
getLinkTarget(org.apache.hadoop.fs.Path)
getUri()
initialize(java.net.URI, org.apache.hadoop.conf.Configuration)
mkdirs(org.apache.hadoop.fs.Path)
mkOneDir(java.io.File)
moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
pathToFile(org.apache.hadoop.fs.Path)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setTimes(org.apache.hadoop.fs.Path, long, long)
startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
toString()
useStatIfAvailable()jj_nt
token_source
disable_tracing()
enable_tracing()
generateParseException()
getToken(int)
Input()
Map()
ModuleName()
RecordList()
ReInit(java.io.InputStream, java.lang.String)
ReInit(java.io.Reader)
usage()
token
driver(java.lang.String[])
Field()
getNextToken()
Include()
main(java.lang.String[])
Module()
Record()
ReInit(java.io.InputStream)
ReInit(org.apache.hadoop.record.compiler.generated.RccTokenManager)
Type()
Vector()
addFileset(org.apache.tools.ant.types.FileSet)
setDestdir(java.io.File)
setFile(java.io.File)
execute()
setFailonerror(boolean)
setLanguage(java.lang.String)
curChar
input_stream
jjstrLiteralImages
getNextToken()
ReInit(org.apache.hadoop.record.compiler.generated.SimpleCharStream)
setDebugStream(java.io.PrintStream)
debugStream
jjnewLexState
lexStateNames
jjFillToken()
ReInit(org.apache.hadoop.record.compiler.generated.SimpleCharStream, int)
SwitchTo(int)valueOf(java.lang.String)
values()getEndTime()
getStatus()
stopped()
getStartTime()
hasTask()compareTo(java.lang.Object)
deserialize(org.apache.hadoop.record.RecordInput, java.lang.String)
serialize(org.apache.hadoop.record.RecordOutput)
toString()
deserialize(org.apache.hadoop.record.RecordInput)
readFields(java.io.DataInput)
serialize(org.apache.hadoop.record.RecordOutput, java.lang.String)
write(java.io.DataOutput)
RecordComparator(java.lang.Class)
compare(byte[], int, int, byte[], int, int)
define(java.lang.Class, org.apache.hadoop.record.RecordComparator)
endMap(java.lang.String)
endVector(java.lang.String)
readBuffer(java.lang.String)
readDouble(java.lang.String)
readInt(java.lang.String)
readString(java.lang.String)
startRecord(java.lang.String)
endRecord(java.lang.String)
readBool(java.lang.String)
readByte(java.lang.String)
readFloat(java.lang.String)
readLong(java.lang.String)
startMap(java.lang.String)
startVector(java.lang.String)
endMap(java.util.TreeMap, java.lang.String)
endVector(java.util.ArrayList, java.lang.String)
startRecord(org.apache.hadoop.record.Record, java.lang.String)
writeBool(boolean, java.lang.String)
writeByte(byte, java.lang.String)
writeFloat(float, java.lang.String)
writeLong(long, java.lang.String)
endRecord(org.apache.hadoop.record.Record, java.lang.String)
startMap(java.util.TreeMap, java.lang.String)
startVector(java.util.ArrayList, java.lang.String)
writeBuffer(org.apache.hadoop.record.Buffer, java.lang.String)
writeDouble(double, java.lang.String)
writeInt(int, java.lang.String)
writeString(java.lang.String, java.lang.String)
close()
getCurrentValue()
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
getCurrentKey()
getProgress()
nextKeyValue()addField(java.lang.String, org.apache.hadoop.record.meta.TypeID)
deserialize(org.apache.hadoop.record.RecordInput, java.lang.String)
getName()
serialize(org.apache.hadoop.record.RecordOutput, java.lang.String)
compareTo(java.lang.Object)
getFieldTypeInfos()
getNestedStructTypeInfo(java.lang.String)
setName(java.lang.String)
close(org.apache.hadoop.mapreduce.TaskAttemptContext)
write(K, V)
getValues()
nextKey()cleanup(org.apache.hadoop.mapreduce.Reducer.Context)
run(org.apache.hadoop.mapreduce.Reducer.Context)
run(org.apache.hadoop.mapreduce.Reducer.Context)
reduce(KEYIN, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer.Context)
setup(org.apache.hadoop.mapreduce.Reducer.Context)cloneWritableInto(org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable)
getClass(T)
getDeclaredMethodsIncludingInherited(java.lang.Class)
newInstance(java.lang.Class, org.apache.hadoop.conf.Configuration)
setConf(java.lang.Object, org.apache.hadoop.conf.Configuration)
copy(org.apache.hadoop.conf.Configuration, T, T)
getDeclaredFieldsIncludingInherited(java.lang.Class)
logThreadInfo(org.apache.commons.logging.Log, java.lang.String, long)
printThreadInfo(java.io.PrintStream, java.lang.String)
setContentionTracing(boolean)compile(java.lang.String)
GROUP
map(K, org.apache.hadoop.io.Text, org.apache.hadoop.mapreduce.Mapper.Context)
PATTERN
setup(org.apache.hadoop.mapreduce.Mapper.Context)getHost()
getTrackingUrl()
setHost(java.lang.String)
setTrackingUrl(java.lang.String)
getRpcPort()
newInstance(java.lang.String, int, java.lang.String)
setRpcPort(int)getApplicationACLs()
getContainersFromPreviousAttempts()
getNMTokensFromPreviousAttempts()
getSchedulerResourceTypes()
setQueue(java.lang.String)
getClientToAMTokenMasterKey()
getMaximumResourceCapability()
getQueue()
setClientToAMTokenMasterKey(java.nio.ByteBuffer)
supplyBindingInformation()
addWriteAccessor(java.lang.String, java.lang.String)
clearWriteAccessors()
exists(java.lang.String)
mknode(java.lang.String, boolean)
stat(java.lang.String)
bind(java.lang.String, org.apache.hadoop.registry.client.types.ServiceRecord, int)
delete(java.lang.String, boolean)
list(java.lang.String)
resolve(java.lang.String)
bind(java.lang.String, org.apache.hadoop.registry.client.types.ServiceRecord, int)
exists(java.lang.String)
list(java.lang.String)
resolve(java.lang.String)
validatePath(java.lang.String)
delete(java.lang.String, boolean)
getClientAcls()
mknode(java.lang.String, boolean)
stat(java.lang.String)
children
size
equals(java.lang.Object)
toString()
path
time
hashCode()getAddressField(java.util.Map, java.lang.String)
hostnamePortPair(java.lang.String, int)
ipcEndpoint(java.lang.String, java.net.InetSocketAddress)
requireAddressType(java.lang.String, org.apache.hadoop.registry.client.types.Endpoint)
retrieveAddressesUriType(org.apache.hadoop.registry.client.types.Endpoint)
uri(java.lang.String)
validateEndpoint(java.lang.String, org.apache.hadoop.registry.client.types.Endpoint)
webEndpoint(java.lang.String, java.net.URI...)
hostnamePortPair(java.net.InetSocketAddress)
inetAddrEndpoint(java.lang.String, java.lang.String, java.lang.String, int)
map(java.lang.String, java.lang.String)
restEndpoint(java.lang.String, java.net.URI...)
retrieveAddressURLs(org.apache.hadoop.registry.client.types.Endpoint)
urlEndpoint(java.lang.String, java.lang.String, java.net.URI...)
validateServiceRecord(java.lang.String, org.apache.hadoop.registry.client.types.ServiceRecord)
componentListPath(java.lang.String, java.lang.String, java.lang.String)
convertUsername(java.lang.String)
extractServiceRecords(org.apache.hadoop.registry.client.api.RegistryOperations, java.lang.String)
extractServiceRecords(org.apache.hadoop.registry.client.api.RegistryOperations, java.lang.String, java.util.Map)
homePathForCurrentUser()
listServiceRecords(org.apache.hadoop.registry.client.api.RegistryOperations, java.lang.String)
servicePath(java.lang.String, java.lang.String, java.lang.String)
componentPath(java.lang.String, java.lang.String, java.lang.String, java.lang.String)
currentUser()
extractServiceRecords(org.apache.hadoop.registry.client.api.RegistryOperations, java.lang.String, java.util.Collection)
getCurrentUsernameUnencoded(java.lang.String)
homePathForUser(java.lang.String)
serviceclassPath(java.lang.String, java.lang.String)
statChildren(org.apache.hadoop.registry.client.api.RegistryOperations, java.lang.String)
getAppId()
setAppId(org.apache.hadoop.yarn.api.records.ApplicationId)
getResourceKey()
setResourceKey(java.lang.String)
getDelegationToken()
setDelegationToken(org.apache.hadoop.yarn.api.records.Token)
getNextExpirationTime()
setNextExpirationTime(long)NULL
getCounter(java.lang.Enum)
getInputSplit()
incrCounter(java.lang.Enum, long)
setStatus(java.lang.String)
getCounter(java.lang.String, java.lang.String)
getProgress()
incrCounter(java.lang.String, java.lang.String, long)
getArrival()
getReservationName()
newInstance(long, long, org.apache.hadoop.yarn.api.records.ReservationRequests, java.lang.String)
setDeadline(long)
setReservationRequests(org.apache.hadoop.yarn.api.records.ReservationRequests)
getDeadline()
getReservationRequests()
setArrival(long)
setReservationName(java.lang.String)
getReservationId()
setReservationId(org.apache.hadoop.yarn.api.records.ReservationId)
newInstance(org.apache.hadoop.yarn.api.records.ReservationId)
clusterTimestamp
build()
equals(java.lang.Object)
getId()
parseReservationId(java.lang.String)
id
compareTo(org.apache.hadoop.yarn.api.records.ReservationId)
getClusterTimestamp()
hashCode()
toString()compare(org.apache.hadoop.yarn.api.records.ReservationRequest, org.apache.hadoop.yarn.api.records.ReservationRequest)
compareTo(org.apache.hadoop.yarn.api.records.ReservationRequest)
getCapability()
getDuration()
hashCode()
newInstance(org.apache.hadoop.yarn.api.records.Resource, int, int, long)
setConcurrency(int)
setNumContainers(int)
equals(java.lang.Object)
getConcurrency()
getNumContainers()
newInstance(org.apache.hadoop.yarn.api.records.Resource, int)
setCapability(org.apache.hadoop.yarn.api.records.Resource)
setDuration(long)valueOf(java.lang.String)
values()getInterpreter()
newInstance(java.util.List, org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter)
setReservationResources(java.util.List)
getReservationResources()
setInterpreter(org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter)
getQueue()
newInstance(org.apache.hadoop.yarn.api.records.ReservationDefinition, java.lang.String)
setReservationDefinition(org.apache.hadoop.yarn.api.records.ReservationDefinition)
getReservationDefinition()
setQueue(java.lang.String)
getReservationId()
getReservationDefinition()
newInstance(org.apache.hadoop.yarn.api.records.ReservationDefinition, org.apache.hadoop.yarn.api.records.ReservationId)
setReservationId(org.apache.hadoop.yarn.api.records.ReservationId)
getReservationId()
setReservationDefinition(org.apache.hadoop.yarn.api.records.ReservationDefinition)
add(T)
close()
next(T)
reset()
clear()
hasNext()
replay(T)
equals(java.lang.Object)
getVirtualCores()
newInstance(int, int)
setVirtualCores(int)
getMemory()
hashCode()
setMemory(int)
toString()getBlacklistAdditions()
newInstance(java.util.List, java.util.List)
setBlacklistRemovals(java.util.List)
getBlacklistRemovals()
setBlacklistAdditions(java.util.List)
UNAVAILABLE
checkPidPgrpidForMatch()
getCumulativeCpuTime()
getCumulativeRssmem(int)
getCumulativeVmem(int)
getResourceCalculatorProcessTree(java.lang.String, java.lang.Class, org.apache.hadoop.conf.Configuration)
getRssMemorySize(int)
getVirtualMemorySize(int)
getCpuUsagePercent()
getCumulativeRssmem()
getCumulativeVmem()
getProcessTreeDump()
getRssMemorySize()
getVirtualMemorySize()
updateProcessTree()newInstance(org.apache.hadoop.yarn.api.records.Resource, int)
toString()compare(org.apache.hadoop.yarn.api.records.ResourceRequest, org.apache.hadoop.yarn.api.records.ResourceRequest)
ANY
compareTo(org.apache.hadoop.yarn.api.records.ResourceRequest)
getCapability()
getNumContainers()
getRelaxLocality()
hashCode()
newInstance(org.apache.hadoop.yarn.api.records.Priority, java.lang.String, org.apache.hadoop.yarn.api.records.Resource, int)
newInstance(org.apache.hadoop.yarn.api.records.Priority, java.lang.String, org.apache.hadoop.yarn.api.records.Resource, int, boolean, java.lang.String)
setNodeLabelExpression(java.lang.String)
setPriority(org.apache.hadoop.yarn.api.records.Priority)
setResourceName(java.lang.String)
equals(java.lang.Object)
getNodeLabelExpression()
getPriority()
getResourceName()
isAnyLocation(java.lang.String)
newInstance(org.apache.hadoop.yarn.api.records.Priority, java.lang.String, org.apache.hadoop.yarn.api.records.Resource, int, boolean)
setCapability(org.apache.hadoop.yarn.api.records.Resource)
setNumContainers(int)
setRelaxLocality(boolean)add(org.apache.hadoop.util.bloom.Key)
addFalsePositive(org.apache.hadoop.util.bloom.Key)
addFalsePositive(java.util.List)
selectiveClearing(org.apache.hadoop.util.bloom.Key, short)
addFalsePositive(java.util.Collection)
addFalsePositive(org.apache.hadoop.util.bloom.Key[])
readFields(java.io.DataInput)
write(java.io.DataOutput)
cleanupProgress()
getCounters()
getHistoryUrl()
getJobFile()
getJobName()
getJobStatus()
getTaskDiagnostics(org.apache.hadoop.mapred.TaskAttemptID)
isComplete()
isSuccessful()
killTask(java.lang.String, boolean)
killTask(org.apache.hadoop.mapred.TaskAttemptID, boolean)
mapProgress()
setJobPriority(java.lang.String)
waitForCompletion()
getConfiguration()
getFailureInfo()
getID()
getJobID()
getID()
getJobState()
getTaskCompletionEvents(int)
getTrackingURL()
isRetired()
killJob()
killTask(org.apache.hadoop.mapred.TaskAttemptID, boolean)
reduceProgress()
setupProgress()
append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable)
delete(org.apache.hadoop.fs.Path, boolean)
getDefaultBlockSize()
getScheme()
getWorkingDirectory()
isFile(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
getCanonicalServiceName()
getFileStatus(org.apache.hadoop.fs.Path)
getUri()
initialize(java.net.URI, org.apache.hadoop.conf.Configuration)
listStatus(org.apache.hadoop.fs.Path)
open(org.apache.hadoop.fs.Path, int)
setWorkingDirectory(org.apache.hadoop.fs.Path)
getKerberosInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
getTokenInfo(java.lang.Class, org.apache.hadoop.conf.Configuration)
NO_SCRIPT
toString()
getConf()
toString()
setConf(org.apache.hadoop.conf.Configuration)
getPos()
seek(long)SYNC_INTERVAL
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile.CompressionType, org.apache.hadoop.io.compress.CompressionCodec)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile.CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile.Metadata)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile.CompressionType, org.apache.hadoop.io.compress.CompressionCodec)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile.CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile.Metadata)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
getDefaultCompressionType(org.apache.hadoop.conf.Configuration)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile.CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile.Metadata)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile.CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile.Metadata, java.util.EnumSet, org.apache.hadoop.fs.Options.CreateOpts...)
createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile.CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile.Metadata)
createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile.CompressionType)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile.CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile.CompressionType, org.apache.hadoop.util.Progressable)
createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.Writer.Option...)
setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile.CompressionType)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
KEY_CLASS
checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
getSequenceFileOutputKeyClass(org.apache.hadoop.mapreduce.JobContext)
setSequenceFileOutputKeyClass(org.apache.hadoop.mapreduce.Job, java.lang.Class)
VALUE_CLASS
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
getSequenceFileOutputValueClass(org.apache.hadoop.mapreduce.JobContext)
setSequenceFileOutputValueClass(org.apache.hadoop.mapreduce.Job, java.lang.Class)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
close()
getCurrentValue()
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
getCurrentKey()
getProgress()
nextKeyValue()FILTER_CLASS
FILTER_REGEX
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
FILTER_FREQUENCY
LOG
setFilterClass(org.apache.hadoop.mapreduce.Job, java.lang.Class)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
listStatus(org.apache.hadoop.mapreduce.JobContext)
getFormatMinSplitSize()getOutputCompressionType(org.apache.hadoop.mapreduce.JobContext)
getSequenceWriter(org.apache.hadoop.mapreduce.TaskAttemptContext, java.lang.Class, java.lang.Class)
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
setOutputCompressionType(org.apache.hadoop.mapreduce.Job, org.apache.hadoop.io.SequenceFile.CompressionType)
conf
close()
getCurrentValue()
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
getCurrentKey()
getProgress()
nextKeyValue()createRetriableProxy(org.apache.hadoop.conf.Configuration, java.lang.Class, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.yarn.ipc.YarnRPC, java.net.InetSocketAddress, org.apache.hadoop.io.retry.RetryPolicy)
createRetryPolicy(org.apache.hadoop.conf.Configuration, java.lang.String, long, java.lang.String, long)
parse(java.lang.String, int)
close()
getConfig()
getFailureState()
getFailureCause()
getName()
getStartTime()
isInState(org.apache.hadoop.service.Service.STATE)
start()
unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)
getBlockers()
getFailureCause()
getLifecycleHistory()
getServiceState()
init(org.apache.hadoop.conf.Configuration)
registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)
stop()
waitForServiceToStop(long)
stop(org.apache.hadoop.service.Service)
stopQuietly(org.apache.hadoop.service.Service)
stopQuietly(org.apache.commons.logging.Log, org.apache.hadoop.service.Service)
description
internal
type
addExternalEndpoint(org.apache.hadoop.registry.client.types.Endpoint)
attributes()
get(java.lang.String)
getExternalEndpoint(java.lang.String)
set(java.lang.String, java.lang.Object)
attributes
external
RECORD_TYPE
addInternalEndpoint(org.apache.hadoop.registry.client.types.Endpoint)
clone()
get(java.lang.String, java.lang.String)
getInternalEndpoint(java.lang.String)
toString()stateChanged(org.apache.hadoop.service.Service)
convert(java.lang.String, java.lang.Throwable)
convert(java.lang.Throwable)
checkStateTransition(java.lang.String, org.apache.hadoop.service.Service.STATE, org.apache.hadoop.service.Service.STATE)
enterState(org.apache.hadoop.service.Service.STATE)
isInState(org.apache.hadoop.service.Service.STATE)
toString()
ensureCurrentState(org.apache.hadoop.service.Service.STATE)
getState()
isValidStateTransition(org.apache.hadoop.service.Service.STATE, org.apache.hadoop.service.Service.STATE)
computeChecksum(java.io.InputStream)
getChecksum(org.apache.hadoop.conf.Configuration)
createSharedCacheClient()
release(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String)
getFileChecksum(org.apache.hadoop.fs.Path)
use(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String)
compareTo(org.apache.hadoop.io.ShortWritable)
get()
readFields(java.io.DataInput)
toString()
equals(java.lang.Object)
hashCode()
set(short)
write(java.io.DataOutput)
bufcolumn
bufline
column
inputStream
maxNextCharInd
prevCharIsLF
tabSize
adjustBeginLineColumn(int, int)
BeginToken()
ExpandBuff(boolean)
getBeginColumn()
getEndColumn()
GetImage()
getTabSize(int)
ReInit(java.io.InputStream)
ReInit(java.io.InputStream, int, int, int)
ReInit(java.io.InputStream, java.lang.String, int, int)
ReInit(java.io.Reader)
ReInit(java.io.Reader, int, int, int)
UpdateLineColumn(char)
buffer
bufpos
inBuf
line
prevCharIsCR
staticFlag
backup(int)
Done()
FillBuff()
getBeginLine()
getEndLine()
GetSuffix(int)
readChar()
ReInit(java.io.InputStream, int, int)
ReInit(java.io.InputStream, java.lang.String)
ReInit(java.io.InputStream, java.lang.String, int, int, int)
ReInit(java.io.Reader, int, int)
setTabSize(int)transition(OPERAND, EVENT)
COUNTER_GROUP
COUNTER_REDUCE_PROCESSED_GROUPS
getAttemptsToStartSkipping(org.apache.hadoop.conf.Configuration)
getAutoIncrReducerProcCount(org.apache.hadoop.conf.Configuration)
COUNTER_REDUCE_PROCESSED_GROUPS
getReducerMaxSkipGroups(org.apache.hadoop.conf.Configuration)
setAttemptsToStartSkipping(org.apache.hadoop.conf.Configuration, int)
setAutoIncrReducerProcCount(org.apache.hadoop.conf.Configuration, boolean)
COUNTER_REDUCE_PROCESSED_GROUPS
setReducerMaxSkipGroups(org.apache.hadoop.conf.Configuration, long)
COUNTER_MAP_PROCESSED_RECORDS
getAutoIncrMapperProcCount(org.apache.hadoop.conf.Configuration)
COUNTER_MAP_PROCESSED_RECORDS
getMapperMaxSkipRecords(org.apache.hadoop.conf.Configuration)
getSkipOutputPath(org.apache.hadoop.conf.Configuration)
setAutoIncrMapperProcCount(org.apache.hadoop.conf.Configuration, boolean)
COUNTER_MAP_PROCESSED_RECORDS
setMapperMaxSkipRecords(org.apache.hadoop.conf.Configuration, long)
setSkipOutputPath(org.apache.hadoop.mapred.JobConf, org.apache.hadoop.fs.Path)
createSocket()
createSocket(java.net.InetAddress, int, java.net.InetAddress, int)
createSocket(java.lang.String, int, java.net.InetAddress, int)
getConf()
setConf(org.apache.hadoop.conf.Configuration)
createSocket(java.net.InetAddress, int)
createSocket(java.lang.String, int)
equals(java.lang.Object)
hashCode()clear()
containsKey(java.lang.Object)
entrySet()
firstKey()
hashCode()
isEmpty()
lastKey()
putAll(java.util.Map)
remove(java.lang.Object)
subMap(org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.WritableComparable)
values()
comparator()
containsValue(java.lang.Object)
equals(java.lang.Object)
get(java.lang.Object)
headMap(org.apache.hadoop.io.WritableComparable)
keySet()
put(org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.Writable)
readFields(java.io.DataInput)
size()
tailMap(org.apache.hadoop.io.WritableComparable)
write(java.io.DataOutput)
getClassName()
getId()addConfigurationPair(java.lang.String, java.lang.String)
build()getAdjustedEnd()
setEnd(long)
getAdjustedStart()
setStart(long)getLocation()
isOnDisk()
isInMemory()createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, long, long, org.apache.hadoop.io.compress.SplittableCompressionCodec.READ_MODE)
createSocket()
createSocket(java.net.InetAddress, int, java.net.InetAddress, int)
createSocket(java.lang.String, int, java.net.InetAddress, int)
hashCode()
createSocket(java.net.InetAddress, int)
createSocket(java.lang.String, int)
equals(java.lang.Object)
getContainerLaunchContext()
newInstance(org.apache.hadoop.yarn.api.records.ContainerLaunchContext, org.apache.hadoop.yarn.api.records.Token)
setContainerToken(org.apache.hadoop.yarn.api.records.Token)
getContainerToken()
setContainerLaunchContext(org.apache.hadoop.yarn.api.records.ContainerLaunchContext)
getStartContainerRequests()
setStartContainerRequests(java.util.List)
newInstance(java.util.List)
getAllServicesMetaData()
getSuccessfullyStartedContainers()
getFailedRequests()doTransition(EVENTTYPE, EVENT)
getCurrentState()addTransition(STATE, java.util.Set, EVENTTYPE, org.apache.hadoop.yarn.state.MultipleArcTransition)
addTransition(STATE, STATE, EVENTTYPE, org.apache.hadoop.yarn.state.SingleArcTransition)
addTransition(STATE, STATE, java.util.Set, org.apache.hadoop.yarn.state.SingleArcTransition)
installTopology()
make(OPERAND, STATE)
addTransition(STATE, STATE, EVENTTYPE)
addTransition(STATE, STATE, java.util.Set)
generateStateGraph(java.lang.String)
make(OPERAND)
getContainerIds()
setContainerIds(java.util.List)
newInstance(java.util.List)
getFailedRequests()
getSuccessfullyStoppedContainers()DEFAULT
asList()
getTypesSupportingQuota()
isTransient()
parseStorageType(java.lang.String)
valueOf(java.lang.String)
EMPTY_ARRAY
getMovableTypes()
isMovable()
parseStorageType(int)
supportTypeQuota()
values()add(X)
close()
next(X)
reset()
clear()
hasNext()
replay(X)
getContainers()
strongIntern(java.lang.String)
weakIntern(java.lang.String)
addNextValue(java.lang.Object)
getReport()
reset()
getCombinerOutput()
getVal()addNextValue(java.lang.Object)
getReport()
reset()
getCombinerOutput()
getVal()close()
toString(T)
fromString(java.lang.String)
equals(java.lang.Object)
hashCode()
getFieldTypeInfos()getApplicationSubmissionContext()
setApplicationSubmissionContext(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext)
newInstance(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext)
EXECUTABLE
INTERPRETOR
IS_JAVA_REDUCE
IS_JAVA_RW
PARTITIONER
PRESERVE_COMMANDFILE
getExecutable(org.apache.hadoop.mapred.JobConf)
getIsJavaRecordReader(org.apache.hadoop.mapred.JobConf)
getIsJavaReducer(org.apache.hadoop.mapred.JobConf)
jobSubmit(org.apache.hadoop.mapred.JobConf)
run(java.lang.String[])
setExecutable(org.apache.hadoop.mapred.JobConf, java.lang.String)
setIsJavaRecordReader(org.apache.hadoop.mapred.JobConf, boolean)
setIsJavaReducer(org.apache.hadoop.mapred.JobConf, boolean)
submitJob(org.apache.hadoop.mapred.JobConf)
runJob(org.apache.hadoop.mapred.JobConf)
INPUT_FORMAT
IS_JAVA_MAP
IS_JAVA_RR
LOG
PORT
getIsJavaMapper(org.apache.hadoop.mapred.JobConf)
getIsJavaRecordWriter(org.apache.hadoop.mapred.JobConf)
getKeepCommandFile(org.apache.hadoop.mapred.JobConf)
main(java.lang.String[])
runJob(org.apache.hadoop.mapred.JobConf)
setIsJavaMapper(org.apache.hadoop.mapred.JobConf, boolean)
setIsJavaRecordWriter(org.apache.hadoop.mapred.JobConf, boolean)
setKeepCommandFile(org.apache.hadoop.mapred.JobConf, boolean)
hflush()
sync()
hsync()getTime()
COMPARATOR_JCLASS
COMPRESSION_GZ
COMPRESSION_NONE
getSupportedCompressionAlgorithms()
makeComparator(java.lang.String)
COMPARATOR_MEMCMP
COMPRESSION_LZO
main(java.lang.String[])
getConf()
setConf(org.apache.hadoop.conf.Configuration)
reloadCachedMappings()getCounter(java.lang.Enum)
getProgress()
getTaskAttemptID()
getCounter(java.lang.String, java.lang.String)
getStatus()
setStatus(java.lang.String)
ATTEMPT
appendTo(java.lang.StringBuilder)
equals(java.lang.Object)
getJobID()
getTaskType()
isMap()
toString()
compareTo(org.apache.hadoop.mapreduce.ID)
forName(java.lang.String)
getTaskID()
hashCode()
readFields(java.io.DataInput)
write(java.io.DataOutput)
valueOf(java.lang.String)
values()
EMPTY_ARRAY
equals(java.lang.Object)
getStatus()
getTaskRunTime()
hashCode()
isMapTask()
setEventId(int)
setTaskRunTime(int)
setTaskTrackerHttp(java.lang.String)
write(java.io.DataOutput)
getEventId()
getTaskAttemptId()
getTaskTrackerHttp()
idWithinJob()
readFields(java.io.DataInput)
setTaskAttemptId(org.apache.hadoop.mapreduce.TaskAttemptID)
setTaskStatus(org.apache.hadoop.mapreduce.TaskCompletionEvent.Status)
toString()valueOf(java.lang.String)
values()idFormat
appendTo(java.lang.StringBuilder)
equals(java.lang.Object)
getAllTaskTypes()
getRepresentingCharacter(org.apache.hadoop.mapreduce.TaskType)
getTaskType(char)
isMap()
toString()
TASK
compareTo(org.apache.hadoop.mapreduce.ID)
forName(java.lang.String)
getJobID()
getTaskType()
hashCode()
readFields(java.io.DataInput)
write(java.io.DataOutput)
getCurrentKey()
getOutputCommitter()
write(KEYOUT, VALUEOUT)
getCurrentValue()
nextKeyValue()getCounters()
getSuccessfulTaskAttempt()
getTaskID()
setRunningTaskAttempts(java.util.Collection)
setSuccessfulAttempt(org.apache.hadoop.mapred.TaskAttemptID)
getRunningTaskAttempts()
getTaskId()
setFinishTime(long)
setStartTime(long)getBlacklistReport()
getTaskTrackerName()
readFields(java.io.DataInput)
getReasonForBlacklist()
isBlacklisted()
write(java.io.DataOutput)
valueOf(java.lang.String)
values()DEFAULT_MAX_LEN
append(byte[], int, int)
charAt(int)
copyBytes()
decode(byte[], int, int)
encode(java.lang.String)
equals(java.lang.Object)
find(java.lang.String, int)
getLength()
readFields(java.io.DataInput)
readString(java.io.DataInput)
readWithKnownLength(java.io.DataInput, int)
set(byte[], int, int)
set(org.apache.hadoop.io.Text)
toString()
validateUTF8(byte[])
write(java.io.DataOutput)
writeString(java.io.DataOutput, java.lang.String)
bytesToCodePoint(java.nio.ByteBuffer)
clear()
decode(byte[])
decode(byte[], int, int, boolean)
encode(java.lang.String, boolean)
find(java.lang.String)
getBytes()
getLength()
hashCode()
readFields(java.io.DataInput, int)
readString(java.io.DataInput, int)
set(byte[])
set(java.lang.String)
skip(java.io.DataInput)
utf8Length(java.lang.String)
validateUTF8(byte[], int, int)
write(java.io.DataOutput, int)
writeString(java.io.DataOutput, java.lang.String, int)
createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
isSplitable(org.apache.hadoop.mapreduce.JobContext, org.apache.hadoop.fs.Path)
SEPERATOR
getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
split(org.apache.hadoop.conf.Configuration, java.sql.ResultSet, java.lang.String)
TimelineClient(java.lang.String)
cancelDelegationToken(org.apache.hadoop.security.token.Token)
getDelegationToken(java.lang.String)
putEntities(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity...)
createTimelineClient()
putDomain(org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)
renewDelegationToken(org.apache.hadoop.security.token.Token)
KIND_NAME
getKind()
getContent()
setContent(java.lang.Object)
getType()
setType(java.lang.String)
selectToken(org.apache.hadoop.io.Text, java.util.Collection)
getCreatedTime()
getId()
getOwner()
getWriters()
setDescription(java.lang.String)
setModifiedTime(java.lang.Long)
setReaders(java.lang.String)
getDescription()
getModifiedTime()
getReaders()
setCreatedTime(java.lang.Long)
setId(java.lang.String)
setOwner(java.lang.String)
setWriters(java.lang.String)
addDomain(org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)
getDomains()
addDomains(java.util.List)
setDomains(java.util.List)
addEntities(java.util.List)
getEntities()
addEntity(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)
setEntities(java.util.List)
addEvent(org.apache.hadoop.yarn.api.records.timeline.TimelineEvent)
addOtherInfo(java.util.Map)
addPrimaryFilter(java.lang.String, java.lang.Object)
addRelatedEntities(java.util.Map)
compareTo(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)
getDomainId()
getEntityType()
getOtherInfo()
getRelatedEntities()
hashCode()
setEntityId(java.lang.String)
setEvents(java.util.List)
setPrimaryFilters(java.util.Map)
setStartTime(java.lang.Long)
addEvents(java.util.List)
addOtherInfo(java.lang.String, java.lang.Object)
addPrimaryFilters(java.util.Map)
addRelatedEntity(java.lang.String, java.lang.String)
equals(java.lang.Object)
getEntityId()
getEvents()
getPrimaryFilters()
getStartTime()
setDomainId(java.lang.String)
setEntityType(java.lang.String)
setOtherInfo(java.util.Map)
setRelatedEntities(java.util.Map)
addEventInfo(java.util.Map)
compareTo(org.apache.hadoop.yarn.api.records.timeline.TimelineEvent)
getEventInfo()
getTimestamp()
setEventInfo(java.util.Map)
setTimestamp(long)
addEventInfo(java.lang.String, java.lang.Object)
equals(java.lang.Object)
getEventType()
hashCode()
setEventType(java.lang.String)
addEvent(org.apache.hadoop.yarn.api.records.timeline.TimelineEvent)
getEntityId()
getEvents()
setEntityType(java.lang.String)
addEvents(java.util.List)
getEntityType()
setEntityId(java.lang.String)
setEvents(java.util.List)
addEvent(org.apache.hadoop.yarn.api.records.timeline.TimelineEvents.EventsOfOneEntity)
getAllEvents()
addEvents(java.util.List)
setEvents(java.util.List)
ACCESS_DENIED
IO_EXCEPTION
NO_START_TIME
getEntityId()
getErrorCode()
setEntityType(java.lang.String)
FORBIDDEN_RELATION
NO_DOMAIN
SYSTEM_FILTER_CONFLICT
getEntityType()
setEntityId(java.lang.String)
setErrorCode(int)
addError(org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError)
getErrors()
addErrors(java.util.List)
setErrors(java.util.List)
buildTimelineTokenService(org.apache.hadoop.conf.Configuration)
dumpTimelineRecordtoJSON(java.lang.Object, boolean)
dumpTimelineRecordtoJSON(java.lang.Object)
getTimelineTokenServiceAddress(org.apache.hadoop.conf.Configuration)
getIdentifier()
getPassword()
getKind()
getService()cleanUpTokenReferral(org.apache.hadoop.conf.Configuration)
obtainTokensForNamenodes(org.apache.hadoop.security.Credentials, org.apache.hadoop.fs.Path[], org.apache.hadoop.conf.Configuration)
getSecretKey(org.apache.hadoop.security.Credentials, org.apache.hadoop.io.Text)
map(K, org.apache.hadoop.io.Text, org.apache.hadoop.mapred.OutputCollector, org.apache.hadoop.mapred.Reporter)
map(java.lang.Object, org.apache.hadoop.io.Text, org.apache.hadoop.mapreduce.Mapper.Context)
addEscapes(java.lang.String)
LexicalError(boolean, int, int, int, java.lang.String, char)
getMessage()run(java.lang.String[])
confirmPrompt(java.lang.String)
run(org.apache.hadoop.conf.Configuration, org.apache.hadoop.util.Tool, java.lang.String[])
run(java.lang.String[])
printGenericCommandUsage(java.io.PrintStream)
run(org.apache.hadoop.util.Tool, java.lang.String[])
DEFAULT_PATH
NATURAL_ORDER
getConf()
getPartitionFile(org.apache.hadoop.conf.Configuration)
setPartitionFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path)
MAX_TRIE_DEPTH
PARTITIONER_PATH
getPartition(K, V, int)
setConf(org.apache.hadoop.conf.Configuration)
versionID
addSpanReceiver(org.apache.hadoop.tracing.SpanReceiverInfo)
removeSpanReceiver(long)
listSpanReceivers()
checkpoint()
getEmptier()
moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration)
expunge()
isEnabled()
moveToTrash(org.apache.hadoop.fs.Path)
deletionInterval
trash
createCheckpoint()
getCurrentTrashDir()
getInstance(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)
isEnabled()
fs
deleteCheckpoint()
getEmptier()
initialize(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)
moveToTrash(org.apache.hadoop.fs.Path)
written
equals(java.lang.Object)
has(int)
iterator()
size()
write(java.io.DataOutput)
get(int)
hashCode()
readFields(java.io.DataInput)
toString()get()
set(org.apache.hadoop.io.Writable[][])
write(java.io.DataOutput)
readFields(java.io.DataInput)
toArray()BoolTypeID
ByteTypeID
FloatTypeID
LongTypeID
typeVal
equals(java.lang.Object)
hashCode()
BufferTypeID
DoubleTypeID
IntTypeID
StringTypeID
getTypeVal()getFile()
getPort()
getUserInfo()
setFile(java.lang.String)
setPort(int)
setUserInfo(java.lang.String)
getHost()
getScheme()
newInstance(java.lang.String, java.lang.String, int, java.lang.String)
setHost(java.lang.String)
setScheme(java.lang.String)
getTime()
MAX_NUM_UNIQUE_VALUES
addNextValue(java.lang.Object)
getReport()
reset()
getCombinerOutput()
getUniqueItems()
setMaxItems(long)
getAppId()
setAppId(org.apache.hadoop.yarn.api.records.ApplicationId)
getResourceKey()
setResourceKey(java.lang.String)
getPath()
setPath(java.lang.String)
theAggregatorDescriptor
configure(org.apache.hadoop.conf.Configuration)
generateKeyValPairs(java.lang.Object, java.lang.Object)
createInstance(java.lang.String)
toString()getAuthMethod()
valueOf(java.lang.String)
valueOf(org.apache.hadoop.security.SaslRpcServer.AuthMethod)
values()parse(java.lang.String, int)
hexchars
compareBytes(byte[], int, int, byte[], int, int)
readDouble(byte[], int)
readVInt(byte[], int)
readVLong(byte[], int)
writeVInt(java.io.DataOutput, int)
getVIntSize(long)
readFloat(byte[], int)
readVInt(java.io.DataInput)
readVLong(java.io.DataInput)
writeVLong(java.io.DataOutput, long)
compareTo(org.apache.hadoop.io.VIntWritable)
get()
readFields(java.io.DataInput)
toString()
equals(java.lang.Object)
hashCode()
set(int)
write(java.io.DataOutput)
compareTo(org.apache.hadoop.io.VLongWritable)
get()
readFields(java.io.DataInput)
toString()
equals(java.lang.Object)
hashCode()
set(long)
write(java.io.DataOutput)
addNextValue(java.lang.Object)
getReport()
getCombinerOutput()
reset()DOUBLE_VALUE_SUM
LONG_VALUE_MAX
LONG_VALUE_SUM
STRING_VALUE_MIN
VALUE_HISTOGRAM
configure(org.apache.hadoop.conf.Configuration)
generateKeyValPairs(java.lang.Object, java.lang.Object)
inputFile
LONG_VALUE_MIN
STRING_VALUE_MAX
UNIQ_VALUE_COUNT
generateEntry(java.lang.String, java.lang.String, org.apache.hadoop.io.Text)
generateValueAggregator(java.lang.String, long)
reduce(org.apache.hadoop.io.Text, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer.Context)
ONE
configure(org.apache.hadoop.conf.Configuration)
TYPE_SEPARATOR
generateKeyValPairs(java.lang.Object, java.lang.Object)
createValueAggregatorJob(org.apache.hadoop.conf.Configuration, java.lang.String[])
createValueAggregatorJobs(java.lang.String[])
main(java.lang.String[])
createValueAggregatorJob(java.lang.String[], java.lang.Class[])
createValueAggregatorJobs(java.lang.String[], java.lang.Class[])
setAggregatorDescriptors(java.lang.Class[])
aggregatorDescriptorList
DESCRIPTOR_NUM
getAggregatorDescriptors(org.apache.hadoop.conf.Configuration)
logSpec()
DESCRIPTOR
USER_JAR
getValueAggregatorDescriptor(java.lang.String, org.apache.hadoop.conf.Configuration)
setup(org.apache.hadoop.conf.Configuration)
map(K1, V1, org.apache.hadoop.mapreduce.Mapper.Context)
setup(org.apache.hadoop.mapreduce.Mapper.Context)reduce(org.apache.hadoop.io.Text, java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer.Context)
setup(org.apache.hadoop.mapreduce.Reducer.Context)addNextValue(java.lang.Object)
getReport()
getReportItems()
getCombinerOutput()
getReportDetails()
reset()equals(java.lang.Object)
hashCode()
getElementTypeID()toString()
getVersion()
write(java.io.DataOutput)
readFields(java.io.DataInput)
access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction)
create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable)
delete(org.apache.hadoop.fs.Path)
getAclStatus(org.apache.hadoop.fs.Path)
getContentSummary(org.apache.hadoop.fs.Path)
getDefaultBlockSize(org.apache.hadoop.fs.Path)
getDefaultReplication(org.apache.hadoop.fs.Path)
getFileChecksum(org.apache.hadoop.fs.Path)
getHomeDirectory()
getScheme()
getServerDefaults(org.apache.hadoop.fs.Path)
getUri()
getXAttr(org.apache.hadoop.fs.Path, java.lang.String)
getXAttrs(org.apache.hadoop.fs.Path, java.util.List)
listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter)
listXAttrs(org.apache.hadoop.fs.Path)
modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List)
removeAcl(org.apache.hadoop.fs.Path)
removeDefaultAcl(org.apache.hadoop.fs.Path)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
setAcl(org.apache.hadoop.fs.Path, java.util.List)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
setTimes(org.apache.hadoop.fs.Path, long, long)
setWorkingDirectory(org.apache.hadoop.fs.Path)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet)
append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable)
createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet, int, short, long, org.apache.hadoop.util.Progressable)
delete(org.apache.hadoop.fs.Path, boolean)
getChildFileSystems()
getDefaultBlockSize()
getDefaultReplication()
getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long)
getFileStatus(org.apache.hadoop.fs.Path)
getMountPoints()
getServerDefaults()
getTrashCanLocation(org.apache.hadoop.fs.Path)
getWorkingDirectory()
getXAttrs(org.apache.hadoop.fs.Path)
initialize(java.net.URI, org.apache.hadoop.conf.Configuration)
listStatus(org.apache.hadoop.fs.Path)
mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
open(org.apache.hadoop.fs.Path, int)
removeAclEntries(org.apache.hadoop.fs.Path, java.util.List)
removeXAttr(org.apache.hadoop.fs.Path, java.lang.String)
resolvePath(org.apache.hadoop.fs.Path)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setReplication(org.apache.hadoop.fs.Path, short)
setVerifyChecksum(boolean)
setWriteChecksum(boolean)
truncate(org.apache.hadoop.fs.Path, long)
access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction)
access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
getAclStatus(org.apache.hadoop.fs.Path)
getFileBlockLocations(org.apache.hadoop.fs.Path, long, long)
getFileBlockLocations(org.apache.hadoop.fs.Path, long, long)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getFileLinkStatus(org.apache.hadoop.fs.Path)
getFsStatus()
getFsStatus(org.apache.hadoop.fs.Path)
getLinkTarget(org.apache.hadoop.fs.Path)
getServerDefaults()
getXAttr(org.apache.hadoop.fs.Path, java.lang.String)
getXAttrs(org.apache.hadoop.fs.Path, java.util.List)
listStatus(org.apache.hadoop.fs.Path)
listXAttrs(org.apache.hadoop.fs.Path)
modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List)
removeAcl(org.apache.hadoop.fs.Path)
removeDefaultAcl(org.apache.hadoop.fs.Path)
renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options.Rename...)
resolvePath(org.apache.hadoop.fs.Path)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String)
setReplication(org.apache.hadoop.fs.Path, short)
setReplication(org.apache.hadoop.fs.Path, short)
setVerifyChecksum(boolean)
setVerifyChecksum(boolean, org.apache.hadoop.fs.Path)
supportsSymlinks()
createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options.ChecksumOpt, boolean)
create(org.apache.hadoop.fs.Path, java.util.EnumSet, org.apache.hadoop.fs.Options.CreateOpts...)
delete(org.apache.hadoop.fs.Path, boolean)
delete(org.apache.hadoop.fs.Path, boolean)
getDelegationTokens(java.lang.String)
getFileChecksum(org.apache.hadoop.fs.Path)
getFileChecksum(org.apache.hadoop.fs.Path)
getFileStatus(org.apache.hadoop.fs.Path)
getFileStatus(org.apache.hadoop.fs.Path)
getHomeDirectory()
getMountPoints()
getUriDefaultPort()
getXAttrs(org.apache.hadoop.fs.Path)
isValidName(java.lang.String)
listStatusIterator(org.apache.hadoop.fs.Path)
listStatus(org.apache.hadoop.fs.Path)
mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean)
mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean)
open(org.apache.hadoop.fs.Path, int)
open(org.apache.hadoop.fs.Path, int)
removeAclEntries(org.apache.hadoop.fs.Path, java.util.List)
removeXAttr(org.apache.hadoop.fs.Path, java.lang.String)
renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options.Rename...)
setAcl(org.apache.hadoop.fs.Path, java.util.List)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission)
setTimes(org.apache.hadoop.fs.Path, long, long)
setTimes(org.apache.hadoop.fs.Path, long, long)
setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet)
truncate(org.apache.hadoop.fs.Path, long)
truncate(org.apache.hadoop.fs.Path, long)compareTo(org.apache.hadoop.fs.VolumeId)
hashCode()
equals(java.lang.Object)
getUriDefaultPort()
getPathNameWarning()
run(java.lang.String[])
main(java.lang.String[])
setMockFileSystemForTesting(org.apache.hadoop.fs.FileSystem)
getMapContext(org.apache.hadoop.mapreduce.MapContext)
cmp
WrappedRecordReader(int)
accept(org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader.JoinCollector, K)
compareTo(org.apache.hadoop.mapreduce.lib.join.ComposableRecordReader)
createValue()
getCurrentKey()
getProgress()
hasNext()
initialize(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
key(K)
skip(K)
empty
close()
createKey()
equals(java.lang.Object)
getCurrentValue()
hashCode()
id()
key()
nextKeyValue()getReducerContext(org.apache.hadoop.mapreduce.ReduceContext)
readFields(java.io.DataInput)
write(java.io.DataOutput)
WritableComparator()
WritableComparator(java.lang.Class, boolean)
compare(byte[], int, int, byte[], int, int)
compare(org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.WritableComparable)
define(java.lang.Class, org.apache.hadoop.io.WritableComparator)
get(java.lang.Class, org.apache.hadoop.conf.Configuration)
getKeyClass()
hashBytes(byte[], int, int)
readDouble(byte[], int)
readInt(byte[], int)
readUnsignedShort(byte[], int)
readVLong(byte[], int)
WritableComparator(java.lang.Class)
WritableComparator(java.lang.Class, org.apache.hadoop.conf.Configuration, boolean)
compare(java.lang.Object, java.lang.Object)
compareBytes(byte[], int, int, byte[], int, int)
get(java.lang.Class)
getConf()
hashBytes(byte[], int)
newKey()
readFloat(byte[], int)
readLong(byte[], int)
readVInt(byte[], int)
setConf(org.apache.hadoop.conf.Configuration)
getFactory(java.lang.Class)
newInstance(java.lang.Class, org.apache.hadoop.conf.Configuration)
newInstance(java.lang.Class)
setFactory(java.lang.Class, org.apache.hadoop.io.WritableFactory)
newInstance()
clone(T, org.apache.hadoop.conf.Configuration)
decodeVIntSize(byte)
getVIntSize(long)
readCompressedByteArray(java.io.DataInput)
readCompressedStringArray(java.io.DataInput)
readString(java.io.DataInput)
readStringSafely(java.io.DataInput, int)
readVIntInRange(java.io.DataInput, int, int)
skipCompressedByteArray(java.io.DataInput)
toByteArray(org.apache.hadoop.io.Writable...)
writeCompressedString(java.io.DataOutput, java.lang.String)
writeEnum(java.io.DataOutput, java.lang.Enum)
writeStringArray(java.io.DataOutput, java.lang.String[])
writeVLong(java.io.DataOutput, long)
cloneInto(org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable)
displayByteArray(byte[])
isNegativeVInt(byte)
readCompressedString(java.io.DataInput)
readEnum(java.io.DataInput, java.lang.Class)
readStringArray(java.io.DataInput)
readVInt(java.io.DataInput)
readVLong(java.io.DataInput)
skipFully(java.io.DataInput, int)
writeCompressedByteArray(java.io.DataOutput, byte[])
writeCompressedStringArray(java.io.DataOutput, java.lang.String[])
writeString(java.io.DataOutput, java.lang.String)
writeVInt(java.io.DataOutput, int)
decodeValue(java.lang.String)
valueOf(java.lang.String)
encodeValue(byte[], org.apache.hadoop.fs.XAttrCodec)
values()validate(java.lang.String, boolean, java.util.EnumSet)
values()
valueOf(java.lang.String)
endMap(java.lang.String)
endVector(java.lang.String)
readBuffer(java.lang.String)
readDouble(java.lang.String)
readInt(java.lang.String)
readString(java.lang.String)
startRecord(java.lang.String)
endRecord(java.lang.String)
readBool(java.lang.String)
readByte(java.lang.String)
readFloat(java.lang.String)
readLong(java.lang.String)
startMap(java.lang.String)
startVector(java.lang.String)
endMap(java.util.TreeMap, java.lang.String)
endVector(java.util.ArrayList, java.lang.String)
startRecord(org.apache.hadoop.record.Record, java.lang.String)
writeBool(boolean, java.lang.String)
writeByte(byte, java.lang.String)
writeFloat(float, java.lang.String)
writeLong(long, java.lang.String)
endRecord(org.apache.hadoop.record.Record, java.lang.String)
startMap(java.util.TreeMap, java.lang.String)
startVector(java.util.ArrayList, java.lang.String)
writeBuffer(org.apache.hadoop.record.Buffer, java.lang.String)
writeDouble(double, java.lang.String)
writeInt(int, java.lang.String)
writeString(java.lang.String, java.lang.String)
valueOf(java.lang.String)
values()valueOf(java.lang.String)
values()YarnClient(java.lang.String)
createApplication()
deleteReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationDeleteRequest)
getAMRMToken(org.apache.hadoop.yarn.api.records.ApplicationId)
getApplicationAttempts(org.apache.hadoop.yarn.api.records.ApplicationId)
getApplications()
getApplications(java.util.Set)
getChildQueueInfos(java.lang.String)
getContainerReport(org.apache.hadoop.yarn.api.records.ContainerId)
getLabelsToNodes()
getNodeReports(org.apache.hadoop.yarn.api.records.NodeState...)
getQueueAclsInfo()
getRMDelegationToken(org.apache.hadoop.io.Text)
getYarnClusterMetrics()
moveApplicationAcrossQueues(org.apache.hadoop.yarn.api.records.ApplicationId, java.lang.String)
submitReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest)
createYarnClient()
getAllQueues()
getApplicationAttemptReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
getApplicationReport(org.apache.hadoop.yarn.api.records.ApplicationId)
getApplications(java.util.EnumSet)
getApplications(java.util.Set, java.util.EnumSet)
getClusterNodeLabels()
getContainers(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
getLabelsToNodes(java.util.Set)
getNodeToLabels()
getQueueInfo(java.lang.String)
getRootQueueInfos()
killApplication(org.apache.hadoop.yarn.api.records.ApplicationId)
submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext)
updateReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationUpdateRequest)
getApplicationSubmissionContext()
getNewApplicationResponse()getNumNodeManagers()
APPLICATION_HISTORY_MAX_APPS
APPLICATION_MAX_TAGS
AUTO_FAILOVER_EMBEDDED
AUTO_FAILOVER_PREFIX
CLIENT_FAILOVER_MAX_ATTEMPTS
CLIENT_FAILOVER_PROXY_PROVIDER
CLIENT_FAILOVER_RETRIES_ON_SOCKET_TIMEOUTS
CLIENT_FAILOVER_SLEEPTIME_MAX_MS
CLIENT_NM_CONNECT_RETRY_INTERVAL_MS
DEFAULT_APPLICATION_HISTORY_MAX_APPS
DEFAULT_APPLICATION_TYPE
DEFAULT_AUTO_FAILOVER_ENABLED
DEFAULT_CLIENT_FAILOVER_PROXY_PROVIDER
DEFAULT_CLIENT_FAILOVER_RETRIES_ON_SOCKET_TIMEOUTS
DEFAULT_CLIENT_NM_CONNECT_RETRY_INTERVAL_MS
DEFAULT_DISPATCHER_DRAIN_EVENTS_TIMEOUT
DEFAULT_FS_NODE_LABELS_STORE_RETRY_POLICY_SPEC
DEFAULT_FS_RM_STATE_STORE_RETRY_INTERVAL_MS
DEFAULT_IN_MEMORY_CHECK_PERIOD_MINS
DEFAULT_IN_MEMORY_STALENESS_PERIOD_MINS
DEFAULT_IPC_RECORD_FACTORY_CLASS
DEFAULT_IPC_SERVER_FACTORY_CLASS
DEFAULT_LOG_AGGREGATION_RETAIN_CHECK_INTERVAL_SECONDS
DEFAULT_NM_ADDRESS
DEFAULT_NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE
DEFAULT_NM_CONTAINER_EXECUTOR_SCHED_PRIORITY
DEFAULT_NM_CONTAINER_MON_INTERVAL_MS
DEFAULT_NM_DISK_HEALTH_CHECK_INTERVAL_MS
DEFAULT_NM_HEALTH_CHECK_INTERVAL_MS
DEFAULT_NM_LINUX_CONTAINER_CGROUPS_DELETE_DELAY
DEFAULT_NM_LINUX_CONTAINER_CGROUPS_STRICT_RESOURCE_USAGE
DEFAULT_NM_LOCAL_DIRS
DEFAULT_NM_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS
DEFAULT_NM_LOCALIZER_CLIENT_THREAD_COUNT
DEFAULT_NM_LOCALIZER_PORT
DEFAULT_NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS
DEFAULT_NM_LOG_DIRS
DEFAULT_NM_MAX_PER_DISK_UTILIZATION_PERCENTAGE
DEFAULT_NM_MIN_PER_DISK_FREE_SPACE_MB
DEFAULT_NM_NONSECURE_MODE_LOCAL_USER
DEFAULT_NM_PMEM_CHECK_ENABLED
DEFAULT_NM_PORT
DEFAULT_NM_RECOVERY_ENABLED
DEFAULT_NM_REMOTE_APP_LOG_DIR_SUFFIX
DEFAULT_NM_RESOURCEMANAGER_MINIMUM_VERSION
DEFAULT_NM_USER_HOME_DIR
DEFAULT_NM_VMEM_CHECK_ENABLED
DEFAULT_NM_WEBAPP_ADDRESS
DEFAULT_NM_WEBAPP_HTTPS_ADDRESS
DEFAULT_NM_WEBAPP_PORT
DEFAULT_NM_WINDOWS_CONTAINER_MEMORY_LIMIT_ENABLED
DEFAULT_PROCFS_USE_SMAPS_BASED_RSS_ENABLED
DEFAULT_PROXY_PORT
DEFAULT_RESOURCEMANAGER_CONNECT_MAX_WAIT_MS
DEFAULT_RM_ADDRESS
DEFAULT_RM_ADMIN_CLIENT_THREAD_COUNT
DEFAULT_RM_AM_EXPIRY_INTERVAL_MS
DEFAULT_RM_AMLAUNCHER_THREAD_COUNT
DEFAULT_RM_CLIENT_THREAD_COUNT
DEFAULT_RM_CONTAINER_ALLOC_EXPIRY_INTERVAL_MS
DEFAULT_RM_DELAYED_DELEGATION_TOKEN_REMOVAL_INTERVAL_MS
DEFAULT_RM_HA_ENABLED
DEFAULT_RM_MAX_COMPLETED_APPLICATIONS
DEFAULT_RM_NM_EXPIRY_INTERVAL_MS
DEFAULT_RM_NMTOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS
DEFAULT_RM_NODEMANAGER_MINIMUM_VERSION
DEFAULT_RM_NODES_INCLUDE_FILE_PATH
DEFAULT_RM_PROXY_USER_PRIVILEGES_ENABLED
DEFAULT_RM_RESERVATION_SYSTEM_ENABLE
DEFAULT_RM_RESOURCE_TRACKER_ADDRESS
DEFAULT_RM_RESOURCE_TRACKER_PORT
DEFAULT_RM_SCHEDULER_ADDRESS
DEFAULT_RM_SCHEDULER_ENABLE_MONITORS
DEFAULT_RM_SCHEDULER_MAXIMUM_ALLOCATION_VCORES
DEFAULT_RM_SCHEDULER_MINIMUM_ALLOCATION_VCORES
DEFAULT_RM_SCHEDULER_USE_PORT_FOR_NODE_NAME
DEFAULT_RM_SYSTEM_METRICS_PUBLISHER_DISPATCHER_POOL_SIZE
DEFAULT_RM_WEBAPP_ADDRESS
DEFAULT_RM_WEBAPP_ENABLE_CORS_FILTER
DEFAULT_RM_WEBAPP_HTTPS_PORT
DEFAULT_RM_WEBAPP_UI_ACTIONS_ENABLED
DEFAULT_RM_ZK_ACL
DEFAULT_RM_ZK_TIMEOUT_MS
DEFAULT_SCM_ADMIN_CLIENT_THREAD_COUNT
DEFAULT_SCM_APP_CHECKER_CLASS
DEFAULT_SCM_CLEANER_PERIOD_MINS
DEFAULT_SCM_CLIENT_SERVER_ADDRESS
DEFAULT_SCM_CLIENT_SERVER_THREAD_COUNT
DEFAULT_SCM_UPLOADER_SERVER_ADDRESS
DEFAULT_SCM_UPLOADER_SERVER_THREAD_COUNT
DEFAULT_SCM_WEBAPP_PORT
DEFAULT_SHARED_CACHE_ENABLED
DEFAULT_SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR
DEFAULT_SHARED_CACHE_ROOT
DEFAULT_TIMELINE_DELEGATION_TOKEN_MAX_LIFETIME
DEFAULT_TIMELINE_SERVICE_ADDRESS
DEFAULT_TIMELINE_SERVICE_CLIENT_MAX_RETRIES
DEFAULT_TIMELINE_SERVICE_CLIENT_THREAD_COUNT
DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE
DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE
DEFAULT_TIMELINE_SERVICE_PORT
DEFAULT_TIMELINE_SERVICE_TTL_MS
DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS
DEFAULT_TIMELINE_SERVICE_WEBAPP_PORT
DEFAULT_YARN_ADMIN_ACL
DEFAULT_YARN_APPLICATION_CLASSPATH
DEFAULT_YARN_CLIENT_APPLICATION_CLIENT_PROTOCOL_POLL_TIMEOUT_MS
DEFAULT_YARN_FAIL_FAST
DEFAULT_YARN_MINICLUSTER_FIXED_PORTS
DEFAULT_YARN_MINICLUSTER_USE_RPC
DEFAULT_ZK_RM_STATE_STORE_PARENT_PATH
FS_BASED_RM_CONF_STORE
FS_NODE_LABELS_STORE_ROOT_DIR
FS_RM_STATE_STORE_RETRY_INTERVAL_MS
FS_RM_STATE_STORE_URI
IN_MEMORY_INITIAL_DELAY_MINS
IN_MEMORY_STORE_PREFIX
IPC_PREFIX
IPC_RPC_IMPL
IS_MINI_YARN_CLUSTER
LOG_AGGREGATION_RETAIN_CHECK_INTERVAL_SECONDS
NM_ADDRESS
NM_AUX_SERVICE_FMT
NM_BIND_HOST
NM_CLIENT_MAX_NM_PROXIES
NM_CONTAINER_EXECUTOR_SCHED_PRIORITY
NM_CONTAINER_MON_INTERVAL_MS
NM_CONTAINER_MON_RESOURCE_CALCULATOR
NM_DELETE_THREAD_COUNT
NM_DISK_HEALTH_CHECK_INTERVAL_MS
NM_DOCKER_CONTAINER_EXECUTOR_IMAGE_NAME
NM_HEALTH_CHECK_INTERVAL_MS
NM_HEALTH_CHECK_SCRIPT_PATH
NM_KEYTAB
NM_LINUX_CONTAINER_CGROUPS_DELETE_TIMEOUT
NM_LINUX_CONTAINER_CGROUPS_MOUNT
NM_LINUX_CONTAINER_CGROUPS_STRICT_RESOURCE_USAGE
NM_LINUX_CONTAINER_GROUP
NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY
NM_LOCALIZER_ADDRESS
NM_LOCALIZER_CACHE_TARGET_SIZE_MB
NM_LOCALIZER_FETCH_THREAD_COUNT
NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS
NM_LOG_DIRS
NM_MAX_PER_DISK_UTILIZATION_PERCENTAGE
NM_MIN_PER_DISK_FREE_SPACE_MB
NM_NONSECURE_MODE_LOCAL_USER_KEY
NM_PMEM_CHECK_ENABLED
NM_PREFIX
NM_PROCESS_KILL_WAIT_MS
NM_RECOVERY_ENABLED
NM_REMOTE_APP_LOG_DIR
NM_RESOURCE_PERCENTAGE_PHYSICAL_CPU_LIMIT
NM_SLEEP_DELAY_BEFORE_SIGKILL_MS
NM_VCORES
NM_VMEM_PMEM_RATIO
NM_WEBAPP_ENABLE_CORS_FILTER
NM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY
NM_WINDOWS_CONTAINER_CPU_LIMIT_ENABLED
NM_WINDOWS_SECURE_CONTAINER_GROUP
NODE_LABELS_PREFIX
PROXY_ADDRESS
PROXY_PREFIX
RECOVERY_ENABLED
RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS
RM_ADMIN_ADDRESS
RM_AM_EXPIRY_INTERVAL_MS
RM_AMLAUNCHER_THREAD_COUNT
RM_BIND_HOST
RM_CLUSTER_ID
RM_CONTAINER_ALLOC_EXPIRY_INTERVAL_MS
RM_DELAYED_DELEGATION_TOKEN_REMOVAL_INTERVAL_MS
RM_DELEGATION_KEY_UPDATE_INTERVAL_KEY
RM_DELEGATION_TOKEN_MAX_LIFETIME_KEY
RM_DELEGATION_TOKEN_RENEW_INTERVAL_KEY
RM_FAIL_FAST
RM_HA_ID
RM_HA_PREFIX
RM_HOSTNAME
RM_LEVELDB_STORE_PATH
RM_METRICS_RUNTIME_BUCKETS
RM_NM_HEARTBEAT_INTERVAL_MS
RM_NODEMANAGER_CONNECT_RETIRES
RM_NODES_EXCLUDE_FILE_PATH
RM_PREFIX
RM_PROXY_USER_PREFIX
RM_RESERVATION_SYSTEM_CLASS
RM_RESERVATION_SYSTEM_PLAN_FOLLOWER
RM_RESOURCE_TRACKER_ADDRESS
RM_SCHEDULER
RM_SCHEDULER_CLIENT_THREAD_COUNT
RM_SCHEDULER_INCLUDE_PORT_IN_NODE_NAME
RM_SCHEDULER_MAXIMUM_ALLOCATION_VCORES
RM_SCHEDULER_MINIMUM_ALLOCATION_VCORES
RM_STATE_STORE_MAX_COMPLETED_APPLICATIONS
RM_SYSTEM_METRICS_PUBLISHER_DISPATCHER_POOL_SIZE
RM_WEBAPP_ADDRESS
RM_WEBAPP_ENABLE_CORS_FILTER
RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY
RM_WEBAPP_UI_ACTIONS_ENABLED
RM_ZK_ACL
RM_ZK_AUTH
RM_ZK_PREFIX
RM_ZK_TIMEOUT_MS
SCM_ADMIN_CLIENT_THREAD_COUNT
SCM_CLEANER_INITIAL_DELAY_MINS
SCM_CLEANER_RESOURCE_SLEEP_MS
SCM_CLIENT_SERVER_THREAD_COUNT
SCM_STORE_PREFIX
SCM_UPLOADER_SERVER_THREAD_COUNT
SHARED_CACHE_CHECKSUM_ALGO_IMPL
SHARED_CACHE_NESTED_LEVEL
SHARED_CACHE_NM_UPLOADER_THREAD_COUNT
SHARED_CACHE_ROOT
TIMELINE_DELEGATION_TOKEN_MAX_LIFETIME
TIMELINE_SERVICE_ADDRESS
TIMELINE_SERVICE_CLIENT_BEST_EFFORT
TIMELINE_SERVICE_CLIENT_PREFIX
TIMELINE_SERVICE_ENABLED
TIMELINE_SERVICE_HTTP_CROSS_ORIGIN_ENABLED
TIMELINE_SERVICE_KEYTAB
TIMELINE_SERVICE_LEVELDB_PREFIX
TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE
TIMELINE_SERVICE_LEVELDB_STATE_STORE_PATH
TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS
TIMELINE_SERVICE_PRINCIPAL
TIMELINE_SERVICE_STATE_STORE_CLASS
TIMELINE_SERVICE_TTL_ENABLE
TIMELINE_SERVICE_UI_NAMES
TIMELINE_SERVICE_UI_WEB_PATH_PREFIX
TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS
YARN_ADMIN_ACL
YARN_APP_CONTAINER_LOG_DIR
YARN_APPLICATION_CLASSPATH
YARN_CLIENT_APP_SUBMISSION_POLL_INTERVAL_MS
YARN_CLIENT_APPLICATION_CLIENT_PROTOCOL_POLL_TIMEOUT_MS
YARN_HTTP_POLICY_DEFAULT
YARN_LOG_SERVER_URL
YARN_MINICLUSTER_CONTROL_RESOURCE_MONITORING
YARN_MINICLUSTER_NM_PMEM_MB
YARN_PREFIX
YARN_SECURITY_SERVICE_AUTHORIZATION_APPLICATIONHISTORY_PROTOCOL
YARN_SECURITY_SERVICE_AUTHORIZATION_CONTAINER_MANAGEMENT_PROTOCOL
YARN_SECURITY_SERVICE_AUTHORIZATION_RESOURCEMANAGER_ADMINISTRATION_PROTOCOL
YARN_SSL_CLIENT_HTTPS_NEED_AUTH_DEFAULT
YARN_TRACKING_URL_GENERATOR
ZK_RM_STATE_STORE_ROOT_NODE_ACL
getSocketAddr(java.lang.String, java.lang.String, int)
updateConnectAddr(java.lang.String, java.net.InetSocketAddress)
APPLICATION_MAX_TAG_LENGTH
APPLICATION_TYPE_LENGTH
AUTO_FAILOVER_ENABLED
AUTO_FAILOVER_ZK_BASE_PATH
CLIENT_FAILOVER_PREFIX
CLIENT_FAILOVER_RETRIES
CLIENT_FAILOVER_SLEEPTIME_BASE_MS
CLIENT_NM_CONNECT_MAX_WAIT_MS
DEBUG_NM_DELETE_DELAY_SEC
DEFAULT_APPLICATION_NAME
DEFAULT_AUTO_FAILOVER_EMBEDDED
DEFAULT_AUTO_FAILOVER_ZK_BASE_PATH
DEFAULT_CLIENT_FAILOVER_RETRIES
DEFAULT_CLIENT_NM_CONNECT_MAX_WAIT_MS
DEFAULT_CONTAINER_TEMP_DIR
DEFAULT_FS_BASED_RM_CONF_STORE
DEFAULT_FS_RM_STATE_STORE_NUM_RETRIES
DEFAULT_FS_RM_STATE_STORE_RETRY_POLICY_SPEC
DEFAULT_IN_MEMORY_INITIAL_DELAY_MINS
DEFAULT_IPC_CLIENT_FACTORY_CLASS
DEFAULT_IPC_RPC_IMPL
DEFAULT_LOG_AGGREGATION_ENABLED
DEFAULT_LOG_AGGREGATION_RETAIN_SECONDS
DEFAULT_NM_ADMIN_USER_ENV
DEFAULT_NM_CLIENT_MAX_NM_PROXIES
DEFAULT_NM_CONTAINER_MGR_THREAD_COUNT
DEFAULT_NM_DELETE_THREAD_COUNT
DEFAULT_NM_ENV_WHITELIST
DEFAULT_NM_HEALTH_CHECK_SCRIPT_TIMEOUT_MS
DEFAULT_NM_LINUX_CONTAINER_CGROUPS_DELETE_TIMEOUT
DEFAULT_NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY
DEFAULT_NM_LOCALIZER_ADDRESS
DEFAULT_NM_LOCALIZER_CACHE_TARGET_SIZE_MB
DEFAULT_NM_LOCALIZER_FETCH_THREAD_COUNT
DEFAULT_NM_LOG_AGG_COMPRESSION_TYPE
DEFAULT_NM_LOG_DELETE_THREAD_COUNT
DEFAULT_NM_LOG_RETAIN_SECONDS
DEFAULT_NM_MIN_HEALTHY_DISKS_FRACTION
DEFAULT_NM_NONSECURE_MODE_LIMIT_USERS
DEFAULT_NM_NONSECURE_MODE_USER_PATTERN
DEFAULT_NM_PMEM_MB
DEFAULT_NM_PROCESS_KILL_WAIT_MS
DEFAULT_NM_REMOTE_APP_LOG_DIR
DEFAULT_NM_RESOURCE_PERCENTAGE_PHYSICAL_CPU_LIMIT
DEFAULT_NM_SLEEP_DELAY_BEFORE_SIGKILL_MS
DEFAULT_NM_VCORES
DEFAULT_NM_VMEM_PMEM_RATIO
DEFAULT_NM_WEBAPP_ENABLE_CORS_FILTER
DEFAULT_NM_WEBAPP_HTTPS_PORT
DEFAULT_NM_WINDOWS_CONTAINER_CPU_LIMIT_ENABLED
DEFAULT_NODE_LABELS_ENABLED
DEFAULT_PROXY_ADDRESS
DEFAULT_QUEUE_NAME
DEFAULT_RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS
DEFAULT_RM_ADMIN_ADDRESS
DEFAULT_RM_ADMIN_PORT
DEFAULT_RM_AM_MAX_ATTEMPTS
DEFAULT_RM_AMRM_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS
DEFAULT_RM_CONFIGURATION_PROVIDER_CLASS
DEFAULT_RM_CONTAINER_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS
DEFAULT_RM_DELEGATION_TOKEN_RENEWER_THREAD_COUNT
DEFAULT_RM_HISTORY_WRITER_MULTI_THREADED_DISPATCHER_POOL_SIZE
DEFAULT_RM_METRICS_RUNTIME_BUCKETS
DEFAULT_RM_NM_HEARTBEAT_INTERVAL_MS
DEFAULT_RM_NODEMANAGER_CONNECT_RETIRES
DEFAULT_RM_NODES_EXCLUDE_FILE_PATH
DEFAULT_RM_PORT
DEFAULT_RM_RECOVERY_ENABLED
DEFAULT_RM_RESERVATION_SYSTEM_PLAN_FOLLOWER_TIME_STEP
DEFAULT_RM_RESOURCE_TRACKER_CLIENT_THREAD_COUNT
DEFAULT_RM_SCHEDULER
DEFAULT_RM_SCHEDULER_CLIENT_THREAD_COUNT
DEFAULT_RM_SCHEDULER_MAXIMUM_ALLOCATION_MB
DEFAULT_RM_SCHEDULER_MINIMUM_ALLOCATION_MB
DEFAULT_RM_SCHEDULER_PORT
DEFAULT_RM_STATE_STORE_MAX_COMPLETED_APPLICATIONS
DEFAULT_RM_SYSTEM_METRICS_PUBLISHER_ENABLED
DEFAULT_RM_WEBAPP_DELEGATION_TOKEN_AUTH_FILTER
DEFAULT_RM_WEBAPP_HTTPS_ADDRESS
DEFAULT_RM_WEBAPP_PORT
DEFAULT_RM_WORK_PRESERVING_RECOVERY_SCHEDULING_WAIT_MS
DEFAULT_RM_ZK_RETRY_INTERVAL_MS
DEFAULT_SCM_ADMIN_ADDRESS
DEFAULT_SCM_ADMIN_PORT
DEFAULT_SCM_CLEANER_INITIAL_DELAY_MINS
DEFAULT_SCM_CLEANER_RESOURCE_SLEEP_MS
DEFAULT_SCM_CLIENT_SERVER_PORT
DEFAULT_SCM_STORE_CLASS
DEFAULT_SCM_UPLOADER_SERVER_PORT
DEFAULT_SCM_WEBAPP_ADDRESS
DEFAULT_SHARED_CACHE_CHECKSUM_ALGO_IMPL
DEFAULT_SHARED_CACHE_NESTED_LEVEL
DEFAULT_SHARED_CACHE_NM_UPLOADER_THREAD_COUNT
DEFAULT_TIMELINE_DELEGATION_KEY_UPDATE_INTERVAL
DEFAULT_TIMELINE_DELEGATION_TOKEN_RENEW_INTERVAL
DEFAULT_TIMELINE_SERVICE_CLIENT_BEST_EFFORT
DEFAULT_TIMELINE_SERVICE_CLIENT_RETRY_INTERVAL_MS
DEFAULT_TIMELINE_SERVICE_ENABLED
DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE
DEFAULT_TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS
DEFAULT_TIMELINE_SERVICE_RECOVERY_ENABLED
DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS
DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_PORT
DEFAULT_YARN_ACL_ENABLE
DEFAULT_YARN_APP_ACL
DEFAULT_YARN_CLIENT_APPLICATION_CLIENT_PROTOCOL_POLL_INTERVAL_MS
DEFAULT_YARN_CROSS_PLATFORM_APPLICATION_CLASSPATH
DEFAULT_YARN_MINICLUSTER_CONTROL_RESOURCE_MONITORING
DEFAULT_YARN_MINICLUSTER_NM_PMEM_MB
DEFAULT_ZK_RM_NUM_RETRIES
DISPATCHER_DRAIN_EVENTS_TIMEOUT
FS_NODE_LABELS_STORE_RETRY_POLICY_SPEC
FS_RM_STATE_STORE_NUM_RETRIES
FS_RM_STATE_STORE_RETRY_POLICY_SPEC
IN_MEMORY_CHECK_PERIOD_MINS
IN_MEMORY_STALENESS_PERIOD_MINS
IPC_CLIENT_FACTORY_CLASS
IPC_RECORD_FACTORY_CLASS
IPC_SERVER_FACTORY_CLASS
LOG_AGGREGATION_ENABLED
LOG_AGGREGATION_RETAIN_SECONDS
NM_ADMIN_USER_ENV
NM_AUX_SERVICES
NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE
NM_CONTAINER_EXECUTOR
NM_CONTAINER_MGR_THREAD_COUNT
NM_CONTAINER_MON_PROCESS_TREE
NM_DEFAULT_DOCKER_CONTAINER_EXECUTOR_EXEC_NAME
NM_DISK_HEALTH_CHECK_ENABLE
NM_DOCKER_CONTAINER_EXECUTOR_EXEC_NAME
NM_ENV_WHITELIST
NM_HEALTH_CHECK_SCRIPT_OPTS
NM_HEALTH_CHECK_SCRIPT_TIMEOUT_MS
NM_LINUX_CONTAINER_CGROUPS_DELETE_DELAY
NM_LINUX_CONTAINER_CGROUPS_HIERARCHY
NM_LINUX_CONTAINER_CGROUPS_MOUNT_PATH
NM_LINUX_CONTAINER_EXECUTOR_PATH
NM_LINUX_CONTAINER_RESOURCES_HANDLER
NM_LOCAL_DIRS
NM_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS
NM_LOCALIZER_CLIENT_THREAD_COUNT
NM_LOG_AGG_COMPRESSION_TYPE
NM_LOG_DELETION_THREADS_COUNT
NM_LOG_RETAIN_SECONDS
NM_MIN_HEALTHY_DISKS_FRACTION
NM_NONSECURE_MODE_LIMIT_USERS
NM_NONSECURE_MODE_USER_PATTERN_KEY
NM_PMEM_MB
NM_PRINCIPAL
NM_RECOVERY_DIR
NM_RECOVERY_PREFIX
NM_REMOTE_APP_LOG_DIR_SUFFIX
NM_RESOURCEMANAGER_MINIMUM_VERSION
NM_USER_HOME_DIR
NM_VMEM_CHECK_ENABLED
NM_WEBAPP_ADDRESS
NM_WEBAPP_HTTPS_ADDRESS
NM_WEBAPP_SPNEGO_USER_NAME_KEY
NM_WINDOWS_CONTAINER_MEMORY_LIMIT_ENABLED
NODE_LABELS_ENABLED
PROCFS_USE_SMAPS_BASED_RSS_ENABLED
PROXY_KEYTAB
PROXY_PRINCIPAL
RESOURCEMANAGER_CONNECT_MAX_WAIT_MS
RM_ADDRESS
RM_ADMIN_CLIENT_THREAD_COUNT
RM_AM_MAX_ATTEMPTS
RM_AMRM_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS
RM_CLIENT_THREAD_COUNT
RM_CONFIGURATION_PROVIDER_CLASS
RM_CONTAINER_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS
RM_DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT
RM_DELEGATION_TOKEN_MAX_LIFETIME_DEFAULT
RM_DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT
RM_DELEGATION_TOKEN_RENEWER_THREAD_COUNT
RM_HA_ENABLED
RM_HA_IDS
RM_HISTORY_WRITER_MULTI_THREADED_DISPATCHER_POOL_SIZE
RM_KEYTAB
RM_MAX_COMPLETED_APPLICATIONS
RM_NM_EXPIRY_INTERVAL_MS
RM_NMTOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS
RM_NODEMANAGER_MINIMUM_VERSION
RM_NODES_INCLUDE_FILE_PATH
RM_PRINCIPAL
RM_PROXY_USER_PRIVILEGES_ENABLED
RM_RESERVATION_SYSTEM_ENABLE
RM_RESERVATION_SYSTEM_PLAN_FOLLOWER_TIME_STEP
RM_RESOURCE_TRACKER_CLIENT_THREAD_COUNT
RM_SCHEDULER_ADDRESS
RM_SCHEDULER_ENABLE_MONITORS
RM_SCHEDULER_MAXIMUM_ALLOCATION_MB
RM_SCHEDULER_MINIMUM_ALLOCATION_MB
RM_SCHEDULER_MONITOR_POLICIES
RM_STORE
RM_SYSTEM_METRICS_PUBLISHER_ENABLED
RM_WEBAPP_DELEGATION_TOKEN_AUTH_FILTER
RM_WEBAPP_HTTPS_ADDRESS
RM_WEBAPP_SPNEGO_USER_NAME_KEY
RM_WORK_PRESERVING_RECOVERY_SCHEDULING_WAIT_MS
RM_ZK_ADDRESS
RM_ZK_NUM_RETRIES
RM_ZK_RETRY_INTERVAL_MS
SCM_ADMIN_ADDRESS
SCM_APP_CHECKER_CLASS
SCM_CLEANER_PERIOD_MINS
SCM_CLIENT_SERVER_ADDRESS
SCM_STORE_CLASS
SCM_UPLOADER_SERVER_ADDRESS
SCM_WEBAPP_ADDRESS
SHARED_CACHE_ENABLED
SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR
SHARED_CACHE_PREFIX
TIMELINE_DELEGATION_KEY_UPDATE_INTERVAL
TIMELINE_DELEGATION_TOKEN_RENEW_INTERVAL
TIMELINE_SERVICE_BIND_HOST
TIMELINE_SERVICE_CLIENT_MAX_RETRIES
TIMELINE_SERVICE_CLIENT_RETRY_INTERVAL_MS
TIMELINE_SERVICE_HANDLER_THREAD_COUNT
TIMELINE_SERVICE_HTTP_CROSS_ORIGIN_ENABLED_DEFAULT
TIMELINE_SERVICE_LEVELDB_PATH
TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE
TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE
TIMELINE_SERVICE_LEVELDB_STATE_STORE_PREFIX
TIMELINE_SERVICE_PREFIX
TIMELINE_SERVICE_RECOVERY_ENABLED
TIMELINE_SERVICE_STORE
TIMELINE_SERVICE_TTL_MS
TIMELINE_SERVICE_UI_ON_DISK_PATH_PREFIX
TIMELINE_SERVICE_WEBAPP_ADDRESS
YARN_ACL_ENABLE
YARN_APP_CONTAINER_LOG_BACKUPS
YARN_APP_CONTAINER_LOG_SIZE
YARN_AUTHORIZATION_PROVIDER
YARN_CLIENT_APPLICATION_CLIENT_PROTOCOL_POLL_INTERVAL_MS
YARN_FAIL_FAST
YARN_HTTP_POLICY_KEY
YARN_MC_PREFIX
YARN_MINICLUSTER_FIXED_PORTS
YARN_MINICLUSTER_USE_RPC
YARN_SECURITY_SERVICE_AUTHORIZATION_APPLICATIONCLIENT_PROTOCOL
YARN_SECURITY_SERVICE_AUTHORIZATION_APPLICATIONMASTER_PROTOCOL
YARN_SECURITY_SERVICE_AUTHORIZATION_RESOURCE_LOCALIZER
YARN_SECURITY_SERVICE_AUTHORIZATION_RESOURCETRACKER_PROTOCOL
YARN_SSL_SERVER_RESOURCE_DEFAULT
ZK_RM_STATE_STORE_PARENT_PATH
ZK_STATE_STORE_PREFIX
shouldRMFailFast(org.apache.hadoop.conf.Configuration)
useHttps(org.apache.hadoop.conf.Configuration)
uncaughtException(java.lang.Thread, java.lang.Throwable)
